id,title,abstract,categories,authors,published_date,updated_date
2503.17363v1,Dancing with Critiques: Enhancing LLM Reasoning with Stepwise Natural Language Self-Critique,"Enhancing the reasoning capabilities of large language models (LLMs),
particularly for complex tasks requiring multi-step logical deductions, remains
a significant challenge. Traditional inference time scaling methods utilize
scalar reward signals from process reward models to evaluate candidate
reasoning steps, but these scalar rewards lack the nuanced qualitative
information essential for understanding and justifying each step. In this
paper, we propose a novel inference-time scaling approach -- stepwise natural
language self-critique (PANEL), which employs self-generated natural language
critiques as feedback to guide the step-level search process. By generating
rich, human-readable critiques for each candidate reasoning step, PANEL retains
essential qualitative information, facilitating better-informed decision-making
during inference. This approach bypasses the need for task-specific verifiers
and the associated training overhead, making it broadly applicable across
diverse tasks. Experimental results on challenging reasoning benchmarks,
including AIME and GPQA, demonstrate that PANEL significantly enhances
reasoning performance, outperforming traditional scalar reward-based methods.
Our code is available at https://github.com/puddingyeah/PANEL to support and
encourage future research in this promising field.",['cs.CL'],"['Yansi Li', 'Jiahao Xu', 'Tian Liang', 'Xingyu Chen', 'Zhiwei He', 'Qiuzhi Liu', 'Rui Wang', 'Zhuosheng Zhang', 'Zhaopeng Tu', 'Haitao Mi', 'Dong Yu']",2025-03-21,2025-03-21
2503.17361v1,Gumbel-Softmax Flow Matching with Straight-Through Guidance for Controllable Biological Sequence Generation,"Flow matching in the continuous simplex has emerged as a promising strategy
for DNA sequence design, but struggles to scale to higher simplex dimensions
required for peptide and protein generation. We introduce Gumbel-Softmax Flow
and Score Matching, a generative framework on the simplex based on a novel
Gumbel-Softmax interpolant with a time-dependent temperature. Using this
interpolant, we introduce Gumbel-Softmax Flow Matching by deriving a
parameterized velocity field that transports from smooth categorical
distributions to distributions concentrated at a single vertex of the simplex.
We alternatively present Gumbel-Softmax Score Matching which learns to regress
the gradient of the probability density. Our framework enables high-quality,
diverse generation and scales efficiently to higher-dimensional simplices. To
enable training-free guidance, we propose Straight-Through Guided Flows
(STGFlow), a classifier-based guidance method that leverages straight-through
estimators to steer the unconditional velocity field toward optimal vertices of
the simplex. STGFlow enables efficient inference-time guidance using
classifiers pre-trained on clean sequences, and can be used with any discrete
flow method. Together, these components form a robust framework for
controllable de novo sequence generation. We demonstrate state-of-the-art
performance in conditional DNA promoter design, sequence-only protein
generation, and target-binding peptide design for rare disease treatment.","['cs.LG', 'q-bio.BM']","['Sophia Tang', 'Yinuo Zhang', 'Alexander Tong', 'Pranam Chatterjee']",2025-03-21,2025-03-21
2503.17359v1,Position: Interactive Generative Video as Next-Generation Game Engine,"Modern game development faces significant challenges in creativity and cost
due to predetermined content in traditional game engines. Recent breakthroughs
in video generation models, capable of synthesizing realistic and interactive
virtual environments, present an opportunity to revolutionize game creation. In
this position paper, we propose Interactive Generative Video (IGV) as the
foundation for Generative Game Engines (GGE), enabling unlimited novel content
generation in next-generation gaming. GGE leverages IGV's unique strengths in
unlimited high-quality content synthesis, physics-aware world modeling,
user-controlled interactivity, long-term memory capabilities, and causal
reasoning. We present a comprehensive framework detailing GGE's core modules
and a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work
charts a new course for game development in the AI era, envisioning a future
where AI-powered generative systems fundamentally reshape how games are created
and experienced.",['cs.CV'],"['Jiwen Yu', 'Yiran Qin', 'Haoxuan Che', 'Quande Liu', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Xihui Liu']",2025-03-21,2025-03-21
2503.17358v1,Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image,"In many robotics and VR/AR applications, fast camera motions cause a high
level of motion blur, causing existing camera pose estimation methods to fail.
In this work, we propose a novel framework that leverages motion blur as a rich
cue for motion estimation rather than treating it as an unwanted artifact. Our
approach works by predicting a dense motion flow field and a monocular depth
map directly from a single motion-blurred image. We then recover the
instantaneous camera velocity by solving a linear least squares problem under
the small motion assumption. In essence, our method produces an IMU-like
measurement that robustly captures fast and aggressive camera movements. To
train our model, we construct a large-scale dataset with realistic synthetic
motion blur derived from ScanNet++v2 and further refine our model by training
end-to-end on real data using our fully differentiable pipeline. Extensive
evaluations on real-world benchmarks demonstrate that our method achieves
state-of-the-art angular and translational velocity estimates, outperforming
current methods like MASt3R and COLMAP.",['cs.CV'],"['Jerred Chen', 'Ronald Clark']",2025-03-21,2025-03-21
2503.17355v1,Glivenko-Cantelli for $f$-divergence,"We extend the celebrated Glivenko-Cantelli theorem, sometimes called the
fundamental theorem of statistics, from its standard setting of total variation
distance to all $f$-divergences. A key obstacle in this endeavor is to define
$f$-divergence on a subcollection of a $\sigma$-algebra that forms a
$\pi$-system but not a $\sigma$-subalgebra. This is a side contribution of our
work. We will show that this notion of $f$-divergence on the $\pi$-system of
rays preserves nearly all known properties of standard $f$-divergence, yields a
novel integral representation of the Kolmogorov-Smirnov distance, and has a
Glivenko-Cantelli theorem.","['math.ST', 'cs.LG', 'stat.TH', '60B10, 60F15, 60F25']","['Haoming Wang', 'Lek-Heng Lim']",2025-03-21,2025-03-21
2503.17354v1,HCAST: Human-Calibrated Autonomy Software Tasks,"To understand and predict the societal impacts of highly autonomous AI
systems, we need benchmarks with grounding, i.e., metrics that directly connect
AI performance to real-world effects we care about. We present HCAST
(Human-Calibrated Autonomy Software Tasks), a benchmark of 189 machine learning
engineering, cybersecurity, software engineering, and general reasoning tasks.
We collect 563 human baselines (totaling over 1500 hours) from people skilled
in these domains, working under identical conditions as AI agents, which lets
us estimate that HCAST tasks take humans between one minute and 8+ hours.
Measuring the time tasks take for humans provides an intuitive metric for
evaluating AI capabilities, helping answer the question ""can an agent be
trusted to complete a task that would take a human X hours?"" We evaluate the
success rates of AI agents built on frontier foundation models, and we find
that current agents succeed 70-80% of the time on tasks that take humans less
than one hour, and less than 20% of the time on tasks that take humans more
than 4 hours.","['cs.AI', 'I.2.0']","['David Rein', 'Joel Becker', 'Amy Deng', 'Seraphina Nix', 'Chris Canal', ""Daniel O'Connel"", 'Pip Arnott', 'Ryan Bloom', 'Thomas Broadley', 'Katharyn Garcia', 'Brian Goodrich', 'Max Hasin', 'Sami Jawhar', 'Megan Kinniment', 'Thomas Kwa', 'Aron Lajko', 'Nate Rush', 'Lucas Jun Koba Sato', 'Sydney Von Arx', 'Ben West', 'Lawrence Chan', 'Elizabeth Barnes']",2025-03-21,2025-03-21
2503.17353v1,NdLinear Is All You Need for Representation Learning,"Many high-impact machine learning tasks involve multi-dimensional data (e.g.,
images, volumetric medical scans, multivariate time-series). Yet, most neural
architectures flatten inputs, discarding critical cross-dimension information.
We introduce NdLinear, a novel linear transformation that preserves these
structures without extra overhead. By operating separately along each
dimension, NdLinear captures dependencies that standard fully connected layers
overlook. Extensive experiments across convolutional, recurrent, and
transformer-based networks show significant improvements in representational
power and parameter efficiency. Crucially, NdLinear serves as a foundational
building block for large-scale foundation models by operating on any unimodal
or multimodal data in its native form. This removes the need for flattening or
modality-specific preprocessing. Ndlinear rethinks core architectural
priorities beyond attention, enabling more expressive, context-aware models at
scale. We propose NdLinear as a drop-in replacement for standard linear layers
-- marking an important step toward next-generation neural architectures.","['cs.LG', 'cs.AI']","['Alex Reneau', 'Jerry Yao-Chieh Hu', 'Zhongfang Zhuang', 'Ting-Chun Liu']",2025-03-21,2025-03-21
2503.17352v1,OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement,"Recent advancements demonstrated by DeepSeek-R1 have shown that complex
reasoning abilities in large language models (LLMs), including sophisticated
behaviors such as self-verification and self-correction, can be achieved by RL
with verifiable rewards and significantly improves model performance on
challenging tasks such as AIME. Motivated by these findings, our study
investigates whether similar reasoning capabilities can be successfully
integrated into large vision-language models (LVLMs) and assesses their impact
on challenging multimodal reasoning tasks. We consider an approach that
iteratively leverages supervised fine-tuning (SFT) on lightweight training data
and Reinforcement Learning (RL) to further improve model generalization.
Initially, reasoning capabilities were distilled from pure-text R1 models by
generating reasoning steps using high-quality captions of the images sourced
from diverse visual datasets. Subsequently, iterative RL training further
enhance reasoning skills, with each iteration's RL-improved model generating
refined SFT datasets for the next round. This iterative process yielded
OpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on
challenging benchmarks such as MathVista, MathVerse, and MathVision,
demonstrating the potential of our strategy for robust vision-language
reasoning. The code, model and data are held at
https://github.com/yihedeng9/OpenVLThinker.","['cs.CV', 'cs.CL']","['Yihe Deng', 'Hritik Bansal', 'Fan Yin', 'Nanyun Peng', 'Wei Wang', 'Kai-Wei Chang']",2025-03-21,2025-03-21
2503.17351v1,Time-Series U-Net with Recurrence for Noise-Robust Imaging Photoplethysmography,"Remote estimation of vital signs enables health monitoring for situations in
which contact-based devices are either not available, too intrusive, or too
expensive. In this paper, we present a modular, interpretable pipeline for
pulse signal estimation from video of the face that achieves state-of-the-art
results on publicly available datasets.Our imaging photoplethysmography (iPPG)
system consists of three modules: face and landmark detection, time-series
extraction, and pulse signal/pulse rate estimation. Unlike many deep learning
methods that make use of a single black-box model that maps directly from input
video to output signal or heart rate, our modular approach enables each of the
three parts of the pipeline to be interpreted individually. The pulse signal
estimation module, which we call TURNIP (Time-Series U-Net with Recurrence for
Noise-Robust Imaging Photoplethysmography), allows the system to faithfully
reconstruct the underlying pulse signal waveform and uses it to measure heart
rate and pulse rate variability metrics, even in the presence of motion. When
parts of the face are occluded due to extreme head poses, our system explicitly
detects such ""self-occluded"" regions and maintains estimation robustness
despite the missing information. Our algorithm provides reliable heart rate
estimates without the need for specialized sensors or contact with the skin,
outperforming previous iPPG methods on both color (RGB) and near-infrared (NIR)
datasets.",['cs.CV'],"['Vineet R. Shenoy', 'Shaoju Wu', 'Armand Comas', 'Tim K. Marks', 'Suhas Lohit', 'Hassan Mansour']",2025-03-21,2025-03-21
2503.17350v1,Decouple and Track: Benchmarking and Improving Video Diffusion Transformers for Motion Transfer,"The motion transfer task involves transferring motion from a source video to
newly generated videos, requiring the model to decouple motion from appearance.
Previous diffusion-based methods primarily rely on separate spatial and
temporal attention mechanisms within 3D U-Net. In contrast, state-of-the-art
video Diffusion Transformers (DiT) models use 3D full attention, which does not
explicitly separate temporal and spatial information. Thus, the interaction
between spatial and temporal dimensions makes decoupling motion and appearance
more challenging for DiT models. In this paper, we propose DeT, a method that
adapts DiT models to improve motion transfer ability. Our approach introduces a
simple yet effective temporal kernel to smooth DiT features along the temporal
dimension, facilitating the decoupling of foreground motion from background
appearance. Meanwhile, the temporal kernel effectively captures temporal
variations in DiT features, which are closely related to motion. Moreover, we
introduce explicit supervision along dense trajectories in the latent feature
space to further enhance motion consistency. Additionally, we present MTBench,
a general and challenging benchmark for motion transfer. We also introduce a
hybrid motion fidelity metric that considers both the global and local motion
similarity. Therefore, our work provides a more comprehensive evaluation than
previous works. Extensive experiments on MTBench demonstrate that DeT achieves
the best trade-off between motion fidelity and edit fidelity.",['cs.CV'],"['Qingyu Shi', 'Jianzong Wu', 'Jinbin Bai', 'Jiangning Zhang', 'Lu Qi', 'Xiangtai Li', 'Yunhai Tong']",2025-03-21,2025-03-21
2503.17349v1,Beyond Semantics: Rediscovering Spatial Awareness in Vision-Language Models,"Vision-Language Models (VLMs) excel at identifying and describing objects but
struggle with spatial reasoning such as accurately understanding the relative
positions of objects. Inspired by the dual-pathway (ventral-dorsal) model of
human vision, we investigate why VLMs fail spatial tasks despite strong object
recognition capabilities. Our interpretability-driven analysis reveals a
critical underlying cause: vision embeddings in VLMs are treated primarily as
semantic ``bag-of-tokens,"" overshadowing subtle yet crucial positional cues due
to their disproportionately large embedding norms. We validate this insight
through extensive diagnostic experiments, demonstrating minimal performance
impact when token orders or fine-grained spatial details are removed. Guided by
these findings, we propose simple, interpretable interventions, including
normalizing vision embedding norms and extracting mid-layer spatially rich
features, to restore spatial awareness. Empirical results on both our synthetic
data and standard benchmarks demonstrate improved spatial reasoning
capabilities, highlighting the value of interpretability-informed design
choices. Our study not only uncovers fundamental limitations in current VLM
architectures but also provides actionable insights for enhancing structured
perception of visual scenes.",['cs.CV'],"['Jianing Qi', 'Jiawei Liu', 'Hao Tang', 'Zhigang Zhu']",2025-03-21,2025-03-21
2503.17347v1,Dereflection Any Image with Diffusion Priors and Diversified Data,"Reflection removal of a single image remains a highly challenging task due to
the complex entanglement between target scenes and unwanted reflections.
Despite significant progress, existing methods are hindered by the scarcity of
high-quality, diverse data and insufficient restoration priors, resulting in
limited generalization across various real-world scenarios. In this paper, we
propose Dereflection Any Image, a comprehensive solution with an efficient data
preparation pipeline and a generalizable model for robust reflection removal.
First, we introduce a dataset named Diverse Reflection Removal (DRR) created by
randomly rotating reflective mediums in target scenes, enabling variation of
reflection angles and intensities, and setting a new benchmark in scale,
quality, and diversity. Second, we propose a diffusion-based framework with
one-step diffusion for deterministic outputs and fast inference. To ensure
stable learning, we design a three-stage progressive training strategy,
including reflection-invariant finetuning to encourage consistent outputs
across varying reflection patterns that characterize our dataset. Extensive
experiments show that our method achieves SOTA performance on both common
benchmarks and challenging in-the-wild images, showing superior generalization
across diverse real-world scenes.",['cs.CV'],"['Jichen Hu', 'Chen Yang', 'Zanwei Zhou', 'Jiemin Fang', 'Xiaokang Yang', 'Qi Tian', 'Wei Shen']",2025-03-21,2025-03-21
2503.17340v1,Align Your Rhythm: Generating Highly Aligned Dance Poses with Gating-Enhanced Rhythm-Aware Feature Representation,"Automatically generating natural, diverse and rhythmic human dance movements
driven by music is vital for virtual reality and film industries. However,
generating dance that naturally follows music remains a challenge, as existing
methods lack proper beat alignment and exhibit unnatural motion dynamics. In
this paper, we propose Danceba, a novel framework that leverages gating
mechanism to enhance rhythm-aware feature representation for music-driven dance
generation, which achieves highly aligned dance poses with enhanced rhythmic
sensitivity. Specifically, we introduce Phase-Based Rhythm Extraction (PRE) to
precisely extract rhythmic information from musical phase data, capitalizing on
the intrinsic periodicity and temporal structures of music. Additionally, we
propose Temporal-Gated Causal Attention (TGCA) to focus on global rhythmic
features, ensuring that dance movements closely follow the musical rhythm. We
also introduce Parallel Mamba Motion Modeling (PMMM) architecture to separately
model upper and lower body motions along with musical features, thereby
improving the naturalness and diversity of generated dance movements. Extensive
experiments confirm that Danceba outperforms state-of-the-art methods,
achieving significantly better rhythmic alignment and motion diversity. Project
page: https://danceba.github.io/ .","['cs.MM', 'cs.AI', 'cs.CV', 'cs.SD', 'eess.AS']","['Congyi Fan', 'Jian Guan', 'Xuanjia Zhao', 'Dongli Xu', 'Youtian Lin', 'Tong Ye', 'Pengming Feng', 'Haiwei Pan']",2025-03-21,2025-03-21
2503.17339v1,Can AI expose tax loopholes? Towards a new generation of legal policy assistants,"The legislative process is the backbone of a state built on solid
institutions. Yet, due to the complexity of laws -- particularly tax law --
policies may lead to inequality and social tensions. In this study, we
introduce a novel prototype system designed to address the issues of tax
loopholes and tax avoidance. Our hybrid solution integrates a natural language
interface with a domain-specific language tailored for planning. We demonstrate
on a case study how tax loopholes and avoidance schemes can be exposed. We
conclude that our prototype can help enhance social welfare by systematically
identifying and addressing tax gaps stemming from loopholes.","['cs.CY', 'cs.AI']","['Peter Fratrič', 'Nils Holzenberger', 'David Restrepo Amariles']",2025-03-21,2025-03-21
2503.17338v1,Capturing Individual Human Preferences with Reward Features,"Reinforcement learning from human feedback usually models preferences using a
reward model that does not distinguish between people. We argue that this is
unlikely to be a good design choice in contexts with high potential for
disagreement, like in the training of large language models. We propose a
method to specialise a reward model to a person or group of people. Our
approach builds on the observation that individual preferences can be captured
as a linear combination of a set of general reward features. We show how to
learn such features and subsequently use them to quickly adapt the reward model
to a specific individual, even if their preferences are not reflected in the
training data. We present experiments with large language models comparing the
proposed architecture with a non-adaptive reward model and also adaptive
counterparts, including models that do in-context personalisation. Depending on
how much disagreement there is in the training data, our model either
significantly outperforms the baselines or matches their performance with a
simpler architecture and more stable training.","['cs.AI', 'cs.LG', 'stat.ML']","['André Barreto', 'Vincent Dumoulin', 'Yiran Mao', 'Nicolas Perez-Nieves', 'Bobak Shahriari', 'Yann Dauphin', 'Doina Precup', 'Hugo Larochelle']",2025-03-21,2025-03-21
2503.17336v1,Efficient Intent-Based Filtering for Multi-Party Conversations Using Knowledge Distillation from LLMs,"Large language models (LLMs) have showcased remarkable capabilities in
conversational AI, enabling open-domain responses in chat-bots, as well as
advanced processing of conversations like summarization, intent classification,
and insights generation. However, these models are resource-intensive,
demanding substantial memory and computational power. To address this, we
propose a cost-effective solution that filters conversational snippets of
interest for LLM processing, tailored to the target downstream application,
rather than processing every snippet. In this work, we introduce an innovative
approach that leverages knowledge distillation from LLMs to develop an
intent-based filter for multi-party conversations, optimized for compute power
constrained environments. Our method combines different strategies to create a
diverse multi-party conversational dataset, that is annotated with the target
intents and is then used to fine-tune the MobileBERT model for multi-label
intent classification. This model achieves a balance between efficiency and
performance, effectively filtering conversation snippets based on their
intents. By passing only the relevant snippets to the LLM for further
processing, our approach significantly reduces overall operational costs
depending on the intents and the data distribution as demonstrated in our
experiments.","['cs.CL', 'cs.AI']","['Reem Gody', 'Mohamed Abdelghaffar', 'Mohammed Jabreel', 'Ahmed Tawfik']",2025-03-21,2025-03-21
2503.17332v1,CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities,"Large language model (LLM) agents are increasingly capable of autonomously
conducting cyberattacks, posing significant threats to existing applications.
This growing risk highlights the urgent need for a real-world benchmark to
evaluate the ability of LLM agents to exploit web application vulnerabilities.
However, existing benchmarks fall short as they are limited to abstracted
Capture the Flag competitions or lack comprehensive coverage. Building a
benchmark for real-world vulnerabilities involves both specialized expertise to
reproduce exploits and a systematic approach to evaluating unpredictable
threats. To address this challenge, we introduce CVE-Bench, a real-world
cybersecurity benchmark based on critical-severity Common Vulnerabilities and
Exposures. In CVE-Bench, we design a sandbox framework that enables LLM agents
to exploit vulnerable web applications in scenarios that mimic real-world
conditions, while also providing effective evaluation of their exploits. Our
evaluation shows that the state-of-the-art agent framework can resolve up to
13% of vulnerabilities.","['cs.CR', 'cs.AI', 'I.2.1; I.2.7']","['Yuxuan Zhu', 'Antony Kellermann', 'Dylan Bowman', 'Philip Li', 'Akul Gupta', 'Adarsh Danda', 'Richard Fang', 'Conner Jensen', 'Eric Ihli', 'Jason Benn', 'Jet Geronimo', 'Avi Dhir', 'Sudhit Rao', 'Kaicheng Yu', 'Twm Stone', 'Daniel Kang']",2025-03-21,2025-03-21
2503.17331v1,A Topological Data Analysis Framework for Quantifying Necrosis in Glioblastomas,"In this paper, we introduce a shape descriptor that we call ""interior
function"". This is a Topological Data Analysis (TDA) based descriptor that
refines previous descriptors for image analysis. Using this concept, we define
subcomplex lacunarity, a new index that quantifies geometric characteristics of
necrosis in tumors such as conglomeration. Building on this framework, we
propose a set of indices to analyze necrotic morphology and construct a diagram
that captures the distinct structural and geometric properties of necrotic
regions in tumors. We present an application of this framework in the study of
MRIs of Glioblastomas (GB). Using cluster analysis, we identify four distinct
subtypes of Glioblastomas that reflect geometric properties of necrotic
regions.","['math.AT', 'cs.CV']","['Francisco Tellez', 'Enrique Torres-Giese']",2025-03-21,2025-03-21
2503.17329v1,Predicting Potential Customer Support Needs and Optimizing Search Ranking in a Two-Sided Marketplace,"Airbnb is an online marketplace that connects hosts and guests to unique
stays and experiences. When guests stay at homes booked on Airbnb, there are a
small fraction of stays that lead to support needed from Airbnb's Customer
Support (CS), which may cause inconvenience to guests and hosts and require
Airbnb resources to resolve. In this work, we show that instances where CS
support is needed may be predicted based on hosts and guests behavior. We build
a model to predict the likelihood of CS support needs for each match of guest
and host. The model score is incorporated into Airbnb's search ranking
algorithm as one of the many factors. The change promotes more reliable matches
in search results and significantly reduces bookings that require CS support.",['cs.LG'],"['Do-kyum Kim', 'Han Zhao', 'Huiji Gao', 'Liwei He', 'Malay Haldar', 'Sanjeev Katariya']",2025-03-21,2025-03-21
2503.17316v1,Pow3R: Empowering Unconstrained 3D Reconstruction with Camera and Scene Priors,"We present Pow3r, a novel large 3D vision regression model that is highly
versatile in the input modalities it accepts. Unlike previous feed-forward
models that lack any mechanism to exploit known camera or scene priors at test
time, Pow3r incorporates any combination of auxiliary information such as
intrinsics, relative pose, dense or sparse depth, alongside input images,
within a single network. Building upon the recent DUSt3R paradigm, a
transformer-based architecture that leverages powerful pre-training, our
lightweight and versatile conditioning acts as additional guidance for the
network to predict more accurate estimates when auxiliary information is
available. During training we feed the model with random subsets of modalities
at each iteration, which enables the model to operate under different levels of
known priors at test time. This in turn opens up new capabilities, such as
performing inference in native image resolution, or point-cloud completion. Our
experiments on 3D reconstruction, depth completion, multi-view depth
prediction, multi-view stereo, and multi-view pose estimation tasks yield
state-of-the-art results and confirm the effectiveness of Pow3r at exploiting
all available information. The project webpage is
https://europe.naverlabs.com/pow3r.",['cs.CV'],"['Wonbong Jang', 'Philippe Weinzaepfel', 'Vincent Leroy', 'Lourdes Agapito', 'Jerome Revaud']",2025-03-21,2025-03-21
2503.17309v1,LLM+MAP: Bimanual Robot Task Planning using Large Language Models and Planning Domain Definition Language,"Bimanual robotic manipulation provides significant versatility, but also
presents an inherent challenge due to the complexity involved in the spatial
and temporal coordination between two hands. Existing works predominantly focus
on attaining human-level manipulation skills for robotic hands, yet little
attention has been paid to task planning on long-horizon timescales. With their
outstanding in-context learning and zero-shot generation abilities, Large
Language Models (LLMs) have been applied and grounded in diverse robotic
embodiments to facilitate task planning. However, LLMs still suffer from errors
in long-horizon reasoning and from hallucinations in complex robotic tasks,
lacking a guarantee of logical correctness when generating the plan. Previous
works, such as LLM+P, extended LLMs with symbolic planners. However, none have
been successfully applied to bimanual robots. New challenges inevitably arise
in bimanual manipulation, necessitating not only effective task decomposition
but also efficient task allocation. To address these challenges, this paper
introduces LLM+MAP, a bimanual planning framework that integrates LLM reasoning
and multi-agent planning, automating effective and efficient bimanual task
planning. We conduct simulated experiments on various long-horizon manipulation
tasks of differing complexity. Our method is built using GPT-4o as the backend,
and we compare its performance against plans generated directly by LLMs,
including GPT-4o, V3 and also recent strong reasoning models o1 and R1. By
analyzing metrics such as planning time, success rate, group debits, and
planning-step reduction rate, we demonstrate the superior performance of
LLM+MAP, while also providing insights into robotic reasoning. Code is
available at https://github.com/Kchu/LLM-MAP.","['cs.RO', 'cs.AI']","['Kun Chu', 'Xufeng Zhao', 'Cornelius Weber', 'Stefan Wermter']",2025-03-21,2025-03-21
2503.17308v1,On Quantum Perceptron Learning via Quantum Search,"With the growing interest in quantum machine learning, the perceptron -- a
fundamental building block in traditional machine learning -- has emerged as a
valuable model for exploring quantum advantages. Two quantum perceptron
algorithms based on Grover's search, were developed in arXiv:1602.04799 to
accelerate training and improve statistical efficiency in perceptron learning.
This paper points out and corrects a mistake in the proof of Theorem 2 in
arXiv:1602.04799. Specifically, we show that the probability of sampling from a
normal distribution for a $D$-dimensional hyperplane that perfectly classifies
the data scales as $\Omega(\gamma^{D})$ instead of $\Theta({\gamma})$, where
$\gamma$ is the margin. We then revisit two well-established linear programming
algorithms -- the ellipsoid method and the cutting plane random walk algorithm
-- in the context of perceptron learning, and show how quantum search
algorithms can be leveraged to enhance the overall complexity. Specifically,
both algorithms gain a sub-linear speed-up $O(\sqrt{N})$ in the number of data
points $N$ as a result of Grover's algorithm and an additional $O(D^{1.5})$
speed-up is possible for cutting plane random walk algorithm employing quantum
walk search.","['quant-ph', 'cs.LG', 'stat.ML']","['Xiaoyu Sun', 'Mathieu Roget', 'Giuseppe Di Molfetta', 'Hachem Kadri']",2025-03-21,2025-03-21
2503.17299v1,Preference-Guided Diffusion for Multi-Objective Offline Optimization,"Offline multi-objective optimization aims to identify Pareto-optimal
solutions given a dataset of designs and their objective values. In this work,
we propose a preference-guided diffusion model that generates Pareto-optimal
designs by leveraging a classifier-based guidance mechanism. Our guidance
classifier is a preference model trained to predict the probability that one
design dominates another, directing the diffusion model toward optimal regions
of the design space. Crucially, this preference model generalizes beyond the
training distribution, enabling the discovery of Pareto-optimal solutions
outside the observed dataset. We introduce a novel diversity-aware preference
guidance, augmenting Pareto dominance preference with diversity criteria. This
ensures that generated solutions are optimal and well-distributed across the
objective space, a capability absent in prior generative methods for offline
multi-objective optimization. We evaluate our approach on various continuous
offline multi-objective optimization tasks and find that it consistently
outperforms other inverse/generative approaches while remaining competitive
with forward/surrogate-based optimization methods. Our results highlight the
effectiveness of classifier-guided diffusion models in generating diverse and
high-quality solutions that approximate the Pareto front well.","['cs.LG', 'cs.AI']","['Yashas Annadani', 'Syrine Belakaria', 'Stefano Ermon', 'Stefan Bauer', 'Barbara E Engelhardt']",2025-03-21,2025-03-21
2503.17290v1,Calibration Strategies for Robust Causal Estimation: Theoretical and Empirical Insights on Propensity Score Based Estimators,"The partitioning of data for estimation and calibration critically impacts
the performance of propensity score based estimators like inverse probability
weighting (IPW) and double/debiased machine learning (DML) frameworks. We
extend recent advances in calibration techniques for propensity score
estimation, improving the robustness of propensity scores in challenging
settings such as limited overlap, small sample sizes, or unbalanced data. Our
contributions are twofold: First, we provide a theoretical analysis of the
properties of calibrated estimators in the context of DML. To this end, we
refine existing calibration frameworks for propensity score models, with a
particular emphasis on the role of sample-splitting schemes in ensuring valid
causal inference. Second, through extensive simulations, we show that
calibration reduces variance of inverse-based propensity score estimators while
also mitigating bias in IPW, even in small-sample regimes. Notably, calibration
improves stability for flexible learners (e.g., gradient boosting) while
preserving the doubly robust properties of DML. A key insight is that, even
when methods perform well without calibration, incorporating a calibration step
does not degrade performance, provided that an appropriate sample-splitting
approach is chosen.","['stat.ML', 'cs.LG', 'econ.EM', 'stat.ME']","['Jan Rabenseifner', 'Sven Klaassen', 'Jannis Kueck', 'Philipp Bach']",2025-03-21,2025-03-21
2503.17289v1,3D Neural Operator-Based Flow Surrogates around 3D geometries: Signed Distance Functions and Derivative Constraints,"Accurate modeling of fluid dynamics around complex geometries is critical for
applications such as aerodynamic optimization and biomedical device design.
While advancements in numerical methods and high-performance computing have
improved simulation capabilities, the computational cost of high-fidelity 3D
flow simulations remains a significant challenge. Scientific machine learning
(SciML) offers an efficient alternative, enabling rapid and reliable flow
predictions. In this study, we evaluate Deep Operator Networks (DeepONet) and
Geometric-DeepONet, a variant that incorporates geometry information via signed
distance functions (SDFs), on steady-state 3D flow over complex objects. Our
dataset consists of 1,000 high-fidelity simulations spanning Reynolds numbers
from 10 to 1,000, enabling comprehensive training and evaluation across a range
of flow regimes. To assess model generalization, we test our models on a random
and extrapolatory train-test splitting. Additionally, we explore a
derivative-informed training strategy that augments standard loss functions
with velocity gradient penalties and incompressibility constraints, improving
physics consistency in 3D flow prediction. Our results show that
Geometric-DeepONet improves boundary-layer accuracy by up to 32% compared to
standard DeepONet. Moreover, incorporating derivative constraints enhances
gradient accuracy by 25% in interpolation tasks and up to 45% in extrapolatory
test scenarios, suggesting significant improvement in generalization
capabilities to unseen 3D Reynolds numbers.",['cs.LG'],"['Ali Rabeh', 'Adarsh Krishnamurthy', 'Baskar Ganapathysubramanian']",2025-03-21,2025-03-21
2503.17288v1,Exploring a Principled Framework for Deep Subspace Clustering,"Subspace clustering is a classical unsupervised learning task, built on a
basic assumption that high-dimensional data can be approximated by a union of
subspaces (UoS). Nevertheless, the real-world data are often deviating from the
UoS assumption. To address this challenge, state-of-the-art deep subspace
clustering algorithms attempt to jointly learn UoS representations and
self-expressive coefficients. However, the general framework of the existing
algorithms suffers from a catastrophic feature collapse and lacks a theoretical
guarantee to learn desired UoS representation. In this paper, we present a
Principled fRamewOrk for Deep Subspace Clustering (PRO-DSC), which is designed
to learn structured representations and self-expressive coefficients in a
unified manner. Specifically, in PRO-DSC, we incorporate an effective
regularization on the learned representations into the self-expressive model,
prove that the regularized self-expressive model is able to prevent feature
space collapse, and demonstrate that the learned optimal representations under
certain condition lie on a union of orthogonal subspaces. Moreover, we provide
a scalable and efficient approach to implement our PRO-DSC and conduct
extensive experiments to verify our theoretical findings and demonstrate the
superior performance of our proposed deep subspace clustering approach. The
code is available at https://github.com/mengxianghan123/PRO-DSC.","['cs.CV', 'cs.LG']","['Xianghan Meng', 'Zhiyuan Huang', 'Wei He', 'Xianbiao Qi', 'Rong Xiao', 'Chun-Guang Li']",2025-03-21,2025-03-21
2503.17287v1,FastCuRL: Curriculum Reinforcement Learning with Progressive Context Extension for Efficient Training R1-like Reasoning Models,"In this paper, we propose \textbf{\textsc{FastCuRL}}, a simple yet efficient
\textbf{Cu}rriculum \textbf{R}einforcement \textbf{L}earning approach with
context window extending strategy to accelerate the reinforcement learning
training efficiency for R1-like reasoning models while enhancing their
performance in tackling complex reasoning tasks with long chain-of-thought
rationales, particularly with a 1.5B parameter language model.
\textbf{\textsc{FastCuRL}} consists of two main procedures: length-aware
training data segmentation and context window extension training. Specifically,
the former first splits the original training data into three different levels
by the input prompt length, and then the latter leverages segmented training
datasets with a progressively increasing context window length to train the
reasoning model. Experimental results demonstrate that
\textbf{\textsc{FastCuRL}}-1.5B-Preview surpasses DeepScaleR-1.5B-Preview
across all five datasets (including MATH 500, AIME 2024, AMC 2023, Minerva
Math, and OlympiadBench) while only utilizing 50\% of training steps.
Furthermore, all training stages for FastCuRL-1.5B-Preview are completed using
just a single node with 8 GPUs.",['cs.CL'],"['Mingyang Song', 'Mao Zheng', 'Zheng Li', 'Wenjie Yang', 'Xuan Luo', 'Yue Pan', 'Feng Zhang']",2025-03-21,2025-03-21
2503.17286v1,Offline Model-Based Optimization: Comprehensive Review,"Offline optimization is a fundamental challenge in science and engineering,
where the goal is to optimize black-box functions using only offline datasets.
This setting is particularly relevant when querying the objective function is
prohibitively expensive or infeasible, with applications spanning protein
engineering, material discovery, neural architecture search, and beyond. The
main difficulty lies in accurately estimating the objective landscape beyond
the available data, where extrapolations are fraught with significant epistemic
uncertainty. This uncertainty can lead to objective hacking(reward hacking),
exploiting model inaccuracies in unseen regions, or other spurious
optimizations that yield misleadingly high performance estimates outside the
training distribution. Recent advances in model-based optimization(MBO) have
harnessed the generalization capabilities of deep neural networks to develop
offline-specific surrogate and generative models. Trained with carefully
designed strategies, these models are more robust against out-of-distribution
issues, facilitating the discovery of improved designs. Despite its growing
impact in accelerating scientific discovery, the field lacks a comprehensive
review. To bridge this gap, we present the first thorough review of offline
MBO. We begin by formalizing the problem for both single-objective and
multi-objective settings and by reviewing recent benchmarks and evaluation
metrics. We then categorize existing approaches into two key areas: surrogate
modeling, which emphasizes accurate function approximation in
out-of-distribution regions, and generative modeling, which explores
high-dimensional design spaces to identify high-performing designs. Finally, we
examine the key challenges and propose promising directions for advancement in
this rapidly evolving field including safe control of superintelligent systems.",['cs.LG'],"['Minsu Kim', 'Jiayao Gu', 'Ye Yuan', 'Taeyoung Yun', 'Zixuan Liu', 'Yoshua Bengio', 'Can Chen']",2025-03-21,2025-03-21
2503.17285v1,An Iterative Feedback Mechanism for Improving Natural Language Class Descriptions in Open-Vocabulary Object Detection,"Recent advances in open-vocabulary object detection models will enable
Automatic Target Recognition systems to be sustainable and repurposed by
non-technical end-users for a variety of applications or missions. New, and
potentially nuanced, classes can be defined with natural language text
descriptions in the field, immediately before runtime, without needing to
retrain the model. We present an approach for improving non-technical users'
natural language text descriptions of their desired targets of interest, using
a combination of analysis techniques on the text embeddings, and proper
combinations of embeddings for contrastive examples. We quantify the
improvement that our feedback mechanism provides by demonstrating performance
with multiple publicly-available open-vocabulary object detection models.","['cs.CV', 'cs.CL']","['Louis Y. Kim', 'Michelle Karker', 'Victoria Valledor', 'Seiyoung C. Lee', 'Karl F. Brzoska', 'Margaret Duff', 'Anthony Palladino']",2025-03-21,2025-03-21
2503.17279v1,CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement,"The meaning conveyed by a sentence often depends on the context in which it
appears. Despite the progress of sentence embedding methods, it remains unclear
how to best modify a sentence embedding conditioned on its context. To address
this problem, we propose Condition-Aware Sentence Embeddings (CASE), an
efficient and accurate method to create an embedding for a sentence under a
given condition. First, CASE creates an embedding for the condition using a
Large Language Model (LLM), where the sentence influences the attention scores
computed for the tokens in the condition during pooling. Next, a supervised
nonlinear projection is learned to reduce the dimensionality of the LLM-based
text embeddings. We show that CASE significantly outperforms previously
proposed Conditional Semantic Textual Similarity (C-STS) methods on an existing
standard benchmark dataset. We find that subtracting the condition embedding
consistently improves the C-STS performance of LLM-based text embeddings.
Moreover, we propose a supervised dimensionality reduction method that not only
reduces the dimensionality of LLM-based embeddings but also significantly
improves their performance.",['cs.CL'],"['Gaifan Zhang', 'Yi Zhou', 'Danushka Bollegala']",2025-03-21,2025-03-21
2503.17276v1,HyperNVD: Accelerating Neural Video Decomposition via Hypernetworks,"Decomposing a video into a layer-based representation is crucial for easy
video editing for the creative industries, as it enables independent editing of
specific layers. Existing video-layer decomposition models rely on implicit
neural representations (INRs) trained independently for each video, making the
process time-consuming when applied to new videos. Noticing this limitation, we
propose a meta-learning strategy to learn a generic video decomposition model
to speed up the training on new videos. Our model is based on a hypernetwork
architecture which, given a video-encoder embedding, generates the parameters
for a compact INR-based neural video decomposition model. Our strategy
mitigates the problem of single-video overfitting and, importantly, shortens
the convergence of video decomposition on new, unseen videos. Our code is
available at: https://hypernvd.github.io/",['cs.CV'],"['Maria Pilligua', 'Danna Xue', 'Javier Vazquez-Corral']",2025-03-21,2025-03-21
2503.17275v1,Vision Transformer Based Semantic Communications for Next Generation Wireless Networks,"In the evolving landscape of 6G networks, semantic communications are poised
to revolutionize data transmission by prioritizing the transmission of semantic
meaning over raw data accuracy. This paper presents a Vision Transformer
(ViT)-based semantic communication framework that has been deliberately
designed to achieve high semantic similarity during image transmission while
simultaneously minimizing the demand for bandwidth. By equipping ViT as the
encoder-decoder framework, the proposed architecture can proficiently encode
images into a high semantic content at the transmitter and precisely
reconstruct the images, considering real-world fading and noise consideration
at the receiver. Building on the attention mechanisms inherent to ViTs, our
model outperforms Convolution Neural Network (CNNs) and Generative Adversarial
Networks (GANs) tailored for generating such images. The architecture based on
the proposed ViT network achieves the Peak Signal-to-noise Ratio (PSNR) of 38
dB, which is higher than other Deep Learning (DL) approaches in maintaining
semantic similarity across different communication environments. These findings
establish our ViT-based approach as a significant breakthrough in semantic
communications.","['eess.IV', 'cs.CV', 'eess.SP']","['Muhammad Ahmed Mohsin', 'Muhammad Jazib', 'Zeeshan Alam', 'Muhmmad Farhan Khan', 'Muhammad Saad', 'Muhammad Ali Jamshed']",2025-03-21,2025-03-21
2503.17272v1,Revisiting End To End Sparse Autoencoder Training -- A Short Finetune is All You Need,"Sparse autoencoders (SAEs) are widely used for interpreting language model
activations. A key evaluation metric is the increase in cross-entropy loss when
replacing model activations with SAE reconstructions. Typically, SAEs are
trained solely on mean squared error (MSE) using precomputed, shuffled
activations. Recent work introduced training SAEs directly with a combination
of KL divergence and MSE (""end-to-end"" SAEs), significantly improving
reconstruction accuracy at the cost of substantially increased computation,
which has limited their widespread adoption. We propose a brief KL+MSE
fine-tuning step applied only to the final 25M training tokens (just a few
percent of typical training budgets) that achieves comparable improvements,
reducing the cross-entropy loss gap by 20-50%, while incurring minimal
additional computational cost. We further find that multiple fine-tuning
methods (KL fine-tuning, LoRA adapters, linear adapters) yield similar,
non-additive cross-entropy improvements, suggesting a common, easily
correctable error source in MSE-trained SAEs. We demonstrate a straightforward
method for effectively transferring hyperparameters and sparsity penalties
despite scale differences between KL and MSE losses. While both ReLU and TopK
SAEs see significant cross-entropy loss improvements, evaluations on supervised
SAEBench metrics yield mixed results, suggesting practical benefits depend on
both SAE architecture and the specific downstream task. Nonetheless, our method
offers meaningful improvements in interpretability applications such as circuit
analysis with minor additional cost.",['cs.LG'],['Adam Karvonen'],2025-03-21,2025-03-21
2503.17269v1,Recovering Pulse Waves from Video Using Deep Unrolling and Deep Equilibrium Models,"Camera-based monitoring of vital signs, also known as imaging
photoplethysmography (iPPG), has seen applications in driver-monitoring,
perfusion assessment in surgical settings, affective computing, and more. iPPG
involves sensing the underlying cardiac pulse from video of the skin and
estimating vital signs such as the heart rate or a full pulse waveform. Some
previous iPPG methods impose model-based sparse priors on the pulse signals and
use iterative optimization for pulse wave recovery, while others use end-to-end
black-box deep learning methods. In contrast, we introduce methods that combine
signal processing and deep learning methods in an inverse problem framework.
Our methods estimate the underlying pulse signal and heart rate from facial
video by learning deep-network-based denoising operators that leverage deep
algorithm unfolding and deep equilibrium models. Experiments show that our
methods can denoise an acquired signal from the face and infer the correct
underlying pulse rate, achieving state-of-the-art heart rate estimation
performance on well-known benchmarks, all with less than one-fifth the number
of learnable parameters as the closest competing method.","['cs.CV', 'eess.IV']","['Vineet R Shenoy', 'Suhas Lohit', 'Hassan Mansour', 'Rama Chellappa', 'Tim K. Marks']",2025-03-21,2025-03-21
2503.17267v1,Physical Plausibility-aware Trajectory Prediction via Locomotion Embodiment,"Humans can predict future human trajectories even from momentary observations
by using human pose-related cues. However, previous Human Trajectory Prediction
(HTP) methods leverage the pose cues implicitly, resulting in implausible
predictions. To address this, we propose Locomotion Embodiment, a framework
that explicitly evaluates the physical plausibility of the predicted trajectory
by locomotion generation under the laws of physics. While the plausibility of
locomotion is learned with an indifferentiable physics simulator, it is
replaced by our differentiable Locomotion Value function to train an HTP
network in a data-driven manner. In particular, our proposed Embodied
Locomotion loss is beneficial for efficiently training a stochastic HTP network
using multiple heads. Furthermore, the Locomotion Value filter is proposed to
filter out implausible trajectories at inference. Experiments demonstrate that
our method enhances even the state-of-the-art HTP methods across diverse
datasets and problem settings. Our code is available at:
https://github.com/ImIntheMiddle/EmLoco.",['cs.CV'],"['Hiromu Taketsugu', 'Takeru Oba', 'Takahiro Maeda', 'Shohei Nobuhara', 'Norimichi Ukita']",2025-03-21,2025-03-21
2503.17265v1,Learning to Solve Related Linear Systems,"Solving multiple parametrised related systems is an essential component of
many numerical tasks. Borrowing strength from the solved systems and learning
will make this process faster. In this work, we propose a novel probabilistic
linear solver over the parameter space. This leverages information from the
solved linear systems in a regression setting to provide an efficient posterior
mean and covariance. We advocate using this as companion regression model for
the preconditioned conjugate gradient method, and discuss the favourable
properties of the posterior mean and covariance as the initial guess and
preconditioner. We also provide several design choices for this companion
solver. Numerical experiments showcase the benefits of using our novel solver
in a hyperparameter optimisation problem.","['stat.ML', 'cs.LG', 'cs.NA', 'math.NA']","['Disha Hegde', 'Jon Cockayne']",2025-03-21,2025-03-21
2503.17262v1,Unsupervised Joint Learning of Optical Flow and Intensity with Event Cameras,"Event cameras rely on motion to obtain information about scene appearance. In
other words, for event cameras, motion and appearance are seen both or neither,
which are encoded in the output event stream. Previous works consider
recovering these two visual quantities as separate tasks, which does not fit
with the nature of event cameras and neglects the inherent relations between
both tasks. In this paper, we propose an unsupervised learning framework that
jointly estimates optical flow (motion) and image intensity (appearance), with
a single network. Starting from the event generation model, we newly derive the
event-based photometric error as a function of optical flow and image
intensity, which is further combined with the contrast maximization framework,
yielding a comprehensive loss function that provides proper constraints for
both flow and intensity estimation. Exhaustive experiments show that our model
achieves state-of-the-art performance for both optical flow (achieves 20% and
25% improvement in EPE and AE respectively in the unsupervised learning
category) and intensity estimation (produces competitive results with other
baselines, particularly in high dynamic range scenarios). Last but not least,
our model achieves shorter inference time than all the other optical flow
models and many of the image reconstruction models, while they output only one
quantity. Project page: https://github.com/tub-rip/e2fai","['cs.CV', 'cs.LG', 'eess.IV']","['Shuang Guo', 'Friedhelm Hamann', 'Guillermo Gallego']",2025-03-21,2025-03-21
2503.17261v1,Cross-Modal Interactive Perception Network with Mamba for Lung Tumor Segmentation in PET-CT Images,"Lung cancer is a leading cause of cancer-related deaths globally. PET-CT is
crucial for imaging lung tumors, providing essential metabolic and anatomical
information, while it faces challenges such as poor image quality, motion
artifacts, and complex tumor morphology. Deep learning-based models are
expected to address these problems, however, existing small-scale and private
datasets limit significant performance improvements for these methods. Hence,
we introduce a large-scale PET-CT lung tumor segmentation dataset, termed
PCLT20K, which comprises 21,930 pairs of PET-CT images from 605 patients.
Furthermore, we propose a cross-modal interactive perception network with Mamba
(CIPA) for lung tumor segmentation in PET-CT images. Specifically, we design a
channel-wise rectification module (CRM) that implements a channel state space
block across multi-modal features to learn correlated representations and helps
filter out modality-specific noise. A dynamic cross-modality interaction module
(DCIM) is designed to effectively integrate position and context information,
which employs PET images to learn regional position information and serves as a
bridge to assist in modeling the relationships between local features of CT
images. Extensive experiments on a comprehensive benchmark demonstrate the
effectiveness of our CIPA compared to the current state-of-the-art segmentation
methods. We hope our research can provide more exploration opportunities for
medical image segmentation. The dataset and code are available at
https://github.com/mj129/CIPA.","['eess.IV', 'cs.CV']","['Jie Mei', 'Chenyu Lin', 'Yu Qiu', 'Yaonan Wang', 'Hui Zhang', 'Ziyang Wang', 'Dong Dai']",2025-03-21,2025-03-21
2503.17252v1,On Privately Estimating a Single Parameter,"We investigate differentially private estimators for individual parameters
within larger parametric models. While generic private estimators exist, the
estimators we provide repose on new local notions of estimand stability, and
these notions allow procedures that provide private certificates of their own
stability. By leveraging these private certificates, we provide computationally
and statistical efficient mechanisms that release private statistics that are,
at least asymptotically in the sample size, essentially unimprovable: they
achieve instance optimal bounds. Additionally, we investigate the practicality
of the algorithms both in simulated data and in real-world data from the
American Community Survey and US Census, highlighting scenarios in which the
new procedures are successful and identifying areas for future work.","['cs.LG', 'cs.CR', 'math.ST', 'stat.TH']","['Hilal Asi', 'John C. Duchi', 'Kunal Talwar']",2025-03-21,2025-03-21
2503.17251v1,Breaking the Symmetries of Indistinguishable Objects,"Indistinguishable objects often occur when modelling problems in constraint
programming, as well as in other related paradigms. They occur when objects can
be viewed as being drawn from a set of unlabelled objects, and the only
operation allowed on them is equality testing. For example, the golfers in the
social golfer problem are indistinguishable. If we do label the golfers, then
any relabelling of the golfers in one solution gives another valid solution.
Therefore, we can regard the symmetric group of size $n$ as acting on a set of
$n$ indistinguishable objects. In this paper, we show how we can break the
symmetries resulting from indistinguishable objects. We show how symmetries on
indistinguishable objects can be defined properly in complex types, for example
in a matrix indexed by indistinguishable objects. We then show how the
resulting symmetries can be broken correctly. In Essence, a high-level
modelling language, indistinguishable objects are encapsulated in ""unnamed
types"". We provide an implementation of complete symmetry breaking for unnamed
types in Essence.",['cs.AI'],"['Ozgur Akgun', 'Mun See Chang', 'Ian P. Gent', 'Christopher Jefferson']",2025-03-21,2025-03-21
2503.17247v1,"KL3M Tokenizers: A Family of Domain-Specific and Character-Level Tokenizers for Legal, Financial, and Preprocessing Applications","We present the KL3M tokenizers, a family of specialized tokenizers for legal,
financial, and governmental text. Despite established work on tokenization,
specialized tokenizers for professional domains remain understudied. Our paper
offers two main contributions to this area.
  First, we introduce domain-specific BPE tokenizers for legal, financial, and
governmental text. Our kl3m-004-128k-cased tokenizer uses 9-17% fewer tokens
than GPT-4o and Llama3 for domain-specific documents, despite having a smaller
vocabulary. For specialized terminology, our cased tokenizer is even more
efficient, using up to 83% fewer tokens for legal terms and 39% fewer tokens
for financial terms.
  Second, we develop character-level BPE tokenizers (4K, 8K, and 16K vocabulary
sizes) for text correction tasks like OCR post-processing. These tokenizers
keep consistent token boundaries between error-containing and correct text,
making it easier for models to learn correction patterns.
  These tokenizers help professional applications by fitting more text in
context windows, reducing computational needs, and preserving the meaning of
domain-specific terms. Our analysis shows these efficiency gains directly
benefit the processing of long legal and financial documents. We release all
tokenizers and code through GitHub and Hugging Face to support further research
in specialized tokenization.","['cs.CL', 'cs.AI']","['Michael J Bommarito', 'Daniel Martin Katz', 'Jillian Bommarito']",2025-03-21,2025-03-21
2503.17244v1,Deep End-to-End Posterior ENergy (DEEPEN) for image recovery,"Current end-to-end (E2E) and plug-and-play (PnP) image reconstruction
algorithms approximate the maximum a posteriori (MAP) estimate but cannot offer
sampling from the posterior distribution, like diffusion models. By contrast,
it is challenging for diffusion models to be trained in an E2E fashion. This
paper introduces a Deep End-to-End Posterior ENergy (DEEPEN) framework, which
enables MAP estimation as well as sampling. We learn the parameters of the
posterior, which is the sum of the data consistency error and the negative
log-prior distribution, using maximum likelihood optimization in an E2E
fashion. The proposed approach does not require algorithm unrolling, and hence
has a smaller computational and memory footprint than current E2E methods,
while it does not require contraction constraints typically needed by current
PnP methods. Our results demonstrate that DEEPEN offers improved performance
than current E2E and PnP models in the MAP setting, while it also offers faster
sampling compared to diffusion models. In addition, the learned energy-based
model is observed to be more robust to changes in image acquisition settings.","['eess.IV', 'cs.CV', 'cs.LG']","['Jyothi Rikhab Chand', 'Mathews Jacob']",2025-03-21,2025-03-21
2503.17239v1,SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging,"Fine-tuning large language models (LLMs) on downstream tasks can
inadvertently erode their safety alignment, even for benign fine-tuning
datasets. We address this challenge by proposing SafeMERGE, a post-fine-tuning
framework that preserves safety while maintaining task utility. It achieves
this by selectively merging fine-tuned and safety-aligned model layers only
when those deviate from safe behavior, measured by a cosine similarity
criterion. We evaluate SafeMERGE against other fine-tuning- and
post-fine-tuning-stage approaches for Llama-2-7B-Chat and Qwen-2-7B-Instruct
models on GSM8K and PubMedQA tasks while exploring different merging
strategies. We find that SafeMERGE consistently reduces harmful outputs
compared to other baselines without significantly sacrificing performance,
sometimes even enhancing it. The results suggest that our selective,
subspace-guided, and per-layer merging method provides an effective safeguard
against the inadvertent loss of safety in fine-tuned LLMs while outperforming
simpler post-fine-tuning-stage defenses.","['cs.CL', 'cs.AI']","['Aladin Djuhera', 'Swanand Ravindra Kadhe', 'Farhan Ahmed', 'Syed Zawad', 'Holger Boche']",2025-03-21,2025-03-21
2503.17238v1,Slide-Level Prompt Learning with Vision Language Models for Few-Shot Multiple Instance Learning in Histopathology,"In this paper, we address the challenge of few-shot classification in
histopathology whole slide images (WSIs) by utilizing foundational
vision-language models (VLMs) and slide-level prompt learning. Given the
gigapixel scale of WSIs, conventional multiple instance learning (MIL) methods
rely on aggregation functions to derive slide-level (bag-level) predictions
from patch representations, which require extensive bag-level labels for
training. In contrast, VLM-based approaches excel at aligning visual embeddings
of patches with candidate class text prompts but lack essential pathological
prior knowledge. Our method distinguishes itself by utilizing pathological
prior knowledge from language models to identify crucial local tissue types
(patches) for WSI classification, integrating this within a VLM-based MIL
framework. Our approach effectively aligns patch images with tissue types, and
we fine-tune our model via prompt learning using only a few labeled WSIs per
category. Experimentation on real-world pathological WSI datasets and ablation
studies highlight our method's superior performance over existing MIL- and
VLM-based methods in few-shot WSI classification tasks. Our code is publicly
available at https://github.com/LTS5/SLIP.",['cs.CV'],"['Devavrat Tomar', 'Guillaume Vray', 'Dwarikanath Mahapatra', 'Sudipta Roy', 'Jean-Philippe Thiran', 'Behzad Bozorgtabar']",2025-03-21,2025-03-21
2503.17237v1,Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID,"Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal
infrared video is inherently challenging due to low contrast, environmental
noise, and small target sizes. This paper provides a straightforward approach
to address multi-UAV tracking in thermal infrared video, leveraging recent
advances in detection and tracking. Instead of relying on the YOLOv5 with the
DeepSORT pipeline, we present a tracking framework built on YOLOv12 and
BoT-SORT, enhanced with tailored training and inference strategies. We evaluate
our approach following the metrics from the 4th Anti-UAV Challenge and
demonstrate competitive performance. Notably, we achieve strong results without
using contrast enhancement or temporal information fusion to enrich UAV
features, highlighting our approach as a ""Strong Baseline"" for the multi-UAV
tracking task. We provide implementation details, in-depth experimental
analysis, and a discussion of potential improvements. The code is available at
https://github.com/wish44165/YOLOv12-BoT-SORT-ReID .","['cs.CV', 'cs.AI']",['Yu-Hsi Chen'],2025-03-21,2025-03-21
2503.17231v1,LoGoFair: Post-Processing for Local and Global Fairness in Federated Learning,"Federated learning (FL) has garnered considerable interest for its capability
to learn from decentralized data sources. Given the increasing application of
FL in decision-making scenarios, addressing fairness issues across different
sensitive groups (e.g., female, male) in FL is crucial. Current research often
focuses on facilitating fairness at each client's data (local fairness) or
within the entire dataset across all clients (global fairness). However,
existing approaches that focus exclusively on either local or global fairness
fail to address two key challenges: (\textbf{CH1}) Under statistical
heterogeneity, global fairness does not imply local fairness, and vice versa.
(\textbf{CH2}) Achieving fairness under model-agnostic setting. To tackle the
aforementioned challenges, this paper proposes a novel post-processing
framework for achieving both Local and Global Fairness in the FL context,
namely LoGoFair. To address CH1, LoGoFair endeavors to seek the Bayes optimal
classifier under local and global fairness constraints, which strikes the
optimal accuracy-fairness balance in the probabilistic sense. To address CH2,
LoGoFair employs a model-agnostic federated post-processing procedure that
enables clients to collaboratively optimize global fairness while ensuring
local fairness, thereby achieving the optimal fair classifier within FL.
Experimental results on three real-world datasets further illustrate the
effectiveness of the proposed LoGoFair framework.","['cs.LG', 'cs.DC']","['Li Zhang', 'Chaochao Chen', 'Zhongxuan Han', 'Qiyong Zhong', 'Xiaolin Zheng']",2025-03-21,2025-03-21
2503.17229v1,FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs,"Large Language Models (LLMs) frequently generate hallucinated content, posing
significant challenges for applications where factuality is crucial. While
existing hallucination detection methods typically operate at the sentence
level or passage level, we propose FactSelfCheck, a novel black-box
sampling-based method that enables fine-grained fact-level detection. Our
approach represents text as knowledge graphs consisting of facts in the form of
triples. Through analyzing factual consistency across multiple LLM responses,
we compute fine-grained hallucination scores without requiring external
resources or training data. Our evaluation demonstrates that FactSelfCheck
performs competitively with leading sampling-based methods while providing more
detailed insights. Most notably, our fact-level approach significantly improves
hallucination correction, achieving a 35% increase in factual content compared
to the baseline, while sentence-level SelfCheckGPT yields only an 8%
improvement. The granular nature of our detection enables more precise
identification and correction of hallucinated content.","['cs.LG', 'cs.AI', 'cs.CL']","['Albert Sawczyn', 'Jakub Binkowski', 'Denis Janiak', 'Bogdan Gabrys', 'Tomasz Kajdanowicz']",2025-03-21,2025-03-21
2503.17226v1,Leveraging Text-to-Image Generation for Handling Spurious Correlation,"Deep neural networks trained with Empirical Risk Minimization (ERM) perform
well when both training and test data come from the same domain, but they often
fail to generalize to out-of-distribution samples. In image classification,
these models may rely on spurious correlations that often exist between labels
and irrelevant features of images, making predictions unreliable when those
features do not exist. We propose a technique to generate training samples with
text-to-image (T2I) diffusion models for addressing the spurious correlation
problem. First, we compute the best describing token for the visual features
pertaining to the causal components of samples by a textual inversion
mechanism. Then, leveraging a language segmentation method and a diffusion
model, we generate new samples by combining the causal component with the
elements from other classes. We also meticulously prune the generated samples
based on the prediction probabilities and attribution scores of the ERM model
to ensure their correct composition for our objective. Finally, we retrain the
ERM model on our augmented dataset. This process reduces the model's reliance
on spurious correlations by learning from carefully crafted samples for in
which this correlation does not exist. Our experiments show that across
different benchmarks, our technique achieves better worst-group accuracy than
the existing state-of-the-art methods.",['cs.CV'],"['Aryan Yazdan Parast', 'Basim Azam', 'Naveed Akhtar']",2025-03-21,2025-03-21
2503.17224v1,Neuro-Symbolic Scene Graph Conditioning for Synthetic Image Dataset Generation,"As machine learning models increase in scale and complexity, obtaining
sufficient training data has become a critical bottleneck due to acquisition
costs, privacy constraints, and data scarcity in specialised domains. While
synthetic data generation has emerged as a promising alternative, a notable
performance gap remains compared to models trained on real data, particularly
as task complexity grows. Concurrently, Neuro-Symbolic methods, which combine
neural networks' learning strengths with symbolic reasoning's structured
representations, have demonstrated significant potential across various
cognitive tasks. This paper explores the utility of Neuro-Symbolic conditioning
for synthetic image dataset generation, focusing specifically on improving the
performance of Scene Graph Generation models. The research investigates whether
structured symbolic representations in the form of scene graphs can enhance
synthetic data quality through explicit encoding of relational constraints. The
results demonstrate that Neuro-Symbolic conditioning yields significant
improvements of up to +2.59% in standard Recall metrics and +2.83% in No Graph
Constraint Recall metrics when used for dataset augmentation. These findings
establish that merging Neuro-Symbolic and generative approaches produces
synthetic data with complementary structural information that enhances model
performance when combined with real data, providing a novel approach to
overcome data scarcity limitations even for complex visual reasoning tasks.","['cs.CV', 'cs.AI', 'cs.LG']","['Giacomo Savazzi', 'Eugenio Lomurno', 'Cristian Sbrolli', 'Agnese Chiatti', 'Matteo Matteucci']",2025-03-21,2025-03-21
2503.17222v1,Automating Adjudication of Cardiovascular Events Using Large Language Models,"Cardiovascular events, such as heart attacks and strokes, remain a leading
cause of mortality globally, necessitating meticulous monitoring and
adjudication in clinical trials. This process, traditionally performed manually
by clinical experts, is time-consuming, resource-intensive, and prone to
inter-reviewer variability, potentially introducing bias and hindering trial
progress. This study addresses these critical limitations by presenting a novel
framework for automating the adjudication of cardiovascular events in clinical
trials using Large Language Models (LLMs). We developed a two-stage approach:
first, employing an LLM-based pipeline for event information extraction from
unstructured clinical data and second, using an LLM-based adjudication process
guided by a Tree of Thoughts approach and clinical endpoint committee (CEC)
guidelines. Using cardiovascular event-specific clinical trial data, the
framework achieved an F1-score of 0.82 for event extraction and an accuracy of
0.68 for adjudication. Furthermore, we introduce the CLEART score, a novel,
automated metric specifically designed for evaluating the quality of
AI-generated clinical reasoning in adjudicating cardiovascular events. This
approach demonstrates significant potential for substantially reducing
adjudication time and costs while maintaining high-quality, consistent, and
auditable outcomes in clinical trials. The reduced variability and enhanced
standardization also allow for faster identification and mitigation of risks
associated with cardiovascular therapies.","['cs.CL', 'cs.AI']","['Sonish Sivarajkumar', 'Kimia Ameri', 'Chuqin Li', 'Yanshan Wang', 'Min Jiang']",2025-03-21,2025-03-21
2503.17221v1,UniCon: Unidirectional Information Flow for Effective Control of Large-Scale Diffusion Models,"We introduce UniCon, a novel architecture designed to enhance control and
efficiency in training adapters for large-scale diffusion models. Unlike
existing methods that rely on bidirectional interaction between the diffusion
model and control adapter, UniCon implements a unidirectional flow from the
diffusion network to the adapter, allowing the adapter alone to generate the
final output. UniCon reduces computational demands by eliminating the need for
the diffusion model to compute and store gradients during adapter training. Our
results indicate that UniCon reduces GPU memory usage by one-third and
increases training speed by 2.3 times, while maintaining the same adapter
parameter size. Additionally, without requiring extra computational resources,
UniCon enables the training of adapters with double the parameter volume of
existing ControlNets. In a series of image conditional generation tasks, UniCon
has demonstrated precise responsiveness to control inputs and exceptional
generation capabilities.",['cs.CV'],"['Fanghua Yu', 'Jinjin Gu', 'Jinfan Hu', 'Zheyuan Li', 'Chao Dong']",2025-03-21,2025-03-21
2503.17214v1,ML-Based Bidding Price Prediction for Pay-As-Bid Ancillary Services Markets: A Use Case in the German Control Reserve Market,"The increasing integration of renewable energy sources has led to greater
volatility and unpredictability in electricity generation, posing challenges to
grid stability. Ancillary service markets, such as the German control reserve
market, allow industrial consumers and producers to offer flexibility in their
power consumption or generation, contributing to grid stability while earning
additional income. However, many participants use simple bidding strategies
that may not maximize their revenues. This paper presents a methodology for
forecasting bidding prices in pay-as-bid ancillary service markets, focusing on
the German control reserve market. We evaluate various machine learning models,
including Support Vector Regression, Decision Trees, and k-Nearest Neighbors,
and compare their performance against benchmark models. To address the
asymmetry in the revenue function of pay-as-bid markets, we introduce an offset
adjustment technique that enhances the practical applicability of the
forecasting models. Our analysis demonstrates that the proposed approach
improves potential revenues by 27.43 % to 37.31 % compared to baseline models.
When analyzing the relationship between the model forecasting errors and the
revenue, a negative correlation is measured for three markets; according to the
results, a reduction of 1 EUR/MW model price forecasting error (MAE)
statistically leads to a yearly revenue increase between 483 EUR/MW and 3,631
EUR/MW. The proposed methodology enables industrial participants to optimize
their bidding strategies, leading to increased earnings and contributing to the
efficiency and stability of the electrical grid.","['cs.LG', 'cs.CE', 'stat.AP', 'stat.ML']","['Vincent Bezold', 'Lukas Baur', 'Alexander Sauer']",2025-03-21,2025-03-21
2503.17213v1,PP-DocLayout: A Unified Document Layout Detection Model to Accelerate Large-Scale Data Construction,"Document layout analysis is a critical preprocessing step in document
intelligence, enabling the detection and localization of structural elements
such as titles, text blocks, tables, and formulas. Despite its importance,
existing layout detection models face significant challenges in generalizing
across diverse document types, handling complex layouts, and achieving
real-time performance for large-scale data processing. To address these
limitations, we present PP-DocLayout, which achieves high precision and
efficiency in recognizing 23 types of layout regions across diverse document
formats. To meet different needs, we offer three models of varying scales.
PP-DocLayout-L is a high-precision model based on the RT-DETR-L detector,
achieving 90.4% mAP@0.5 and an end-to-end inference time of 13.4 ms per page on
a T4 GPU. PP-DocLayout-M is a balanced model, offering 75.2% mAP@0.5 with an
inference time of 12.7 ms per page on a T4 GPU. PP-DocLayout-S is a
high-efficiency model designed for resource-constrained environments and
real-time applications, with an inference time of 8.1 ms per page on a T4 GPU
and 14.5 ms on a CPU. This work not only advances the state of the art in
document layout analysis but also provides a robust solution for constructing
high-quality training data, enabling advancements in document intelligence and
multimodal AI systems. Code and models are available at
https://github.com/PaddlePaddle/PaddleX .","['cs.CV', 'cs.AI']","['Ting Sun', 'Cheng Cui', 'Yuning Du', 'Yi Liu']",2025-03-21,2025-03-21
2503.17212v1,A Deep Learning Framework for Visual Attention Prediction and Analysis of News Interfaces,"News outlets' competition for attention in news interfaces has highlighted
the need for demographically-aware saliency prediction models. Despite recent
advancements in saliency detection applied to user interfaces (UI), existing
datasets are limited in size and demographic representation. We present a deep
learning framework that enhances the SaRa (Saliency Ranking) model with
DeepGaze IIE, improving Salient Object Ranking (SOR) performance by 10.7%. Our
framework optimizes three key components: saliency map generation, grid segment
scoring, and map normalization. Through a two-fold experiment using
eye-tracking (30 participants) and mouse-tracking (375 participants aged
13--70), we analyze attention patterns across demographic groups. Statistical
analysis reveals significant age-based variations (p < 0.05, {\epsilon^2} =
0.042), with older users (36--70) engaging more with textual content and
younger users (13--35) interacting more with images. Mouse-tracking data
closely approximates eye-tracking behavior (sAUC = 0.86) and identifies UI
elements that immediately stand out, validating its use in large-scale studies.
We conclude that saliency studies should prioritize gathering data from a
larger, demographically representative sample and report exact demographic
distributions.","['cs.CV', 'cs.HC']","['Matthew Kenely', 'Dylan Seychell', 'Carl James Debono', 'Chris Porter']",2025-03-21,2025-03-21
2503.17211v1,A Language Anchor-Guided Method for Robust Noisy Domain Generalization,"Real-world machine learning applications often struggle with two major
challenges: distribution shift and label noise. Models tend to overfit by
focusing on redundant and uninformative features in the training data, which
makes it hard for them to generalize to the target domain. Noisy data worsens
this problem by causing further overfitting to the noise, meaning that existing
methods often fail to tell the difference between true, invariant features and
misleading, spurious ones. To tackle these issues, we introduce Anchor
Alignment and Adaptive Weighting (A3W). This new algorithm uses sample
reweighting guided by natural language processing (NLP) anchors to extract more
representative features. In simple terms, A3W leverages semantic
representations from natural language models as a source of domain-invariant
prior knowledge. Additionally, it employs a weighted loss function that adjusts
each sample's contribution based on its similarity to the corresponding NLP
anchor. This adjustment makes the model more robust to noisy labels. Extensive
experiments on standard benchmark datasets show that A3W consistently
outperforms state-of-the-art domain generalization methods, offering
significant improvements in both accuracy and robustness across different
datasets and noise levels.","['cs.CL', 'cs.CV', 'cs.LG']","['Zilin Dai', 'Lehong Wang', 'Fangzhou Lin', 'Yidong Wang', 'Zhigang Li', 'Kazunori D Yamada', 'Ziming Zhang', 'Wang Lu']",2025-03-21,2025-03-21
2503.17198v1,Jailbreaking the Non-Transferable Barrier via Test-Time Data Disguising,"Non-transferable learning (NTL) has been proposed to protect model
intellectual property (IP) by creating a ""non-transferable barrier"" to restrict
generalization from authorized to unauthorized domains. Recently, well-designed
attack, which restores the unauthorized-domain performance by fine-tuning NTL
models on few authorized samples, highlights the security risks of NTL-based
applications. However, such attack requires modifying model weights, thus being
invalid in the black-box scenario. This raises a critical question: can we
trust the security of NTL models deployed as black-box systems? In this work,
we reveal the first loophole of black-box NTL models by proposing a novel
attack method (dubbed as JailNTL) to jailbreak the non-transferable barrier
through test-time data disguising. The main idea of JailNTL is to disguise
unauthorized data so it can be identified as authorized by the NTL model,
thereby bypassing the non-transferable barrier without modifying the NTL model
weights. Specifically, JailNTL encourages unauthorized-domain disguising in two
levels, including: (i) data-intrinsic disguising (DID) for eliminating domain
discrepancy and preserving class-related content at the input-level, and (ii)
model-guided disguising (MGD) for mitigating output-level statistics difference
of the NTL model. Empirically, when attacking state-of-the-art (SOTA) NTL
models in the black-box scenario, JailNTL achieves an accuracy increase of up
to 55.7% in the unauthorized domain by using only 1% authorized samples,
largely exceeding existing SOTA white-box attacks.","['cs.CR', 'cs.CV', 'cs.LG']","['Yongli Xiang', 'Ziming Hong', 'Lina Yao', 'Dadong Wang', 'Tongliang Liu']",2025-03-21,2025-03-21
2503.17197v1,FreeUV: Ground-Truth-Free Realistic Facial UV Texture Recovery via Cross-Assembly Inference Strategy,"Recovering high-quality 3D facial textures from single-view 2D images is a
challenging task, especially under constraints of limited data and complex
facial details such as makeup, wrinkles, and occlusions. In this paper, we
introduce FreeUV, a novel ground-truth-free UV texture recovery framework that
eliminates the need for annotated or synthetic UV data. FreeUV leverages
pre-trained stable diffusion model alongside a Cross-Assembly inference
strategy to fulfill this objective. In FreeUV, separate networks are trained
independently to focus on realistic appearance and structural consistency, and
these networks are combined during inference to generate coherent textures. Our
approach accurately captures intricate facial features and demonstrates robust
performance across diverse poses and occlusions. Extensive experiments validate
FreeUV's effectiveness, with results surpassing state-of-the-art methods in
both quantitative and qualitative metrics. Additionally, FreeUV enables new
applications, including local editing, facial feature interpolation, and
multi-view texture recovery. By reducing data requirements, FreeUV offers a
scalable solution for generating high-fidelity 3D facial textures suitable for
real-world scenarios.",['cs.CV'],"['Xingchao Yang', 'Takafumi Taketomi', 'Yuki Endo', 'Yoshihiro Kanamori']",2025-03-21,2025-03-21
2503.17195v1,TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided Subspace Partitioning,"Model customization requires high-quality and diverse datasets, but acquiring
such data remains challenging and costly. Although large language models (LLMs)
can synthesize training data, current approaches are constrained by limited
seed data, model bias and insufficient control over the generation process,
resulting in limited diversity and biased distribution with the increase of
data scales. To tackle this challenge, we present TreeSynth, a tree-guided
subspace-based data synthesis framework that recursively partitions the entire
data space into hierar-chical subspaces, enabling comprehensive and diverse
scaling of data synthesis. Briefly, given a task-specific description, we
construct a data space partitioning tree by iteratively executing criteria
determination and subspace coverage steps. This hierarchically divides the
whole space (i.e., root node) into mutually exclusive and complementary atomic
subspaces (i.e., leaf nodes). By collecting synthesized data according to the
attributes of each leaf node, we obtain a diverse dataset that fully covers the
data space. Empirically, our extensive experiments demonstrate that TreeSynth
surpasses both human-designed datasets and the state-of-the-art data synthesis
baselines, achieving maximum improvements of 45.2% in data diversity and 17.6%
in downstream task performance across various models and tasks. Hopefully,
TreeSynth provides a scalable solution to synthesize diverse and comprehensive
datasets from scratch without human intervention.","['cs.LG', 'cs.AI']","['Sheng Wang', 'Pengan Chen', 'Jingqi Zhou', 'Qintong Li', 'Jingwei Dong', 'Jiahui Gao', 'Boyang Xue', 'Jiyue Jiang', 'Lingpeng Kong', 'Chuan Wu']",2025-03-21,2025-03-21
2503.17194v1,Curriculum RL meets Monte Carlo Planning: Optimization of a Real World Container Management Problem,"In this work, we augment reinforcement learning with an inference-time
collision model to ensure safe and efficient container management in a
waste-sorting facility with limited processing capacity. Each container has two
optimal emptying volumes that trade off higher throughput against overflow
risk. Conventional reinforcement learning (RL) approaches struggle under
delayed rewards, sparse critical events, and high-dimensional uncertainty --
failing to consistently balance higher-volume empties with the risk of
safety-limit violations. To address these challenges, we propose a hybrid
method comprising: (1) a curriculum-learning pipeline that incrementally trains
a PPO agent to handle delayed rewards and class imbalance, and (2) an offline
pairwise collision model used at inference time to proactively avert collisions
with minimal online cost. Experimental results show that our targeted
inference-time collision checks significantly improve collision avoidance,
reduce safety-limit violations, maintain high throughput, and scale effectively
across varying container-to-PU ratios. These findings offer actionable
guidelines for designing safe and efficient container-management systems in
real-world facilities.",['cs.LG'],"['Abhijeet Pendyala', 'Tobias Glasmachers']",2025-03-21,2025-03-21
2503.17193v1,MSCA-Net:Multi-Scale Context Aggregation Network for Infrared Small Target Detection,"Detecting infrared small targets in complex backgrounds remains a challenging
task because of the low contrast and high noise levels inherent in infrared
images. These factors often lead to the loss of crucial details during feature
extraction. Moreover, existing detection methods have limitations in adequately
integrating global and local information, which constrains the efficiency and
accuracy of infrared small target detection. To address these challenges, this
paper proposes a novel network architecture named MSCA-Net, which integrates
three key components: Multi-Scale Enhanced Detection Attention
mechanism(MSEDA), Positional Convolutional Block Attention Module (PCBAM), and
Channel Aggregation Block (CAB). Specifically, MSEDA employs a multi-scale
feature fusion attention mechanism to adaptively aggregate information across
different scales, enriching feature representation. PCBAM captures the
correlation between global and local features through a correlation
matrix-based strategy, enabling deep feature interaction. Moreover, CAB
redistributes input feature channels, facilitating the efficient transmission
of beneficial features and further enhancing the model detection capability in
complex backgrounds. The experimental results demonstrate that MSCA-Net
achieves outstanding small target detection performance in complex backgrounds.
Specifically, it attains mIoU scores of 78.43\%, 94.56\%, and 67.08\% on the
NUAA-SIRST, NUDT-SIRST, and IRTSD-1K datasets, respectively, underscoring its
effectiveness and strong potential for real-world applications.",['cs.CV'],"['Xiaojin Lu', 'Taoran yue', 'Jiaxi cai', 'Shibing Chu']",2025-03-21,2025-03-21
2503.17184v1,D2Fusion: Dual-domain Fusion with Feature Superposition for Deepfake Detection,"Deepfake detection is crucial for curbing the harm it causes to society.
However, current Deepfake detection methods fail to thoroughly explore artifact
information across different domains due to insufficient intrinsic
interactions. These interactions refer to the fusion and coordination after
feature extraction processes across different domains, which are crucial for
recognizing complex forgery clues. Focusing on more generalized Deepfake
detection, in this work, we introduce a novel bi-directional attention module
to capture the local positional information of artifact clues from the spatial
domain. This enables accurate artifact localization, thus addressing the coarse
processing with artifact features. To further address the limitation that the
proposed bi-directional attention module may not well capture global subtle
forgery information in the artifact feature (e.g., textures or edges), we
employ a fine-grained frequency attention module in the frequency domain. By
doing so, we can obtain high-frequency information in the fine-grained
features, which contains the global and subtle forgery information. Although
these features from the diverse domains can be effectively and independently
improved, fusing them directly does not effectively improve the detection
performance. Therefore, we propose a feature superposition strategy that
complements information from spatial and frequency domains. This strategy turns
the feature components into the form of wave-like tokens, which are updated
based on their phase, such that the distinctions between authentic and artifact
features can be amplified. Our method demonstrates significant improvements
over state-of-the-art (SOTA) methods on five public Deepfake datasets in
capturing abnormalities across different manipulated operations and real-life.","['cs.CV', 'cs.AI']","['Xueqi Qiu', 'Xingyu Miao', 'Fan Wan', 'Haoran Duan', 'Tejal Shah', 'Varun Ojhab', 'Yang Longa', 'Rajiv Ranjan']",2025-03-21,2025-03-21
2503.17182v1,Radar-Guided Polynomial Fitting for Metric Depth Estimation,"We propose PolyRad, a novel radar-guided depth estimation method that
introduces polynomial fitting to transform scaleless depth predictions from
pretrained monocular depth estimation (MDE) models into metric depth maps.
Unlike existing approaches that rely on complex architectures or expensive
sensors, our method is grounded in a simple yet fundamental insight: using
polynomial coefficients predicted from cheap, ubiquitous radar data to
adaptively adjust depth predictions non-uniformly across depth ranges. Although
MDE models often infer reasonably accurate local depth structure within each
object or local region, they may misalign these regions relative to one
another, making a linear scale-and-shift transformation insufficient given
three or more of these regions. In contrast, PolyRad generalizes beyond linear
transformations and is able to correct such misalignments by introducing
inflection points. Importantly, our polynomial fitting framework preserves
structural consistency through a novel training objective that enforces
monotonicity via first-derivative regularization. PolyRad achieves
state-of-the-art performance on the nuScenes, ZJU-4DRadarCam, and View-of-Delft
datasets, outperforming existing methods by 30.3% in MAE and 37.2% in RMSE.",['cs.CV'],"['Patrick Rim', 'Hyoungseob Park', 'Vadim Ezhov', 'Jeffrey Moon', 'Alex Wong']",2025-03-21,2025-03-21
2503.17181v1,LLMs Love Python: A Study of LLMs' Bias for Programming Languages and Libraries,"Programming language and library choices are crucial to software reliability
and security. Poor or inconsistent choices can lead to increased technical
debt, security vulnerabilities, and even catastrophic failures in
safety-critical systems. As Large Language Models (LLMs) play an increasing
role in code generation, it is essential to understand how they make these
decisions. However, little is known about their preferences when selecting
programming languages and libraries for different coding tasks. To fill this
gap, this study provides the first in-depth investigation into LLM preferences
for programming languages and libraries used when generating code. We assess
the preferences of eight diverse LLMs by prompting them to complete various
coding tasks, including widely-studied benchmarks and the more practical task
of generating the initial structural code for new projects (a crucial step that
often determines a project's language or library choices).
  Our findings reveal that LLMs heavily favour Python when solving
language-agnostic problems, using it in 90%-97% of cases for benchmark tasks.
Even when generating initial project code where Python is not a suitable
language, it remains the most-used language in 58% of instances. Moreover, LLMs
contradict their own language recommendations in 83% of project initialisation
tasks, raising concerns about their reliability in guiding language selection.
Similar biases toward well-established libraries further create serious
discoverability challenges for newer open-source projects. These results
highlight the need to improve LLMs' adaptability to diverse programming
contexts and to develop mechanisms for mitigating programming language and
library bias.","['cs.SE', 'cs.AI']","['Lukas Twist', 'Jie M. Zhang', 'Mark Harman', 'Don Syme', 'Joost Noppen', 'Detlef Nauck']",2025-03-21,2025-03-21
2503.17175v1,Which2comm: An Efficient Collaborative Perception Framework for 3D Object Detection,"Collaborative perception allows real-time inter-agent information exchange
and thus offers invaluable opportunities to enhance the perception capabilities
of individual agents. However, limited communication bandwidth in practical
scenarios restricts the inter-agent data transmission volume, consequently
resulting in performance declines in collaborative perception systems. This
implies a trade-off between perception performance and communication cost. To
address this issue, we propose Which2comm, a novel multi-agent 3D object
detection framework leveraging object-level sparse features. By integrating
semantic information of objects into 3D object detection boxes, we introduce
semantic detection boxes (SemDBs). Innovatively transmitting these
information-rich object-level sparse features among agents not only
significantly reduces the demanding communication volume, but also improves 3D
object detection performance. Specifically, a fully sparse network is
constructed to extract SemDBs from individual agents; a temporal fusion
approach with a relative temporal encoding mechanism is utilized to obtain the
comprehensive spatiotemporal features. Extensive experiments on the V2XSet and
OPV2V datasets demonstrate that Which2comm consistently outperforms other
state-of-the-art methods on both perception performance and communication cost,
exhibiting better robustness to real-world latency. These results present that
for multi-agent collaborative 3D object detection, transmitting only
object-level sparse features is sufficient to achieve high-precision and robust
performance.",['cs.CV'],"['Duanrui Yu', 'Jing You', 'Xin Pei', 'Anqi Qu', 'Dingyu Wang', 'Shaocheng Jia']",2025-03-21,2025-03-21
2503.17173v1,Robustness of deep learning classification to adversarial input on GPUs: asynchronous parallel accumulation is a source of vulnerability,"The ability of machine learning (ML) classification models to resist small,
targeted input perturbations - known as adversarial attacks - is a key measure
of their safety and reliability. We show that floating-point non associativity
(FPNA) coupled with asynchronous parallel programming on GPUs is sufficient to
result in misclassification, without any perturbation to the input.
Additionally, we show this misclassification is particularly significant for
inputs close to the decision boundary and that standard adversarial robustness
results may be overestimated up to 4.6% when not considering machine-level
details. We first study a linear classifier, before focusing on standard Graph
Neural Network (GNN) architectures and datasets. We present a novel black-box
attack using Bayesian optimization to determine external workloads that bias
the output of reductions on GPUs and reliably lead to misclassification.
Motivated by these results, we present a new learnable permutation (LP)
gradient-based approach, to learn floating point operation orderings that lead
to misclassifications, making the assumption that any reduction or permutation
ordering is possible. This LP approach provides a worst-case estimate in a
computationally efficient manner, avoiding the need to run identical
experiments tens of thousands of times over a potentially large set of possible
GPU states or architectures. Finally, we investigate parallel reduction
ordering across different GPU architectures for a reduction under three
conditions: (1) executing external background workloads, (2) utilizing
multi-GPU virtualization, and (3) applying power capping. Our results
demonstrate that parallel reduction ordering varies significantly across
architectures under the first two conditions. The results and methods developed
here can help to include machine-level considerations into adversarial
robustness assessments.","['cs.LG', 'cs.DC', 'I.2.11; B.8.1']","['Sanjif Shanmugavelu', 'Mathieu Taillefumier', 'Christopher Culver', 'Vijay Ganesh', 'Oscar Hernandez', 'Ada Sedova']",2025-03-21,2025-03-21
2503.17172v1,Principal Eigenvalue Regularization for Improved Worst-Class Certified Robustness of Smoothed Classifiers,"Recent studies have identified a critical challenge in deep neural networks
(DNNs) known as ``robust fairness"", where models exhibit significant
disparities in robust accuracy across different classes. While prior work has
attempted to address this issue in adversarial robustness, the study of
worst-class certified robustness for smoothed classifiers remains unexplored.
Our work bridges this gap by developing a PAC-Bayesian bound for the
worst-class error of smoothed classifiers. Through theoretical analysis, we
demonstrate that the largest eigenvalue of the smoothed confusion matrix
fundamentally influences the worst-class error of smoothed classifiers. Based
on this insight, we introduce a regularization method that optimizes the
largest eigenvalue of smoothed confusion matrix to enhance worst-class accuracy
of the smoothed classifier and further improve its worst-class certified
robustness. We provide extensive experimental validation across multiple
datasets and model architectures to demonstrate the effectiveness of our
approach.",['cs.LG'],"['Gaojie Jin', 'Tianjin Huang', 'Ronghui Mu', 'Xiaowei Huang']",2025-03-21,2025-03-21
2503.17171v1,Generative adversarial framework to calibrate excursion set models for the 3D morphology of all-solid-state battery cathodes,"This paper presents a computational method for generating virtual 3D
morphologies of functional materials using low-parametric stochastic geometry
models, i.e., digital twins, calibrated with 2D microscopy images. These
digital twins allow systematic parameter variations to simulate various
morphologies, that can be deployed for virtual materials testing by means of
spatially resolved numerical simulations of macroscopic properties. Generative
adversarial networks (GANs) have gained popularity for calibrating models to
generate realistic 3D morphologies. However, GANs often comprise of numerous
uninterpretable parameters make systematic variation of morphologies for
virtual materials testing challenging. In contrast, low-parametric stochastic
geometry models (e.g., based on Gaussian random fields) enable targeted
variation but may struggle to mimic complex morphologies. Combining GANs with
advanced stochastic geometry models (e.g., excursion sets of more general
random fields) addresses these limitations, allowing model calibration solely
from 2D image data. This approach is demonstrated by generating a digital twin
of all-solid-state battery (ASSB) cathodes. Since the digital twins are
parametric, they support systematic exploration of structural scenarios and
their macroscopic properties. The proposed method facilitates simulation
studies for optimizing 3D morphologies, benefiting not only ASSB cathodes but
also other materials with similar structures.","['stat.ML', 'cs.LG']","['Orkun Furat', 'Sabrina Weber', 'Johannes Schubert', 'René Rekers', 'Maximilian Luczak', 'Erik Glatt', 'Andreas Wiegmann', 'Jürgen Janek', 'Anja Bielefeld', 'Volker Schmidt']",2025-03-21,2025-03-21
2503.17168v1,Hi-ALPS -- An Experimental Robustness Quantification of Six LiDAR-based Object Detection Systems for Autonomous Driving,"Light Detection and Ranging (LiDAR) is an essential sensor technology for
autonomous driving as it can capture high-resolution 3D data. As 3D object
detection systems (OD) can interpret such point cloud data, they play a key
role in the driving decisions of autonomous vehicles. Consequently, such 3D OD
must be robust against all types of perturbations and must therefore be
extensively tested. One approach is the use of adversarial examples, which are
small, sometimes sophisticated perturbations in the input data that change,
i.e., falsify, the prediction of the OD. These perturbations are carefully
designed based on the weaknesses of the OD. The robustness of the OD cannot be
quantified with adversarial examples in general, because if the OD is
vulnerable to a given attack, it is unclear whether this is due to the
robustness of the OD or whether the attack algorithm produces particularly
strong adversarial examples. The contribution of this work is Hi-ALPS --
Hierarchical Adversarial-example-based LiDAR Perturbation Level System, where
higher robustness of the OD is required to withstand the perturbations as the
perturbation levels increase. In doing so, the Hi-ALPS levels successively
implement a heuristic followed by established adversarial example approaches.
In a series of comprehensive experiments using Hi-ALPS, we quantify the
robustness of six state-of-the-art 3D OD under different types of
perturbations. The results of the experiments show that none of the OD is
robust against all Hi-ALPS levels; an important factor for the ranking is that
human observers can still correctly recognize the perturbed objects, as the
respective perturbations are small. To increase the robustness of the OD, we
discuss the applicability of state-of-the-art countermeasures. In addition, we
derive further suggestions for countermeasures based on our experimental
results.","['cs.CV', 'cs.LG']","['Alexandra Arzberger', 'Ramin Tavakoli Kolagari']",2025-03-21,2025-03-21
2503.17167v1,DiTEC-WDN: A Large-Scale Dataset of Water Distribution Network Scenarios under Diverse Hydraulic Conditions,"Privacy restrictions hinder the sharing of real-world Water Distribution
Network (WDN) models, limiting the application of emerging data-driven machine
learning, which typically requires extensive observations. To address this
challenge, we propose the dataset DiTEC-WDN that comprises 36,000 unique
scenarios simulated over either short-term (24 hours) or long-term (1 year)
periods. We constructed this dataset using an automated pipeline that optimizes
crucial parameters (e.g., pressure, flow rate, and demand patterns),
facilitates large-scale simulations, and records discrete, synthetic but
hydraulically realistic states under standard conditions via rule validation
and post-hoc analysis. With a total of 228 million generated graph-based
states, DiTEC-WDN can support a variety of machine-learning tasks, including
graph-level, node-level, and link-level regression, as well as time-series
forecasting. This contribution, released under a public license, encourages
open scientific research in the critical water sector, eliminates the risk of
exposing sensitive data, and fulfills the need for a large-scale water
distribution network benchmark for study comparisons and scenario analysis.","['cs.LG', 'cs.AI']","['Huy Truong', 'Andrés Tello', 'Alexander Lazovik', 'Victoria Degeler']",2025-03-21,2025-03-21
2503.17162v1,CoRLD: Contrastive Representation Learning Of Deformable Shapes In Images,"Deformable shape representations, parameterized by deformations relative to a
given template, have proven effective for improved image analysis tasks.
However, their broader applicability is hindered by two major challenges.
First, existing methods mainly rely on a known template during testing, which
is impractical and limits flexibility. Second, they often struggle to capture
fine-grained, voxel-level distinctions between similar shapes (e.g., anatomical
variations among healthy individuals, those with mild cognitive impairment, and
diseased states). To address these limitations, we propose a novel framework -
Contrastive Representation Learning of Deformable shapes (CoRLD) in learned
deformation spaces and demonstrate its effectiveness in the context of image
classification. Our CoRLD leverages a class-aware contrastive supervised
learning objective in latent deformation spaces, promoting proximity among
representations of similar classes while ensuring separation of dissimilar
groups. In contrast to previous deep learning networks that require a reference
image as input to predict deformation changes, our approach eliminates this
dependency. Instead, template images are utilized solely as ground truth in the
loss function during the training process, making our model more flexible and
generalizable to a wide range of medical applications. We validate CoRLD on
diverse datasets, including real brain magnetic resonance imaging (MRIs) and
adrenal shapes derived from computed tomography (CT) scans. Experimental
results show that our model effectively extracts deformable shape features,
which can be easily integrated with existing classifiers to substantially boost
the classification accuracy. Our code is available at GitHub.",['cs.CV'],['Tonmoy Hossain ana Miaomiao Zhang'],2025-03-21,2025-03-21
2503.17155v1,D2C: Unlocking the Potential of Continuous Autoregressive Image Generation with Discrete Tokens,"In the domain of image generation, latent-based generative models occupy a
dominant status; however, these models rely heavily on image tokenizer. To meet
modeling requirements, autoregressive models possessing the characteristics of
scalability and flexibility embrace a discrete-valued tokenizer, but face the
challenge of poor image generation quality. In contrast, diffusion models take
advantage of the continuous-valued tokenizer to achieve better generation
quality but are subject to low efficiency and complexity. The existing hybrid
models are mainly to compensate for information loss and simplify the diffusion
learning process. The potential of merging discrete-valued and
continuous-valued tokens in the field of image generation has not yet been
explored. In this paper, we propose D2C, a novel two-stage method to enhance
model generation capacity. In the first stage, the discrete-valued tokens
representing coarse-grained image features are sampled by employing a small
discrete-valued generator. Then in the second stage, the continuous-valued
tokens representing fine-grained image features are learned conditioned on the
discrete token sequence. In addition, we design two kinds of fusion modules for
seamless interaction. On the ImageNet-256 benchmark, extensive experiment
results validate that our model achieves superior performance compared with
several continuous-valued and discrete-valued generative models on the
class-conditional image generation tasks.",['cs.CV'],"['Panpan Wang', 'Liqiang Niu', 'Fandong Meng', 'Jinan Xu', 'Yufeng Chen', 'Jie Zhou']",2025-03-21,2025-03-21
2503.17153v1,Enhancing Steering Estimation with Semantic-Aware GNNs,"Steering estimation is a critical task in autonomous driving, traditionally
relying on 2D image-based models. In this work, we explore the advantages of
incorporating 3D spatial information through hybrid architectures that combine
3D neural network models with recurrent neural networks (RNNs) for temporal
modeling, using LiDAR-based point clouds as input. We systematically evaluate
four hybrid 3D models, all of which outperform the 2D-only baseline, with the
Graph Neural Network (GNN) - RNN model yielding the best results.
  To reduce reliance on LiDAR, we leverage a pretrained unified model to
estimate depth from monocular images, reconstructing pseudo-3D point clouds. We
then adapt the GNN-RNN model, originally designed for LiDAR-based point clouds,
to work with these pseudo-3D representations, achieving comparable or even
superior performance compared to the LiDAR-based model. Additionally, the
unified model provides semantic labels for each point, enabling a more
structured scene representation. To further optimize graph construction, we
introduce an efficient connectivity strategy where connections are
predominantly formed between points of the same semantic class, with only 20\%
of inter-class connections retained. This targeted approach reduces graph
complexity and computational cost while preserving critical spatial
relationships.
  Finally, we validate our approach on the KITTI dataset, achieving a 71%
improvement over 2D-only models. Our findings highlight the advantages of 3D
spatial information and efficient graph construction for steering estimation,
while maintaining the cost-effectiveness of monocular images and avoiding the
expense of LiDAR-based systems.",['cs.CV'],"['Fouad Makiyeh', 'Huy-Dung Nguyen', 'Patrick Chareyre', 'Ramin Hasani', 'Marc Blanchon', 'Daniela Rus']",2025-03-21,2025-03-21
2503.17142v1,Not Only Text: Exploring Compositionality of Visual Representations in Vision-Language Models,"Vision-Language Models (VLMs) learn a shared feature space for text and
images, enabling the comparison of inputs of different modalities. While prior
works demonstrated that VLMs organize natural language representations into
regular structures encoding composite meanings, it remains unclear if
compositional patterns also emerge in the visual embedding space. In this work,
we investigate compositionality in the image domain, where the analysis of
compositional properties is challenged by noise and sparsity of visual data. We
address these problems and propose a framework, called Geodesically
Decomposable Embeddings (GDE), that approximates image representations with
geometry-aware compositional structures in the latent space. We demonstrate
that visual embeddings of pre-trained VLMs exhibit a compositional arrangement,
and evaluate the effectiveness of this property in the tasks of compositional
classification and group robustness. GDE achieves stronger performance in
compositional classification compared to its counterpart method that assumes
linear geometry of the latent space. Notably, it is particularly effective for
group robustness, where we achieve higher results than task-specific solutions.
Our results indicate that VLMs can automatically develop a human-like form of
compositional reasoning in the visual domain, making their underlying processes
more interpretable. Code is available at
https://github.com/BerasiDavide/vlm_image_compositionality.","['cs.CV', 'cs.LG']","['Davide Berasi', 'Matteo Farina', 'Massimiliano Mancini', 'Elisa Ricci', 'Nicola Strisciuglio']",2025-03-21,2025-03-21
2503.17141v1,HiFi-Stream: Streaming Speech Enhancement with Generative Adversarial Networks,"Speech Enhancement techniques have become core technologies in mobile devices
and voice software simplifying downstream speech tasks. Still, modern Deep
Learning (DL) solutions often require high amount of computational resources
what makes their usage on low-resource devices challenging. We present
HiFi-Stream, an optimized version of recently published HiFi++ model. Our
experiments demonstrate that HiFiStream saves most of the qualities of the
original model despite its size and computational complexity: the lightest
version has only around 490k parameters which is 3.5x reduction in comparison
to the original HiFi++ making it one of the smallest and fastest models
available. The model is evaluated in streaming setting where it demonstrates
its superior performance in comparison to modern baselines.","['cs.SD', 'cs.LG', 'eess.AS']","['Ekaterina Dmitrieva', 'Maksim Kaledin']",2025-03-21,2025-03-21
2503.17140v1,Adiabatic Fine-Tuning of Neural Quantum States Enables Detection of Phase Transitions in Weight Space,"Neural quantum states (NQS) have emerged as a powerful tool for approximating
quantum wavefunctions using deep learning. While these models achieve
remarkable accuracy, understanding how they encode physical information remains
an open challenge. In this work, we introduce adiabatic fine-tuning, a scheme
that trains NQS across a phase diagram, leading to strongly correlated weight
representations across different models. This correlation in weight space
enables the detection of phase transitions in quantum systems by analyzing the
trained network weights alone. We validate our approach on the transverse field
Ising model and the J1-J2 Heisenberg model, demonstrating that phase
transitions manifest as distinct structures in weight space. Our results
establish a connection between physical phase transitions and the geometry of
neural network parameters, opening new directions for the interpretability of
machine learning models in physics.","['quant-ph', 'cs.LG']","['Vinicius Hernandes', 'Thomas Spriggs', 'Saqar Khaleefah', 'Eliska Greplova']",2025-03-21,2025-03-21
2503.17138v1,Structure Is Not Enough: Leveraging Behavior for Neural Network Weight Reconstruction,"The weights of neural networks (NNs) have recently gained prominence as a new
data modality in machine learning, with applications ranging from accuracy and
hyperparameter prediction to representation learning or weight generation. One
approach to leverage NN weights involves training autoencoders (AEs), using
contrastive and reconstruction losses. This allows such models to be applied to
a wide variety of downstream tasks, and they demonstrate strong predictive
performance and low reconstruction error. However, despite the low
reconstruction error, these AEs reconstruct NN models with deteriorated
performance compared to the original ones, limiting their usability with regard
to model weight generation. In this paper, we identify a limitation of
weight-space AEs, specifically highlighting that a structural loss, that uses
the Euclidean distance between original and reconstructed weights, fails to
capture some features critical for reconstructing high-performing models. We
analyze the addition of a behavioral loss for training AEs in weight space,
where we compare the output of the reconstructed model with that of the
original one, given some common input. We show a strong synergy between
structural and behavioral signals, leading to increased performance in all
downstream tasks evaluated, in particular NN weights reconstruction and
generation.",['cs.LG'],"['Léo Meynent', 'Ivan Melev', 'Konstantin Schürholt', 'Göran Kauermann', 'Damian Borth']",2025-03-21,2025-03-21
2503.17136v1,CoKe: Customizable Fine-Grained Story Evaluation via Chain-of-Keyword Rationalization,"Evaluating creative text such as human-written stories using language models
has always been a challenging task -- owing to the subjectivity of
multi-annotator ratings. To mimic the thinking process of humans, chain of
thought (CoT) generates free-text explanations that help guide a model's
predictions and Self-Consistency (SC) marginalizes predictions over multiple
generated explanations. In this study, we discover that the widely-used
self-consistency reasoning methods cause suboptimal results due to an objective
mismatch between generating 'fluent-looking' explanations vs. actually leading
to a good rating prediction for an aspect of a story. To overcome this
challenge, we propose $\textbf{C}$hain-$\textbf{o}$f-$\textbf{Ke}$ywords
(CoKe), that generates a sequence of keywords $\textit{before}$ generating a
free-text rationale, that guide the rating prediction of our evaluation
language model. Then, we generate a diverse set of such keywords, and aggregate
the scores corresponding to these generations. On the StoryER dataset, CoKe
based on our small fine-tuned evaluation models not only reach human-level
performance and significantly outperform GPT-4 with a 2x boost in correlation
with human annotators, but also requires drastically less number of parameters.",['cs.CL'],"['Brihi Joshi', 'Sriram Venkatapathy', 'Mohit Bansal', 'Nanyun Peng', 'Haw-Shiuan Chang']",2025-03-21,2025-03-21
2503.17132v1,Temporal-Guided Spiking Neural Networks for Event-Based Human Action Recognition,"This paper explores the promising interplay between spiking neural networks
(SNNs) and event-based cameras for privacy-preserving human action recognition
(HAR). The unique feature of event cameras in capturing only the outlines of
motion, combined with SNNs' proficiency in processing spatiotemporal data
through spikes, establishes a highly synergistic compatibility for event-based
HAR. Previous studies, however, have been limited by SNNs' ability to process
long-term temporal information, essential for precise HAR. In this paper, we
introduce two novel frameworks to address this: temporal segment-based SNN
(\textit{TS-SNN}) and 3D convolutional SNN (\textit{3D-SNN}). The
\textit{TS-SNN} extracts long-term temporal information by dividing actions
into shorter segments, while the \textit{3D-SNN} replaces 2D spatial elements
with 3D components to facilitate the transmission of temporal information. To
promote further research in event-based HAR, we create a dataset,
\textit{FallingDetection-CeleX}, collected using the high-resolution CeleX-V
event camera $(1280 \times 800)$, comprising 7 distinct actions. Extensive
experimental results show that our proposed frameworks surpass state-of-the-art
SNN methods on our newly collected dataset and three other neuromorphic
datasets, showcasing their effectiveness in handling long-range temporal
information for event-based HAR.","['cs.CV', 'cs.AI', 'cs.CR', 'cs.NE']","['Siyuan Yang', 'Shilin Lu', 'Shizheng Wang', 'Meng Hwa Er', 'Zengwei Zheng', 'Alex C. Kot']",2025-03-21,2025-03-21
2503.17126v1,Modifying Large Language Model Post-Training for Diverse Creative Writing,"As creative writing tasks do not have singular correct answers, large
language models (LLMs) trained to perform these tasks should be able to
generate diverse valid outputs. However, LLM post-training often focuses on
improving generation quality but neglects to facilitate output diversity.
Hence, in creative writing generation, we investigate post-training approaches
to promote both output diversity and quality. Our core idea is to include
deviation -- the degree of difference between a training sample and all other
samples with the same prompt -- in the training objective to facilitate
learning from rare high-quality instances. By adopting our approach to direct
preference optimization (DPO) and odds ratio preference optimization (ORPO), we
demonstrate that we can promote the output diversity of trained models while
minimally decreasing quality. Our best model with 8B parameters could achieve
on-par diversity as a human-created dataset while having output quality similar
to the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We
further validate our approaches with a human evaluation, an ablation, and a
comparison to an existing diversification approach, DivPO.","['cs.CL', 'cs.LG']","['John Joon Young Chung', 'Vishakh Padmakumar', 'Melissa Roemmele', 'Yuqian Sun', 'Max Kreminski']",2025-03-21,2025-03-21
2503.17125v1,Leveraging Language Models for Out-of-Distribution Recovery in Reinforcement Learning,"Deep Reinforcement Learning (DRL) has demonstrated strong performance in
robotic control but remains susceptible to out-of-distribution (OOD) states,
often resulting in unreliable actions and task failure. While previous methods
have focused on minimizing or preventing OOD occurrences, they largely neglect
recovery once an agent encounters such states. Although the latest research has
attempted to address this by guiding agents back to in-distribution states,
their reliance on uncertainty estimation hinders scalability in complex
environments. To overcome this limitation, we introduce Language Models for
Out-of-Distribution Recovery (LaMOuR), which enables recovery learning without
relying on uncertainty estimation. LaMOuR generates dense reward codes that
guide the agent back to a state where it can successfully perform its original
task, leveraging the capabilities of LVLMs in image description, logical
reasoning, and code generation. Experimental results show that LaMOuR
substantially enhances recovery efficiency across diverse locomotion tasks and
even generalizes effectively to complex environments, including humanoid
locomotion and mobile manipulation, where existing methods struggle. The code
and supplementary materials are available at
\href{https://lamour-rl.github.io/}{https://lamour-rl.github.io/}.","['cs.RO', 'cs.AI']","['Chan Kim', 'Seung-Woo Seo', 'Seong-Woo Kim']",2025-03-21,2025-03-21
2503.17122v1,R-LiViT: A LiDAR-Visual-Thermal Dataset Enabling Vulnerable Road User Focused Roadside Perception,"In autonomous driving, the integration of roadside perception systems is
essential for overcoming occlusion challenges and enhancing the safety of
Vulnerable Road Users (VRUs). While LiDAR and visual (RGB) sensors are commonly
used, thermal imaging remains underrepresented in datasets, despite its
acknowledged advantages for VRU detection in extreme lighting conditions. In
this paper, we present R-LiViT, the first dataset to combine LiDAR, RGB, and
thermal imaging from a roadside perspective, with a strong focus on VRUs.
R-LiViT captures three intersections during both day and night, ensuring a
diverse dataset. It includes 10,000 LiDAR frames and 2,400 temporally and
spatially aligned RGB and thermal images across over 150 traffic scenarios,
with 6 and 8 annotated classes respectively, providing a comprehensive resource
for tasks such as object detection and tracking. The dataset1 and the code for
reproducing our evaluation results2 are made publicly available.",['cs.CV'],"['Jonas Mirlach', 'Lei Wan', 'Andreas Wiedholz', 'Hannan Ejaz Keen', 'Andreas Eich']",2025-03-21,2025-03-21
2503.17117v1,A New Statistical Model of Star Speckles for Learning to Detect and Characterize Exoplanets in Direct Imaging Observations,"The search for exoplanets is an active field in astronomy, with direct
imaging as one of the most challenging methods due to faint exoplanet signals
buried within stronger residual starlight. Successful detection requires
advanced image processing to separate the exoplanet signal from this nuisance
component. This paper presents a novel statistical model that captures nuisance
fluctuations using a multi-scale approach, leveraging problem symmetries and a
joint spectral channel representation grounded in physical principles. Our
model integrates into an interpretable, end-to-end learnable framework for
simultaneous exoplanet detection and flux estimation. The proposed algorithm is
evaluated against the state of the art using datasets from the SPHERE
instrument operating at the Very Large Telescope (VLT). It significantly
improves the precision-recall trade-off, notably on challenging datasets that
are otherwise unusable by astronomers. The proposed approach is computationally
efficient, robust to varying data quality, and well suited for large-scale
observational surveys.","['astro-ph.IM', 'astro-ph.EP', 'cs.CV', 'cs.LG', 'stat.AP']","['Théo Bodrito', 'Olivier Flasseur', 'Julien Mairal', 'Jean Ponce', 'Maud Langlois', 'Anne-Marie Lagrange']",2025-03-21,2025-03-21
2503.17116v1,The CASTLE 2024 Dataset: Advancing the Art of Multimodal Understanding,"Egocentric video has seen increased interest in recent years, as it is used
in a range of areas. However, most existing datasets are limited to a single
perspective. In this paper, we present the CASTLE 2024 dataset, a multimodal
collection containing ego- and exo-centric (i.e., first- and third-person
perspective) video and audio from 15 time-aligned sources, as well as other
sensor streams and auxiliary data. The dataset was recorded by volunteer
participants over four days in a fixed location and includes the point of view
of 10 participants, with an additional 5 fixed cameras providing an exocentric
perspective. The entire dataset contains over 600 hours of UHD video recorded
at 50 frames per second. In contrast to other datasets, CASTLE 2024 does not
contain any partial censoring, such as blurred faces or distorted audio. The
dataset is available via https://castle-dataset.github.io/.","['cs.MM', 'cs.AI', 'cs.CV', 'cs.IR']","['Luca Rossetto', 'Werner Bailer', 'Duc-Tien Dang-Nguyen', 'Graham Healy', 'Björn Þór Jónsson', 'Onanong Kongmeesub', 'Hoang-Bao Le', 'Stevan Rudinac', 'Klaus Schöffmann', 'Florian Spiess', 'Allie Tran', 'Minh-Triet Tran', 'Quang-Linh Tran', 'Cathal Gurrin']",2025-03-21,2025-03-21
2503.17110v1,Beyond Accuracy: What Matters in Designing Well-Behaved Models?,"Deep learning has become an essential part of computer vision, with deep
neural networks (DNNs) excelling in predictive performance. However, they often
fall short in other critical quality dimensions, such as robustness,
calibration, or fairness. While existing studies have focused on a subset of
these quality dimensions, none have explored a more general form of
""well-behavedness"" of DNNs. With this work, we address this gap by
simultaneously studying nine different quality dimensions for image
classification. Through a large-scale study, we provide a bird's-eye view by
analyzing 326 backbone models and how different training paradigms and model
architectures affect the quality dimensions. We reveal various new insights
such that (i) vision-language models exhibit high fairness on ImageNet-1k
classification and strong robustness against domain changes; (ii)
self-supervised learning is an effective training paradigm to improve almost
all considered quality dimensions; and (iii) the training dataset size is a
major driver for most of the quality dimensions. We conclude our study by
introducing the QUBA score (Quality Understanding Beyond Accuracy), a novel
metric that ranks models across multiple dimensions of quality, enabling
tailored recommendations based on specific user needs.","['cs.CV', 'cs.LG']","['Robin Hesse', 'Doğukan Bağcı', 'Bernt Schiele', 'Simone Schaub-Meyer', 'Stefan Roth']",2025-03-21,2025-03-21
2503.17109v1,Missing Target-Relevant Information Prediction with World Model for Accurate Zero-Shot Composed Image Retrieval,"Zero-Shot Composed Image Retrieval (ZS-CIR) involves diverse tasks with a
broad range of visual content manipulation intent across domain, scene, object,
and attribute. The key challenge for ZS-CIR tasks is to modify a reference
image according to manipulation text to accurately retrieve a target image,
especially when the reference image is missing essential target content. In
this paper, we propose a novel prediction-based mapping network, named
PrediCIR, to adaptively predict the missing target visual content in reference
images in the latent space before mapping for accurate ZS-CIR. Specifically, a
world view generation module first constructs a source view by omitting certain
visual content of a target view, coupled with an action that includes the
manipulation intent derived from existing image-caption pairs. Then, a target
content prediction module trains a world model as a predictor to adaptively
predict the missing visual information guided by user intention in manipulating
text at the latent space. The two modules map an image with the predicted
relevant information to a pseudo-word token without extra supervision. Our
model shows strong generalization ability on six ZS-CIR tasks. It obtains
consistent and significant performance boosts ranging from 1.73% to 4.45% over
the best methods and achieves new state-of-the-art results on ZS-CIR. Our code
is available at https://github.com/Pter61/predicir.",['cs.CV'],"['Yuanmin Tang', 'Jing Yu', 'Keke Gai', 'Jiamin Zhuang', 'Gang Xiong', 'Gaopeng Gou', 'Qi Wu']",2025-03-21,2025-03-21
2503.17107v1,Exploring Few-Shot Object Detection on Blood Smear Images: A Case Study of Leukocytes and Schistocytes,"The detection of blood disorders often hinges upon the quantification of
specific blood cell types. Variations in cell counts may indicate the presence
of pathological conditions. Thus, the significance of developing precise
automatic systems for blood cell enumeration is underscored. The investigation
focuses on a novel approach termed DE-ViT. This methodology is employed in a
Few-Shot paradigm, wherein training relies on a limited number of images. Two
distinct datasets are utilised for experimental purposes: the Raabin-WBC
dataset for Leukocyte detection and a local dataset for Schistocyte
identification. In addition to the DE-ViT model, two baseline models, Faster
R-CNN 50 and Faster R-CNN X 101, are employed, with their outcomes being
compared against those of the proposed model. While DE-ViT has demonstrated
state-of-the-art performance on the COCO and LVIS datasets, both baseline
models surpassed its performance on the Raabin-WBC dataset. Moreover, only
Faster R-CNN X 101 yielded satisfactory results on the SC-IDB. The observed
disparities in performance may possibly be attributed to domain shift
phenomena.","['eess.IV', 'cs.CV']","['Davide Antonio Mura', 'Michela Pinna', 'Lorenzo Putzu', 'Andrea Loddo', 'Alessandra Perniciano', 'Olga Mulas', 'Cecilia Di Ruberto']",2025-03-21,2025-03-21
2503.17106v1,GAA-TSO: Geometry-Aware Assisted Depth Completion for Transparent and Specular Objects,"Transparent and specular objects are frequently encountered in daily life,
factories, and laboratories. However, due to the unique optical properties, the
depth information on these objects is usually incomplete and inaccurate, which
poses significant challenges for downstream robotics tasks. Therefore, it is
crucial to accurately restore the depth information of transparent and specular
objects. Previous depth completion methods for these objects usually use RGB
information as an additional channel of the depth image to perform depth
prediction. Due to the poor-texture characteristics of transparent and specular
objects, these methods that rely heavily on color information tend to generate
structure-less depth predictions. Moreover, these 2D methods cannot effectively
explore the 3D structure hidden in the depth channel, resulting in depth
ambiguity. To this end, we propose a geometry-aware assisted depth completion
method for transparent and specular objects, which focuses on exploring the 3D
structural cues of the scene. Specifically, besides extracting 2D features from
RGB-D input, we back-project the input depth to a point cloud and build the 3D
branch to extract hierarchical scene-level 3D structural features. To exploit
3D geometric information, we design several gated cross-modal fusion modules to
effectively propagate multi-level 3D geometric features to the image branch. In
addition, we propose an adaptive correlation aggregation strategy to
appropriately assign 3D features to the corresponding 2D features. Extensive
experiments on ClearGrasp, OOD, TransCG, and STD datasets show that our method
outperforms other state-of-the-art methods. We further demonstrate that our
method significantly enhances the performance of downstream robotic grasping
tasks.","['cs.CV', 'cs.RO']","['Yizhe Liu', 'Tong Jia', 'Da Cai', 'Hao Wang', 'Dongyue Chen']",2025-03-21,2025-03-21
2503.17105v1,A Comparative Analysis of Image Descriptors for Histopathological Classification of Gastric Cancer,"Gastric cancer ranks as the fifth most common and fourth most lethal cancer
globally, with a dismal 5-year survival rate of approximately 20%. Despite
extensive research on its pathobiology, the prognostic predictability remains
inadequate, compounded by pathologists' high workload and potential diagnostic
errors. Thus, automated, accurate histopathological diagnosis tools are
crucial. This study employs Machine Learning and Deep Learning techniques to
classify histopathological images into healthy and cancerous categories. Using
handcrafted and deep features with shallow learning classifiers on the
GasHisSDB dataset, we offer a comparative analysis and insights into the most
robust and high-performing combinations of features and classifiers for
distinguishing between normal and abnormal histopathological images without
fine-tuning strategies. With the RF classifier, our approach can reach F1 of
93.4%, demonstrating its validity.","['eess.IV', 'cs.CV']","['Marco Usai', 'Andrea Loddo', 'Alessandra Perniciano', 'Maurizio Atzori', 'Cecilia Di Ruberto']",2025-03-21,2025-03-21
2503.17101v1,Large Language Model Compression via the Nested Activation-Aware Decomposition,"In this paper, we tackle the critical challenge of compressing large language
models (LLMs) to facilitate their practical deployment and broader adoption. We
introduce a novel post-training compression paradigm that focuses on low-rank
decomposition of LLM weights. Our analysis identifies two main challenges in
this task: the variability in LLM activation distributions and handling unseen
activations from different datasets and models.
  To address these challenges, we propose a nested activation-aware framework
(NSVD) for LLMs, a training-free approach designed to enhance the accuracy of
low-rank decompositions by managing activation outliers through transforming
the weight matrix based on activation distribution and the original weight
matrix. This method allows for the absorption of outliers into the transformed
weight matrix, improving decomposition accuracy. Our comprehensive evaluation
across eight datasets and six models from three distinct LLM families
demonstrates the superiority of NSVD over current state-of-the-art methods,
especially at medium to large compression ratios or in multilingual and
multitask settings.",['cs.LG'],"['Jun Lu', 'Tianyi Xu', 'Bill Ding', 'David Li', 'Yu Kang']",2025-03-21,2025-03-21
2503.17097v1,R2LDM: An Efficient 4D Radar Super-Resolution Framework Leveraging Diffusion Model,"We introduce R2LDM, an innovative approach for generating dense and accurate
4D radar point clouds, guided by corresponding LiDAR point clouds. Instead of
utilizing range images or bird's eye view (BEV) images, we represent both LiDAR
and 4D radar point clouds using voxel features, which more effectively capture
3D shape information. Subsequently, we propose the Latent Voxel Diffusion Model
(LVDM), which performs the diffusion process in the latent space. Additionally,
a novel Latent Point Cloud Reconstruction (LPCR) module is utilized to
reconstruct point clouds from high-dimensional latent voxel features. As a
result, R2LDM effectively generates LiDAR-like point clouds from paired raw
radar data. We evaluate our approach on two different datasets, and the
experimental results demonstrate that our model achieves 6- to 10-fold
densification of radar point clouds, outperforming state-of-the-art baselines
in 4D radar point cloud super-resolution. Furthermore, the enhanced radar point
clouds generated by our method significantly improve downstream tasks,
achieving up to 31.7% improvement in point cloud registration recall rate and
24.9% improvement in object detection accuracy.",['cs.CV'],"['Boyuan Zheng', 'Shouyi Lu', 'Renbo Huang', 'Minqing Huang', 'Fan Lu', 'Wei Tian', 'Guirong Zhuo', 'Lu Xiong']",2025-03-21,2025-03-21
2503.17096v1,Multi-modal Multi-platform Person Re-Identification: Benchmark and Method,"Conventional person re-identification (ReID) research is often limited to
single-modality sensor data from static cameras, which fails to address the
complexities of real-world scenarios where multi-modal signals are increasingly
prevalent. For instance, consider an urban ReID system integrating stationary
RGB cameras, nighttime infrared sensors, and UAVs equipped with dynamic
tracking capabilities. Such systems face significant challenges due to
variations in camera perspectives, lighting conditions, and sensor modalities,
hindering effective person ReID. To address these challenges, we introduce the
MP-ReID benchmark, a novel dataset designed specifically for multi-modality and
multi-platform ReID. This benchmark uniquely compiles data from 1,930
identities across diverse modalities, including RGB, infrared, and thermal
imaging, captured by both UAVs and ground-based cameras in indoor and outdoor
environments. Building on this benchmark, we introduce Uni-Prompt ReID, a
framework with specific-designed prompts, tailored for cross-modality and
cross-platform scenarios. Our method consistently outperforms state-of-the-art
approaches, establishing a robust foundation for future research in complex and
dynamic ReID environments. Our dataset are available
at:https://mp-reid.github.io/.",['cs.CV'],"['Ruiyang Ha', 'Songyi Jiang', 'Bin Li', 'Bikang Pan', 'Yihang Zhu', 'Junjie Zhang', 'Xiatian Zhu', 'Shaogang Gong', 'Jingya Wang']",2025-03-21,2025-03-21
2503.17095v1,FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields,"Recent 3D face editing methods using masks have produced high-quality edited
images by leveraging Neural Radiance Fields (NeRF). Despite their impressive
performance, existing methods often provide limited user control due to the use
of pre-trained segmentation masks. To utilize masks with a desired layout, an
extensive training dataset is required, which is challenging to gather. We
present FFaceNeRF, a NeRF-based face editing technique that can overcome the
challenge of limited user control due to the use of fixed mask layouts. Our
method employs a geometry adapter with feature injection, allowing for
effective manipulation of geometry attributes. Additionally, we adopt latent
mixing for tri-plane augmentation, which enables training with a few samples.
This facilitates rapid model adaptation to desired mask layouts, crucial for
applications in fields like personalized medical imaging or creative face
editing. Our comparative evaluations demonstrate that FFaceNeRF surpasses
existing mask based face editing methods in terms of flexibility, control, and
generated image quality, paving the way for future advancements in customized
and high-fidelity 3D face editing. The code is available on the
{\href{https://kwanyun.github.io/FFaceNeRF_page/}{project-page}}.","['cs.GR', 'cs.AI', 'cs.CV', '68T45, 68U05', 'I.3.3; I.3.8']","['Kwan Yun', 'Chaelin Kim', 'Hangyeul Shin', 'Junyong Noh']",2025-03-21,2025-03-21
2503.17093v1,ColabSfM: Collaborative Structure-from-Motion by Point Cloud Registration,"Structure-from-Motion (SfM) is the task of estimating 3D structure and camera
poses from images. We define Collaborative SfM (ColabSfM) as sharing
distributed SfM reconstructions. Sharing maps requires estimating a joint
reference frame, which is typically referred to as registration. However, there
is a lack of scalable methods and training datasets for registering SfM
reconstructions. In this paper, we tackle this challenge by proposing the
scalable task of point cloud registration for SfM reconstructions. We find that
current registration methods cannot register SfM point clouds when trained on
existing datasets. To this end, we propose a SfM registration dataset
generation pipeline, leveraging partial reconstructions from synthetically
generated camera trajectories for each scene. Finally, we propose a simple but
impactful neural refiner on top of the SotA registration method RoITr that
yields significant improvements, which we call RefineRoITr. Our extensive
experimental evaluation shows that our proposed pipeline and model enables
ColabSfM. Code is available at https://github.com/EricssonResearch/ColabSfM",['cs.CV'],"['Johan Edstedt', 'André Mateus', 'Alberto Jaenal']",2025-03-21,2025-03-21
2503.17089v1,Does a Rising Tide Lift All Boats? Bias Mitigation for AI-based CMR Segmentation,"Artificial intelligence (AI) is increasingly being used for medical imaging
tasks. However, there can be biases in the resulting models, particularly when
they were trained using imbalanced training datasets. One such example has been
the strong race bias effect in cardiac magnetic resonance (CMR) image
segmentation models. Although this phenomenon has been reported in a number of
publications, little is known about the effectiveness of bias mitigation
algorithms in this domain. We aim to investigate the impact of common bias
mitigation methods to address bias between Black and White subjects in AI-based
CMR segmentation models. Specifically, we use oversampling, importance
reweighing and Group DRO as well as combinations of these techniques to
mitigate the race bias. Furthermore, motivated by recent findings on the root
causes of AI-based CMR segmentation bias, we evaluate the same methods using
models trained and evaluated on cropped CMR images. We find that bias can be
mitigated using oversampling, significantly improving performance for the
underrepresented Black subjects whilst not significantly reducing the majority
White subjects' performance. Group DRO also improves performance for Black
subjects but not significantly, while reweighing decreases performance for
Black subjects. Using a combination of oversampling and Group DRO also improves
performance for Black subjects but not significantly. Using cropped images
increases performance for both races and reduces the bias, whilst adding
oversampling as a bias mitigation technique with cropped images reduces the
bias further.","['eess.IV', 'cs.AI', 'cs.CV']","['Tiarna Lee', 'Esther Puyol-Antón', 'Bram Ruijsink', 'Miaojing Shi', 'Andrew P. King']",2025-03-21,2025-03-21
2503.17085v1,Deterministic AI Agent Personality Expression through Standard Psychological Diagnostics,"Artificial intelligence (AI) systems powered by large language models have
become increasingly prevalent in modern society, enabling a wide range of
applications through natural language interaction. As AI agents proliferate in
our daily lives, their generic and uniform expressiveness presents a
significant limitation to their appeal and adoption. Personality expression
represents a key prerequisite for creating more human-like and distinctive AI
systems. We show that AI models can express deterministic and consistent
personalities when instructed using established psychological frameworks, with
varying degrees of accuracy depending on model capabilities. We find that more
advanced models like GPT-4o and o1 demonstrate the highest accuracy in
expressing specified personalities across both Big Five and Myers-Briggs
assessments, and further analysis suggests that personality expression emerges
from a combination of intelligence and reasoning capabilities. Our results
reveal that personality expression operates through holistic reasoning rather
than question-by-question optimization, with response-scale metrics showing
higher variance than test-scale metrics. Furthermore, we find that model
fine-tuning affects communication style independently of personality expression
accuracy. These findings establish a foundation for creating AI agents with
diverse and consistent personalities, which could significantly enhance
human-AI interaction across applications from education to healthcare, while
additionally enabling a broader range of more unique AI agents. The ability to
quantitatively assess and implement personality expression in AI systems opens
new avenues for research into more relatable, trustworthy, and ethically
designed AI.","['cs.LG', 'cs.AI', 'cs.CY', 'cs.HC']","['J. M. Diederik Kruijssen', 'Nicholas Emmons']",2025-03-21,2025-03-21
2503.17080v1,Seeing What Matters: Empowering CLIP with Patch Generation-to-Selection,"The CLIP model has demonstrated significant advancements in aligning visual
and language modalities through large-scale pre-training on image-text pairs,
enabling strong zero-shot classification and retrieval capabilities on various
domains. However, CLIP's training remains computationally intensive, with high
demands on both data processing and memory. To address these challenges, recent
masking strategies have emerged, focusing on the selective removal of image
patches to improve training efficiency. Although effective, these methods often
compromise key semantic information, resulting in suboptimal alignment between
visual features and text descriptions. In this work, we present a concise yet
effective approach called Patch Generation-to-Selection to enhance CLIP's
training efficiency while preserving critical semantic content. Our method
introduces a gradual masking process in which a small set of candidate patches
is first pre-selected as potential mask regions. Then, we apply Sobel edge
detection across the entire image to generate an edge mask that prioritizes the
retention of the primary object areas. Finally, similarity scores between the
candidate mask patches and their neighboring patches are computed, with optimal
transport normalization refining the selection process to ensure a balanced
similarity matrix. Our approach, CLIP-PGS, sets new state-of-the-art results in
zero-shot classification and retrieval tasks, achieving superior performance in
robustness evaluation and language compositionality benchmarks.",['cs.CV'],"['Gensheng Pei', 'Tao Chen', 'Yujia Wang', 'Xinhao Cai', 'Xiangbo Shu', 'Tianfei Zhou', 'Yazhou Yao']",2025-03-21,2025-03-21
2503.17076v1,Halton Scheduler For Masked Generative Image Transformer,"Masked Generative Image Transformers (MaskGIT) have emerged as a scalable and
efficient image generation framework, able to deliver high-quality visuals with
low inference costs. However, MaskGIT's token unmasking scheduler, an essential
component of the framework, has not received the attention it deserves. We
analyze the sampling objective in MaskGIT, based on the mutual information
between tokens, and elucidate its shortcomings. We then propose a new sampling
strategy based on our Halton scheduler instead of the original Confidence
scheduler. More precisely, our method selects the token's position according to
a quasi-random, low-discrepancy Halton sequence. Intuitively, that method
spreads the tokens spatially, progressively covering the image uniformly at
each step. Our analysis shows that it allows reducing non-recoverable sampling
errors, leading to simpler hyper-parameters tuning and better quality images.
Our scheduler does not require retraining or noise injection and may serve as a
simple drop-in replacement for the original sampling strategy. Evaluation of
both class-to-image synthesis on ImageNet and text-to-image generation on the
COCO dataset demonstrates that the Halton scheduler outperforms the Confidence
scheduler quantitatively by reducing the FID and qualitatively by generating
more diverse and more detailed images. Our code is at
https://github.com/valeoai/Halton-MaskGIT.",['cs.CV'],"['Victor Besnier', 'Mickael Chen', 'David Hurych', 'Eduardo Valle', 'Matthieu Cord']",2025-03-21,2025-03-21
2503.17074v1,"Zero-Shot Styled Text Image Generation, but Make It Autoregressive","Styled Handwritten Text Generation (HTG) has recently received attention from
the computer vision and document analysis communities, which have developed
several solutions, either GAN- or diffusion-based, that achieved promising
results. Nonetheless, these strategies fail to generalize to novel styles and
have technical constraints, particularly in terms of maximum output length and
training efficiency. To overcome these limitations, in this work, we propose a
novel framework for text image generation, dubbed Emuru. Our approach leverages
a powerful text image representation model (a variational autoencoder) combined
with an autoregressive Transformer. Our approach enables the generation of
styled text images conditioned on textual content and style examples, such as
specific fonts or handwriting styles. We train our model solely on a diverse,
synthetic dataset of English text rendered in over 100,000 typewritten and
calligraphy fonts, which gives it the capability to reproduce unseen styles
(both fonts and users' handwriting) in zero-shot. To the best of our knowledge,
Emuru is the first autoregressive model for HTG, and the first designed
specifically for generalization to novel styles. Moreover, our model generates
images without background artifacts, which are easier to use for downstream
applications. Extensive evaluation on both typewritten and handwritten,
any-length text image generation scenarios demonstrates the effectiveness of
our approach.",['cs.CV'],"['Vittorio Pippi', 'Fabio Quattrini', 'Silvia Cascianelli', 'Alessio Tonioni', 'Rita Cucchiara']",2025-03-21,2025-03-21
2503.17073v1,A Study into Investigating Temporal Robustness of LLMs,"Large Language Models (LLMs) encapsulate a surprising amount of factual world
knowledge. However, their performance on temporal questions and historical
knowledge is limited because they often cannot understand temporal scope and
orientation or neglect the temporal aspect altogether. In this study, we aim to
measure precisely how robust LLMs are for question answering based on their
ability to process temporal information and perform tasks requiring temporal
reasoning and temporal factual knowledge. Specifically, we design eight
time-sensitive robustness tests for factual information to check the
sensitivity of six popular LLMs in the zero-shot setting. Overall, we find LLMs
lacking temporal robustness, especially to temporal reformulations and the use
of different granularities of temporal references. We show how a selection of
these eight tests can be used automatically to judge a model's temporal
robustness for user questions on the fly. Finally, we apply the findings of
this study to improve the temporal QA performance by up to 55 percent.","['cs.CL', 'cs.IR', '68T50', 'I.2.7']","['Jonas Wallat', 'Abdelrahman Abdallah', 'Adam Jatowt', 'Avishek Anand']",2025-03-21,2025-03-21
2503.17072v1,Multi-Span Optical Power Spectrum Evolution Modeling using ML-based Multi-Decoder Attention Framework,"We implement a ML-based attention framework with component-specific decoders,
improving optical power spectrum prediction in multi-span networks. By reducing
the need for in-depth training on each component, the framework can be scaled
to multi-span topologies with minimal data collection, making it suitable for
brown-field scenarios.","['cs.LG', 'cs.NI']","['Agastya Raj', 'Zehao Wang', 'Frank Slyne', 'Tingjun Chen', 'Dan Kilper', 'Marco Ruffini']",2025-03-21,2025-03-21
2503.17071v1,Superpowering Open-Vocabulary Object Detectors for X-ray Vision,"Open-vocabulary object detection (OvOD) is set to revolutionize security
screening by enabling systems to recognize any item in X-ray scans. However,
developing effective OvOD models for X-ray imaging presents unique challenges
due to data scarcity and the modality gap that prevents direct adoption of
RGB-based solutions. To overcome these limitations, we propose RAXO, a
training-free framework that repurposes off-the-shelf RGB OvOD detectors for
robust X-ray detection. RAXO builds high-quality X-ray class descriptors using
a dual-source retrieval strategy. It gathers relevant RGB images from the web
and enriches them via a novel X-ray material transfer mechanism, eliminating
the need for labeled databases. These visual descriptors replace text-based
classification in OvOD, leveraging intra-modal feature distances for robust
detection. Extensive experiments demonstrate that RAXO consistently improves
OvOD performance, providing an average mAP increase of up to 17.0 points over
base detectors. To further support research in this emerging field, we also
introduce DET-COMPASS, a new benchmark featuring bounding box annotations for
over 300 object categories, enabling large-scale evaluation of OvOD in X-ray.
Code and dataset available at: https://github.com/PAGF188/RAXO.",['cs.CV'],"['Pablo Garcia-Fernandez', 'Lorenzo Vaquero', 'Mingxuan Liu', 'Feng Xue', 'Daniel Cores', 'Nicu Sebe', 'Manuel Mucientes', 'Elisa Ricci']",2025-03-21,2025-03-21
2503.17070v1,A Thorough Assessment of the Non-IID Data Impact in Federated Learning,"Federated learning (FL) allows collaborative machine learning (ML) model
training among decentralized clients' information, ensuring data privacy. The
decentralized nature of FL deals with non-independent and identically
distributed (non-IID) data. This open problem has notable consequences, such as
decreased model performance and more significant convergence times. Despite its
importance, experimental studies systematically addressing all types of data
heterogeneity (a.k.a. non-IIDness) remain scarce. We aim to fill this gap by
assessing and quantifying the non-IID effect through a thorough empirical
analysis. We use the Hellinger Distance (HD) to measure differences in
distribution among clients. Our study benchmarks four state-of-the-art
strategies for handling non-IID data, including label, feature, quantity, and
spatiotemporal skewness, under realistic and controlled conditions. This is the
first comprehensive analysis of the spatiotemporal skew effect in FL. Our
findings highlight the significant impact of label and spatiotemporal skew
non-IID types on FL model performance, with notable performance drops occurring
at specific HD thresholds. Additionally, the FL performance is heavily affected
mainly when the non-IIDness is extreme. Thus, we provide recommendations for FL
research to tackle data heterogeneity effectively. Our work represents the most
extensive examination of non-IIDness in FL, offering a robust foundation for
future research.","['cs.LG', 'cs.AI', 'stat.ML']","['Daniel M. Jimenez-Gutierrez', 'Mehrdad Hassanzadeh', 'Aris Anagnostopoulos', 'Ioannis Chatzigiannakis', 'Andrea Vitaletti']",2025-03-21,2025-03-21
2503.17069v1,PVChat: Personalized Video Chat with One-Shot Learning,"Video large language models (ViLLMs) excel in general video understanding,
e.g., recognizing activities like talking and eating, but struggle with
identity-aware comprehension, such as ""Wilson is receiving chemotherapy"" or
""Tom is discussing with Sarah"", limiting their applicability in smart
healthcare and smart home environments. To address this limitation, we propose
a one-shot learning framework PVChat, the first personalized ViLLM that enables
subject-aware question answering (QA) from a single video for each subject. Our
approach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically
augmented video-QA dataset, leveraging a progressive image-to-video learning
strategy. Specifically, we introduce an automated augmentation pipeline that
synthesizes identity-preserving positive samples and retrieves hard negatives
from existing video corpora, generating a diverse training dataset with four QA
types: existence, appearance, action, and location inquiries. To enhance
subject-specific learning, we propose a ReLU Routing MoH attention mechanism,
alongside two novel objectives: (1) Smooth Proximity Regularization for
progressive learning through exponential distance scaling and (2) Head
Activation Enhancement for balanced attention routing. Finally, we adopt a
two-stage training strategy, transitioning from image pre-training to video
fine-tuning, enabling a gradual learning process from static attributes to
dynamic representations. We evaluate PVChat on diverse datasets covering
medical scenarios, TV series, anime, and real-world footage, demonstrating its
superiority in personalized feature understanding after learning from a single
video, compared to state-of-the-art ViLLMs.","['cs.CV', 'cs.AI']","['Yufei Shi', 'Weilong Yan', 'Gang Xu', 'Yumeng Li', 'Yuchen Li', 'Zhenxi Li', 'Fei Richard Yu', 'Ming Li', 'Si Yong Yeo']",2025-03-21,2025-03-21
2503.17061v1,Replay4NCL: An Efficient Memory Replay-based Methodology for Neuromorphic Continual Learning in Embedded AI Systems,"Neuromorphic Continual Learning (NCL) paradigm leverages Spiking Neural
Networks (SNNs) to enable continual learning (CL) capabilities for AI systems
to adapt to dynamically changing environments. Currently, the state-of-the-art
employ a memory replay-based method to maintain the old knowledge. However,
this technique relies on long timesteps and compression-decompression steps,
thereby incurring significant latency and energy overheads, which are not
suitable for tightly-constrained embedded AI systems (e.g., mobile
agents/robotics). To address this, we propose Replay4NCL, a novel efficient
memory replay-based methodology for enabling NCL in embedded AI systems.
Specifically, Replay4NCL compresses the latent data (old knowledge), then
replays them during the NCL training phase with small timesteps, to minimize
the processing latency and energy consumption. To compensate the information
loss from reduced spikes, we adjust the neuron threshold potential and learning
rate settings. Experimental results on the class-incremental scenario with the
Spiking Heidelberg Digits (SHD) dataset show that Replay4NCL can preserve old
knowledge with Top-1 accuracy of 90.43% compared to 86.22% from the
state-of-the-art, while effectively learning new tasks, achieving 4.88x latency
speed-up, 20% latent memory saving, and 36.43% energy saving. These results
highlight the potential of our Replay4NCL methodology to further advances NCL
capabilities for embedded AI systems.","['cs.NE', 'cs.AI', 'cs.LG']","['Mishal Fatima Minhas', 'Rachmad Vidya Wicaksana Putra', 'Falah Awwad', 'Osman Hasan', 'Muhammad Shafique']",2025-03-21,2025-03-21
2503.17059v1,DIDiffGes: Decoupled Semi-Implicit Diffusion Models for Real-time Gesture Generation from Speech,"Diffusion models have demonstrated remarkable synthesis quality and diversity
in generating co-speech gestures. However, the computationally intensive
sampling steps associated with diffusion models hinder their practicality in
real-world applications. Hence, we present DIDiffGes, for a Decoupled
Semi-Implicit Diffusion model-based framework, that can synthesize
high-quality, expressive gestures from speech using only a few sampling steps.
Our approach leverages Generative Adversarial Networks (GANs) to enable
large-step sampling for diffusion model. We decouple gesture data into body and
hands distributions and further decompose them into marginal and conditional
distributions. GANs model the marginal distribution implicitly, while L2
reconstruction loss learns the conditional distributions exciplictly. This
strategy enhances GAN training stability and ensures expressiveness of
generated full-body gestures. Our framework also learns to denoise root noise
conditioned on local body representation, guaranteeing stability and realism.
DIDiffGes can generate gestures from speech with just 10 sampling steps,
without compromising quality and expressiveness, reducing the number of
sampling steps by a factor of 100 compared to existing methods. Our user study
reveals that our method outperforms state-of-the-art approaches in human
likeness, appropriateness, and style correctness. Project is
https://cyk990422.github.io/DIDiffGes.","['cs.GR', 'cs.CV', 'cs.SD']","['Yongkang Cheng', 'Shaoli Huang', 'Xuelin Chen', 'Jifeng Ning', 'Mingming Gong']",2025-03-21,2025-03-21
2503.17057v1,Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks,"Accurate segmentation of ultrasound (US) images of the cervical muscles is
crucial for precision healthcare. The demand for automatic computer-assisted
methods is high. However, the scarcity of labeled data hinders the development
of these methods. Advanced semi-supervised learning approaches have displayed
promise in overcoming this challenge by utilizing labeled and unlabeled data.
This study introduces a novel semi-supervised learning (SSL) framework that
integrates dual neural networks. This SSL framework utilizes both networks to
generate pseudo-labels and cross-supervise each other at the pixel level.
Additionally, a self-supervised contrastive learning strategy is introduced,
which employs a pair of deep representations to enhance feature learning
capabilities, particularly on unlabeled data. Our framework demonstrates
competitive performance in cervical segmentation tasks. Our codes are publicly
available on https://github.com/13204942/SSL\_Cervical\_Segmentation.","['eess.IV', 'cs.CV']","['Fangyijie Wang', 'Kathleen M. Curran', 'Guénolé Silvestre']",2025-03-21,2025-03-21
2503.17055v1,Data-Driven Optimization of EV Charging Station Placement Using Causal Discovery,"This paper addresses the critical challenge of optimizing electric vehicle
charging station placement through a novel data-driven methodology employing
causal discovery techniques. While traditional approaches prioritize economic
factors or power grid constraints, they often neglect empirical charging
patterns that ultimately determine station utilization. We analyze extensive
charging data from Palo Alto and Boulder (337,344 events across 100 stations)
to uncover latent relationships between station characteristics and
utilization. Applying structural learning algorithms (NOTEARS and DAGMA) to
this data reveals that charging demand is primarily determined by three
factors: proximity to amenities, EV registration density, and adjacency to
high-traffic routes. These findings, consistent across multiple algorithms and
urban contexts, challenge conventional infrastructure distribution strategies.
We develop an optimization framework that translates these insights into
actionable placement recommendations, identifying locations likely to
experience high utilization based on the discovered dependency structures. The
resulting site selection model prioritizes strategic clustering in high-amenity
areas with substantial EV populations rather than uniform spatial distribution.
Our approach contributes a framework that integrates empirical charging
behavior into infrastructure planning, potentially enhancing both station
utilization and user convenience. By focusing on data-driven insights instead
of theoretical distribution models, we provide a more effective strategy for
expanding charging networks that can adjust to various stages of EV market
development.","['cs.LG', 'cs.AI']","['Julius Stephan Junker', 'Rong Hu', 'Ziyue Li', 'Wolfgang Ketter']",2025-03-21,2025-03-21
2503.17050v1,"Scoring, Remember, and Reference: Catching Camouflaged Objects in Videos","Video Camouflaged Object Detection (VCOD) aims to segment objects whose
appearances closely resemble their surroundings, posing a challenging and
emerging task. Existing vision models often struggle in such scenarios due to
the indistinguishable appearance of camouflaged objects and the insufficient
exploitation of dynamic information in videos. To address these challenges, we
propose an end-to-end VCOD framework inspired by human memory-recognition,
which leverages historical video information by integrating memory reference
frames for camouflaged sequence processing. Specifically, we design a
dual-purpose decoder that simultaneously generates predicted masks and scores,
enabling reference frame selection based on scores while introducing auxiliary
supervision to enhance feature extraction.Furthermore, this study introduces a
novel reference-guided multilevel asymmetric attention mechanism, effectively
integrating long-term reference information with short-term motion cues for
comprehensive feature extraction. By combining these modules, we develop the
Scoring, Remember, and Reference (SRR) framework, which efficiently extracts
information to locate targets and employs memory guidance to improve subsequent
processing. With its optimized module design and effective utilization of video
data, our model achieves significant performance improvements, surpassing
existing approaches by 10% on benchmark datasets while requiring fewer
parameters (54M) and only a single pass through the video. The code will be
made publicly available.",['cs.CV'],"['Yuang Feng', 'Shuyong Gao', 'Fuzhen Yan', 'Yicheng Song', 'Lingyi Hong', 'Junjie Hu', 'Wenqiang Zhang']",2025-03-21,2025-03-21
2503.17046v1,HAPI: A Model for Learning Robot Facial Expressions from Human Preferences,"Automatic robotic facial expression generation is crucial for human-robot
interaction, as handcrafted methods based on fixed joint configurations often
yield rigid and unnatural behaviors. Although recent automated techniques
reduce the need for manual tuning, they tend to fall short by not adequately
bridging the gap between human preferences and model predictions-resulting in a
deficiency of nuanced and realistic expressions due to limited degrees of
freedom and insufficient perceptual integration. In this work, we propose a
novel learning-to-rank framework that leverages human feedback to address this
discrepancy and enhanced the expressiveness of robotic faces. Specifically, we
conduct pairwise comparison annotations to collect human preference data and
develop the Human Affective Pairwise Impressions (HAPI) model, a Siamese
RankNet-based approach that refines expression evaluation. Results obtained via
Bayesian Optimization and online expression survey on a 35-DOF android platform
demonstrate that our approach produces significantly more realistic and
socially resonant expressions of Anger, Happiness, and Surprise than those
generated by baseline and expert-designed methods. This confirms that our
framework effectively bridges the gap between human preferences and model
predictions while robustly aligning robotic expression generation with human
affective responses.","['cs.RO', 'cs.AI', 'cs.CV', 'cs.LG']","['Dongsheng Yang', 'Qianying Liu', 'Wataru Sato', 'Takashi Minato', 'Chaoran Liu', ""Shin'ya Nishida""]",2025-03-21,2025-03-21
2503.17044v1,ExCap3D: Expressive 3D Scene Understanding via Object Captioning with Varying Detail,"Generating text descriptions of objects in 3D indoor scenes is an important
building block of embodied understanding. Existing methods do this by
describing objects at a single level of detail, which often does not capture
fine-grained details such as varying textures, materials, and shapes of the
parts of objects. We propose the task of expressive 3D captioning: given an
input 3D scene, describe objects at multiple levels of detail: a high-level
object description, and a low-level description of the properties of its parts.
To produce such captions, we present ExCap3D, an expressive 3D captioning model
which takes as input a 3D scan, and for each detected object in the scan,
generates a fine-grained collective description of the parts of the object,
along with an object-level description conditioned on the part-level
description. We design ExCap3D to encourage semantic consistency between the
generated text descriptions, as well as textual similarity in the latent space,
to further increase the quality of the generated captions. To enable this task,
we generated the ExCap3D Dataset by leveraging a visual-language model (VLM)
for multi-view captioning. The ExCap3D Dataset contains captions on the
ScanNet++ dataset with varying levels of detail, comprising 190k text
descriptions of 34k 3D objects in 947 indoor scenes. Our experiments show that
the object- and part-level of detail captions generated by ExCap3D are of
higher quality than those produced by state-of-the-art methods, with a Cider
score improvement of 17% and 124% for object- and part-level details
respectively. Our code, dataset and models will be made publicly available.",['cs.CV'],"['Chandan Yeshwanth', 'David Rozenberszki', 'Angela Dai']",2025-03-21,2025-03-21
2503.17039v1,Summarization Metrics for Spanish and Basque: Do Automatic Scores and LLM-Judges Correlate with Humans?,"Studies on evaluation metrics and LLM-as-a-Judge models for automatic text
summarization have largely been focused on English, limiting our understanding
of their effectiveness in other languages. Through our new dataset BASSE
(BAsque and Spanish Summarization Evaluation), we address this situation by
collecting human judgments on 2,040 abstractive summaries in Basque and
Spanish, generated either manually or by five LLMs with four different prompts.
For each summary, annotators evaluated five criteria on a 5-point Likert scale:
coherence, consistency, fluency, relevance, and 5W1H. We use these data to
reevaluate traditional automatic metrics used for evaluating summaries, as well
as several LLM-as-a-Judge models that show strong performance on this task in
English. Our results show that currently proprietary judge LLMs have the
highest correlation with human judgments, followed by criteria-specific
automatic metrics, while open-sourced judge LLMs perform poorly. We release
BASSE and our code publicly, along with the first large-scale Basque
summarization dataset containing 22,525 news articles with their subheads.","['cs.CL', 'cs.AI']","['Jeremy Barnes', 'Naiara Perez', 'Alba Bonet-Jover', 'Begoña Altuna']",2025-03-21,2025-03-21
2503.17037v1,Unitless Unrestricted Markov-Consistent SCM Generation: Better Benchmark Datasets for Causal Discovery,"Causal discovery aims to extract qualitative causal knowledge in the form of
causal graphs from data. Because causal ground truth is rarely known in the
real world, simulated data plays a vital role in evaluating the performance of
the various causal discovery algorithms proposed in the literature. But recent
work highlighted certain artifacts of commonly used data generation techniques
for a standard class of structural causal models (SCM) that may be nonphysical,
including var- and R2-sortability, where the variables' variance and
coefficients of determination (R2) after regressing on all other variables,
respectively, increase along the causal order. Some causal methods exploit such
artifacts, leading to unrealistic expectations for their performance on
real-world data. Some modifications have been proposed to remove these
artifacts; notably, the internally-standardized structural causal model (iSCM)
avoids varsortability and largely alleviates R2-sortability on sparse causal
graphs, but exhibits a reversed R2-sortability pattern for denser graphs not
featured in their work. We analyze which sortability patterns we expect to see
in real data, and propose a method for drawing coefficients that we argue more
effectively samples the space of SCMs. Finally, we propose a novel extension of
our SCM generation method to the time series setting.",['cs.LG'],"['Rebecca J. Herman', 'Jonas Wahl', 'Urmi Ninad', 'Jakob Runge']",2025-03-21,2025-03-21
2503.17034v1,An Attentive Representative Sample Selection Strategy Combined with Balanced Batch Training for Skin Lesion Segmentation,"An often overlooked problem in medical image segmentation research is the
effective selection of training subsets to annotate from a complete set of
unlabelled data. Many studies select their training sets at random, which may
lead to suboptimal model performance, especially in the minimal supervision
setting where each training image has a profound effect on performance
outcomes. This work aims to address this issue. We use prototypical contrasting
learning and clustering to extract representative and diverse samples for
annotation. We improve upon prior works with a bespoke cluster-based image
selection process. Additionally, we introduce the concept of unsupervised
balanced batch dataloading to medical image segmentation, which aims to improve
model learning with minimally annotated data. We evaluated our method on a
public skin lesion dataset (ISIC 2018) and compared it to another
state-of-the-art data sampling method. Our method achieved superior performance
in a low annotation budget scenario.","['cs.CV', 'cs.AI']","['Stephen Lloyd-Brown', 'Susan Francis', 'Caroline Hoad', 'Penny Gowland', 'Karen Mullinger', 'Andrew French', 'Xin Chen']",2025-03-21,2025-03-21
2503.17032v1,TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented Reality via 3D Gaussian Splatting,"Realistic 3D full-body talking avatars hold great potential in AR, with
applications ranging from e-commerce live streaming to holographic
communication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike
avatar creation, existing methods struggle with fine-grained control of facial
expressions and body movements in full-body talking tasks. Additionally, they
often lack sufficient details and cannot run in real-time on mobile devices. We
present TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking
avatar driven by various signals. Our approach starts by creating a
personalized clothed human parametric template that binds Gaussians to
represent appearances. We then pre-train a StyleUnet-based network to handle
complex pose-dependent non-rigid deformation, which can capture high-frequency
appearance details but is too resource-intensive for mobile devices. To
overcome this, we ""bake"" the non-rigid deformations into a lightweight
MLP-based network using a distillation technique and develop blend shapes to
compensate for details. Extensive experiments show that TaoAvatar achieves
state-of-the-art rendering quality while running in real-time across various
devices, maintaining 90 FPS on high-definition stereo devices such as the Apple
Vision Pro.",['cs.CV'],"['Jianchuan Chen', 'Jingchuan Hu', 'Gaige Wang', 'Zhonghua Jiang', 'Tiansong Zhou', 'Zhiwen Chen', 'Chengfei Lv']",2025-03-21,2025-03-21
2503.17030v1,Exploring the Efficacy of Partial Denoising Using Bit Plane Slicing for Enhanced Fracture Identification: A Comparative Study of Deep Learning-Based Approaches and Handcrafted Feature Extraction Techniques,"Computer vision has transformed medical diagnosis, treatment, and research
through advanced image processing and machine learning techniques. Fracture
classification, a critical area in healthcare, has greatly benefited from these
advancements, yet accurate detection is challenged by complex patterns and
image noise. Bit plane slicing enhances medical images by reducing noise
interference and extracting informative features. This research explores
partial denoising techniques to provide practical solutions for improved
fracture analysis, ultimately enhancing patient care. The study explores deep
learning model DenseNet and handcrafted feature extraction. Decision Tree and
Random Forest, were employed to train and evaluate distinct image
representations. These include the original image, the concatenation of the
four bit planes from the LSB as well as MSB, the fully denoised image, and an
image consisting of 6 bit planes from MSB and 2 denoised bit planes from LSB.
The purpose of forming these diverse image representations is to analyze SNR as
well as classification accuracy and identify the bit planes that contain the
most informative features. Moreover, the study delves into the significance of
partial denoising techniques in preserving crucial features, leading to
improvements in classification results. Notably, this study shows that
employing the Random Forest classifier, the partially denoised image
representation exhibited a testing accuracy of 95.61% surpassing the
performance of other image representations. The outcomes of this research
provide valuable insights into the development of efficient preprocessing,
feature extraction and classification approaches for fracture identification.
By enhancing diagnostic accuracy, these advancements hold the potential to
positively impact patient care and overall medical outcomes.","['eess.IV', 'cs.AI', 'cs.CV']","['Snigdha Paul', 'Sambit Mallick', 'Anindya Sen']",2025-03-21,2025-03-21
2503.17029v1,AnimatePainter: A Self-Supervised Rendering Framework for Reconstructing Painting Process,"Humans can intuitively decompose an image into a sequence of strokes to
create a painting, yet existing methods for generating drawing processes are
limited to specific data types and often rely on expensive human-annotated
datasets. We propose a novel self-supervised framework for generating drawing
processes from any type of image, treating the task as a video generation
problem. Our approach reverses the drawing process by progressively removing
strokes from a reference image, simulating a human-like creation sequence.
Crucially, our method does not require costly datasets of real human drawing
processes; instead, we leverage depth estimation and stroke rendering to
construct a self-supervised dataset. We model human drawings as ""refinement""
and ""layering"" processes and introduce depth fusion layers to enable video
generation models to learn and replicate human drawing behavior. Extensive
experiments validate the effectiveness of our approach, demonstrating its
ability to generate realistic drawings without the need for real drawing
process data.",['cs.CV'],"['Junjie Hu', 'Shuyong Gao', 'Qianyu Guo', 'Yan Wang', 'Qishan Wang', 'Yuang Feng', 'Wenqiang Zhang']",2025-03-21,2025-03-21
2503.17027v1,RAW-Adapter: Adapting Pre-trained Visual Model to Camera RAW Images and A Benchmark,"In the computer vision community, the preference for pre-training visual
models has largely shifted toward sRGB images due to their ease of acquisition
and compact storage. However, camera RAW images preserve abundant physical
details across diverse real-world scenarios. Despite this, most existing visual
perception methods that utilize RAW data directly integrate image signal
processing (ISP) stages with subsequent network modules, often overlooking
potential synergies at the model level. Building on recent advances in
adapter-based methodologies in both NLP and computer vision, we propose
RAW-Adapter, a novel framework that incorporates learnable ISP modules as
input-level adapters to adjust RAW inputs. At the same time, it employs
model-level adapters to seamlessly bridge ISP processing with high-level
downstream architectures. Moreover, RAW-Adapter serves as a general framework
applicable to various computer vision frameworks.
  Furthermore, we introduce RAW-Bench, which incorporates 17 types of RAW-based
common corruptions, including lightness degradations, weather effects,
blurriness, camera imaging degradations, and variations in camera color
response. Using this benchmark, we systematically compare the performance of
RAW-Adapter with state-of-the-art (SOTA) ISP methods and other RAW-based
high-level vision algorithms. Additionally, we propose a RAW-based data
augmentation strategy to further enhance RAW-Adapter's performance and improve
its out-of-domain (OOD) generalization ability. Extensive experiments
substantiate the effectiveness and efficiency of RAW-Adapter, highlighting its
robust performance across diverse scenarios.",['cs.CV'],"['Ziteng Cui', 'Jianfei Yang', 'Tatsuya Harada']",2025-03-21,2025-03-21
2503.17025v1,A Guide to Bayesian Networks Software Packages for Structure and Parameter Learning -- 2025 Edition,"A representation of the cause-effect mechanism is needed to enable artificial
intelligence to represent how the world works. Bayesian Networks (BNs) have
proven to be an effective and versatile tool for this task. BNs require
constructing a structure of dependencies among variables and learning the
parameters that govern these relationships. These tasks, referred to as
structural learning and parameter learning, are actively investigated by the
research community, with several algorithms proposed and no single method
having established itself as standard. A wide range of software, tools, and
packages have been developed for BNs analysis and made available to academic
researchers and industry practitioners. As a consequence of having no
one-size-fits-all solution, moving the first practical steps and getting
oriented into this field is proving to be challenging to outsiders and
beginners. In this paper, we review the most relevant tools and software for
BNs structural and parameter learning to date, providing our subjective
recommendations directed to an audience of beginners. In addition, we provide
an extensive easy-to-consult overview table summarizing all software packages
and their main features. By improving the reader understanding of which
available software might best suit their needs, we improve accessibility to the
field and make it easier for beginners to take their first step into it.","['cs.AI', 'I.2']","['Joverlyn Gaudillo', 'Nicole Astrologo', 'Fabio Stella', 'Enzo Acerbi', 'Francesco Canonaco']",2025-03-21,2025-03-21
2503.17024v1,A Tale of Two Classes: Adapting Supervised Contrastive Learning to Binary Imbalanced Datasets,"Supervised contrastive learning (SupCon) has proven to be a powerful
alternative to the standard cross-entropy loss for classification of
multi-class balanced datasets. However, it struggles to learn well-conditioned
representations of datasets with long-tailed class distributions. This problem
is potentially exacerbated for binary imbalanced distributions, which are
commonly encountered during many real-world problems such as medical diagnosis.
In experiments on seven binary datasets of natural and medical images, we show
that the performance of SupCon decreases with increasing class imbalance. To
substantiate these findings, we introduce two novel metrics that evaluate the
quality of the learned representation space. By measuring the class
distribution in local neighborhoods, we are able to uncover structural
deficiencies of the representation space that classical metrics cannot detect.
Informed by these insights, we propose two new supervised contrastive learning
strategies tailored to binary imbalanced datasets that improve the structure of
the representation space and increase downstream classification accuracy over
standard SupCon by up to 35%. We make our code available.","['cs.LG', 'cs.CV']","['David Mildenberger', 'Paul Hager', 'Daniel Rueckert', 'Martin J Menten']",2025-03-21,2025-03-21
2503.17020v1,Benign Overfitting with Quantum Kernels,"Quantum kernels quantify similarity between data points by measuring the
inner product between quantum states, computed through quantum circuit
measurements. By embedding data into quantum systems, quantum kernel feature
maps, that may be classically intractable to compute, could efficiently exploit
high-dimensional Hilbert spaces to capture complex patterns. However, designing
effective quantum feature maps remains a major challenge. Many quantum kernels,
such as the fidelity kernel, suffer from exponential concentration, leading to
near-identity kernel matrices that fail to capture meaningful data correlations
and lead to overfitting and poor generalization. In this paper, we propose a
novel strategy for constructing quantum kernels that achieve good
generalization performance, drawing inspiration from benign overfitting in
classical machine learning. Our approach introduces the concept of local-global
quantum kernels, which combine two complementary components: a local quantum
kernel based on measurements of small subsystems and a global quantum kernel
derived from full-system measurements. Through numerical experiments, we
demonstrate that local-global quantum kernels exhibit benign overfitting,
supporting the effectiveness of our approach in enhancing quantum kernel
methods.","['quant-ph', 'cs.LG', 'stat.ML']","['Joachim Tomasi', 'Sandrine Anthoine', 'Hachem Kadri']",2025-03-21,2025-03-21
2503.17018v1,Symbolic Audio Classification via Modal Decision Tree Learning,"The range of potential applications of acoustic analysis is wide.
Classification of sounds, in particular, is a typical machine learning task
that received a lot of attention in recent years. The most common approaches to
sound classification are sub-symbolic, typically based on neural networks, and
result in black-box models with high performances but very low transparency. In
this work, we consider several audio tasks, namely, age and gender recognition,
emotion classification, and respiratory disease diagnosis, and we approach them
with a symbolic technique, that is, (modal) decision tree learning. We prove
that such tasks can be solved using the same symbolic pipeline, that allows to
extract simple rules with very high accuracy and low complexity. In principle,
all such tasks could be associated to an autonomous conversation system, which
could be useful in different contexts, such as an automatic reservation agent
for an hospital or a clinic.","['cs.SD', 'cs.AI', 'cs.LG', 'eess.AS', '68T05', 'I.2.6']","['Enrico Marzano', 'Giovanni Pagliarini', 'Riccardo Pasini', 'Guido Sciavicco', 'Ionel Eduard Stan']",2025-03-21,2025-03-21
2503.17017v1,Specifying What You Know or Not for Multi-Label Class-Incremental Learning,"Existing class incremental learning is mainly designed for single-label
classification task, which is ill-equipped for multi-label scenarios due to the
inherent contradiction of learning objectives for samples with incomplete
labels. We argue that the main challenge to overcome this contradiction in
multi-label class-incremental learning (MLCIL) lies in the model's inability to
clearly distinguish between known and unknown knowledge. This ambiguity hinders
the model's ability to retain historical knowledge, master current classes, and
prepare for future learning simultaneously. In this paper, we target at
specifying what is known or not to accommodate Historical, Current, and
Prospective knowledge for MLCIL and propose a novel framework termed as HCP.
Specifically, (i) we clarify the known classes by dynamic feature purification
and recall enhancement with distribution prior, enhancing the precision and
retention of known information. (ii) We design prospective knowledge mining to
probe the unknown, preparing the model for future learning. Extensive
experiments validate that our method effectively alleviates catastrophic
forgetting in MLCIL, surpassing the previous state-of-the-art by 3.3% on
average accuracy for MS-COCO B0-C10 setting without replay buffers.","['cs.LG', 'cs.CV']","['Aoting Zhang', 'Dongbao Yang', 'Chang Liu', 'Xiaopeng Hong', 'Yu Zhou']",2025-03-21,2025-03-21
2503.17015v1,Do regularization methods for shortcut mitigation work as intended?,"Mitigating shortcuts, where models exploit spurious correlations in training
data, remains a significant challenge for improving generalization.
Regularization methods have been proposed to address this issue by enhancing
model generalizability. However, we demonstrate that these methods can
sometimes overregularize, inadvertently suppressing causal features along with
spurious ones. In this work, we analyze the theoretical mechanisms by which
regularization mitigates shortcuts and explore the limits of its effectiveness.
Additionally, we identify the conditions under which regularization can
successfully eliminate shortcuts without compromising causal features. Through
experiments on synthetic and real-world datasets, our comprehensive analysis
provides valuable insights into the strengths and limitations of regularization
techniques for addressing shortcuts, offering guidance for developing more
robust models.","['cs.LG', 'stat.ML']","['Haoyang Hong', 'Ioanna Papanikolaou', 'Sonali Parbhoo']",2025-03-21,2025-03-21
2503.17013v1,Developing Critical Thinking in Second Language Learners: Exploring Generative AI like ChatGPT as a Tool for Argumentative Essay Writing,"This study employs the Paul-Elder Critical Thinking Model and Tan's
argumentative writing framework to create a structured methodology. This
methodology, ChatGPT Guideline for Critical Argumentative Writing (CGCAW)
framework, integrates the models with ChatGPT's capabilities to guide L2
learners in utilizing ChatGPT to enhance their critical thinking skills. A
quantitative experiment was conducted with 10 participants from a state
university, divided into experimental and control groups. The experimental
group utilized the CGCAW framework, while the control group used ChatGPT
without specific guidelines. Participants wrote an argumentative essay within a
40-minute timeframe, and essays were evaluated by three assessors: ChatGPT,
Grammarly, and a course instructor. Results indicated that the experimental
group showed improvements in clarity, logical coherence, and use of evidence,
demonstrating ChatGPT's potential to enhance specific aspects of argumentative
writing. However, the control group performed better in overall language
mechanics and articulation of main arguments, indicating areas where the CGCAW
framework could be further refined. This study highlights the need for further
research to optimize the use of AI tools like ChatGPT in L2 learning
environments to enhance critical thinking and writing skills.","['cs.HC', 'cs.AI', 'I.2.7; K.3.1']","['Simon Suh', 'Jihyuk Bang', 'Ji Woo Han']",2025-03-21,2025-03-21
2503.17004v1,Text2Model: Generating dynamic chemical reactor models using large language models (LLMs),"As large language models have shown remarkable capabilities in conversing via
natural language, the question arises as to how LLMs could potentially assist
chemical engineers in research and industry with domain-specific tasks. We
generate dynamic chemical reactor models in Modelica code format from textual
descriptions as user input. We fine-tune Llama 3.1 8B Instruct on synthetically
generated Modelica code for different reactor scenarios. We compare the
performance of our fine-tuned model to the baseline Llama 3.1 8B Instruct model
and GPT4o. We manually assess the models' predictions regarding the syntactic
and semantic accuracy of the generated dynamic models. We find that
considerable improvements are achieved by the fine-tuned model with respect to
both the semantic and the syntactic accuracy of the Modelica models. However,
the fine-tuned model lacks a satisfactory ability to generalize to unseen
scenarios compared to GPT4o.","['cs.PL', 'cs.CL']","['Sophia Rupprecht', 'Yassine Hounat', 'Monisha Kumar', 'Giacomo Lastrucci', 'Artur M. Schweidtmann']",2025-03-21,2025-03-21
2503.17003v1,A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications,"Large Language Models (LLMs) have demonstrated remarkable capabilities, yet
their transition to real-world applications reveals a critical limitation: the
inability to adapt to individual preferences while maintaining alignment with
universal human values. Current alignment techniques adopt a one-size-fits-all
approach that fails to accommodate users' diverse backgrounds and needs. This
paper presents the first comprehensive survey of personalized alignment-a
paradigm that enables LLMs to adapt their behavior within ethical boundaries
based on individual preferences. We propose a unified framework comprising
preference memory management, personalized generation, and feedback-based
alignment, systematically analyzing implementation approaches and evaluating
their effectiveness across various scenarios. By examining current techniques,
potential risks, and future challenges, this survey provides a structured
foundation for developing more adaptable and ethically-aligned LLMs.",['cs.CL'],"['Jian Guan', 'Junfei Wu', 'Jia-Nan Li', 'Chuanqi Cheng', 'Wei Wu']",2025-03-21,2025-03-21
2503.17002v1,Targetless 6DoF Calibration of LiDAR and 2D Scanning Radar Based on Cylindrical Occupancy,"Owing to the capability for reliable and all-weather long-range sensing, the
fusion of LiDAR and Radar has been widely applied to autonomous vehicles for
robust perception. In practical operation, well manually calibrated extrinsic
parameters, which are crucial for the fusion of multi-modal sensors, may drift
due to the vibration. To address this issue, we present a novel targetless
calibration approach, termed LiRaCo, for the extrinsic 6DoF calibration of
LiDAR and Radar sensors. Although both types of sensors can obtain geometric
information, bridging the geometric correspondences between multi-modal data
without any clues of explicit artificial markers is nontrivial, mainly due to
the low vertical resolution of scanning Radar. To achieve the targetless
calibration, LiRaCo leverages a spatial occupancy consistency between LiDAR
point clouds and Radar scans in a common cylindrical representation,
considering the increasing data sparsity with distance for both sensors.
Specifically, LiRaCo expands the valid Radar scanned pixels into 3D occupancy
grids to constrain LiDAR point clouds based on spatial consistency.
Consequently, a cost function involving extrinsic calibration parameters is
formulated based on the spatial overlap of 3D grids and LiDAR points. Extrinsic
parameters are finally estimated by optimizing the cost function. Comprehensive
quantitative and qualitative experiments on two real outdoor datasets with
different LiDAR sensors demonstrate the feasibility and accuracy of the
proposed method. The source code will be publicly available.","['cs.RO', 'cs.AI']","['Weimin Wang', 'Yu Du', 'Ting Yang', 'Yu Liu']",2025-03-21,2025-03-21
2503.16997v1,Steady Progress Beats Stagnation: Mutual Aid of Foundation and Conventional Models in Mixed Domain Semi-Supervised Medical Image Segmentation,"Large pretrained visual foundation models exhibit impressive general
capabilities. However, the extensive prior knowledge inherent in these models
can sometimes be a double-edged sword when adapting them to downstream tasks in
specific domains. In the context of semi-supervised medical image segmentation
with domain shift, foundation models like MedSAM tend to make overconfident
predictions, some of which are incorrect. The error accumulation hinders the
effective utilization of unlabeled data and limits further improvements. In
this paper, we introduce a Synergistic training framework for Foundation and
Conventional models (SynFoC) to address the issue. We observe that a
conventional model trained from scratch has the ability to correct the
high-confidence mispredictions of the foundation model, while the foundation
model can supervise it with high-quality pseudo-labels in the early training
stages. Furthermore, to enhance the collaborative training effectiveness of
both models and promote reliable convergence towards optimization, the
consensus-divergence consistency regularization is proposed. We demonstrate the
superiority of our method across four public multi-domain datasets. In
particular, our method improves the Dice score by 10.31\% on the Prostate
dataset. Our code is available at https://github.com/MQinghe/SynFoC .",['cs.CV'],"['Qinghe Ma', 'Jian Zhang', 'Zekun Li', 'Lei Qi', 'Qian Yu', 'Yinghuan Shi']",2025-03-21,2025-03-21
2503.16991v1,TRACE: Time SeRies PArameter EffiCient FinE-tuning,"We propose an efficient fine-tuning method for time series foundation models,
termed TRACE: Time Series Parameter Efficient Fine-tuning. While pretrained
time series foundation models are gaining popularity, they face the following
challenges: (1) Unlike natural language tasks, time series data vary in
frequency, channel numbers, historical/prediction lengths. For long-term
forecasting tasks in particular, tailored fine-tuning can significantly enhance
performance.(2) Existing parameter-efficient tuning methods like LoRA remain
applicable but require adaptation to temporal characteristics.
  To address these challenges, our TRACE framework introduces two key
innovations: (1) Gated DSIC (Gated Dynamic Simulation Importance Calculation),
an unbiased LoRA module importance selection mechanism that ensures conditional
parameter consistency before and after masking. Experiments demonstrate that
Gated DSIC outperforms common fine-tuning. (2) Reconstructed prediction heads
for long-term forecasting tasks, which achieve comparable or superior
performance to linear probing heads while drastically reducing parameter
counts.
  Extensive experiments on long-/short-term forecasting and anomaly detection
tasks across diverse datasets, coupled with ablation studies, validate the
effectiveness of our method.",['cs.LG'],"['Yuze Li', 'Wei Zhu']",2025-03-21,2025-03-21
2503.16988v1,High Accuracy Pulmonary Vessel Segmentation for Contrast and Non-contrast CT Images and Its Clinical Evaluation,"Accurate segmentation of pulmonary vessels plays a very critical role in
diagnosing and assessing various lung diseases. In clinical practice, diagnosis
is typically carried out using CTPA images. However, there is a lack of
high-precision pulmonary vessel segmentation algorithms for CTPA, and pulmonary
vessel segmentation for NCCT poses an even greater challenge. In this study, we
propose a 3D image segmentation algorithm for automated pulmonary vessel
segmentation from both contrast and non-contrast CT images. In the network, we
designed a Vessel Lumen Structure Optimization Module (VLSOM), which extracts
the centerline of vessels and adjusts the weights based on the positional
information and adds a Cl-Dice-Loss to supervise the stability of the vessels
structure. In addition, we designed a method for generating vessel GT from CTPA
to NCCT for training models that support both CTPA and NCCT. In this work, we
used 427 sets of high-precision annotated CT data from multiple vendors and
countries. Finally, our experimental model achieved Cl-Recall, Cl-DICE and
Recall values of 0.879, 0.909, 0.934 (CTPA) and 0.928, 0.936, 0.955 (NCCT)
respectively. This shows that our model has achieved good performance in both
accuracy and completeness of pulmonary vessel segmentation. In clinical visual
evaluation, our model also had good segmentation performance on various disease
types and can assist doctors in medical diagnosis, verifying the great
potential of this method in clinical application.","['eess.IV', 'cs.CV']","['Ying Ming', 'Shaoze Luo', 'Longfei Zhao', 'Qiqi Xu', 'Wei Song']",2025-03-21,2025-03-21
2503.16983v1,Enabling Versatile Controls for Video Diffusion Models,"Despite substantial progress in text-to-video generation, achieving precise
and flexible control over fine-grained spatiotemporal attributes remains a
significant unresolved challenge in video generation research. To address these
limitations, we introduce VCtrl (also termed PP-VCtrl), a novel framework
designed to enable fine-grained control over pre-trained video diffusion models
in a unified manner. VCtrl integrates diverse user-specified control
signals-such as Canny edges, segmentation masks, and human keypoints-into
pretrained video diffusion models via a generalizable conditional module
capable of uniformly encoding multiple types of auxiliary signals without
modifying the underlying generator. Additionally, we design a unified control
signal encoding pipeline and a sparse residual connection mechanism to
efficiently incorporate control representations. Comprehensive experiments and
human evaluations demonstrate that VCtrl effectively enhances controllability
and generation quality. The source code and pre-trained models are publicly
available and implemented using the PaddlePaddle framework at
http://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl.","['cs.CV', 'cs.AI']","['Xu Zhang', 'Hao Zhou', 'Haoming Qin', 'Xiaobin Lu', 'Jiaxing Yan', 'Guanzhong Wang', 'Zeyu Chen', 'Yi Liu']",2025-03-21,2025-03-21
2503.16980v1,Token Dynamics: Towards Efficient and Dynamic Video Token Representation for Video Large Language Models,"Token-based video representation has emerged as a promising approach for
enabling large language models to interpret video content. However, existing
token reduction techniques, such as token pruning and token merging, often
disrupt essential spatial-temporal positional embeddings, failing to adequately
balance computational efficiency with fewer tokens. Consequently, these methods
result in relatively lengthy token sequences, limiting their applicability in
scenarios requiring extreme token compression, such as video large language
models. In this paper, we introduce the novel task of extreme short token
reduction, aiming to represent extensive video sequences with a minimal number
of tokens. To address this challenge, we propose Token Dynamics, a new video
representation framework that dynamically reduces token count while preserving
spatial-temporal coherence. Specifically, we disentangle video representations
by separating visual embeddings from grid-level motion information, structuring
them into: 1. a concise token base, created by clustering tokens that describe
object-level content; 2. a token dynamics map, capturing detailed
spatial-temporal motion patterns across grids. Furthermore, we introduce a
cross-dynamics attention mechanism that integrates motion features into the
token base without increasing token length, thereby maintaining compactness and
spatial-temporal integrity. The experiments demonstrate a reduction of token
count to merely 0.07% of the original tokens, with only a minor performance
drop of 1.13%. Additionally, we propose two novel subtasks within extreme token
reduction (fixed-length and adaptive-length compression), both effectively
representing long token sequences for video-language tasks. Our method offers
significantly lower theoretical complexity, fewer tokens, and enhanced
throughput, thus providing an efficient solution for video LLMs.","['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG']","['Haichao Zhang', 'Zhuowei Li', 'Dimitris Metaxas', 'Yun Fu']",2025-03-21,2025-03-21
2503.16979v1,Instant Gaussian Stream: Fast and Generalizable Streaming of Dynamic Scene Reconstruction via Gaussian Splatting,"Building Free-Viewpoint Videos in a streaming manner offers the advantage of
rapid responsiveness compared to offline training methods, greatly enhancing
user experience. However, current streaming approaches face challenges of high
per-frame reconstruction time (10s+) and error accumulation, limiting their
broader application. In this paper, we propose Instant Gaussian Stream (IGS), a
fast and generalizable streaming framework, to address these issues. First, we
introduce a generalized Anchor-driven Gaussian Motion Network, which projects
multi-view 2D motion features into 3D space, using anchor points to drive the
motion of all Gaussians. This generalized Network generates the motion of
Gaussians for each target frame in the time required for a single inference.
Second, we propose a Key-frame-guided Streaming Strategy that refines each key
frame, enabling accurate reconstruction of temporally complex scenes while
mitigating error accumulation. We conducted extensive in-domain and
cross-domain evaluations, demonstrating that our approach can achieve streaming
with a average per-frame reconstruction time of 2s+, alongside a enhancement in
view synthesis quality.",['cs.CV'],"['Jinbo Yan', 'Rui Peng', 'Zhiyan Wang', 'Luyang Tang', 'Jiayu Yang', 'Jie Liang', 'Jiahao Wu', 'Ronggang Wang']",2025-03-21,2025-03-21
2503.16978v1,Real-Time Diffusion Policies for Games: Enhancing Consistency Policies with Q-Ensembles,"Diffusion models have shown impressive performance in capturing complex and
multi-modal action distributions for game agents, but their slow inference
speed prevents practical deployment in real-time game environments. While
consistency models offer a promising approach for one-step generation, they
often suffer from training instability and performance degradation when applied
to policy learning. In this paper, we present CPQE (Consistency Policy with
Q-Ensembles), which combines consistency models with Q-ensembles to address
these challenges.CPQE leverages uncertainty estimation through Q-ensembles to
provide more reliable value function approximations, resulting in better
training stability and improved performance compared to classic double
Q-network methods. Our extensive experiments across multiple game scenarios
demonstrate that CPQE achieves inference speeds of up to 60 Hz -- a significant
improvement over state-of-the-art diffusion policies that operate at only 20 Hz
-- while maintaining comparable performance to multi-step diffusion approaches.
CPQE consistently outperforms state-of-the-art consistency model approaches,
showing both higher rewards and enhanced training stability throughout the
learning process. These results indicate that CPQE offers a practical solution
for deploying diffusion-based policies in games and other real-time
applications where both multi-modal behavior modeling and rapid inference are
critical requirements.",['cs.AI'],"['Ruoqi Zhang', 'Ziwei Luo', 'Jens Sjölund', 'Per Mattsson', 'Linus Gisslén', 'Alessandro Sestini']",2025-03-21,2025-03-21
2503.16976v1,GeoT: Geometry-guided Instance-dependent Transition Matrix for Semi-supervised Tooth Point Cloud Segmentation,"Achieving meticulous segmentation of tooth point clouds from intra-oral scans
stands as an indispensable prerequisite for various orthodontic applications.
Given the labor-intensive nature of dental annotation, a significant amount of
data remains unlabeled, driving increasing interest in semi-supervised
approaches. One primary challenge of existing semi-supervised medical
segmentation methods lies in noisy pseudo labels generated for unlabeled data.
To address this challenge, we propose GeoT, the first framework that employs
instance-dependent transition matrix (IDTM) to explicitly model noise in pseudo
labels for semi-supervised dental segmentation. Specifically, to handle the
extensive solution space of IDTM arising from tens of thousands of dental
points, we introduce tooth geometric priors through two key components:
point-level geometric regularization (PLGR) to enhance consistency between
point adjacency relationships in 3D and IDTM spaces, and class-level geometric
smoothing (CLGS) to leverage the fixed spatial distribution of tooth categories
for optimal IDTM estimation. Extensive experiments performed on the public
Teeth3DS dataset and private dataset demonstrate that our method can make full
utilization of unlabeled data to facilitate segmentation, achieving performance
comparable to fully supervised methods with only $20\%$ of the labeled data.","['cs.CV', 'cs.AI']","['Weihao Yu', 'Xiaoqing Guo', 'Chenxin Li', 'Yifan Liu', 'Yixuan Yuan']",2025-03-21,2025-03-21
2503.16975v1,EasyRobust: A Comprehensive and Easy-to-use Toolkit for Robust and Generalized Vision,"Deep neural networks (DNNs) has shown great promise in computer vision tasks.
However, machine vision achieved by DNNs cannot be as robust as human
perception. Adversarial attacks and data distribution shifts have been known as
two major scenarios which degrade machine performance and obstacle the wide
deployment of machines ""in the wild"". In order to break these obstructions and
facilitate the research of model robustness, we develop EasyRobust, a
comprehensive and easy-to-use toolkit for training, evaluation and analysis of
robust vision models. EasyRobust targets at two types of robustness: 1)
Adversarial robustness enables the model to defense against malicious inputs
crafted by worst-case perturbations, also known as adversarial examples; 2)
Non-adversarial robustness enhances the model performance on natural test
images with corruptions or distribution shifts. Thorough benchmarks on image
classification enable EasyRobust to provide an accurate robustness evaluation
on vision models. We wish our EasyRobust can help for training
practically-robust models and promote academic and industrial progress in
closing the gap between human and machine vision. Codes and models of
EasyRobust have been open-sourced in https://github.com/alibaba/easyrobust.",['cs.CV'],"['Xiaofeng Mao', 'Yuefeng Chen', 'Rong Zhang', 'Hui Xue', 'Zhao Li', 'Hang Su']",2025-03-21,2025-03-21
2503.16974v1,Assessing Consistency and Reproducibility in the Outputs of Large Language Models: Evidence Across Diverse Finance and Accounting Tasks,"This study provides the first comprehensive assessment of consistency and
reproducibility in Large Language Model (LLM) outputs in finance and accounting
research. We evaluate how consistently LLMs produce outputs given identical
inputs through extensive experimentation with 50 independent runs across five
common tasks: classification, sentiment analysis, summarization, text
generation, and prediction. Using three OpenAI models (GPT-3.5-turbo,
GPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse
financial source texts and data, covering MD&As, FOMC statements, finance news
articles, earnings call transcripts, and financial statements. Our findings
reveal substantial but task-dependent consistency, with binary classification
and sentiment analysis achieving near-perfect reproducibility, while complex
tasks show greater variability. More advanced models do not consistently
demonstrate better consistency and reproducibility, with task-specific patterns
emerging. LLMs significantly outperform expert human annotators in consistency
and maintain high agreement even where human experts significantly disagree. We
further find that simple aggregation strategies across 3-5 runs dramatically
improve consistency. Simulation analysis reveals that despite measurable
inconsistency in LLM outputs, downstream statistical inferences remain
remarkably robust. These findings address concerns about what we term
""G-hacking,"" the selective reporting of favorable outcomes from multiple
Generative AI runs, by demonstrating that such risks are relatively low for
finance and accounting tasks.","['q-fin.GN', 'cs.AI', 'cs.CE', 'cs.CL', 'cs.LG']","['Julian Junyan Wang', 'Victor Xiaoqi Wang']",2025-03-21,2025-03-21
2503.16973v1,ARFlow: Human Action-Reaction Flow Matching with Physical Guidance,"Human action-reaction synthesis, a fundamental challenge in modeling causal
human interactions, plays a critical role in applications ranging from virtual
reality to social robotics. While diffusion-based models have demonstrated
promising performance, they exhibit two key limitations for interaction
synthesis: reliance on complex noise-to-reaction generators with intricate
conditional mechanisms, and frequent physical violations in generated motions.
To address these issues, we propose Action-Reaction Flow Matching (ARFlow), a
novel framework that establishes direct action-to-reaction mappings,
eliminating the need for complex conditional mechanisms. Our approach
introduces two key innovations: an x1-prediction method that directly outputs
human motions instead of velocity fields, enabling explicit constraint
enforcement; and a training-free, gradient-based physical guidance mechanism
that effectively prevents body penetration artifacts during sampling. Extensive
experiments on NTU120 and Chi3D datasets demonstrate that ARFlow not only
outperforms existing methods in terms of Fr\'echet Inception Distance and
motion diversity but also significantly reduces body collisions, as measured by
our new Intersection Volume and Intersection Frequency metrics.","['cs.CV', 'cs.AI']","['Wentao Jiang', 'Jingya Wang', 'Haotao Lu', 'Kaiyang Ji', 'Baoxiong Jia', 'Siyuan Huang', 'Ye Shi']",2025-03-21,2025-03-21
2503.16970v1,Distilling Monocular Foundation Model for Fine-grained Depth Completion,"Depth completion involves predicting dense depth maps from sparse LiDAR
inputs. However, sparse depth annotations from sensors limit the availability
of dense supervision, which is necessary for learning detailed geometric
features. In this paper, we propose a two-stage knowledge distillation
framework that leverages powerful monocular foundation models to provide dense
supervision for depth completion. In the first stage, we introduce a
pre-training strategy that generates diverse training data from natural images,
which distills geometric knowledge to depth completion. Specifically, we
simulate LiDAR scans by utilizing monocular depth and mesh reconstruction,
thereby creating training data without requiring ground-truth depth. Besides,
monocular depth estimation suffers from inherent scale ambiguity in real-world
settings. To address this, in the second stage, we employ a scale- and
shift-invariant loss (SSI Loss) to learn real-world scales when fine-tuning on
real-world datasets. Our two-stage distillation framework enables depth
completion models to harness the strengths of monocular foundation models.
Experimental results demonstrate that models trained with our two-stage
distillation framework achieve state-of-the-art performance, ranking
\textbf{first place} on the KITTI benchmark. Code is available at
https://github.com/Sharpiless/DMD3C",['cs.CV'],"['Yingping Liang', 'Yutao Hu', 'Wenqi Shao', 'Ying Fu']",2025-03-21,2025-03-21
2503.16965v1,When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only Training For Human-Centered Decision Making,"Embodied decision-making is fundamental for AI agents operating in real-world
environments. While Visual Language Models (VLMs) have advanced this
capability, they still struggle with complex decisions, particularly in
human-centered situations that require deep reasoning about human needs and
values. In this study, we systematically evaluate open-sourced VLMs on
multimodal human-centered decision-making tasks. We find that LLMs receiving
only textual descriptions unexpectedly outperform their VLM counterparts of
similar scale that process actual images, suggesting that visual alignment may
hinder VLM abilities. To address this challenge, we propose a novel text-only
training approach with synthesized textual data. This method strengthens VLMs'
language components and transfers the learned abilities to multimodal
inference, eliminating the need for expensive image-text paired data.
Furthermore, we show that VLMs can achieve substantial performance gains
through self-improvement, using training data generated by their LLM
counterparts rather than relying on larger teacher models like GPT-4. Our
findings establish a more efficient and scalable approach to enhancing VLMs'
human-centered decision-making capabilities, opening new avenues for optimizing
VLMs through self-improvement mechanisms.","['cs.CL', 'cs.CV']","['Zhe Hu', 'Jing Li', 'Yu Yin']",2025-03-21,2025-03-21
2503.16964v1,DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from In-the-Wild Drone Imagery,"Drones have become essential tools for reconstructing wild scenes due to
their outstanding maneuverability. Recent advances in radiance field methods
have achieved remarkable rendering quality, providing a new avenue for 3D
reconstruction from drone imagery. However, dynamic distractors in wild
environments challenge the static scene assumption in radiance fields, while
limited view constraints hinder the accurate capture of underlying scene
geometry. To address these challenges, we introduce DroneSplat, a novel
framework designed for robust 3D reconstruction from in-the-wild drone imagery.
Our method adaptively adjusts masking thresholds by integrating local-global
segmentation heuristics with statistical approaches, enabling precise
identification and elimination of dynamic distractors in static scenes. We
enhance 3D Gaussian Splatting with multi-view stereo predictions and a
voxel-guided optimization strategy, supporting high-quality rendering under
limited view constraints. For comprehensive evaluation, we provide a
drone-captured 3D reconstruction dataset encompassing both dynamic and static
scenes. Extensive experiments demonstrate that DroneSplat outperforms both 3DGS
and NeRF baselines in handling in-the-wild drone imagery.",['cs.CV'],"['Jiadong Tang', 'Yu Gao', 'Dianyi Yang', 'Liqi Yan', 'Yufeng Yue', 'Yi Yang']",2025-03-21,2025-03-21
2503.16963v1,Center-guided Classifier for Semantic Segmentation of Remote Sensing Images,"Compared with natural images, remote sensing images (RSIs) have the unique
characteristic. i.e., larger intraclass variance, which makes semantic
segmentation for remote sensing images more challenging. Moreover, existing
semantic segmentation models for remote sensing images usually employ a vanilla
softmax classifier, which has three drawbacks: (1) non-direct supervision for
the pixel representations during training; (2) inadequate modeling ability of
parametric softmax classifiers under large intraclass variance; and (3) opaque
process of classification decision. In this paper, we propose a novel
classifier (called CenterSeg) customized for RSI semantic segmentation, which
solves the abovementioned problems with multiple prototypes, direct supervision
under Grassmann manifold, and interpretability strategy. Specifically, for each
class, our CenterSeg obtains local class centers by aggregating corresponding
pixel features based on ground-truth masks, and generates multiple prototypes
through hard attention assignment and momentum updating. In addition, we
introduce the Grassmann manifold and constrain the joint embedding space of
pixel features and prototypes based on two additional regularization terms.
Especially, during the inference, CenterSeg can further provide
interpretability to the model by restricting the prototype as a sample of the
training set. Experimental results on three remote sensing segmentation
datasets validate the effectiveness of the model. Besides the superior
performance, CenterSeg has the advantages of simplicity, lightweight,
compatibility, and interpretability. Code is available at
https://github.com/xwmaxwma/rssegmentation.",['cs.CV'],"['Wei Zhang', 'Mengting Ma', 'Yizhen Jiang', 'Rongrong Lian', 'Zhenkai Wu', 'Kangning Cui', 'Xiaowen Ma']",2025-03-21,2025-03-21
2503.16957v1,Uncertainty-Driven Modeling of Microporosity and Permeability in Clastic Reservoirs Using Random Forest,"Predicting microporosity and permeability in clastic reservoirs is a
challenge in reservoir quality assessment, especially in formations where
direct measurements are difficult or expensive. These reservoir properties are
fundamental in determining a reservoir's capacity for fluid storage and
transmission, yet conventional methods for evaluating them, such as Mercury
Injection Capillary Pressure (MICP) and Scanning Electron Microscopy (SEM), are
resource-intensive. The aim of this study is to develop a cost-effective
machine learning model to predict complex reservoir properties using readily
available field data and basic laboratory analyses. A Random Forest classifier
was employed, utilizing key geological parameters such as porosity, grain size
distribution, and spectral gamma-ray (SGR) measurements. An uncertainty
analysis was applied to account for natural variability, expanding the dataset,
and enhancing the model's robustness. The model achieved a high level of
accuracy in predicting microporosity (93%) and permeability levels (88%). By
using easily obtainable data, this model reduces the reliance on expensive
laboratory methods, making it a valuable tool for early-stage exploration,
especially in remote or offshore environments. The integration of machine
learning with uncertainty analysis provides a reliable and cost-effective
approach for evaluating key reservoir properties in siliciclastic formations.
This model offers a practical solution to improve reservoir quality
assessments, enabling more informed decision-making and optimizing exploration
efforts.","['physics.geo-ph', 'cs.LG']","['Muhammad Risha', 'Mohamed Elsaadany', 'Paul Liu']",2025-03-21,2025-03-21
2503.16956v1,From Faces to Voices: Learning Hierarchical Representations for High-quality Video-to-Speech,"The objective of this study is to generate high-quality speech from silent
talking face videos, a task also known as video-to-speech synthesis. A
significant challenge in video-to-speech synthesis lies in the substantial
modality gap between silent video and multi-faceted speech. In this paper, we
propose a novel video-to-speech system that effectively bridges this modality
gap, significantly enhancing the quality of synthesized speech. This is
achieved by learning of hierarchical representations from video to speech.
Specifically, we gradually transform silent video into acoustic feature spaces
through three sequential stages -- content, timbre, and prosody modeling. In
each stage, we align visual factors -- lip movements, face identity, and facial
expressions -- with corresponding acoustic counterparts to ensure the seamless
transformation. Additionally, to generate realistic and coherent speech from
the visual representations, we employ a flow matching model that estimates
direct trajectories from a simple prior distribution to the target speech
distribution. Extensive experiments demonstrate that our method achieves
exceptional generation quality comparable to real utterances, outperforming
existing methods by a significant margin.","['eess.AS', 'cs.AI', 'cs.CV', 'cs.SD']","['Ji-Hoon Kim', 'Jeongsoo Choi', 'Jaehun Kim', 'Chaeyoung Jung', 'Joon Son Chung']",2025-03-21,2025-03-21
2503.16953v1,Neural-Guided Equation Discovery,"Deep learning approaches are becoming increasingly attractive for equation
discovery. We show the advantages and disadvantages of using neural-guided
equation discovery by giving an overview of recent papers and the results of
experiments using our modular equation discovery system MGMT
($\textbf{M}$ulti-Task $\textbf{G}$rammar-Guided $\textbf{M}$onte-Carlo
$\textbf{T}$ree Search for Equation Discovery). The system uses neural-guided
Monte-Carlo Tree Search (MCTS) and supports both supervised and reinforcement
learning, with a search space defined by a context-free grammar. We summarize
seven desirable properties of equation discovery systems, emphasizing the
importance of embedding tabular data sets for such learning approaches. Using
the modular structure of MGMT, we compare seven architectures (among them,
RNNs, CNNs, and Transformers) for embedding tabular datasets on the auxiliary
task of contrastive learning for tabular data sets on an equation discovery
task. For almost all combinations of modules, supervised learning outperforms
reinforcement learning. Moreover, our experiments indicate an advantage of
using grammar rules as action space instead of tokens. Two adaptations of MCTS
-- risk-seeking MCTS and AmEx-MCTS -- can improve equation discovery with that
kind of search.","['cs.AI', 'I.2.6; I.1.1; G.3']","['Jannis Brugger', 'Mattia Cerrato', 'David Richter', 'Cedric Derstroff', 'Daniel Maninger', 'Mira Mezini', 'Stefan Kramer']",2025-03-21,2025-03-21
2503.16948v1,MagicColor: Multi-Instance Sketch Colorization,"We present \textit{MagicColor}, a diffusion-based framework for
multi-instance sketch colorization. The production of multi-instance 2D line
art colorization adheres to an industry-standard workflow, which consists of
three crucial stages: the design of line art characters, the coloring of
individual objects, and the refinement process. The artists are required to
repeat the process of coloring each instance one by one, which is inaccurate
and inefficient. Meanwhile, current generative methods fail to solve this task
due to the challenge of multi-instance pair data collection. To tackle these
challenges, we incorporate three technical designs to ensure precise character
detail transcription and achieve multi-instance sketch colorization in a single
forward. Specifically, we first propose the self-play training strategy to
solve the lack of training data. Then we introduce an instance guider to feed
the color of the instance. To achieve accurate color matching, we present
fine-grained color matching with edge loss to enhance visual quality. Equipped
with the proposed modules, MagicColor enables automatically transforming
sketches into vividly-colored images with accurate consistency and
multi-instance control. Experiments on our collected datasets show that our
model outperforms existing methods regarding chromatic precision. Specifically,
our model critically automates the colorization process with zero manual
adjustments, so novice users can produce stylistically consistent artwork by
providing reference instances and the original line art. Our code and
additional details are available at https://yinhan-zhang.github.io/color",['cs.CV'],"['Yinhan Zhang', 'Yue Ma', 'Bingyuan Wang', 'Qifeng Chen', 'Zeyu Wang']",2025-03-21,2025-03-21
2503.16945v1,PE-CLIP: A Parameter-Efficient Fine-Tuning of Vision Language Models for Dynamic Facial Expression Recognition,"Vision-Language Models (VLMs) like CLIP offer promising solutions for Dynamic
Facial Expression Recognition (DFER) but face challenges such as inefficient
full fine-tuning, high complexity, and poor alignment between textual and
visual representations. Additionally, existing methods struggle with
ineffective temporal modeling. To address these issues, we propose PE-CLIP, a
parameter-efficient fine-tuning (PEFT) framework that adapts CLIP for DFER
while significantly reducing trainable parameters while maintaining high
accuracy. PE-CLIP introduces two specialized adapters: a Temporal Dynamic
Adapter (TDA) and a Shared Adapter (ShA). The TDA is a GRU-based module with
dynamic scaling that captures sequential dependencies while emphasizing
informative temporal features and suppressing irrelevant variations. The ShA is
a lightweight adapter that refines representations within both textual and
visual encoders, ensuring consistency and efficiency. Additionally, we
integrate Multi-modal Prompt Learning (MaPLe), introducing learnable prompts
for visual and action unit-based textual inputs, enhancing semantic alignment
between modalities and enabling efficient CLIP adaptation for dynamic tasks. We
evaluate PE-CLIP on two benchmark datasets, DFEW and FERV39K, achieving
competitive performance compared to state-of-the-art methods while requiring
fewer trainable parameters. By balancing efficiency and accuracy, PE-CLIP sets
a new benchmark in resource-efficient DFER. The source code of the proposed
PE-CLIP will be publicly available at https://github.com/Ibtissam-SAADI/PE-CLIP .",['cs.CV'],"['Ibtissam Saadi', 'Abdenour Hadid', 'Douglas W. Cunningham', 'Abdelmalik Taleb-Ahmed', 'Yassin El Hillali']",2025-03-21,2025-03-21
2503.16944v1,HyperLoRA: Parameter-Efficient Adaptive Generation for Portrait Synthesis,"Personalized portrait synthesis, essential in domains like social
entertainment, has recently made significant progress. Person-wise fine-tuning
based methods, such as LoRA and DreamBooth, can produce photorealistic outputs
but need training on individual samples, consuming time and resources and
posing an unstable risk. Adapter based techniques such as IP-Adapter freeze the
foundational model parameters and employ a plug-in architecture to enable
zero-shot inference, but they often exhibit a lack of naturalness and
authenticity, which are not to be overlooked in portrait synthesis tasks. In
this paper, we introduce a parameter-efficient adaptive generation method,
namely HyperLoRA, that uses an adaptive plug-in network to generate LoRA
weights, merging the superior performance of LoRA with the zero-shot capability
of adapter scheme. Through our carefully designed network structure and
training strategy, we achieve zero-shot personalized portrait generation
(supporting both single and multiple image inputs) with high photorealism,
fidelity, and editability.",['cs.CV'],"['Mengtian Li', 'Jinshu Chen', 'Wanquan Feng', 'Bingchuan Li', 'Fei Dai', 'Songtao Zhao', 'Qian He']",2025-03-21,2025-03-21
2503.16943v1,Model-free front-to-end training of a large high performance laser neural network,"Artificial neural networks (ANNs), have become ubiquitous and revolutionized
many applications ranging from computer vision to medical diagnoses. However,
they offer a fundamentally connectionist and distributed approach to computing,
in stark contrast to classical computers that use the von Neumann architecture.
This distinction has sparked renewed interest in developing unconventional
hardware to support more efficient implementations of ANNs, rather than merely
emulating them on traditional systems. Photonics stands out as a particularly
promising platform, providing scalability, high speed, energy efficiency, and
the ability for parallel information processing. However, fully realized
autonomous optical neural networks (ONNs) with in-situ learning capabilities
are still rare. In this work, we demonstrate a fully autonomous and parallel
ONN using a multimode vertical cavity surface emitting laser (VCSEL) using
off-the-shelf components. Our ONN is highly efficient and is scalable both in
network size and inference bandwidth towards the GHz range. High performance
hardware-compatible optimization algorithms are necessary in order to minimize
reliance on external von Neumann computers to fully exploit the potential of
ONNs. As such we present and extensively study several algorithms which are
broadly compatible with a wide range of systems. We then apply these algorithms
to optimize our ONN, and benchmark them using the MNIST dataset. We show that
our ONN can achieve high accuracy and convergence efficiency, even under
limited hardware resources. Crucially, we compare these different algorithms in
terms of scaling and optimization efficiency in term of convergence time which
is crucial when working with limited external resources. Our work provides some
guidance for the design of future ONNs as well as a simple and flexible way to
train them.","['cs.LG', 'cs.ET']","['Anas Skalli', 'Satoshi Sunada', 'Mirko Goldmann', 'Marcin Gebski', 'Stephan Reitzenstein', 'James A. Lott', 'Tomasz Czyszanowski', 'Daniel Brunner']",2025-03-21,2025-03-21
2503.16942v1,Re-HOLD: Video Hand Object Interaction Reenactment via adaptive Layout-instructed Diffusion Model,"Current digital human studies focusing on lip-syncing and body movement are
no longer sufficient to meet the growing industrial demand, while human video
generation techniques that support interacting with real-world environments
(e.g., objects) have not been well investigated. Despite human hand synthesis
already being an intricate problem, generating objects in contact with hands
and their interactions presents an even more challenging task, especially when
the objects exhibit obvious variations in size and shape. To cope with these
issues, we present a novel video Reenactment framework focusing on Human-Object
Interaction (HOI) via an adaptive Layout-instructed Diffusion model (Re-HOLD).
Our key insight is to employ specialized layout representation for hands and
objects, respectively. Such representations enable effective disentanglement of
hand modeling and object adaptation to diverse motion sequences. To further
improve the generation quality of HOI, we have designed an interactive textural
enhancement module for both hands and objects by introducing two independent
memory banks. We also propose a layout-adjusting strategy for the cross-object
reenactment scenario to adaptively adjust unreasonable layouts caused by
diverse object sizes during inference. Comprehensive qualitative and
quantitative evaluations demonstrate that our proposed framework significantly
outperforms existing methods. Project page: https://fyycs.github.io/Re-HOLD.",['cs.CV'],"['Yingying Fan', 'Quanwei Yang', 'Kaisiyuan Wang', 'Hang Zhou', 'Yingying Li', 'Haocheng Feng', 'Yu Wu', 'Jingdong Wang']",2025-03-21,2025-03-21
2503.16941v1,Sparse Additive Contextual Bandits: A Nonparametric Approach for Online Decision-making with High-dimensional Covariates,"Personalized services are central to today's digital landscape, where online
decision-making is commonly formulated as contextual bandit problems. Two key
challenges emerge in modern applications: high-dimensional covariates and the
need for nonparametric models to capture complex reward-covariate
relationships. We address these challenges by developing a contextual bandit
algorithm based on sparse additive reward models in reproducing kernel Hilbert
spaces. We establish statistical properties of the doubly penalized method
applied to random regions, introducing novel analyses under bandit feedback.
Our algorithm achieves sublinear cumulative regret over the time horizon $T$
while scaling logarithmically with covariate dimensionality $d$. Notably, we
provide the first regret upper bound with logarithmic growth in $d$ for
nonparametric contextual bandits with high-dimensional covariates. We also
establish a lower bound, with the gap to the upper bound vanishing as
smoothness increases. Extensive numerical experiments demonstrate our
algorithm's superior performance in high-dimensional settings compared to
existing approaches.","['stat.ML', 'cs.LG', 'stat.ME']","['Wenjia Wang', 'Qingwen Zhang', 'Xiaowei Zhang']",2025-03-21,2025-03-21
2503.16939v1,On-Sensor Convolutional Neural Networks with Early-Exits,"Tiny Machine Learning (TinyML) is a novel research field aiming at
integrating Machine Learning (ML) within embedded devices with limited memory,
computation, and energy. Recently, a new branch of TinyML has emerged, focusing
on integrating ML directly into the sensors to further reduce the power
consumption of embedded devices. Interestingly, despite their state-of-the-art
performance in many tasks, none of the current solutions in the literature aims
to optimize the implementation of Convolutional Neural Networks (CNNs)
operating directly into sensors. In this paper, we introduce for the first time
in the literature the optimized design and implementation of Depth-First CNNs
operating on the Intelligent Sensor Processing Unit (ISPU) within an Inertial
Measurement Unit (IMU) by STMicroelectronics. Our approach partitions the CNN
between the ISPU and the microcontroller (MCU) and employs an Early-Exit
mechanism to stop the computations on the IMU when enough confidence about the
results is achieved, hence significantly reducing power consumption. When using
a NUCLEO-F411RE board, this solution achieved an average current consumption of
4.8 mA, marking an 11% reduction compared to the regular inference pipeline on
the MCU, while having equal accuracy.","['cs.LG', 'cs.AI']","['Hazem Hesham Yousef Shalby', 'Arianna De Vecchi', 'Alice Scandelli', 'Pietro Bartoli', 'Diana Trojaniello', 'Manuel Roveri', 'Federica Villa']",2025-03-21,2025-03-21
2503.16938v1,Interpretable Machine Learning for Oral Lesion Diagnosis through Prototypical Instances Identification,"Decision-making processes in healthcare can be highly complex and
challenging. Machine Learning tools offer significant potential to assist in
these processes. However, many current methodologies rely on complex models
that are not easily interpretable by experts. This underscores the need to
develop interpretable models that can provide meaningful support in clinical
decision-making. When approaching such tasks, humans typically compare the
situation at hand to a few key examples and representative cases imprinted in
their memory. Using an approach which selects such exemplary cases and grounds
its predictions on them could contribute to obtaining high-performing
interpretable solutions to such problems. To this end, we evaluate PivotTree,
an interpretable prototype selection model, on an oral lesion detection
problem, specifically trying to detect the presence of neoplastic, aphthous and
traumatic ulcerated lesions from oral cavity images. We demonstrate the
efficacy of using such method in terms of performance and offer a qualitative
and quantitative comparison between exemplary cases and ground-truth prototypes
selected by experts.",['cs.AI'],"['Alessio Cascione', 'Mattia Setzu', 'Federico A. Galatolo', 'Mario G. C. A. Cimino', 'Riccardo Guidotti']",2025-03-21,2025-03-21
2503.16932v1,Rude Humans and Vengeful Robots: Examining Human Perceptions of Robot Retaliatory Intentions in Professional Settings,"Humans and robots are increasingly working in personal and professional
settings. In workplace settings, humans and robots may work together as
colleagues, potentially leading to social expectations, or violation thereof.
Extant research has primarily sought to understand social interactions and
expectations in personal rather than professional settings, and none of these
studies have examined negative outcomes arising from violations of social
expectations. This paper reports the results of a 2x3 online experiment that
used a unique first-person perspective video to immerse participants in a
collaborative workplace setting. The results are nuanced and reveal that while
robots are expected to act in accordance with social expectations despite human
behavior, there are benefits for robots perceived as being the bigger person in
the face of human rudeness. Theoretical and practical implications are provided
which discuss the import of these findings for the design of social robots.","['cs.RO', 'cs.AI']","['Kate Letheren', 'Nicole Robinson']",2025-03-21,2025-03-21
2503.16930v1,Vision-Language Gradient Descent-driven All-in-One Deep Unfolding Networks,"Dynamic image degradations, including noise, blur and lighting
inconsistencies, pose significant challenges in image restoration, often due to
sensor limitations or adverse environmental conditions. Existing Deep Unfolding
Networks (DUNs) offer stable restoration performance but require manual
selection of degradation matrices for each degradation type, limiting their
adaptability across diverse scenarios. To address this issue, we propose the
Vision-Language-guided Unfolding Network (VLU-Net), a unified DUN framework for
handling multiple degradation types simultaneously. VLU-Net leverages a
Vision-Language Model (VLM) refined on degraded image-text pairs to align image
features with degradation descriptions, selecting the appropriate transform for
target degradation. By integrating an automatic VLM-based gradient estimation
strategy into the Proximal Gradient Descent (PGD) algorithm, VLU-Net
effectively tackles complex multi-degradation restoration tasks while
maintaining interpretability. Furthermore, we design a hierarchical feature
unfolding structure to enhance VLU-Net framework, efficiently synthesizing
degradation patterns across various levels. VLU-Net is the first all-in-one DUN
framework and outperforms current leading one-by-one and all-in-one end-to-end
methods by 3.74 dB on the SOTS dehazing dataset and 1.70 dB on the Rain100L
deraining dataset.",['cs.CV'],"['Haijin Zeng', 'Xiangming Wang', 'Yongyong Chen', 'Jingyong Su', 'Jie Liu']",2025-03-21,2025-03-21
2503.16929v1,TEMPO: Temporal Preference Optimization of Video LLMs via Difficulty Scheduling and Pre-SFT Alignment,"Video Large Language Models (Video LLMs) have achieved significant success by
leveraging a two-stage paradigm: pretraining on large-scale video-text data for
vision-language alignment, followed by supervised fine-tuning (SFT) for
task-specific capabilities. However, existing approaches struggle with temporal
reasoning due to weak temporal correspondence in the data and reliance on the
next-token prediction paradigm during training. To address these limitations,
we propose TEMPO (TEMporal Preference Optimization), a systematic framework
that enhances Video LLMs' temporal reasoning capabilities through Direct
Preference Optimization (DPO). To facilitate this, we introduce an automated
preference data generation pipeline that systematically constructs preference
pairs by selecting videos that are rich in temporal information, designing
video-specific perturbation strategies, and finally evaluating model responses
on clean and perturbed video inputs. Our temporal alignment features two key
innovations: curriculum learning which that progressively increases
perturbation difficulty to improve model robustness and adaptability; and
``Pre-SFT Alignment'', applying preference optimization before instruction
tuning to prioritize fine-grained temporal comprehension. Extensive experiments
demonstrate that our approach consistently improves Video LLM performance
across multiple benchmarks with a relatively small set of self-generated DPO
data. We further analyze the transferability of DPO data across architectures
and the role of difficulty scheduling in optimization. Our findings highlight
our TEMPO as a scalable and efficient complement to SFT-based methods, paving
the way for developing reliable Video LLMs.","['cs.CV', 'cs.AI']","['Shicheng Li', 'Lei Li', 'Kun Ouyang', 'Shuhuai Ren', 'Yuanxin Liu', 'Yuanxing Zhang', 'Fuzheng Zhang', 'Lingpeng Kong', 'Qi Liu', 'Xu Sun']",2025-03-21,2025-03-21
2503.16928v1,MerGen: Micro-electrode recording synthesis using a generative data-driven approach,"The analysis of electrophysiological data is crucial for certain surgical
procedures such as deep brain stimulation, which has been adopted for the
treatment of a variety of neurological disorders. During the procedure,
auditory analysis of these signals helps the clinical team to infer the
neuroanatomical location of the stimulation electrode and thus optimize
clinical outcomes. This task is complex, and requires an expert who in turn
requires significant training. In this paper, we propose a generative neural
network, called MerGen, capable of simulating de novo electrophysiological
recordings, with a view to providing a realistic learning tool for clinicians
trainees for identifying these signals. We demonstrate that the generated
signals are perceptually indistinguishable from real signals by experts in the
field, and that it is even possible to condition the generation efficiently to
provide a didactic simulator adapted to a particular surgical scenario. The
efficacy of this conditioning is demonstrated, comparing it to intra-observer
and inter-observer variability amongst experts. We also demonstrate the use of
this network for data augmentation for automatic signal classification which
can play a role in decision-making support in the operating theatre.",['cs.LG'],"['Thibault Martin', 'Paul Sauleau', 'Claire Haegelen', 'Pierre Jannin', 'John S. H. Baxter']",2025-03-21,2025-03-21
2503.16924v1,Optimized Minimal 3D Gaussian Splatting,"3D Gaussian Splatting (3DGS) has emerged as a powerful representation for
real-time, high-performance rendering, enabling a wide range of applications.
However, representing 3D scenes with numerous explicit Gaussian primitives
imposes significant storage and memory overhead. Recent studies have shown that
high-quality rendering can be achieved with a substantially reduced number of
Gaussians when represented with high-precision attributes. Nevertheless,
existing 3DGS compression methods still rely on a relatively large number of
Gaussians, focusing primarily on attribute compression. This is because a
smaller set of Gaussians becomes increasingly sensitive to lossy attribute
compression, leading to severe quality degradation. Since the number of
Gaussians is directly tied to computational costs, it is essential to reduce
the number of Gaussians effectively rather than only optimizing storage. In
this paper, we propose Optimized Minimal Gaussians representation (OMG), which
significantly reduces storage while using a minimal number of primitives.
First, we determine the distinct Gaussian from the near ones, minimizing
redundancy without sacrificing quality. Second, we propose a compact and
precise attribute representation that efficiently captures both continuity and
irregularity among primitives. Additionally, we propose a sub-vector
quantization technique for improved irregularity representation, maintaining
fast training with a negligible codebook size. Extensive experiments
demonstrate that OMG reduces storage requirements by nearly 50% compared to the
previous state-of-the-art and enables 600+ FPS rendering while maintaining high
rendering quality. Our source code is available at
https://maincold2.github.io/omg/.",['cs.CV'],"['Joo Chan Lee', 'Jong Hwan Ko', 'Eunbyung Park']",2025-03-21,2025-03-21
2503.16922v1,RustEvo^2: An Evolving Benchmark for API Evolution in LLM-based Rust Code Generation,"Large Language Models (LLMs) have become pivotal tools for automating code
generation in software development. However, these models face significant
challenges in producing version-aware code for rapidly evolving languages like
Rust, where frequent Application Programming Interfaces (API) changes across
versions lead to compatibility issues and correctness errors. Existing
benchmarks lack systematic evaluation of how models navigate API transitions,
relying on labor-intensive manual curation and offering limited
version-specific insights. To address this gap, we present RustEvo, a novel
framework for constructing dynamic benchmarks that evaluate the ability of LLMs
to adapt to evolving Rust APIs. RustEvo automates dataset creation by
synthesizing 588 API changes (380 from Rust standard libraries, 208 from 15
third-party crates) into programming tasks mirroring real-world challenges.
These tasks cover four API evolution categories: Stabilizations, Signature
Changes, Behavioral Changes, and Deprecations, reflecting their actual
distribution in the Rust ecosystem.
  Experiments on state-of-the-art (SOTA) LLMs reveal significant performance
variations: models achieve a 65.8% average success rate on stabilized APIs but
only 38.0% on behavioral changes, highlighting difficulties in detecting
semantic shifts without signature alterations. Knowledge cutoff dates strongly
influence performance, with models scoring 56.1% on before-cutoff APIs versus
32.5% on after-cutoff tasks. Retrieval-Augmented Generation (RAG) mitigates
this gap, improving success rates by 13.5% on average for APIs released after
model training. Our findings underscore the necessity of our evolution-aware
benchmarks to advance the adaptability of LLMs in fast-paced software
ecosystems. The framework and the benchmarks are publicly released at
https://github.com/SYSUSELab/RustEvo.","['cs.SE', 'cs.AI']","['Linxi Liang', 'Jing Gong', 'Mingwei Liu', 'Chong Wang', 'Guangsheng Ou', 'Yanlin Wang', 'Xin Peng', 'Zibin Zheng']",2025-03-21,2025-03-21
2503.16921v1,When Preferences Diverge: Aligning Diffusion Models with Minority-Aware Adaptive DPO,"In recent years, the field of image generation has witnessed significant
advancements, particularly in fine-tuning methods that align models with
universal human preferences. This paper explores the critical role of
preference data in the training process of diffusion models, particularly in
the context of Diffusion-DPO and its subsequent adaptations. We investigate the
complexities surrounding universal human preferences in image generation,
highlighting the subjective nature of these preferences and the challenges
posed by minority samples in preference datasets. Through pilot experiments, we
demonstrate the existence of minority samples and their detrimental effects on
model performance. We propose Adaptive-DPO -- a novel approach that
incorporates a minority-instance-aware metric into the DPO objective. This
metric, which includes intra-annotator confidence and inter-annotator
stability, distinguishes between majority and minority samples. We introduce an
Adaptive-DPO loss function which improves the DPO loss in two ways: enhancing
the model's learning of majority labels while mitigating the negative impact of
minority samples. Our experiments demonstrate that this method effectively
handles both synthetic minority data and real-world preference data, paving the
way for more effective training methodologies in image generation tasks.","['cs.CV', 'cs.AI']","['Lingfan Zhang', 'Chen Liu', 'Chengming Xu', 'Kai Hu', 'Donghao Luo', 'Chengjie Wang', 'Yanwei Fu', 'Yuan Yao']",2025-03-21,2025-03-21
2503.16917v1,Malliavin-Bismut Score-based Diffusion Models,"We introduce a new framework that employs Malliavin calculus to derive
explicit expressions for the score function -- i.e., the gradient of the
log-density -- associated with solutions to stochastic differential equations
(SDEs). Our approach integrates classical integration-by-parts techniques with
modern tools, such as Bismut's formula and Malliavin calculus, to address
linear and nonlinear SDEs. In doing so, we establish a rigorous connection
between the Malliavin derivative, its adjoint (the Malliavin divergence or the
Skorokhod integral), Bismut's formula, and diffusion generative models, thus
providing a systematic method for computing $\nabla \log p_t(x)$. For the
linear case, we present a detailed study proving that our formula is equivalent
to the actual score function derived from the solution of the Fokker--Planck
equation for linear SDEs. Additionally, we derive a closed-form expression for
$\nabla \log p_t(x)$ for nonlinear SDEs with state-independent diffusion
coefficients. These advancements provide fresh theoretical insights into the
smoothness and structure of probability densities and practical implications
for score-based generative modelling, including the design and analysis of new
diffusion models. Moreover, our findings promote the adoption of the robust
Malliavin calculus framework in machine learning research. These results
directly apply to various pure and applied mathematics fields, such as
generative modelling, the study of SDEs driven by fractional Brownian motion,
and the Fokker--Planck equations associated with nonlinear SDEs.","['cs.LG', 'math.PR']","['Ehsan Mirafzali', 'Utkarsh Gupta', 'Patrick Wyrod', 'Frank Proske', 'Daniele Venturi', 'Razvan Marinescu']",2025-03-21,2025-03-21
2503.16916v1,Temporal Action Detection Model Compression by Progressive Block Drop,"Temporal action detection (TAD) aims to identify and localize action
instances in untrimmed videos, which is essential for various video
understanding tasks. However, recent improvements in model performance, driven
by larger feature extractors and datasets, have led to increased computational
demands. This presents a challenge for applications like autonomous driving and
robotics, which rely on limited computational resources. While existing channel
pruning methods can compress these models, reducing the number of channels
often hinders the parallelization efficiency of GPU, due to the inefficient
multiplication between small matrices. Instead of pruning channels, we propose
a Progressive Block Drop method that reduces model depth while retaining layer
width. In this way, we still use large matrices for computation but reduce the
number of multiplications. Our approach iteratively removes redundant blocks in
two steps: first, we drop blocks with minimal impact on model performance; and
second, we employ a parameter-efficient cross-depth alignment technique,
fine-tuning the pruned model to restore model accuracy. Our method achieves a
25% reduction in computational overhead on two TAD benchmarks (THUMOS14 and
ActivityNet-1.3) to achieve lossless compression. More critically, we
empirically show that our method is orthogonal to channel pruning methods and
can be combined with it to yield further efficiency gains.",['cs.CV'],"['Xiaoyong Chen', 'Yong Guo', 'Jiaming Liang', 'Sitong Zhuang', 'Runhao Zeng', 'Xiping Hu']",2025-03-21,2025-03-21
2503.16914v1,A New Segment Routing method with Swap Node Selection Strategy Based on Deep Reinforcement Learning for Software Defined Network,"The existing segment routing (SR) methods need to determine the routing first
and then use path segmentation approaches to select swap nodes to form a
segment routing path (SRP). They require re-segmentation of the path when the
routing changes. Furthermore, they do not consider the flow table issuance
time, which cannot maximize the speed of issuance flow table. To address these
issues, this paper establishes an optimization model that can simultaneously
form routing strategies and path segmentation strategies for selecting the
appropriate swap nodes to reduce flow table issuance time. It also designs an
intelligent segment routing algorithm based on deep reinforcement learning
(DRL-SR) to solve the proposed model. First, a traffic matrix is designed as
the state space for the deep reinforcement learning agent; this matrix includes
multiple QoS performance indicators, flow table issuance time overhead and SR
label stack depth. Second, the action selection strategy and corresponding
reward function are designed, where the agent selects the next node considering
the routing; in addition, the action selection strategy whether the newly added
node is selected as the swap node and the corresponding reward function are
designed considering the time cost factor for the controller to issue the flow
table to the swap node. Finally, a series of experiments and their results show
that, compared with the existing methods, the designed segmented route
optimization model and the intelligent solution algorithm (DRL-SR) can reduce
the time overhead required to complete the segmented route establishment task
while optimizing performance metrics such as throughput, delays and packet
losses.",['cs.AI'],"['Miao Ye', 'Jihao Zheng', 'Qiuxiang Jiang', 'Yuan Huang', 'Ziheng Wang', 'Yong Wang']",2025-03-21,2025-03-21
2503.16910v1,Salient Object Detection in Traffic Scene through the TSOD10K Dataset,"Traffic Salient Object Detection (TSOD) aims to segment the objects critical
to driving safety by combining semantic (e.g., collision risks) and visual
saliency. Unlike SOD in natural scene images (NSI-SOD), which prioritizes
visually distinctive regions, TSOD emphasizes the objects that demand immediate
driver attention due to their semantic impact, even with low visual contrast.
This dual criterion, i.e., bridging perception and contextual risk, re-defines
saliency for autonomous and assisted driving systems. To address the lack of
task-specific benchmarks, we collect the first large-scale TSOD dataset with
pixel-wise saliency annotations, named TSOD10K. TSOD10K covers the diverse
object categories in various real-world traffic scenes under various
challenging weather/illumination variations (e.g., fog, snowstorms,
low-contrast, and low-light). Methodologically, we propose a Mamba-based TSOD
model, termed Tramba. Considering the challenge of distinguishing inconspicuous
visual information from complex traffic backgrounds, Tramba introduces a novel
Dual-Frequency Visual State Space module equipped with shifted window
partitioning and dilated scanning to enhance the perception of fine details and
global structure by hierarchically decomposing high/low-frequency components.
To emphasize critical regions in traffic scenes, we propose a traffic-oriented
Helix 2D-Selective-Scan (Helix-SS2D) mechanism that injects driving attention
priors while effectively capturing global multi-direction spatial dependencies.
We establish a comprehensive benchmark by evaluating Tramba and 22 existing
NSI-SOD models on TSOD10K, demonstrating Tramba's superiority. Our research
establishes the first foundation for safety-aware saliency analysis in
intelligent transportation systems.",['cs.CV'],"['Yu Qiu', 'Yuhang Sun', 'Jie Mei', 'Lin Xiao', 'Jing Xu']",2025-03-21,2025-03-21
2503.16905v1,MAPS: A Multi-Agent Framework Based on Big Seven Personality and Socratic Guidance for Multimodal Scientific Problem Solving,"Multimodal scientific problems (MSPs) involve complex issues that require the
integration of multiple modalities, such as text and diagrams, presenting a
significant challenge in artificial intelligence. While progress has been made
in addressing traditional scientific problems, MSPs still face two primary
issues: the challenge of multi-modal comprehensive reasoning in scientific
problem-solving and the lack of reflective and rethinking capabilities. To
address these issues, we introduce a Multi-Agent framework based on the Big
Seven Personality and Socratic guidance (MAPS). This framework employs seven
distinct agents that leverage feedback mechanisms and the Socratic method to
guide the resolution of MSPs. To tackle the first issue, we propose a
progressive four-agent solving strategy, where each agent focuses on a specific
stage of the problem-solving process. For the second issue, we introduce a
Critic agent, inspired by Socratic questioning, which prompts critical thinking
and stimulates autonomous learning. We conduct extensive experiments on the
EMMA, Olympiad, and MathVista datasets, achieving promising results that
outperform the current SOTA model by 15.84% across all tasks. Meanwhile, the
additional analytical experiments also verify the model's progress as well as
generalization ability.",['cs.AI'],"['Jian Zhang', 'Zhiyuan Wang', 'Zhangqi Wang', 'Xinyu Zhang', 'Fangzhi Xu', 'Qika Lin', 'Rui Mao', 'Erik Cambria', 'Jun Liu']",2025-03-21,2025-03-21
2503.16904v1,Deep Learning for Human Locomotion Analysis in Lower-Limb Exoskeletons: A Comparative Study,"Wearable robotics for lower-limb assistance have become a pivotal area of
research, aiming to enhance mobility for individuals with physical impairments
or augment the performance of able-bodied users. Accurate and adaptive control
systems are essential to ensure seamless interaction between the wearer and the
robotic device, particularly when navigating diverse and dynamic terrains.
Despite the recent advances in neural networks for time series analysis, no
attempts have been directed towards the classification of ground conditions,
categorized into five classes and subsequently determining the ramp's slope and
stair's height. In this respect, this paper presents an experimental comparison
between eight deep neural network backbones to predict high-level locomotion
parameters across diverse terrains.
  All the models are trained on the publicly available CAMARGO 2021 dataset.
IMU-only data equally or outperformed IMU+EMG inputs, promoting a
cost-effective and efficient design. Indeeds, using three IMU sensors, the LSTM
achieved high terrain classification accuracy (0.94 +- 0.04) and precise ramp
slope (1.95 +- 0.58{\deg}) and the CNN-LSTM a stair height (15.65 +- 7.40 mm)
estimations. As a further contribution, SHAP analysis justified sensor
reduction without performance loss, ensuring a lightweight setup. The system
operates with ~2 ms inference time, supporting real-time applications. The code
is code available at
https://github.com/cosbidev/Human-Locomotion-Identification.","['cs.RO', 'cs.AI', 'F.2.2, I.2.7']","['Omar Coser', 'Christian Tamantini', 'Matteo Tortora', 'Leonardo Furia', 'Rosa Sicilia', 'Loredana Zollo', 'Paolo Soda']",2025-03-21,2025-03-21
2503.16901v1,TeMP-TraG: Edge-based Temporal Message Passing in Transaction Graphs,"Transaction graphs, which represent financial and trade transactions between
entities such as bank accounts and companies, can reveal patterns indicative of
financial crimes like money laundering and fraud. However, effective detection
of such cases requires node and edge classification methods capable of
addressing the unique challenges of transaction graphs, including rich edge
features, multigraph structures and temporal dynamics. To tackle these
challenges, we propose TeMP-TraG, a novel graph neural network mechanism that
incorporates temporal dynamics into message passing. TeMP-TraG prioritises more
recent transactions when aggregating node messages, enabling better detection
of time-sensitive patterns. We demonstrate that TeMP-TraG improves four
state-of-the-art graph neural networks by 6.19% on average. Our results
highlight TeMP-TraG as an advancement in leveraging transaction graphs to
combat financial crime.",['cs.LG'],"['Steve Gounoue', 'Ashutosh Sao', 'Simon Gottschalk']",2025-03-21,2025-03-21
2503.16893v1,Improving the End-to-End Efficiency of Offline Inference for Multi-LLM Applications Based on Sampling and Simulation,"As large language models (LLMs) have shown great success in many tasks, they
are used in various applications. While a lot of works have focused on the
efficiency of single-LLM application (e.g., offloading, request scheduling,
parallelism strategy selection), multi-LLM applications receive less attention,
particularly in offline inference scenarios. In this work, we aim to improve
the offline end-to-end inference efficiency of multi-LLM applications in the
single-node multi-GPU environment. The problem involves two key decisions: (1)
determining which LLMs to run concurrently each time (we may not run all the
models at the same time), and (2) selecting a parallelism strategy to use for
each LLM. This problem is NP-hard. Naive solutions may not work well because
the running time for a model to complete a set of requests depends on the
request workload and the selected parallelism strategy, and they lack an
accurate model of the running time. As the LLM output lengths are unknown
before running, to estimate the model running time, we propose a
sampling-then-simulation method which first estimates the output lengths by
sampling from an empirical cumulative function we obtained from a large dataset
in advance, and then simulates the LLM inference process accordingly. Based on
the simulation, we estimate the per-iteration latencys to get the total
latency. A greedy method is proposed to optimize the scheduling of the LLMs in
the application across the GPUs. We then propose a framework SamuLLM which
contains two phases: planning, which calls the greedy method for an application
and running, which runs the application and dynamically adjust the model
scheduling based on the runtime information. Experiments on 3 applications and
a mixed application show that SamuLLM can achieve 1.0-2.4$\times$ end-to-end
speedups compared to the competitors.","['cs.DC', 'cs.LG']","['Jingzhi Fang', 'Yanyan Shen', 'Yue Wang', 'Lei Chen']",2025-03-21,2025-03-21
2503.16883v1,Assessing the Reliability and Validity of GPT-4 in Annotating Emotion Appraisal Ratings,"Appraisal theories suggest that emotions arise from subjective evaluations of
events, referred to as appraisals. The taxonomy of appraisals is quite diverse,
and they are usually given ratings on a Likert scale to be annotated in an
experiencer-annotator or reader-annotator paradigm. This paper studies GPT-4 as
a reader-annotator of 21 specific appraisal ratings in different prompt
settings, aiming to evaluate and improve its performance compared to human
annotators. We found that GPT-4 is an effective reader-annotator that performs
close to or even slightly better than human annotators, and its results can be
significantly improved by using a majority voting of five completions. GPT-4
also effectively predicts appraisal ratings and emotion labels using a single
prompt, but adding instruction complexity results in poorer performance. We
also found that longer event descriptions lead to more accurate annotations for
both model and human annotator ratings. This work contributes to the growing
usage of LLMs in psychology and the strategies for improving GPT-4 performance
in annotating appraisals.",['cs.CL'],"['Deniss Ruder', 'Andero Uusberg', 'Kairit Sirts']",2025-03-21,2025-03-21
2503.16875v1,Federated Cross-Domain Click-Through Rate Prediction With Large Language Model Augmentation,"Accurately predicting click-through rates (CTR) under stringent privacy
constraints poses profound challenges, particularly when user-item interactions
are sparse and fragmented across domains. Conventional cross-domain CTR (CCTR)
methods frequently assume homogeneous feature spaces and rely on centralized
data sharing, neglecting complex inter-domain discrepancies and the subtle
trade-offs imposed by privacy-preserving protocols. Here, we present Federated
Cross-Domain CTR Prediction with Large Language Model Augmentation
(FedCCTR-LM), a federated framework engineered to address these limitations by
synchronizing data augmentation, representation disentanglement, and adaptive
privacy protection. Our approach integrates three core innovations. First, the
Privacy-Preserving Augmentation Network (PrivAugNet) employs large language
models to enrich user and item representations and expand interaction
sequences, mitigating data sparsity and feature incompleteness. Second, the
Independent Domain-Specific Transformer with Contrastive Learning (IDST-CL)
module disentangles domain-specific and shared user preferences, employing
intra-domain representation alignment (IDRA) and crossdomain representation
disentanglement (CDRD) to refine the learned embeddings and enhance knowledge
transfer across domains. Finally, the Adaptive Local Differential Privacy
(AdaLDP) mechanism dynamically calibrates noise injection to achieve an optimal
balance between rigorous privacy guarantees and predictive accuracy. Empirical
evaluations on four real-world datasets demonstrate that FedCCTR-LM
substantially outperforms existing baselines, offering robust,
privacy-preserving, and generalizable cross-domain CTR prediction in
heterogeneous, federated environments.","['cs.IR', 'cs.CL', 'cs.DC']","['Jiangcheng Qin', 'Xueyuan Zhang', 'Baisong Liu', 'Jiangbo Qian', 'Yangyang Wang']",2025-03-21,2025-03-21
2503.16874v1,MARS: A Multi-Agent Framework Incorporating Socratic Guidance for Automated Prompt Optimization,"The basic question-answering format of large language models involves
inputting a prompt and receiving a response, and the quality of the prompt
directly impacts the effectiveness of the response. Automated Prompt
Optimization (APO) aims to break free from the cognitive biases of manually
designed prompts and explores a broader design space for prompts. However,
existing APO methods suffer from limited flexibility of fixed templates and
inefficient search in prompt spaces as key issues. To this end, we propose a
Multi-Agent framework Incorporating Socratic guidance (MARS), which utilizes
multi-agent fusion technology for automatic planning, with gradual continuous
optimization and evaluation. Specifically, MARS comprises seven agents, each
with distinct functionalities, which autonomously use the Planner to devise an
optimization path that ensures flexibility. Additionally, it employs a
Teacher-Critic-Student Socratic dialogue pattern to iteratively optimize the
prompts while conducting effective search. We conduct extensive experiments on
various datasets to validate the effectiveness of our method, and perform
additional analytical experiments to assess the model's advancement as well as
the interpretability.","['cs.CL', 'cs.AI']","['Jian Zhang', 'Zhangqi Wang', 'Haiping Zhu', 'Jun Liu', 'Qika Lin', 'Erik Cambria']",2025-03-21,2025-03-21
2503.16873v1,Classifier-guided CLIP Distillation for Unsupervised Multi-label Classification,"Multi-label classification is crucial for comprehensive image understanding,
yet acquiring accurate annotations is challenging and costly. To address this,
a recent study suggests exploiting unsupervised multi-label classification
leveraging CLIP, a powerful vision-language model. Despite CLIP's proficiency,
it suffers from view-dependent predictions and inherent bias, limiting its
effectiveness. We propose a novel method that addresses these issues by
leveraging multiple views near target objects, guided by Class Activation
Mapping (CAM) of the classifier, and debiasing pseudo-labels derived from CLIP
predictions. Our Classifier-guided CLIP Distillation (CCD) enables selecting
multiple local views without extra labels and debiasing predictions to enhance
classification performance. Experimental results validate our method's
superiority over existing techniques across diverse datasets. The code is
available at https://github.com/k0u-id/CCD.","['cs.CV', 'cs.AI']","['Dongseob Kim', 'Hyunjung Shim']",2025-03-21,2025-03-21
2503.16872v1,Lie Detector: Unified Backdoor Detection via Cross-Examination Framework,"Institutions with limited data and computing resources often outsource model
training to third-party providers in a semi-honest setting, assuming adherence
to prescribed training protocols with pre-defined learning paradigm (e.g.,
supervised or semi-supervised learning). However, this practice can introduce
severe security risks, as adversaries may poison the training data to embed
backdoors into the resulting model. Existing detection approaches predominantly
rely on statistical analyses, which often fail to maintain universally accurate
detection accuracy across different learning paradigms. To address this
challenge, we propose a unified backdoor detection framework in the semi-honest
setting that exploits cross-examination of model inconsistencies between two
independent service providers. Specifically, we integrate central kernel
alignment to enable robust feature similarity measurements across different
model architectures and learning paradigms, thereby facilitating precise
recovery and identification of backdoor triggers. We further introduce backdoor
fine-tuning sensitivity analysis to distinguish backdoor triggers from
adversarial perturbations, substantially reducing false positives. Extensive
experiments demonstrate that our method achieves superior detection
performance, improving accuracy by 5.4%, 1.6%, and 11.9% over SoTA baselines
across supervised, semi-supervised, and autoregressive learning tasks,
respectively. Notably, it is the first to effectively detect backdoors in
multimodal large language models, further highlighting its broad applicability
and advancing secure deep learning.","['cs.LG', 'cs.CV']","['Xuan Wang', 'Siyuan Liang', 'Dongping Liao', 'Han Fang', 'Aishan Liu', 'Xiaochun Cao', 'Yu-liang Lu', 'Ee-Chien Chang', 'Xitong Gao']",2025-03-21,2025-03-21
2503.16870v1,Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs,"Knowledge distillation can be a cost-effective technique to distill knowledge
in Large Language Models, if the teacher output logits can be pre-computed and
cached. However, successfully applying this to pre-training remains largely
unexplored. In this work, we prove that naive approaches for sparse knowledge
distillation such as caching Top-K probabilities, while intuitive, provide
biased estimates of teacher probability distribution to the student, resulting
in suboptimal performance and calibration. We propose an
importance-sampling-based method `Random Sampling Knowledge Distillation',
which provides unbiased estimates, preserves the gradient in expectation, and
requires storing significantly sparser logits. Our method enables faster
training of student models with marginal overhead (<10%) compared to
cross-entropy based training, while maintaining competitive performance
compared to full distillation, across a range of model sizes from 300M to 3B.","['cs.LG', 'cs.AI', 'cs.CL', '68T50', 'I.2.7']","['Anshumann', 'Mohd Abbas Zaidi', 'Akhil Kedia', 'Jinwoo Ahn', 'Taehwak Kwon', 'Kangwook Lee', 'Haejun Lee', 'Joohyung Lee']",2025-03-21,2025-03-21
2503.16868v1,Joint Extraction Matters: Prompt-Based Visual Question Answering for Multi-Field Document Information Extraction,"Visual question answering (VQA) has emerged as a flexible approach for
extracting specific pieces of information from document images. However,
existing work typically queries each field in isolation, overlooking potential
dependencies across multiple items. This paper investigates the merits of
extracting multiple fields jointly versus separately. Through experiments on
multiple large vision language models and datasets, we show that jointly
extracting fields often improves accuracy, especially when the fields share
strong numeric or contextual dependencies. We further analyze how performance
scales with the number of requested items and use a regression based metric to
quantify inter field relationships. Our results suggest that multi field
prompts can mitigate confusion arising from similar surface forms and related
numeric values, providing practical methods for designing robust VQA systems in
document information extraction tasks.","['cs.CL', 'cs.CV']","['Mengsay Loem', 'Taiju Hosaka']",2025-03-21,2025-03-21
2503.16867v1,ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question Generation and Answering,"Precisely evaluating semantic alignment between text prompts and generated
videos remains a challenge in Text-to-Video (T2V) Generation. Existing
text-to-video alignment metrics like CLIPScore only generate coarse-grained
scores without fine-grained alignment details, failing to align with human
preference. To address this limitation, we propose ETVA, a novel Evaluation
method of Text-to-Video Alignment via fine-grained question generation and
answering. First, a multi-agent system parses prompts into semantic scene
graphs to generate atomic questions. Then we design a knowledge-augmented
multi-stage reasoning framework for question answering, where an auxiliary LLM
first retrieves relevant common-sense knowledge (e.g., physical laws), and then
video LLM answers the generated questions through a multi-stage reasoning
mechanism. Extensive experiments demonstrate that ETVA achieves a Spearman's
correlation coefficient of 58.47, showing a much higher correlation with human
judgment than existing metrics which attain only 31.0. We also construct a
comprehensive benchmark specifically designed for text-to-video alignment
evaluation, featuring 2k diverse prompts and 12k atomic questions spanning 10
categories. Through a systematic evaluation of 15 existing text-to-video
models, we identify their key capabilities and limitations, paving the way for
next-generation T2V generation.",['cs.CV'],"['Kaisi Guan', 'Zhengfeng Lai', 'Yuchong Sun', 'Peng Zhang', 'Wei Liu', 'Kieran Liu', 'Meng Cao', 'Ruihua Song']",2025-03-21,2025-03-21
2503.16865v1,Nonparametric Factor Analysis and Beyond,"Nearly all identifiability results in unsupervised representation learning
inspired by, e.g., independent component analysis, factor analysis, and causal
representation learning, rely on assumptions of additive independent noise or
noiseless regimes. In contrast, we study the more general case where noise can
take arbitrary forms, depend on latent variables, and be non-invertibly
entangled within a nonlinear function. We propose a general framework for
identifying latent variables in the nonparametric noisy settings. We first show
that, under suitable conditions, the generative model is identifiable up to
certain submanifold indeterminacies even in the presence of non-negligible
noise. Furthermore, under the structural or distributional variability
conditions, we prove that latent variables of the general nonlinear models are
identifiable up to trivial indeterminacies. Based on the proposed theoretical
framework, we have also developed corresponding estimation methods and
validated them in various synthetic and real-world settings. Interestingly, our
estimate of the true GDP growth from alternative measurements suggests more
insightful information on the economies than official reports. We expect our
framework to provide new insight into how both researchers and practitioners
deal with latent variables in real-world scenarios.","['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']","['Yujia Zheng', 'Yang Liu', 'Jiaxiong Yao', 'Yingyao Hu', 'Kun Zhang']",2025-03-21,2025-03-21
2503.16862v1,City2Scene: Improving Acoustic Scene Classification with City Features,"Acoustic scene recordings are often collected from a diverse range of cities.
Most existing acoustic scene classification (ASC) approaches focus on
identifying common acoustic scene patterns across cities to enhance
generalization. In contrast, we hypothesize that city-specific environmental
and cultural differences in acoustic features are beneficial for the ASC task.
In this paper, we introduce City2Scene, a novel framework that leverages city
features to improve ASC. City2Scene transfers the city-specific knowledge from
city classification models to a scene classification model using knowledge
distillation. We evaluated City2Scene on the DCASE Challenge Task 1 datasets,
where each audio clip is annotated with both scene and city labels.
Experimental results demonstrate that city features provide valuable
information for classifying scenes. By distilling the city-specific knowledge,
City2Scene effectively improves accuracy for various state-of-the-art ASC
backbone models, including both CNNs and Transformers.","['cs.SD', 'cs.CV', 'eess.AS']","['Yiqiang Cai', 'Yizhou Tan', 'Peihong Zhang', 'Yuxuan Liu', 'Shengchen Li', 'Xi Shao', 'Mark D. Plumbley']",2025-03-21,2025-03-21
2503.16861v1,In-House Evaluation Is Not Enough: Towards Robust Third-Party Flaw Disclosure for General-Purpose AI,"The widespread deployment of general-purpose AI (GPAI) systems introduces
significant new risks. Yet the infrastructure, practices, and norms for
reporting flaws in GPAI systems remain seriously underdeveloped, lagging far
behind more established fields like software security. Based on a collaboration
between experts from the fields of software security, machine learning, law,
social science, and policy, we identify key gaps in the evaluation and
reporting of flaws in GPAI systems. We call for three interventions to advance
system safety. First, we propose using standardized AI flaw reports and rules
of engagement for researchers in order to ease the process of submitting,
reproducing, and triaging flaws in GPAI systems. Second, we propose GPAI system
providers adopt broadly-scoped flaw disclosure programs, borrowing from bug
bounties, with legal safe harbors to protect researchers. Third, we advocate
for the development of improved infrastructure to coordinate distribution of
flaw reports across the many stakeholders who may be impacted. These
interventions are increasingly urgent, as evidenced by the prevalence of
jailbreaks and other flaws that can transfer across different providers' GPAI
systems. By promoting robust reporting and coordination in the AI ecosystem,
these proposals could significantly improve the safety, security, and
accountability of GPAI systems.",['cs.AI'],"['Shayne Longpre', 'Kevin Klyman', 'Ruth E. Appel', 'Sayash Kapoor', 'Rishi Bommasani', 'Michelle Sahar', 'Sean McGregor', 'Avijit Ghosh', 'Borhane Blili-Hamelin', 'Nathan Butters', 'Alondra Nelson', 'Amit Elazari', 'Andrew Sellars', 'Casey John Ellis', 'Dane Sherrets', 'Dawn Song', 'Harley Geiger', 'Ilona Cohen', 'Lauren McIlvenny', 'Madhulika Srikumar', 'Mark M. Jaycox', 'Markus Anderljung', 'Nadine Farid Johnson', 'Nicholas Carlini', 'Nicolas Miailhe', 'Nik Marda', 'Peter Henderson', 'Rebecca S. Portnoff', 'Rebecca Weiss', 'Victoria Westerhoff', 'Yacine Jernite', 'Rumman Chowdhury', 'Percy Liang', 'Arvind Narayanan']",2025-03-21,2025-03-21
2503.16860v1,PRIOT: Pruning-Based Integer-Only Transfer Learning for Embedded Systems,"On-device transfer learning is crucial for adapting a common backbone model
to the unique environment of each edge device. Tiny microcontrollers, such as
the Raspberry Pi Pico, are key targets for on-device learning but often lack
floating-point units, necessitating integer-only training. Dynamic computation
of quantization scale factors, which is adopted in former studies, incurs high
computational costs. Therefore, this study focuses on integer-only training
with static scale factors, which is challenging with existing training methods.
We propose a new training method named PRIOT, which optimizes the network by
pruning selected edges rather than updating weights, allowing effective
training with static scale factors. The pruning pattern is determined by the
edge-popup algorithm, which trains a parameter named score assigned to each
edge instead of the original parameters and prunes the edges with low scores
before inference. Additionally, we introduce a memory-efficient variant,
PRIOT-S, which only assigns scores to a small fraction of edges. We implement
PRIOT and PRIOT-S on the Raspberry Pi Pico and evaluate their accuracy and
computational costs using a tiny CNN model on the rotated MNIST dataset and the
VGG11 model on the rotated CIFAR-10 dataset. Our results demonstrate that PRIOT
improves accuracy by 8.08 to 33.75 percentage points over existing methods,
while PRIOT-S reduces memory footprint with minimal accuracy loss.",['cs.LG'],"['Honoka Anada', 'Sefutsu Ryu', 'Masayuki Usui', 'Tatsuya Kaneko', 'Shinya Takamaeda-Yamazaki']",2025-03-21,2025-03-21
2503.16858v1,MTBench: A Multimodal Time Series Benchmark for Temporal Reasoning and Question Answering,"Understanding the relationship between textual news and time-series evolution
is a critical yet under-explored challenge in applied data science. While
multimodal learning has gained traction, existing multimodal time-series
datasets fall short in evaluating cross-modal reasoning and complex question
answering, which are essential for capturing complex interactions between
narrative information and temporal patterns. To bridge this gap, we introduce
Multimodal Time Series Benchmark (MTBench), a large-scale benchmark designed to
evaluate large language models (LLMs) on time series and text understanding
across financial and weather domains. MTbench comprises paired time series and
textual data, including financial news with corresponding stock price movements
and weather reports aligned with historical temperature records. Unlike
existing benchmarks that focus on isolated modalities, MTbench provides a
comprehensive testbed for models to jointly reason over structured numerical
trends and unstructured textual narratives. The richness of MTbench enables
formulation of diverse tasks that require a deep understanding of both text and
time-series data, including time-series forecasting, semantic and technical
trend analysis, and news-driven question answering (QA). These tasks target the
model's ability to capture temporal dependencies, extract key insights from
textual context, and integrate cross-modal information. We evaluate
state-of-the-art LLMs on MTbench, analyzing their effectiveness in modeling the
complex relationships between news narratives and temporal patterns. Our
findings reveal significant challenges in current models, including
difficulties in capturing long-term dependencies, interpreting causality in
financial and weather trends, and effectively fusing multimodal information.","['cs.CL', 'cs.AI']","['Jialin Chen', 'Aosong Feng', 'Ziyu Zhao', 'Juan Garza', 'Gaukhar Nurbek', 'Cheng Qin', 'Ali Maatouk', 'Leandros Tassiulas', 'Yifeng Gao', 'Rex Ying']",2025-03-21,2025-03-21
2503.16856v1,MMCR: Benchmarking Cross-Source Reasoning in Scientific Papers,"Fully comprehending scientific papers by machines reflects a high level of
Artificial General Intelligence, requiring the ability to reason across
fragmented and heterogeneous sources of information, presenting a complex and
practically significant challenge. While Vision-Language Models (VLMs) have
made remarkable strides in various tasks, particularly those involving
reasoning with evidence source from single image or text page, their ability to
use cross-source information for reasoning remains an open problem. This work
presents MMCR, a high-difficulty benchmark designed to evaluate VLMs' capacity
for reasoning with cross-source information from scientific papers. The
benchmark comprises 276 high-quality questions, meticulously annotated by
humans across 7 subjects and 10 task types. Experiments with 18 VLMs
demonstrate that cross-source reasoning presents a substantial challenge for
existing models. Notably, even the top-performing model, GPT-4o, achieved only
48.55% overall accuracy, with only 20% accuracy in multi-table comprehension
tasks, while the second-best model, Qwen2.5-VL-72B, reached 39.86% overall
accuracy. Furthermore, we investigated the impact of the Chain-of-Thought (CoT)
technique on cross-source reasoning and observed a detrimental effect on small
models, whereas larger models demonstrated substantially enhanced performance.
These results highlight the pressing need to develop VLMs capable of
effectively utilizing cross-source information for reasoning.",['cs.CL'],"['Yang Tian', 'Zheng Lu', 'Mingqi Gao', 'Zheng Liu', 'Bo Zhao']",2025-03-21,2025-03-21
2503.16855v1,Stack Transformer Based Spatial-Temporal Attention Model for Dynamic Multi-Culture Sign Language Recognition,"Hand gesture-based Sign Language Recognition (SLR) serves as a crucial
communication bridge between deaf and non-deaf individuals. Existing SLR
systems perform well for their cultural SL but may struggle with multi-cultural
sign languages (McSL). To address these challenges, this paper proposes a Stack
Spatial-Temporal Transformer Network that leverages multi-head attention
mechanisms to capture both spatial and temporal dependencies with hierarchical
features using the Stack Transfer concept. In the proceed, firstly, we applied
a fully connected layer to make a embedding vector which has high expressive
power from the original dataset, then fed them a stack newly proposed
transformer to achieve hierarchical features with short-range and long-range
dependency. The network architecture is composed of several stages that process
spatial and temporal relationships sequentially, ensuring effective feature
extraction. After making the fully connected layer, the embedding vector is
processed by the Spatial Multi-Head Attention Transformer, which captures
spatial dependencies between joints. In the next stage, the Temporal Multi-Head
Attention Transformer captures long-range temporal dependencies, and again, the
features are concatenated with the output using another skip connection. The
processed features are then passed to the Feed-Forward Network (FFN), which
refines the feature representations further. After the FFN, additional skip
connections are applied to combine the output with earlier layers, followed by
a final normalization layer to produce the final output feature tensor. This
process is repeated for 10 transformer blocks. The extensive experiment shows
that the JSL, KSL and ASL datasets achieved good performance accuracy. Our
approach demonstrates improved performance in McSL, and it will be consider as
a novel work in this domain.",['cs.CV'],"['Koki Hirooka', 'Abu Saleh Musa Miah', 'Tatsuya Murakami', 'Yuto Akiba', 'Yong Seok Hwang', 'Jungpil Shin']",2025-03-21,2025-03-21
2503.16854v1,Generative Compositor for Few-Shot Visual Information Extraction,"Visual Information Extraction (VIE), aiming at extracting structured
information from visually rich document images, plays a pivotal role in
document processing. Considering various layouts, semantic scopes, and
languages, VIE encompasses an extensive range of types, potentially numbering
in the thousands. However, many of these types suffer from a lack of training
data, which poses significant challenges. In this paper, we propose a novel
generative model, named Generative Compositor, to address the challenge of
few-shot VIE. The Generative Compositor is a hybrid pointer-generator network
that emulates the operations of a compositor by retrieving words from the
source text and assembling them based on the provided prompts. Furthermore,
three pre-training strategies are employed to enhance the model's perception of
spatial context information. Besides, a prompt-aware resampler is specially
designed to enable efficient matching by leveraging the entity-semantic prior
contained in prompts. The introduction of the prompt-based retrieval mechanism
and the pre-training strategies enable the model to acquire more effective
spatial and semantic clues with limited training samples. Experiments
demonstrate that the proposed method achieves highly competitive results in the
full-sample training, while notably outperforms the baseline in the 1-shot,
5-shot, and 10-shot settings.",['cs.CV'],"['Zhibo Yang', 'Wei Hua', 'Sibo Song', 'Cong Yao', 'Yingying Zhu', 'Wenqing Cheng', 'Xiang Bai']",2025-03-21,2025-03-21
2503.16853v1,Imagine to Hear: Auditory Knowledge Generation can be an Effective Assistant for Language Models,"Language models pretrained on text-only corpora often struggle with tasks
that require auditory commonsense knowledge. Previous work addresses this
problem by augmenting the language model to retrieve knowledge from external
audio databases. This approach has several limitations, such as the potential
lack of relevant audio in databases and the high costs associated with
constructing and querying the databases. To address these issues, we propose
Imagine to Hear, a novel approach that dynamically generates auditory knowledge
using generative models. Our framework detects multiple audio-related textual
spans from the given prompt and generates corresponding auditory knowledge. We
develop several mechanisms to efficiently process multiple auditory knowledge,
including a CLAP-based rejection sampler and a language-audio fusion module.
Our experiments show that our method achieves state-of-the-art performance on
AuditoryBench without relying on external databases, highlighting the
effectiveness of our generation-based approach.","['cs.CL', 'cs.AI', 'cs.LG', 'cs.SD', 'eess.AS']","['Suho Yoo', 'Hyunjong Ok', 'Jaeho Lee']",2025-03-21,2025-03-21
2503.16852v1,Casual Inference via Style Bias Deconfounding for Domain Generalization,"Deep neural networks (DNNs) often struggle with out-of-distribution data,
limiting their reliability in diverse realworld applications. To address this
issue, domain generalization methods have been developed to learn
domain-invariant features from single or multiple training domains, enabling
generalization to unseen testing domains. However, existing approaches usually
overlook the impact of style frequency within the training set. This oversight
predisposes models to capture spurious visual correlations caused by style
confounding factors, rather than learning truly causal representations, thereby
undermining inference reliability. In this work, we introduce Style
Deconfounding Causal Learning (SDCL), a novel causal inference-based framework
designed to explicitly address style as a confounding factor. Our approaches
begins with constructing a structural causal model (SCM) tailored to the domain
generalization problem and applies a backdoor adjustment strategy to account
for style influence. Building on this foundation, we design a style-guided
expert module (SGEM) to adaptively clusters style distributions during
training, capturing the global confounding style. Additionally, a back-door
causal learning module (BDCL) performs causal interventions during feature
extraction, ensuring fair integration of global confounding styles into sample
predictions, effectively reducing style bias. The SDCL framework is highly
versatile and can be seamlessly integrated with state-of-the-art data
augmentation techniques. Extensive experiments across diverse natural and
medical image recognition tasks validate its efficacy, demonstrating superior
performance in both multi-domain and the more challenging single-domain
generalization scenarios.","['cs.CV', 'cs.AI']","['Jiaxi Li', 'Di Lin', 'Hao Chen', 'Hongying Liu', 'Liang Wan', 'Wei Feng']",2025-03-21,2025-03-21
2503.16851v1,Towards LLM Guardrails via Sparse Representation Steering,"Large Language Models (LLMs) have demonstrated remarkable performance in
natural language generation tasks, yet their uncontrolled outputs pose
significant ethical and safety risks. Recently, representation engineering
methods have shown promising results in steering model behavior by modifying
the rich semantic information encoded in activation vectors. However, due to
the difficulty of precisely disentangling semantic directions within
high-dimensional representation space, existing approaches suffer from three
major limitations: lack of fine-grained control, quality degradation of
generated content, and poor interpretability. To address these challenges, we
propose a sparse encoding-based representation engineering method, named SRE,
which decomposes polysemantic activations into a structured, monosemantic
feature space. By leveraging sparse autoencoding, our approach isolates and
adjusts only task-specific sparse feature dimensions, enabling precise and
interpretable steering of model behavior while preserving content quality. We
validate our method on three critical domains, i.e., safety, fairness, and
truthfulness using the open-source LLM Gemma-2-2B-it. Experimental results show
that SRE achieves superior controllability while maintaining the overall
quality of generated content (i.e., controllability and quality), demonstrating
its effectiveness as a fine-grained and interpretable activation steering
framework.","['cs.CR', 'cs.CL']","['Zeqing He', 'Zhibo Wang', 'Huiyu Xu', 'Kui Ren']",2025-03-21,2025-03-21
2503.16850v1,Physics-Informed Neural Network Surrogate Models for River Stage Prediction,"This work investigates the feasibility of using Physics-Informed Neural
Networks (PINNs) as surrogate models for river stage prediction, aiming to
reduce computational cost while maintaining predictive accuracy. Our primary
contribution demonstrates that PINNs can successfully approximate HEC-RAS
numerical solutions when trained on a single river, achieving strong predictive
accuracy with generally low relative errors, though some river segments exhibit
higher deviations.
  By integrating the governing Saint-Venant equations into the learning
process, the proposed PINN-based surrogate model enforces physical consistency
and significantly improves computational efficiency compared to HEC-RAS. We
evaluate the model's performance in terms of accuracy and computational speed,
demonstrating that it closely approximates HEC-RAS predictions while enabling
real-time inference.
  These results highlight the potential of PINNs as effective surrogate models
for single-river hydrodynamics, offering a promising alternative for
computationally efficient river stage forecasting. Future work will explore
techniques to enhance PINN training stability and robustness across a more
generalized multi-river model.","['cs.LG', 'cs.AI']","['Maximilian Zoch', 'Edward Holmberg', 'Pujan Pokhrel', 'Ken Pathak', 'Steven Sloan', 'Kendall Niles', 'Jay Ratcliff', 'Maik Flanagin', 'Elias Ioup', 'Christian Guetl', 'Mahdi Abdelguerfi']",2025-03-21,2025-03-21
2503.16848v1,HSM: Hierarchical Scene Motifs for Multi-Scale Indoor Scene Generation,"Despite advances in indoor 3D scene layout generation, synthesizing scenes
with dense object arrangements remains challenging. Existing methods primarily
focus on large furniture while neglecting smaller objects, resulting in
unrealistically empty scenes. Those that place small objects typically do not
honor arrangement specifications, resulting in largely random placement not
following the text description. We present HSM, a hierarchical framework for
indoor scene generation with dense object arrangements across spatial scales.
Indoor scenes are inherently hierarchical, with surfaces supporting objects at
different scales, from large furniture on floors to smaller objects on tables
and shelves. HSM embraces this hierarchy and exploits recurring cross-scale
spatial patterns to generate complex and realistic indoor scenes in a unified
manner. Our experiments show that HSM outperforms existing methods by
generating scenes that are more realistic and better conform to user input
across room types and spatial configurations.","['cs.GR', 'cs.CV']","['Hou In Derek Pun', 'Hou In Ivan Tam', 'Austin T. Wang', 'Xiaoliang Huo', 'Angel X. Chang', 'Manolis Savva']",2025-03-21,2025-03-21
2503.16846v1,An Accelerated Bregman Algorithm for ReLU-based Symmetric Matrix Decomposition,"Symmetric matrix decomposition is an active research area in machine
learning. This paper focuses on exploiting the low-rank structure of
non-negative and sparse symmetric matrices via the rectified linear unit (ReLU)
activation function. We propose the ReLU-based nonlinear symmetric matrix
decomposition (ReLU-NSMD) model, introduce an accelerated alternating partial
Bregman (AAPB) method for its solution, and present the algorithm's convergence
results. Our algorithm leverages the Bregman proximal gradient framework to
overcome the challenge of estimating the global $L$-smooth constant in the
classic proximal gradient algorithm. Numerical experiments on synthetic and
real datasets validate the effectiveness of our model and algorithm.","['cs.LG', 'math.OC']",['Qingsong Wang'],2025-03-21,2025-03-21
2503.16843v1,LoRASculpt: Sculpting LoRA for Harmonizing General and Specialized Knowledge in Multimodal Large Language Models,"While Multimodal Large Language Models (MLLMs) excel at generalizing across
modalities and tasks, effectively adapting them to specific downstream tasks
while simultaneously retaining both general and specialized knowledge remains
challenging. Although Low-Rank Adaptation (LoRA) is widely used to efficiently
acquire specialized knowledge in MLLMs, it introduces substantial harmful
redundancy during visual instruction tuning, which exacerbates the forgetting
of general knowledge and degrades downstream task performance. To address this
issue, we propose LoRASculpt to eliminate harmful redundant parameters, thereby
harmonizing general and specialized knowledge. Specifically, under theoretical
guarantees, we introduce sparse updates into LoRA to discard redundant
parameters effectively. Furthermore, we propose a Conflict Mitigation
Regularizer to refine the update trajectory of LoRA, mitigating knowledge
conflicts with the pretrained weights. Extensive experimental results
demonstrate that even at very high degree of sparsity ($\le$ 5%), our method
simultaneously enhances generalization and downstream task performance. This
confirms that our approach effectively mitigates the catastrophic forgetting
issue and further promotes knowledge harmonization in MLLMs.",['cs.CV'],"['Jian Liang', 'Wenke Huang', 'Guancheng Wan', 'Qu Yang', 'Mang Ye']",2025-03-21,2025-03-21
2503.16842v1,Downstream Analysis of Foundational Medical Vision Models for Disease Progression,"Medical vision foundational models are used for a wide variety of tasks,
including medical image segmentation and registration. This work evaluates the
ability of these models to predict disease progression using a simple linear
probe. We hypothesize that intermediate layer features of segmentation models
capture structural information, while those of registration models encode
knowledge of change over time. Beyond demonstrating that these features are
useful for disease progression prediction, we also show that registration model
features do not require spatially aligned input images. However, for
segmentation models, spatial alignment is essential for optimal performance.
Our findings highlight the importance of spatial alignment and the utility of
foundation model features for image registration.","['eess.IV', 'cs.CV']","['Basar Demir', 'Soumitri Chattopadhyay', 'Thomas Hastings Greer', 'Boqi Chen', 'Marc Niethammer']",2025-03-21,2025-03-21
2503.16841v1,Preferential Multi-Objective Bayesian Optimization for Drug Discovery,"Despite decades of advancements in automated ligand screening, large-scale
drug discovery remains resource-intensive and requires post-processing hit
selection, a step where chemists manually select a few promising molecules
based on their chemical intuition. This creates a major bottleneck in the
virtual screening process for drug discovery, demanding experts to repeatedly
balance complex trade-offs among drug properties across a vast pool of
candidates. To improve the efficiency and reliability of this process, we
propose a novel human-centered framework named CheapVS that allows chemists to
guide the ligand selection process by providing preferences regarding the
trade-offs between drug properties via pairwise comparison. Our framework
combines preferential multi-objective Bayesian optimization with a docking
model for measuring binding affinity to capture human chemical intuition for
improving hit identification. Specifically, on a library of 100K chemical
candidates targeting EGFR and DRD2, CheapVS outperforms state-of-the-art
screening methods in identifying drugs within a limited computational budget.
Notably, our method can recover up to 16/37 EGFR and 37/58 DRD2 known drugs
while screening only 6% of the library, showcasing its potential to
significantly advance drug discovery.","['cs.LG', 'cs.HC', 'q-bio.BM']","['Tai Dang', 'Long-Hung Pham', 'Sang T. Truong', 'Ari Glenn', 'Wendy Nguyen', 'Edward A. Pham', 'Jeffrey S. Glenn', 'Sanmi Koyejo', 'Thang Luong']",2025-03-21,2025-03-21
2503.16836v1,A Flexible Fairness Framework with Surrogate Loss Reweighting for Addressing Sociodemographic Disparities,"This paper presents a new algorithmic fairness framework called
$\boldsymbol{\alpha}$-$\boldsymbol{\beta}$ Fair Machine Learning
($\boldsymbol{\alpha}$-$\boldsymbol{\beta}$ FML), designed to optimize fairness
levels across sociodemographic attributes. Our framework employs a new family
of surrogate loss functions, paired with loss reweighting techniques, allowing
precise control over fairness-accuracy trade-offs through tunable
hyperparameters $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$. To efficiently
solve the learning objective, we propose Parallel Stochastic Gradient Descent
with Surrogate Loss (P-SGD-S) and establish convergence guarantees for both
convex and nonconvex loss functions. Experimental results demonstrate that our
framework improves overall accuracy while reducing fairness violations,
offering a smooth trade-off between standard empirical risk minimization and
strict minimax fairness. Results across multiple datasets confirm its
adaptability, ensuring fairness improvements without excessive performance
degradation.",['cs.LG'],"['Wen Xu', 'Elham Dolatabadi']",2025-03-21,2025-03-21
2503.16835v1,Safe and Reliable Diffusion Models via Subspace Projection,"Large-scale text-to-image (T2I) diffusion models have revolutionized image
generation, enabling the synthesis of highly detailed visuals from textual
descriptions. However, these models may inadvertently generate inappropriate
content, such as copyrighted works or offensive images. While existing methods
attempt to eliminate specific unwanted concepts, they often fail to ensure
complete removal, allowing the concept to reappear in subtle forms. For
instance, a model may successfully avoid generating images in Van Gogh's style
when explicitly prompted with 'Van Gogh', yet still reproduce his signature
artwork when given the prompt 'Starry Night'. In this paper, we propose SAFER,
a novel and efficient approach for thoroughly removing target concepts from
diffusion models. At a high level, SAFER is inspired by the observed
low-dimensional structure of the text embedding space. The method first
identifies a concept-specific subspace $S_c$ associated with the target concept
c. It then projects the prompt embeddings onto the complementary subspace of
$S_c$, effectively erasing the concept from the generated images. Since
concepts can be abstract and difficult to fully capture using natural language
alone, we employ textual inversion to learn an optimized embedding of the
target concept from a reference image. This enables more precise subspace
estimation and enhances removal performance. Furthermore, we introduce a
subspace expansion strategy to ensure comprehensive and robust concept erasure.
Extensive experiments demonstrate that SAFER consistently and effectively
erases unwanted concepts from diffusion models while preserving generation
quality.","['cs.CV', 'cs.LG']","['Huiqiang Chen', 'Tianqing Zhu', 'Linlin Wang', 'Xin Yu', 'Longxiang Gao', 'Wanlei Zhou']",2025-03-21,2025-03-21
2503.16833v1,The Deployment of End-to-End Audio Language Models Should Take into Account the Principle of Least Privilege,"We are at a turning point for language models that accept audio input. The
latest end-to-end audio language models (Audio LMs) process speech directly
instead of relying on a separate transcription step. This shift preserves
detailed information, such as intonation or the presence of multiple speakers,
that would otherwise be lost in transcription. However, it also introduces new
safety risks, including the potential misuse of speaker identity cues and other
sensitive vocal attributes, which could have legal implications. In this
position paper, we urge a closer examination of how these models are built and
deployed. We argue that the principle of least privilege should guide decisions
on whether to deploy cascaded or end-to-end models. Specifically, evaluations
should assess (1) whether end-to-end modeling is necessary for a given
application; and (2), the appropriate scope of information access. Finally, We
highlight related gaps in current audio LM benchmarks and identify key open
research questions, both technical and policy-related, that must be addressed
to enable the responsible deployment of end-to-end Audio LMs.","['cs.SD', 'cs.AI', 'cs.CL', 'cs.CY']","['Luxi He', 'Xiangyu Qi', 'Michel Liao', 'Inyoung Cheong', 'Prateek Mittal', 'Danqi Chen', 'Peter Henderson']",2025-03-21,2025-03-21
2503.16832v1,Joint Self-Supervised Video Alignment and Action Segmentation,"We introduce a novel approach for simultaneous self-supervised video
alignment and action segmentation based on a unified optimal transport
framework. In particular, we first tackle self-supervised video alignment by
developing a fused Gromov-Wasserstein optimal transport formulation with a
structural prior, which trains efficiently on GPUs and needs only a few
iterations for solving the optimal transport problem. Our single-task method
achieves the state-of-the-art performance on multiple video alignment
benchmarks and outperforms VAVA, which relies on a traditional Kantorovich
optimal transport formulation with an optimality prior. Furthermore, we extend
our approach by proposing a unified optimal transport framework for joint
self-supervised video alignment and action segmentation, which requires
training and storing a single model and saves both time and memory consumption
as compared to two different single-task models. Extensive evaluations on
several video alignment and action segmentation datasets demonstrate that our
multi-task method achieves comparable video alignment yet superior action
segmentation results over previous methods in video alignment and action
segmentation respectively. Finally, to the best of our knowledge, this is the
first work to unify video alignment and action segmentation into a single
model.",['cs.CV'],"['Ali Shah Ali', 'Syed Ahmed Mahmood', 'Mubin Saeed', 'Andrey Konin', 'M. Zeeshan Zia', 'Quoc-Huy Tran']",2025-03-21,2025-03-21
2503.16826v1,When Tom Eats Kimchi: Evaluating Cultural Bias of Multimodal Large Language Models in Cultural Mixture Contexts,"In a highly globalized world, it is important for multi-modal large language
models (MLLMs) to recognize and respond correctly to mixed-cultural inputs. For
example, a model should correctly identify kimchi (Korean food) in an image
both when an Asian woman is eating it, as well as an African man is eating it.
However, current MLLMs show an over-reliance on the visual features of the
person, leading to misclassification of the entities. To examine the robustness
of MLLMs to different ethnicity, we introduce MixCuBe, a cross-cultural bias
benchmark, and study elements from five countries and four ethnicities. Our
findings reveal that MLLMs achieve both higher accuracy and lower sensitivity
to such perturbation for high-resource cultures, but not for low-resource
cultures. GPT-4o, the best-performing model overall, shows up to 58% difference
in accuracy between the original and perturbed cultural settings in
low-resource cultures. Our dataset is publicly available at:
https://huggingface.co/datasets/kyawyethu/MixCuBe.",['cs.CL'],"['Jun Seong Kim', 'Kyaw Ye Thu', 'Javad Ismayilzada', 'Junyeong Park', 'Eunsu Kim', 'Huzama Ahmad', 'Na Min An', 'James Thorne', 'Alice Oh']",2025-03-21,2025-03-21
2503.16825v1,SGFormer: Satellite-Ground Fusion for 3D Semantic Scene Completion,"Recently, camera-based solutions have been extensively explored for scene
semantic completion (SSC). Despite their success in visible areas, existing
methods struggle to capture complete scene semantics due to frequent visual
occlusions. To address this limitation, this paper presents the first
satellite-ground cooperative SSC framework, i.e., SGFormer, exploring the
potential of satellite-ground image pairs in the SSC task. Specifically, we
propose a dual-branch architecture that encodes orthogonal satellite and ground
views in parallel, unifying them into a common domain. Additionally, we design
a ground-view guidance strategy that corrects satellite image biases during
feature encoding, addressing misalignment between satellite and ground views.
Moreover, we develop an adaptive weighting strategy that balances contributions
from satellite and ground views. Experiments demonstrate that SGFormer
outperforms the state of the art on SemanticKITTI and SSCBench-KITTI-360
datasets. Our code is available on https://github.com/gxytcrc/SGFormer.","['cs.CV', 'cs.RO']","['Xiyue Guo', 'Jiarui Hu', 'Junjie Hu', 'Hujun Bao', 'Guofeng Zhang']",2025-03-21,2025-03-21
2503.16822v1,RigGS: Rigging of 3D Gaussians for Modeling Articulated Objects in Videos,"This paper considers the problem of modeling articulated objects captured in
2D videos to enable novel view synthesis, while also being easily editable,
drivable, and re-posable. To tackle this challenging problem, we propose RigGS,
a new paradigm that leverages 3D Gaussian representation and skeleton-based
motion representation to model dynamic objects without utilizing additional
template priors. Specifically, we first propose skeleton-aware node-controlled
deformation, which deforms a canonical 3D Gaussian representation over time to
initialize the modeling process, producing candidate skeleton nodes that are
further simplified into a sparse 3D skeleton according to their motion and
semantic information. Subsequently, based on the resulting skeleton, we design
learnable skin deformations and pose-dependent detailed deformations, thereby
easily deforming the 3D Gaussian representation to generate new actions and
render further high-quality images from novel views. Extensive experiments
demonstrate that our method can generate realistic new actions easily for
objects and achieve high-quality rendering.",['cs.CV'],"['Yuxin Yao', 'Zhi Deng', 'Junhui Hou']",2025-03-21,2025-03-21
2503.16818v1,Depth-Aided Color Image Inpainting in Quaternion Domain,"In this paper, we propose a depth-aided color image inpainting method in the
quaternion domain, called depth-aided low-rank quaternion matrix completion
(D-LRQMC). In conventional quaternion-based inpainting techniques, the color
image is expressed as a quaternion matrix by using the three imaginary parts as
the color channels, whereas the real part is set to zero and has no
information. Our approach incorporates depth information as the real part of
the quaternion representations, leveraging the correlation between color and
depth to improve the result of inpainting. In the proposed method, we first
restore the observed image with the conventional LRQMC and estimate the depth
of the restored result. We then incorporate the estimated depth into the real
part of the observed image and perform LRQMC again. Simulation results
demonstrate that the proposed D-LRQMC can improve restoration accuracy and
visual quality for various images compared to the conventional LRQMC. These
results suggest the effectiveness of the depth information for color image
processing in quaternion domain.","['eess.IV', 'cs.CV']","['Shunki Tatsumi', 'Ryo Hayakawa', 'Youji Iiguni']",2025-03-21,2025-03-21
2503.16816v1,ST-Prompt Guided Histological Hypergraph Learning for Spatial Gene Expression Prediction,"Spatial Transcriptomics (ST) reveals the spatial distribution of gene
expression in tissues, offering critical insights into biological processes and
disease mechanisms. However, predicting ST from H\&E-stained histology images
is challenging due to the heterogeneous relationship between histomorphology
and gene expression, which arises from substantial variability across different
patients and tissue sections. A more practical and valuable approach is to
utilize ST data from a few local regions to predict the spatial transcriptomic
landscape across the remaining regions in H&E slides. In response, we propose
PHG2ST, an ST-prompt guided histological hypergraph learning framework, which
leverages sparse ST signals as prompts to guide histological hypergraph
learning for global spatial gene expression prediction. Our framework fuses
histological hypergraph representations at multiple scales through a masked
ST-prompt encoding mechanism, improving robustness and generalizability.
Benchmark evaluations on two public ST datasets demonstrate that PHG2ST
outperforms the existing state-of-the-art methods and closely aligns with the
ground truth. These results underscore the potential of leveraging sparse local
ST data for scalable and cost-effective spatial gene expression mapping in
real-world biomedical applications.",['cs.CV'],"['Yi Niu', 'Jiashuai Liu', 'Yingkang Zhan', 'Jiangbo Shi', 'Di Zhang', 'Ines Machado', 'Mireia Crispin-Ortuzar', 'Chen Li', 'Zeyu Gao']",2025-03-21,2025-03-21
2503.16814v1,When Debate Fails: Bias Reinforcement in Large Language Models,"Large Language Models $($LLMs$)$ solve complex problems using training-free
methods like prompt engineering and in-context learning, yet ensuring reasoning
correctness remains challenging. While self-correction methods such as
self-consistency and self-refinement aim to improve reliability, they often
reinforce biases due to the lack of effective feedback mechanisms. Multi-Agent
Debate $($MAD$)$ has emerged as an alternative, but we identify two key
limitations: bias reinforcement, where debate amplifies model biases instead of
correcting them, and lack of perspective diversity, as all agents share the
same model and reasoning patterns, limiting true debate effectiveness. To
systematically evaluate these issues, we introduce $\textit{MetaNIM Arena}$, a
benchmark designed to assess LLMs in adversarial strategic decision-making,
where dynamic interactions influence optimal decisions. To overcome MAD's
limitations, we propose $\textbf{DReaMAD}$ $($$\textbf{D}$iverse
$\textbf{Rea}$soning via $\textbf{M}$ulti-$\textbf{A}$gent $\textbf{D}$ebate
with Refined Prompt$)$, a novel framework that $(1)$ refines LLM's strategic
prior knowledge to improve reasoning quality and $(2)$ promotes diverse
viewpoints within a single model by systematically modifying prompts, reducing
bias. Empirical results show that $\textbf{DReaMAD}$ significantly improves
decision accuracy, reasoning diversity, and bias mitigation across multiple
strategic tasks, establishing it as a more effective approach for LLM-based
decision-making.","['cs.LG', 'cs.CL']","['Jihwan Oh', 'Minchan Jeong', 'Jongwoo Ko', 'Se-Young Yun']",2025-03-21,2025-03-21
2503.16811v1,Seg2Box: 3D Object Detection by Point-Wise Semantics Supervision,"LiDAR-based 3D object detection and semantic segmentation are critical tasks
in 3D scene understanding. Traditional detection and segmentation methods
supervise their models through bounding box labels and semantic mask labels.
However, these two independent labels inherently contain significant
redundancy. This paper aims to eliminate the redundancy by supervising 3D
object detection using only semantic labels. However, the challenge arises due
to the incomplete geometry structure and boundary ambiguity of point-cloud
instances, leading to inaccurate pseudo labels and poor detection results. To
address these challenges, we propose a novel method, named Seg2Box. We first
introduce a Multi-Frame Multi-Scale Clustering (MFMS-C) module, which leverages
the spatio-temporal consistency of point clouds to generate accurate box-level
pseudo-labels. Additionally, the Semantic?Guiding Iterative-Mining
Self-Training (SGIM-ST) module is proposed to enhance the performance by
progressively refining the pseudo-labels and mining the instances without
generating pseudo-labels. Experiments on the Waymo Open Dataset and nuScenes
Dataset show that our method significantly outperforms other competitive
methods by 23.7\% and 10.3\% in mAP, respectively. The results demonstrate the
great label-efficient potential and advancement of our method.",['cs.CV'],"['Maoji Zheng', 'Ziyu Xu', 'Qiming Xia', 'Hai Wu', 'Chenglu Wen', 'Cheng Wang']",2025-03-21,2025-03-21
2503.16809v1,Online Selective Conformal Prediction: Errors and Solutions,"In online selective conformal inference, data arrives sequentially, and
prediction intervals are constructed only when an online selection rule is met.
Since online selections may break the exchangeability between the selected test
datum and the rest of the data, one must correct for this by suitably selecting
the calibration data. In this paper, we evaluate existing calibration selection
strategies and pinpoint some fundamental errors in the associated claims that
guarantee selection-conditional coverage and control of the false coverage rate
(FCR). To address these shortcomings, we propose novel calibration selection
strategies that provably preserve the exchangeability of the calibration data
and the selected test datum. Consequently, we demonstrate that online selective
conformal inference with these strategies guarantees both selection-conditional
coverage and FCR control. Our theoretical findings are supported by
experimental evidence examining tradeoffs between valid methods.","['stat.ML', 'cs.LG']","['Yusuf Sale', 'Aaditya Ramdas']",2025-03-21,2025-03-21
2503.16806v1,DyWA: Dynamics-adaptive World Action Model for Generalizable Non-prehensile Manipulation,"Nonprehensile manipulation is crucial for handling objects that are too thin,
large, or otherwise ungraspable in unstructured environments. While
conventional planning-based approaches struggle with complex contact modeling,
learning-based methods have recently emerged as a promising alternative.
However, existing learning-based approaches face two major limitations: they
heavily rely on multi-view cameras and precise pose tracking, and they fail to
generalize across varying physical conditions, such as changes in object mass
and table friction. To address these challenges, we propose the
Dynamics-Adaptive World Action Model (DyWA), a novel framework that enhances
action learning by jointly predicting future states while adapting to dynamics
variations based on historical trajectories. By unifying the modeling of
geometry, state, physics, and robot actions, DyWA enables more robust policy
learning under partial observability. Compared to baselines, our method
improves the success rate by 31.5% using only single-view point cloud
observations in the simulation. Furthermore, DyWA achieves an average success
rate of 68% in real-world experiments, demonstrating its ability to generalize
across diverse object geometries, adapt to varying table friction, and
robustness in challenging scenarios such as half-filled water bottles and
slippery surfaces.","['cs.RO', 'cs.AI']","['Jiangran Lyu', 'Ziming Li', 'Xuesong Shi', 'Chaoyi Xu', 'Yizhou Wang', 'He Wang']",2025-03-21,2025-03-21
2503.16803v1,BEAC: Imitating Complex Exploration and Task-oriented Behaviors for Invisible Object Nonprehensile Manipulation,"Applying imitation learning (IL) is challenging to nonprehensile manipulation
tasks of invisible objects with partial observations, such as excavating buried
rocks. The demonstrator must make such complex action decisions as exploring to
find the object and task-oriented actions to complete the task while estimating
its hidden state, perhaps causing inconsistent action demonstration and high
cognitive load problems. For these problems, work in human cognitive science
suggests that promoting the use of pre-designed, simple exploration rules for
the demonstrator may alleviate the problems of action inconsistency and high
cognitive load. Therefore, when performing imitation learning from
demonstrations using such exploration rules, it is important to accurately
imitate not only the demonstrator's task-oriented behavior but also his/her
mode-switching behavior (exploratory or task-oriented behavior) under partial
observation. Based on the above considerations, this paper proposes a novel
imitation learning framework called Belief Exploration-Action Cloning (BEAC),
which has a switching policy structure between a pre-designed exploration
policy and a task-oriented action policy trained on the estimated belief states
based on past history. In simulation and real robot experiments, we confirmed
that our proposed method achieved the best task performance, higher mode and
action prediction accuracies, while reducing the cognitive load in the
demonstration indicated by a user study.","['cs.RO', 'cs.LG']","['Hirotaka Tahara', 'Takamitsu Matsubara']",2025-03-21,2025-03-21
2503.16801v1,Auto-Regressive Diffusion for Generating 3D Human-Object Interactions,"Text-driven Human-Object Interaction (Text-to-HOI) generation is an emerging
field with applications in animation, video games, virtual reality, and
robotics. A key challenge in HOI generation is maintaining interaction
consistency in long sequences. Existing Text-to-Motion-based approaches, such
as discrete motion tokenization, cannot be directly applied to HOI generation
due to limited data in this domain and the complexity of the modality. To
address the problem of interaction consistency in long sequences, we propose an
autoregressive diffusion model (ARDHOI) that predicts the next continuous
token. Specifically, we introduce a Contrastive Variational Autoencoder (cVAE)
to learn a physically plausible space of continuous HOI tokens, thereby
ensuring that generated human-object motions are realistic and natural. For
generating sequences autoregressively, we develop a Mamba-based context encoder
to capture and maintain consistent sequential actions. Additionally, we
implement an MLP-based denoiser to generate the subsequent token conditioned on
the encoded context. Our model has been evaluated on the OMOMO and BEHAVE
datasets, where it outperforms existing state-of-the-art methods in terms of
both performance and inference speed. This makes ARDHOI a robust and efficient
solution for text-driven HOI tasks","['cs.GR', 'cs.AI', 'cs.CV']","['Zichen Geng', 'Zeeshan Hayder', 'Wei Liu', 'Ajmal Saeed Mian']",2025-03-21,2025-03-21
2503.16799v1,Causally Aligned Curriculum Learning,"A pervasive challenge in Reinforcement Learning (RL) is the ""curse of
dimensionality"" which is the exponential growth in the state-action space when
optimizing a high-dimensional target task. The framework of curriculum learning
trains the agent in a curriculum composed of a sequence of related and more
manageable source tasks. The expectation is that when some optimal decision
rules are shared across source tasks and the target task, the agent could more
quickly pick up the necessary skills to behave optimally in the environment,
thus accelerating the learning process. However, this critical assumption of
invariant optimal decision rules does not necessarily hold in many practical
applications, specifically when the underlying environment contains unobserved
confounders. This paper studies the problem of curriculum RL through causal
lenses. We derive a sufficient graphical condition characterizing causally
aligned source tasks, i.e., the invariance of optimal decision rules holds. We
further develop an efficient algorithm to generate a causally aligned
curriculum, provided with qualitative causal knowledge of the target task.
Finally, we validate our proposed methodology through experiments in discrete
and continuous confounded tasks with pixel observations.","['cs.LG', 'cs.AI']","['Mingxuan Li', 'Junzhe Zhang', 'Elias Bareinboim']",2025-03-21,2025-03-21
2503.16797v1,A Learnability Analysis on Neuro-Symbolic Learning,"This paper analyzes the learnability of neuro-symbolic (NeSy) tasks within
hybrid systems. We show that the learnability of NeSy tasks can be
characterized by their derived constraint satisfaction problems (DCSPs).
Specifically, a task is learnable if the corresponding DCSP has a unique
solution; otherwise, it is unlearnable. For learnable tasks, we establish error
bounds by exploiting the clustering property of the hypothesis space.
Additionally, we analyze the asymptotic error for general NeSy tasks, showing
that the expected error scales with the disagreement among solutions. Our
results offer a principled approach to determining learnability and provide
insights into the design of new algorithms.","['cs.AI', 'cs.LG']","['Hao-Yuan He', 'Ming Li']",2025-03-21,2025-03-21
2503.16795v1,DCEdit: Dual-Level Controlled Image Editing via Precisely Localized Semantics,"This paper presents a novel approach to improving text-guided image editing
using diffusion-based models. Text-guided image editing task poses key
challenge of precisly locate and edit the target semantic, and previous methods
fall shorts in this aspect. Our method introduces a Precise Semantic
Localization strategy that leverages visual and textual self-attention to
enhance the cross-attention map, which can serve as a regional cues to improve
editing performance. Then we propose a Dual-Level Control mechanism for
incorporating regional cues at both feature and latent levels, offering
fine-grained control for more precise edits. To fully compare our methods with
other DiT-based approaches, we construct the RW-800 benchmark, featuring high
resolution images, long descriptive texts, real-world images, and a new text
editing task. Experimental results on the popular PIE-Bench and RW-800
benchmarks demonstrate the superior performance of our approach in preserving
background and providing accurate edits.",['cs.CV'],"['Yihan Hu', 'Jianing Peng', 'Yiheng Lin', 'Ting Liu', 'Xiaochao Qu', 'Luoqi Liu', 'Yao Zhao', 'Yunchao Wei']",2025-03-21,2025-03-21
2503.16793v1,Restoring Forgotten Knowledge in Non-Exemplar Class Incremental Learning through Test-Time Semantic Evolution,"Continual learning aims to accumulate knowledge over a data stream while
mitigating catastrophic forgetting. In Non-exemplar Class Incremental Learning
(NECIL), forgetting arises during incremental optimization because old classes
are inaccessible, hindering the retention of prior knowledge. To solve this,
previous methods struggle in achieving the stability-plasticity balance in the
training stages. However, we note that the testing stage is rarely considered
among them, but is promising to be a solution to forgetting. Therefore, we
propose RoSE, which is a simple yet effective method that
\textbf{R}est\textbf{o}res forgotten knowledge through test-time
\textbf{S}emantic \textbf{E}volution. Specifically designed for minimizing
forgetting, RoSE is a test-time semantic drift compensation framework that
enables more accurate drift estimation in a self-supervised manner. Moreover,
to avoid incomplete optimization during online testing, we derive an analytical
solution as an alternative to gradient descent. We evaluate RoSE on CIFAR-100,
TinyImageNet, and ImageNet100 datasets, under both cold-start and warm-start
settings. Our method consistently outperforms most state-of-the-art (SOTA)
methods across various scenarios, validating the potential and feasibility of
test-time evolution in NECIL.",['cs.CV'],"['Haori Lu', 'Xusheng Cao', 'Linlan Huang', 'Enguang Wang', 'Fei Yang', 'Xialei Liu']",2025-03-21,2025-03-21
2503.16791v1,"""The Diagram is like Guardrails"": Structuring GenAI-assisted Hypotheses Exploration with an Interactive Shared Representation","Data analysis encompasses a spectrum of tasks, from high-level conceptual
reasoning to lower-level execution. While AI-powered tools increasingly support
execution tasks, there remains a need for intelligent assistance in conceptual
tasks. This paper investigates the design of an ordered node-link tree
interface augmented with AI-generated information hints and visualizations, as
a potential shared representation for hypothesis exploration. Through a design
probe (n=22), participants generated diagrams averaging 21.82 hypotheses. Our
findings showed that the node-link diagram acts as ""guardrails"" for hypothesis
exploration, facilitating structured workflows, providing comprehensive
overviews, and enabling efficient backtracking. The AI-generated information
hints, particularly visualizations, aided users in transforming abstract ideas
into data-backed concepts while reducing cognitive load. We further discuss how
node-link diagrams can support both parallel exploration and iterative
refinement in hypothesis formulation, potentially enhancing the breadth and
depth of human-AI collaborative data analysis.","['cs.HC', 'cs.AI']","['Zijian Ding', 'Michelle Brachman', 'Joel Chan', 'Werner Geyer']",2025-03-21,2025-03-21
2503.16789v1,Conversational User-AI Intervention: A Study on Prompt Rewriting for Improved LLM Response Generation,"Human-LLM conversations are increasingly becoming more pervasive in peoples'
professional and personal lives, yet many users still struggle to elicit
helpful responses from LLM Chatbots. One of the reasons for this issue is
users' lack of understanding in crafting effective prompts that accurately
convey their information needs. Meanwhile, the existence of real-world
conversational datasets on the one hand, and the text understanding faculties
of LLMs on the other, present a unique opportunity to study this problem, and
its potential solutions at scale. Thus, in this paper we present the first
LLM-centric study of real human-AI chatbot conversations, focused on
investigating aspects in which user queries fall short of expressing
information needs, and the potential of using LLMs to rewrite suboptimal user
prompts. Our findings demonstrate that rephrasing ineffective prompts can
elicit better responses from a conversational system, while preserving the
user's original intent. Notably, the performance of rewrites improves in longer
conversations, where contextual inferences about user needs can be made more
accurately. Additionally, we observe that LLMs often need to -- and inherently
do -- make \emph{plausible} assumptions about a user's intentions and goals
when interpreting prompts. Our findings largely hold true across conversational
domains, user intents, and LLMs of varying sizes and families, indicating the
promise of using prompt rewriting as a solution for better human-AI
interactions.",['cs.CL'],"['Rupak Sarkar', 'Bahareh Sarrafzadeh', 'Nirupama Chandrasekaran', 'Nagu Rangan', 'Philip Resnik', 'Longqi Yang', 'Sujay Kumar Jauhar']",2025-03-21,2025-03-21
2503.16788v1,Does Chain-of-Thought Reasoning Help Mobile GUI Agent? An Empirical Study,"Reasoning capabilities have significantly improved the performance of
vision-language models (VLMs) in domains such as mathematical problem-solving,
coding, and visual question-answering. However, their impact on real-world
applications remains unclear. This paper presents the first empirical study on
the effectiveness of reasoning-enabled VLMs in mobile GUI agents, a domain that
requires interpreting complex screen layouts, understanding user instructions,
and executing multi-turn interactions. We evaluate two pairs of commercial
models--Gemini 2.0 Flash and Claude 3.7 Sonnet--comparing their base and
reasoning-enhanced versions across two static benchmarks (ScreenSpot and
AndroidControl) and one interactive environment (AndroidWorld). We surprisingly
find the Claude 3.7 Sonnet reasoning model achieves state-of-the-art
performance on AndroidWorld. However, reasoning VLMs generally offer marginal
improvements over non-reasoning models on static benchmarks and even degrade
performance in some agent setups. Notably, reasoning and non-reasoning VLMs
fail on different sets of tasks, suggesting that reasoning does have an impact,
but its benefits and drawbacks counterbalance each other. We attribute these
inconsistencies to the limitations of benchmarks and VLMs. Based on the
findings, we provide insights for further enhancing mobile GUI agents in terms
of benchmarks, VLMs, and their adaptability in dynamically invoking reasoning
VLMs. The experimental data are publicly available at
https://github.com/LlamaTouch/VLM-Reasoning-Traces.",['cs.AI'],"['Li Zhang', 'Longxi Gao', 'Mengwei Xu']",2025-03-21,2025-03-21
2503.16782v1,Learning Part Knowledge to Facilitate Category Understanding for Fine-Grained Generalized Category Discovery,"Generalized Category Discovery (GCD) aims to classify unlabeled data
containing both seen and novel categories. Although existing methods perform
well on generic datasets, they struggle in fine-grained scenarios. We attribute
this difficulty to their reliance on contrastive learning over global image
features to automatically capture discriminative cues, which fails to capture
the subtle local differences essential for distinguishing fine-grained
categories. Therefore, in this paper, we propose incorporating part knowledge
to address fine-grained GCD, which introduces two key challenges: the absence
of annotations for novel classes complicates the extraction of the part
features, and global contrastive learning prioritizes holistic feature
invariance, inadvertently suppressing discriminative local part patterns. To
address these challenges, we propose PartGCD, including 1) Adaptive Part
Decomposition, which automatically extracts class-specific semantic parts via
Gaussian Mixture Models, and 2) Part Discrepancy Regularization, enforcing
explicit separation between part features to amplify fine-grained local part
distinctions.
  Experiments demonstrate state-of-the-art performance across multiple
fine-grained benchmarks while maintaining competitiveness on generic datasets,
validating the effectiveness and robustness of our approach.","['cs.CV', 'cs.AI', 'cs.LG']","['Enguang Wang', 'Zhimao Peng', 'Zhengyuan Xie', 'Haori Lu', 'Fei Yang', 'Xialei Liu']",2025-03-21,2025-03-21
2503.16780v1,A-IDE : Agent-Integrated Denoising Experts,"Recent advances in deep-learning based denoising methods have improved
Low-Dose CT image quality. However, due to distinct HU distributions and
diverse anatomical characteristics, a single model often struggles to
generalize across multiple anatomies. To address this limitation, we introduce
\textbf{Agent-Integrated Denoising Experts (A-IDE)} framework, which integrates
three anatomical region-specialized RED-CNN models under the management of
decision-making LLM agent. The agent analyzes semantic cues from BiomedCLIP to
dynamically route incoming LDCT scans to the most appropriate expert model. We
highlight three major advantages of our approach. A-IDE excels in
heterogeneous, data-scarce environments. The framework automatically prevents
overfitting by distributing tasks among multiple experts. Finally, our
LLM-driven agentic pipeline eliminates the need for manual interventions.
Experimental evaluations on the Mayo-2016 dataset confirm that A-IDE achieves
superior performance in RMSE, PSNR, and SSIM compared to a single unified
denoiser.",['cs.CV'],"['Uihyun Cho', 'Namhun Kim']",2025-03-21,2025-03-21
2503.16779v1,Chain-of-Tools: Utilizing Massive Unseen Tools in the CoT Reasoning of Frozen Language Models,"Tool learning can further broaden the usage scenarios of large language
models (LLMs). However most of the existing methods either need to finetune
that the model can only use tools seen in the training data, or add tool
demonstrations into the prompt with lower efficiency. In this paper, we present
a new Tool Learning method Chain-of-Tools. It makes full use of the powerful
semantic representation capability of frozen LLMs to finish tool calling in CoT
reasoning with a huge and flexible tool pool which may contain unseen tools.
Especially, to validate the effectiveness of our approach in the massive unseen
tool scenario, we construct a new dataset SimpleToolQuestions. We conduct
experiments on two numerical reasoning benchmarks (GSM8K-XL and FuncQA) and two
knowledge-based question answering benchmarks (KAMEL and SimpleToolQuestions).
Experimental results show that our approach performs better than the baseline.
We also identify dimensions of the model output that are critical in tool
selection, enhancing the model interpretability. Our code and data are
available at: https://github.com/fairyshine/Chain-of-Tools .","['cs.CL', 'cs.AI']","['Mengsong Wu', 'Tong Zhu', 'Han Han', 'Xiang Zhang', 'Wenbiao Shao', 'Wenliang Chen']",2025-03-21,2025-03-21
2503.16777v1,Physics-Informed Deep B-Spline Networks for Dynamical Systems,"Physics-informed machine learning provides an approach to combining data and
governing physics laws for solving complex partial differential equations
(PDEs). However, efficiently solving PDEs with varying parameters and changing
initial conditions and boundary conditions (ICBCs) with theoretical guarantees
remains an open challenge. We propose a hybrid framework that uses a neural
network to learn B-spline control points to approximate solutions to PDEs with
varying system and ICBC parameters. The proposed network can be trained
efficiently as one can directly specify ICBCs without imposing losses,
calculate physics-informed loss functions through analytical formulas, and
requires only learning the weights of B-spline functions as opposed to both
weights and basis as in traditional neural operator learning methods. We
provide theoretical guarantees that the proposed B-spline networks serve as
universal approximators for the set of solutions of PDEs with varying ICBCs
under mild conditions and establish bounds on the generalization errors in
physics-informed learning. We also demonstrate in experiments that the proposed
B-spline network can solve problems with discontinuous ICBCs and outperforms
existing methods, and is able to learn solutions of 3D dynamics with diverse
initial conditions.","['cs.LG', 'cs.SY', 'eess.SY']","['Zhuoyuan Wang', 'Raffaele Romagnoli', 'Jasmine Ratchford', 'Yorie Nakahira']",2025-03-21,2025-03-21
2503.16776v1,OpenCity3D: What do Vision-Language Models know about Urban Environments?,"Vision-language models (VLMs) show great promise for 3D scene understanding
but are mainly applied to indoor spaces or autonomous driving, focusing on
low-level tasks like segmentation. This work expands their use to urban-scale
environments by leveraging 3D reconstructions from multi-view aerial imagery.
We propose OpenCity3D, an approach that addresses high-level tasks, such as
population density estimation, building age classification, property price
prediction, crime rate assessment, and noise pollution evaluation. Our findings
highlight OpenCity3D's impressive zero-shot and few-shot capabilities,
showcasing adaptability to new contexts. This research establishes a new
paradigm for language-driven urban analytics, enabling applications in
planning, policy, and environmental monitoring. See our project page:
opencity3d.github.io",['cs.CV'],"['Valentin Bieri', 'Marco Zamboni', 'Nicolas S. Blumer', 'Qingxuan Chen', 'Francis Engelmann']",2025-03-21,2025-03-21
2503.16775v1,Region Masking to Accelerate Video Processing on Neuromorphic Hardware,"The rapidly growing demand for on-chip edge intelligence on
resource-constrained devices has motivated approaches to reduce energy and
latency of deep learning models. Spiking neural networks (SNNs) have gained
particular interest due to their promise to reduce energy consumption using
event-based processing. We assert that while sigma-delta encoding in SNNs can
take advantage of the temporal redundancy across video frames, they still
involve a significant amount of redundant computations due to processing
insignificant events. In this paper, we propose a region masking strategy that
identifies regions of interest at the input of the SNN, thereby eliminating
computation and data movement for events arising from unimportant regions. Our
approach demonstrates that masking regions at the input not only significantly
reduces the overall spiking activity of the network, but also provides
significant improvement in throughput and latency. We apply region masking
during video object detection on Loihi 2, demonstrating that masking
approximately 60% of input regions can reduce energy-delay product by 1.65x
over a baseline sigma-delta network, with a degradation in mAP@0.5 by 1.09%.","['cs.CV', 'cs.NE', 'eess.IV']","['Sreetama Sarkar', 'Sumit Bam Shrestha', 'Yue Che', 'Leobardo Campos-Macias', 'Gourav Datta', 'Peter A. Beerel']",2025-03-21,2025-03-21
2503.16771v1,On Explaining (Large) Language Models For Code Using Global Code-Based Explanations,"In recent years, Language Models for Code (LLM4Code) have significantly
changed the landscape of software engineering (SE) on downstream tasks, such as
code generation, by making software development more efficient. Therefore, a
growing interest has emerged in further evaluating these Language Models to
homogenize the quality assessment of generated code. As the current evaluation
process can significantly overreact on accuracy-based metrics, practitioners
often seek methods to interpret LLM4Code outputs beyond canonical benchmarks.
While the majority of research reports on code generation effectiveness in
terms of expected ground truth, scant attention has been paid to LLMs'
explanations. In essence, the decision-making process to generate code is hard
to interpret. To bridge this evaluation gap, we introduce code rationales
(Code$Q$), a technique with rigorous mathematical underpinning, to identify
subsets of tokens that can explain individual code predictions. We conducted a
thorough Exploratory Analysis to demonstrate the method's applicability and a
User Study to understand the usability of code-based explanations. Our
evaluation demonstrates that Code$Q$ is a powerful interpretability method to
explain how (less) meaningful input concepts (i.e., natural language particle
`at') highly impact output generation. Moreover, participants of this study
highlighted Code$Q$'s ability to show a causal relationship between the input
and output of the model with readable and informative explanations on code
completion and test generation tasks. Additionally, Code$Q$ also helps to
uncover model rationale, facilitating comparison with a human rationale to
promote a fair level of trust and distrust in the model.","['cs.SE', 'cs.LG']","['David N. Palacio', 'Dipin Khati', 'Daniel Rodriguez-Cardenas', 'Alejandro Velasco', 'Denys Poshyvanyk']",2025-03-21,2025-03-21
2503.16768v1,Dynamic Attention Mechanism in Spatiotemporal Memory Networks for Object Tracking,"Mainstream visual object tracking frameworks predominantly rely on template
matching paradigms. Their performance heavily depends on the quality of
template features, which becomes increasingly challenging to maintain in
complex scenarios involving target deformation, occlusion, and background
clutter. While existing spatiotemporal memory-based trackers emphasize memory
capacity expansion, they lack effective mechanisms for dynamic feature
selection and adaptive fusion. To address this gap, we propose a Dynamic
Attention Mechanism in Spatiotemporal Memory Network (DASTM) with two key
innovations: 1) A differentiable dynamic attention mechanism that adaptively
adjusts channel-spatial attention weights by analyzing spatiotemporal
correlations between the templates and memory features; 2) A lightweight gating
network that autonomously allocates computational resources based on target
motion states, prioritizing high-discriminability features in challenging
scenarios. Extensive evaluations on OTB-2015, VOT 2018, LaSOT, and GOT-10K
benchmarks demonstrate our DASTM's superiority, achieving state-of-the-art
performance in success rate, robustness, and real-time efficiency, thereby
offering a novel solution for real-time tracking in complex environments.","['cs.CV', 'cs.AI']","['Meng Zhou', 'Jiadong Xie', 'Mingsheng Xu']",2025-03-21,2025-03-21
2503.16760v1,Rethinking the Role of Spatial Mixing,"Until quite recently, the backbone of nearly every state-of-the-art computer
vision model has been the 2D convolution. At its core, a 2D convolution
simultaneously mixes information across both the spatial and channel dimensions
of a representation. Many recent computer vision architectures consist of
sequences of isotropic blocks that disentangle the spatial and channel-mixing
components. This separation of the operations allows us to more closely
juxtapose the effects of spatial and channel mixing in deep learning. In this
paper, we take an initial step towards garnering a deeper understanding of the
roles of these mixing operations. Through our experiments and analysis, we
discover that on both classical (ResNet) and cutting-edge (ConvMixer) models,
we can reach nearly the same level of classification performance by and leaving
the spatial mixers at their random initializations. Furthermore, we show that
models with random, fixed spatial mixing are naturally more robust to
adversarial perturbations. Lastly, we show that this phenomenon extends past
the classification regime, as such models can also decode pixel-shuffled
images.","['cs.CV', 'cs.LG']","['George Cazenavette', 'Joel Julin', 'Simon Lucey']",2025-03-21,2025-03-21
2503.16759v1,elaTCSF: A Temporal Contrast Sensitivity Function for Flicker Detection and Modeling Variable Refresh Rate Flicker,"The perception of flicker has been a prominent concern in illumination and
electronic display fields for over a century. Traditional approaches often rely
on Critical Flicker Frequency (CFF), primarily suited for high-contrast
(full-on, full-off) flicker. To tackle varying contrast flicker, the
International Committee for Display Metrology (ICDM) introduced a Temporal
Contrast Sensitivity Function TCSF$_{IDMS}$ within the Information Display
Measurements Standard (IDMS). Nevertheless, this standard overlooks crucial
parameters: luminance, eccentricity, and area. Existing models incorporating
these parameters are inadequate for flicker detection, especially at low
spatial frequencies. To address these limitations, we extend the TCSF$_{IDMS}$
and combine it with a new spatial probability summation model to incorporate
the effects of luminance, eccentricity, and area (elaTCSF). We train the
elaTCSF on various flicker detection datasets and establish the first variable
refresh rate flicker detection dataset for further verification. Additionally,
we contribute to resolving a longstanding debate on whether the flicker is more
visible in peripheral vision. We demonstrate how elaTCSF can be used to predict
flicker due to low-persistence in VR headsets, identify flicker-free VRR
operational ranges, and determine flicker sensitivity in lighting design.","['cs.GR', 'cs.CV']","['Yancheng Cai', 'Ali Bozorgian', 'Maliha Ashraf', 'Robert Wanat', 'Rafał K. Mantiuk']",2025-03-21,2025-03-21
2503.16755v1,Fast online node labeling with graph subsampling,"Large data applications rely on storing data in massive, sparse graphs with
millions to trillions of nodes. Graph-based methods, such as node prediction,
aim for computational efficiency regardless of graph size. Techniques like
localized approximate personalized page rank (APPR) solve sparse linear systems
with complexity independent of graph size, but is in terms of the maximum node
degree, which can be much larger in practice than the average node degree for
real-world large graphs. In this paper, we consider an \emph{online subsampled
APPR method}, where messages are intentionally dropped at random. We use tools
from graph sparsifiers and matrix linear algebra to give approximation bounds
on the graph's spectral properties ($O(1/\epsilon^2)$ edges), and node
classification performance (added $O(n\epsilon)$ overhead).","['cs.DS', 'cs.LG']","['Yushen Huang', 'Ertai Luo', 'Reza Babenezhad', 'Yifan Sun']",2025-03-21,2025-03-21
2503.16753v1,EarlyStopping: Implicit Regularization for Iterative Learning Procedures in Python,"Iterative learning procedures are ubiquitous in machine learning and modern
statistics.
  Regularision is typically required to prevent inflating the expected loss of
a procedure in
  later iterations via the propagation of noise inherent in the data.
  Significant emphasis has been placed on achieving this regularisation
implicitly by stopping
  procedures early.
  The EarlyStopping-package provides a toolbox of (in-sample) sequential early
stopping rules for
  several well-known iterative estimation procedures, such as truncated SVD,
Landweber (gradient
  descent), conjugate gradient descent, L2-boosting and regression trees.
  One of the central features of the package is that the algorithms allow the
specification of the
  true data-generating process and keep track of relevant theoretical
quantities.
  In this paper, we detail the principles governing the implementation of the
EarlyStopping-package and provide
  a survey of recent foundational advances in the theoretical literature.
  We demonstrate how to use the EarlyStopping-package to explore core features
of implicit regularisation
  and replicate results from the literature.","['stat.ML', 'cs.LG', 'cs.MS']","['Eric Ziebell', 'Ratmir Miftachov', 'Bernhard Stankewitz', 'Laura Hucker']",2025-03-20,2025-03-20
2503.16747v1,SAGE: Semantic-Driven Adaptive Gaussian Splatting in Extended Reality,"3D Gaussian Splatting (3DGS) has significantly improved the efficiency and
realism of three-dimensional scene visualization in several applications,
ranging from robotics to eXtended Reality (XR). This work presents SAGE
(Semantic-Driven Adaptive Gaussian Splatting in Extended Reality), a novel
framework designed to enhance the user experience by dynamically adapting the
Level of Detail (LOD) of different 3DGS objects identified via a semantic
segmentation. Experimental results demonstrate how SAGE effectively reduces
memory and computational overhead while keeping a desired target visual
quality, thus providing a powerful optimization for interactive XR
applications.","['cs.GR', 'cs.CV', 'cs.MM']","['Chiara Schiavo', 'Elena Camuffo', 'Leonardo Badia', 'Simone Milani']",2025-03-20,2025-03-20
2503.16746v1,Ordered Topological Deep Learning: a Network Modeling Case Study,"Computer networks are the foundation of modern digital infrastructure,
facilitating global communication and data exchange. As demand for reliable
high-bandwidth connectivity grows, advanced network modeling techniques become
increasingly essential to optimize performance and predict network behavior.
Traditional modeling methods, such as packet-level simulators and queueing
theory, have notable limitations --either being computationally expensive or
relying on restrictive assumptions that reduce accuracy. In this context, the
deep learning-based RouteNet family of models has recently redefined network
modeling by showing an unprecedented cost-performance trade-off. In this work,
we revisit RouteNet's sophisticated design and uncover its hidden connection to
Topological Deep Learning (TDL), an emerging field that models higher-order
interactions beyond standard graph-based methods. We demonstrate that, although
originally formulated as a heterogeneous Graph Neural Network, RouteNet serves
as the first instantiation of a new form of TDL. More specifically, this paper
presents OrdGCCN, a novel TDL framework that introduces the notion of ordered
neighbors in arbitrary discrete topological spaces, and shows that RouteNet's
architecture can be naturally described as an ordered topological neural
network. To the best of our knowledge, this marks the first successful
real-world application of state-of-the-art TDL principles --which we confirm
through extensive testbed experiments--, laying the foundation for the next
generation of ordered TDL-driven applications.","['cs.LG', 'cs.NI']","['Guillermo Bernárdez', 'Miquel Ferriol-Galmés', 'Carlos Güemes-Palau', 'Mathilde Papillon', 'Pere Barlet-Ros', 'Albert Cabellos-Aparicio', 'Nina Miolane']",2025-03-20,2025-03-20
2503.16745v1,SPACER: A Parallel Dataset of Speech Production And Comprehension of Error Repairs,"Speech errors are a natural part of communication, yet they rarely lead to
complete communicative failure because both speakers and comprehenders can
detect and correct errors. Although prior research has examined error
monitoring and correction in production and comprehension separately,
integrated investigation of both systems has been impeded by the scarcity of
parallel data. In this study, we present SPACER, a parallel dataset that
captures how naturalistic speech errors are corrected by both speakers and
comprehenders. We focus on single-word substitution errors extracted from the
Switchboard corpus, accompanied by speaker's self-repairs and comprehenders'
responses from an offline text-editing experiment. Our exploratory analysis
suggests asymmetries in error correction strategies: speakers are more likely
to repair errors that introduce greater semantic and phonemic deviations,
whereas comprehenders tend to correct errors that are phonemically similar to
more plausible alternatives or do not fit into prior contexts. Our dataset
enables future research on integrated approaches toward studying language
production and comprehension.",['cs.CL'],"['Shiva Upadhye', 'Jiaxuan Li', 'Richard Futrell']",2025-03-20,2025-03-20
2503.16743v1,SuperARC: A Test for General and Super Intelligence Based on First Principles of Recursion Theory and Algorithmic Probability,"We introduce an open-ended test grounded in algorithmic probability that can
avoid benchmark contamination in the quantitative evaluation of frontier models
in the context of their Artificial General Intelligence (AGI) and
Superintelligence (ASI) claims. Unlike other tests, this test does not rely on
statistical compression methods (such as GZIP or LZW), which are more closely
related to Shannon entropy than to Kolmogorov complexity. The test challenges
aspects related to features of intelligence of fundamental nature such as
synthesis and model creation in the context of inverse problems (generating new
knowledge from observation). We argue that metrics based on model abstraction
and optimal Bayesian inference for planning can provide a robust framework for
testing intelligence, including natural intelligence (human and animal), narrow
AI, AGI, and ASI. Our results show no clear evidence of LLM convergence towards
a defined level of intelligence, particularly AGI or ASI. We found that LLM
model versions tend to be fragile and incremental, as new versions may perform
worse than older ones, with progress largely driven by the size of training
data. The results were compared with a hybrid neurosymbolic approach that
theoretically guarantees model convergence from optimal inference based on the
principles of algorithmic probability and Kolmogorov complexity. The method
outperforms LLMs in a proof-of-concept on short binary sequences. Our findings
confirm suspicions regarding the fundamental limitations of LLMs, exposing them
as systems optimised for the perception of mastery over human language.
Progress among different LLM versions from the same developers was found to be
inconsistent and limited, particularly in the absence of a solid symbolic
counterpart.","['cs.AI', 'cs.IT', 'math.IT']","['Alberto Hernández-Espinosa', 'Luan Ozelim', 'Felipe S. Abrahão', 'Hector Zenil']",2025-03-20,2025-03-20
2503.16742v1,Digitally Prototype Your Eye Tracker: Simulating Hardware Performance using 3D Synthetic Data,"Eye tracking (ET) is a key enabler for Augmented and Virtual Reality (AR/VR).
Prototyping new ET hardware requires assessing the impact of hardware choices
on eye tracking performance. This task is compounded by the high cost of
obtaining data from sufficiently many variations of real hardware, especially
for machine learning, which requires large training datasets. We propose a
method for end-to-end evaluation of how hardware changes impact machine
learning-based ET performance using only synthetic data. We utilize a dataset
of real 3D eyes, reconstructed from light dome data using neural radiance
fields (NeRF), to synthesize captured eyes from novel viewpoints and camera
parameters. Using this framework, we demonstrate that we can predict the
relative performance across various hardware configurations, accounting for
variations in sensor noise, illumination brightness, and optical blur. We also
compare our simulator with the publicly available eye tracking dataset from the
Project Aria glasses, demonstrating a strong correlation with real-world
performance. Finally, we present a first-of-its-kind analysis in which we vary
ET camera positions, evaluating ET performance ranging from on-axis direct
views of the eye to peripheral views on the frame. Such an analysis would have
previously required manufacturing physical devices to capture evaluation data.
In short, our method enables faster prototyping of ET hardware.",['cs.CV'],"['Esther Y. H. Lin', 'Yimin Ding', 'Jogendra Kundu', 'Yatong An', 'Mohamed T. El-Haddad', 'Alexander Fix']",2025-03-20,2025-03-20
2503.16737v1,Optimal Nonlinear Online Learning under Sequential Price Competition via s-Concavity,"We consider price competition among multiple sellers over a selling horizon
of $T$ periods. In each period, sellers simultaneously offer their prices and
subsequently observe their respective demand that is unobservable to
competitors. The demand function for each seller depends on all sellers' prices
through a private, unknown, and nonlinear relationship. To address this
challenge, we propose a semi-parametric least-squares estimation of the
nonlinear mean function, which does not require sellers to communicate demand
information. We show that when all sellers employ our policy, their prices
converge at a rate of $O(T^{-1/7})$ to the Nash equilibrium prices that sellers
would reach if they were fully informed. Each seller incurs a regret of
$O(T^{5/7})$ relative to a dynamic benchmark policy. A theoretical contribution
of our work is proving the existence of equilibrium under shape-constrained
demand functions via the concept of $s$-concavity and establishing regret
bounds of our proposed policy. Technically, we also establish new concentration
results for the least squares estimator under shape constraints. Our findings
offer significant insights into dynamic competition-aware pricing and
contribute to the broader study of non-parametric learning in strategic
decision-making.","['stat.ML', 'cs.LG', 'math.PR', 'math.ST', 'stat.TH']","['Daniele Bracale', 'Moulinath Banerjee', 'Cong Shi', 'Yuekai Sun']",2025-03-20,2025-03-20
2503.16734v1,Towards Agentic Recommender Systems in the Era of Multimodal Large Language Models,"Recent breakthroughs in Large Language Models (LLMs) have led to the
emergence of agentic AI systems that extend beyond the capabilities of
standalone models. By empowering LLMs to perceive external environments,
integrate multimodal information, and interact with various tools, these
agentic systems exhibit greater autonomy and adaptability across complex tasks.
This evolution brings new opportunities to recommender systems (RS): LLM-based
Agentic RS (LLM-ARS) can offer more interactive, context-aware, and proactive
recommendations, potentially reshaping the user experience and broadening the
application scope of RS. Despite promising early results, fundamental
challenges remain, including how to effectively incorporate external knowledge,
balance autonomy with controllability, and evaluate performance in dynamic,
multimodal settings. In this perspective paper, we first present a systematic
analysis of LLM-ARS: (1) clarifying core concepts and architectures; (2)
highlighting how agentic capabilities -- such as planning, memory, and
multimodal reasoning -- can enhance recommendation quality; and (3) outlining
key research questions in areas such as safety, efficiency, and lifelong
personalization. We also discuss open problems and future directions, arguing
that LLM-ARS will drive the next wave of RS innovation. Ultimately, we foresee
a paradigm shift toward intelligent, autonomous, and collaborative
recommendation experiences that more closely align with users' evolving needs
and complex decision-making processes.","['cs.AI', 'cs.IR']","['Chengkai Huang', 'Junda Wu', 'Yu Xia', 'Zixu Yu', 'Ruhan Wang', 'Tong Yu', 'Ruiyi Zhang', 'Ryan A. Rossi', 'Branislav Kveton', 'Dongruo Zhou', 'Julian McAuley', 'Lina Yao']",2025-03-20,2025-03-20
2503.16731v1,Design and Implementation of an FPGA-Based Tiled Matrix Multiplication Accelerator for Transformer Self-Attention on the Xilinx KV260 SoM,"Transformer-based LLMs spend most of their compute in large matrix
multiplications for attention and feed-forward layers. Recognizing that the Q,
K, and V linear projections within the Multi-Head Self-Attention (MHA) module
represent a critical computational bottleneck, we strategically focused our
efforts on accelerating these operations. We present a tiled matrix
multiplication accelerator optimized for such workloads on a Xilinx KV260
on-board FPGA. Key innovations include persistent on-chip storage for one
matrix operand, two-level tiling for data reuse, and a systolic-like unrolled
compute engine. Implemented via high-level synthesis (HLS) and integrated with
DistilBERT for Q, K, V projections, our accelerator achieves significant
speedup and energy efficiency gains over CPU baselines. Standalone GEMM
benchmarks show up to a 7x speedup over an ARM CPU (PyTorch) and ~200x over
naive numpy, with a throughput of up to 3.1 GFLOPs on 768x3072 matrices.
Although the overall end-to-end DistilBERT acceleration is more modest, our
results validate the potential of FPGA-based acceleration for critical
components of Transformer models.","['cs.AR', 'cs.CL', 'cs.LG', 'B.7.1; C.1.4']","['Zhaoqin ""Richie"" Li', 'Sicheng Chen']",2025-03-20,2025-03-20
2503.16728v1,Natural Language Generation,"This article provides a brief overview of the field of Natural Language
Generation. The term Natural Language Generation (NLG), in its broadest
definition, refers to the study of systems that verbalize some form of
information through natural language. That information could be stored in a
large database or knowledge graph (in data-to-text applications), but NLG
researchers may also study summarisation (text-to-text) or image captioning
(image-to-text), for example. As a subfield of Natural Language Processing, NLG
is closely related to other sub-disciplines such as Machine Translation (MT)
and Dialog Systems. Some NLG researchers exclude MT from their definition of
the field, since there is no content selection involved where the system has to
determine what to say. Conversely, dialog systems do not typically fall under
the header of Natural Language Generation since NLG is just one component of
dialog systems (the others being Natural Language Understanding and Dialog
Management). However, with the rise of Large Language Models (LLMs), different
subfields of Natural Language Processing have converged on similar
methodologies for the production of natural language and the evaluation of
automatically generated text.",['cs.CL'],"['Emiel van Miltenburg', 'Chenghua Lin']",2025-03-20,2025-03-20
2503.16726v1,EDiT: Efficient Diffusion Transformers with Linear Compressed Attention,"Diffusion Transformers (DiTs) have emerged as a leading architecture for
text-to-image synthesis, producing high-quality and photorealistic images.
However, the quadratic scaling properties of the attention in DiTs hinder image
generation with higher resolution or on devices with limited resources. This
work introduces an efficient diffusion transformer (EDiT) to alleviate these
efficiency bottlenecks in conventional DiTs and Multimodal DiTs (MM-DiTs).
First, we present a novel linear compressed attention method that uses a
multi-layer convolutional network to modulate queries with local information
while keys and values are spatially aggregated. Second, we formulate a hybrid
attention scheme for multi-modal inputs that combines linear attention for
image-to-image interactions and standard scaled dot-product attention for
interactions involving prompts. Merging these two approaches leads to an
expressive, linear-time Multimodal Efficient Diffusion Transformer (MM-EDiT).
We demonstrate the effectiveness of the EDiT and MM-EDiT architectures by
integrating them into PixArt-Sigma(conventional DiT) and Stable Diffusion
3.5-Medium (MM-DiT), achieving up to 2.2x speedup with comparable image quality
after distillation.","['cs.CV', 'cs.LG']","['Philipp Becker', 'Abhinav Mehrotra', 'Ruchika Chavhan', 'Malcolm Chadwick', 'Luca Morreale', 'Mehdi Noroozi', 'Alberto Gil Ramos', 'Sourav Bhattacharya']",2025-03-20,2025-03-20
2503.16724v1,Towards Automated Semantic Interpretability in Reinforcement Learning via Vision-Language Models,"Semantic Interpretability in Reinforcement Learning (RL) enables
transparency, accountability, and safer deployment by making the agent's
decisions understandable and verifiable. Achieving this, however, requires a
feature space composed of human-understandable concepts, which traditionally
rely on human specification and fail to generalize to unseen environments. In
this work, we introduce Semantically Interpretable Reinforcement Learning with
Vision-Language Models Empowered Automation (SILVA), an automated framework
that leverages pre-trained vision-language models (VLM) for semantic feature
extraction and interpretable tree-based models for policy optimization. SILVA
first queries a VLM to identify relevant semantic features for an unseen
environment, then extracts these features from the environment. Finally, it
trains an Interpretable Control Tree via RL, mapping the extracted features to
actions in a transparent and interpretable manner. To address the computational
inefficiency of extracting features directly with VLMs, we develop a feature
extraction pipeline that generates a dataset for training a lightweight
convolutional network, which is subsequently used during RL. By leveraging VLMs
to automate tree-based RL, SILVA removes the reliance on human annotation
previously required by interpretable models while also overcoming the inability
of VLMs alone to generate valid robot policies, enabling semantically
interpretable reinforcement learning without human-in-the-loop.","['cs.AI', 'cs.LG']","['Zhaoxin Li', 'Zhang Xi-Jia', 'Batuhan Altundas', 'Letian Chen', 'Rohan Paleja', 'Matthew Gombolay']",2025-03-20,2025-03-20
2503.16718v1,CAARMA: Class Augmentation with Adversarial Mixup Regularization,"Speaker verification is a typical zero-shot learning task, where inference of
unseen classes is performed by comparing embeddings of test instances to known
examples. The models performing inference must hence naturally generate
embeddings that cluster same-class instances compactly, while maintaining
separation across classes. In order to learn to do so, they are typically
trained on a large number of classes (speakers), often using specialized
losses. However real-world speaker datasets often lack the class diversity
needed to effectively learn this in a generalizable manner. We introduce
CAARMA, a class augmentation framework that addresses this problem by
generating synthetic classes through data mixing in the embedding space,
expanding the number of training classes. To ensure the authenticity of the
synthetic classes we adopt a novel adversarial refinement mechanism that
minimizes categorical distinctions between synthetic and real classes. We
evaluate CAARMA on multiple speaker verification tasks, as well as other
representative zero-shot comparison-based speech analysis tasks and obtain
consistent improvements: our framework demonstrates a significant improvement
of 8\% over all baseline models. Code for CAARMA will be released.","['cs.SD', 'cs.CL', 'cs.LG']","['Massa Baali', 'Xiang Li', 'Hao Chen', 'Rita Singh', 'Bhiksha Raj']",2025-03-20,2025-03-20
2503.16711v1,Depth Matters: Multimodal RGB-D Perception for Robust Autonomous Agents,"Autonomous agents that rely purely on perception to make real-time control
decisions require efficient and robust architectures. In this work, we
demonstrate that augmenting RGB input with depth information significantly
enhances our agents' ability to predict steering commands compared to using RGB
alone. We benchmark lightweight recurrent controllers that leverage the fused
RGB-D features for sequential decision-making. To train our models, we collect
high-quality data using a small-scale autonomous car controlled by an expert
driver via a physical steering wheel, capturing varying levels of steering
difficulty. Our models, trained under diverse configurations, were successfully
deployed on real hardware. Specifically, our findings reveal that the early
fusion of depth data results in a highly robust controller, which remains
effective even with frame drops and increased noise levels, without
compromising the network's focus on the task.","['cs.RO', 'cs.CV', 'cs.LG']","['Mihaela-Larisa Clement', 'Mónika Farsang', 'Felix Resch', 'Radu Grosu']",2025-03-20,2025-03-20
2503.16710v1,4D Gaussian Splatting SLAM,"Simultaneously localizing camera poses and constructing Gaussian radiance
fields in dynamic scenes establish a crucial bridge between 2D images and the
4D real world. Instead of removing dynamic objects as distractors and
reconstructing only static environments, this paper proposes an efficient
architecture that incrementally tracks camera poses and establishes the 4D
Gaussian radiance fields in unknown scenarios by using a sequence of RGB-D
images. First, by generating motion masks, we obtain static and dynamic priors
for each pixel. To eliminate the influence of static scenes and improve the
efficiency on learning the motion of dynamic objects, we classify the Gaussian
primitives into static and dynamic Gaussian sets, while the sparse control
points along with an MLP is utilized to model the transformation fields of the
dynamic Gaussians. To more accurately learn the motion of dynamic Gaussians, a
novel 2D optical flow map reconstruction algorithm is designed to render
optical flows of dynamic objects between neighbor images, which are further
used to supervise the 4D Gaussian radiance fields along with traditional
photometric and geometric constraints. In experiments, qualitative and
quantitative evaluation results show that the proposed method achieves robust
tracking and high-quality view synthesis performance in real-world
environments.",['cs.CV'],"['Yanyan Li', 'Youxu Fang', 'Zunjie Zhu', 'Kunyi Li', 'Yong Ding', 'Federico Tombari']",2025-03-20,2025-03-20
2503.16709v1,QuartDepth: Post-Training Quantization for Real-Time Depth Estimation on the Edge,"Monocular Depth Estimation (MDE) has emerged as a pivotal task in computer
vision, supporting numerous real-world applications. However, deploying
accurate depth estimation models on resource-limited edge devices, especially
Application-Specific Integrated Circuits (ASICs), is challenging due to the
high computational and memory demands. Recent advancements in foundational
depth estimation deliver impressive results but further amplify the difficulty
of deployment on ASICs. To address this, we propose QuartDepth which adopts
post-training quantization to quantize MDE models with hardware accelerations
for ASICs. Our approach involves quantizing both weights and activations to
4-bit precision, reducing the model size and computation cost. To mitigate the
performance degradation, we introduce activation polishing and compensation
algorithm applied before and after activation quantization, as well as a weight
reconstruction method for minimizing errors in weight quantization.
Furthermore, we design a flexible and programmable hardware accelerator by
supporting kernel fusion and customized instruction programmability, enhancing
throughput and efficiency. Experimental results demonstrate that our framework
achieves competitive accuracy while enabling fast inference and higher energy
efficiency on ASICs, bridging the gap between high-performance depth estimation
and practical edge-device applicability. Code:
https://github.com/shawnricecake/quart-depth","['cs.CV', 'cs.AI']","['Xuan Shen', 'Weize Ma', 'Jing Liu', 'Changdi Yang', 'Rui Ding', 'Quanyi Wang', 'Henghui Ding', 'Wei Niu', 'Yanzhi Wang', 'Pu Zhao', 'Jun Lin', 'Jiuxiang Gu']",2025-03-20,2025-03-20
2503.16708v1,NeuroSep-CP-LCB: A Deep Learning-based Contextual Multi-armed Bandit Algorithm with Uncertainty Quantification for Early Sepsis Prediction,"In critical care settings, timely and accurate predictions can significantly
impact patient outcomes, especially for conditions like sepsis, where early
intervention is crucial. We aim to model patient-specific reward functions in a
contextual multi-armed bandit setting. The goal is to leverage patient-specific
clinical features to optimize decision-making under uncertainty. This paper
proposes NeuroSep-CP-LCB, a novel integration of neural networks with
contextual bandits and conformal prediction tailored for early sepsis
detection. Unlike the algorithm pool selection problem in the previous paper,
where the primary focus was identifying the most suitable pre-trained model for
prediction tasks, this work directly models the reward function using a neural
network, allowing for personalized and adaptive decision-making. Combining the
representational power of neural networks with the robustness of conformal
prediction intervals, this framework explicitly accounts for uncertainty in
offline data distributions and provides actionable confidence bounds on
predictions.",['cs.LG'],"['Anni Zhou', 'Raheem Beyah', 'Rishikesan Kamaleswaran']",2025-03-20,2025-03-20
2503.16707v1,Cross-Modal and Uncertainty-Aware Agglomeration for Open-Vocabulary 3D Scene Understanding,"The lack of a large-scale 3D-text corpus has led recent works to distill
open-vocabulary knowledge from vision-language models (VLMs). owever, these
methods typically rely on a single VLM to align the feature spaces of 3D models
within a common language space, which limits the potential of 3D models to
leverage the diverse spatial and semantic capabilities encapsulated in various
foundation models. In this paper, we propose Cross-modal and Uncertainty-aware
Agglomeration for Open-vocabulary 3D Scene Understanding dubbed CUA-O3D, the
first model to integrate multiple foundation models-such as CLIP, DINOv2, and
Stable Diffusion-into 3D scene understanding. We further introduce a
deterministic uncertainty estimation to adaptively distill and harmonize the
heterogeneous 2D feature embeddings from these models. Our method addresses two
key challenges: (1) incorporating semantic priors from VLMs alongside the
geometric knowledge of spatially-aware vision foundation models, and (2) using
a novel deterministic uncertainty estimation to capture model-specific
uncertainties across diverse semantic and geometric sensitivities, helping to
reconcile heterogeneous representations during training. Extensive experiments
on ScanNetV2 and Matterport3D demonstrate that our method not only advances
open-vocabulary segmentation but also achieves robust cross-domain alignment
and competitive spatial perception capabilities. The code will be available at
\href{https://github.com/TyroneLi/CUA_O3D}{CUA_O3D}.",['cs.CV'],"['Jinlong Li', 'Cristiano Saltori', 'Fabio Poiesi', 'Nicu Sebe']",2025-03-20,2025-03-20
2503.16700v1,Deep Q-Learning with Gradient Target Tracking,"This paper introduces Q-learning with gradient target tracking, a novel
reinforcement learning framework that provides a learned continuous target
update mechanism as an alternative to the conventional hard update paradigm. In
the standard deep Q-network (DQN), the target network is a copy of the online
network's weights, held fixed for a number of iterations before being
periodically replaced via a hard update. While this stabilizes training by
providing consistent targets, it introduces a new challenge: the hard update
period must be carefully tuned to achieve optimal performance. To address this
issue, we propose two gradient-based target update methods: DQN with asymmetric
gradient target tracking (AGT2-DQN) and DQN with symmetric gradient target
tracking (SGT2-DQN). These methods replace the conventional hard target updates
with continuous and structured updates using gradient descent, which
effectively eliminates the need for manual tuning. We provide a theoretical
analysis proving the convergence of these methods in tabular settings.
Additionally, empirical evaluations demonstrate their advantages over standard
DQN baselines, which suggest that gradient-based target updates can serve as an
effective alternative to conventional target update mechanisms in Q-learning.","['cs.LG', 'cs.SY', 'eess.SY']","['Donghwan Lee', 'Bum Geun Park', 'Taeho Lee']",2025-03-20,2025-03-20
2503.16696v1,Universal approximation property of neural stochastic differential equations,"We identify various classes of neural networks that are able to approximate
continuous functions locally uniformly subject to fixed global linear growth
constraints. For such neural networks the associated neural stochastic
differential equations can approximate general stochastic differential
equations, both of It\^o diffusion type, arbitrarily well. Moreover,
quantitative error estimates are derived for stochastic differential equations
with sufficiently regular coefficients.","['math.PR', 'cs.LG', 'math.FA', 'q-fin.MF', 'stat.ML', '41A29, 60H10, 68T07, 91G80']","['Anna P. Kwossek', 'David J. Prömel', 'Josef Teichmann']",2025-03-20,2025-03-20
2503.16693v1,ATOM: A Framework of Detecting Query-Based Model Extraction Attacks for Graph Neural Networks,"Graph Neural Networks (GNNs) have gained traction in Graph-based Machine
Learning as a Service (GMLaaS) platforms, yet they remain vulnerable to
graph-based model extraction attacks (MEAs), where adversaries reconstruct
surrogate models by querying the victim model. Existing defense mechanisms,
such as watermarking and fingerprinting, suffer from poor real-time
performance, susceptibility to evasion, or reliance on post-attack
verification, making them inadequate for handling the dynamic characteristics
of graph-based MEA variants. To address these limitations, we propose ATOM, a
novel real-time MEA detection framework tailored for GNNs. ATOM integrates
sequential modeling and reinforcement learning to dynamically detect evolving
attack patterns, while leveraging $k$-core embedding to capture the structural
properties, enhancing detection precision. Furthermore, we provide theoretical
analysis to characterize query behaviors and optimize detection strategies.
Extensive experiments on multiple real-world datasets demonstrate that ATOM
outperforms existing approaches in detection performance, maintaining stable
across different time steps, thereby offering a more effective defense
mechanism for GMLaaS environments.","['cs.LG', 'cs.CR']","['Zhan Cheng', 'Bolin Shen', 'Tianming Sha', 'Yuan Gao', 'Shibo Li', 'Yushun Dong']",2025-03-20,2025-03-20
2503.16692v1,Limits of trust in medical AI,"Artificial intelligence (AI) is expected to revolutionize the practice of
medicine. Recent advancements in the field of deep learning have demonstrated
success in a variety of clinical tasks: detecting diabetic retinopathy from
images, predicting hospital readmissions, aiding in the discovery of new drugs,
etc. AI's progress in medicine, however, has led to concerns regarding the
potential effects of this technology upon relationships of trust in clinical
practice. In this paper, I will argue that there is merit to these concerns,
since AI systems can be relied upon, and are capable of reliability, but cannot
be trusted, and are not capable of trustworthiness. Insofar as patients are
required to rely upon AI systems for their medical decision-making, there is
potential for this to produce a deficit of trust in relationships in clinical
practice.","['cs.LG', 'cs.AI', 'cs.CY', 'J.3; K.4']",['Joshua Hatherley'],2025-03-20,2025-03-20
2503.16690v1,Making the unmodulated pyramid wavefront sensor smart II. First on-sky demonstration of extreme adaptive optics with deep learning,"Pyramid wavefront sensors (PWFSs) are the preferred choice for current and
future extreme adaptive optics (XAO) systems. Almost all instruments use the
PWFS in its modulated form to mitigate its limited linearity range. However,
this modulation comes at the cost of a reduction in sensitivity, a blindness to
petal-piston modes, and a limit to the sensor's ability to operate at high
speeds. Therefore, there is strong interest to use the PWFS without modulation,
which can be enabled with nonlinear reconstructors. Here, we present the first
on-sky demonstration of XAO with an unmodulated PWFS using a nonlinear
reconstructor based on convolutional neural networks. We discuss the real-time
implementation on the Magellan Adaptive Optics eXtreme (MagAO-X) instrument
using the optimized TensorRT framework and show that inference is fast enough
to run the control loop at >2 kHz frequencies. Our on-sky results demonstrate a
successful closed-loop operation using a model calibrated with internal source
data that delivers stable and robust correction under varying conditions.
Performance analysis reveals that our smart PWFS achieves nearly the same
Strehl ratio as the highly optimized modulated PWFS under favorable conditions
on bright stars. Notably, we observe an improvement in performance on a fainter
star under the influence of strong winds. These findings confirm the
feasibility of using the PWFS in its unmodulated form and highlight its
potential for next-generation instruments. Future efforts will focus on
achieving even higher control loop frequencies (>3 kHz), optimizing the
calibration procedures, and testing its performance on fainter stars, where
more gain is expected for the unmodulated PWFS compared to its modulated
counterpart.","['astro-ph.IM', 'astro-ph.EP', 'cs.LG', 'physics.optics']","['R. Landman', 'S. Y. Haffert', 'J. D. Long', 'J. R. Males', 'L. M. Close', 'W. B. Foster', 'K. Van Gorkom', 'O. Guyon', 'A. D. Hedglen', 'P. T. Johnson', 'M. Y. Kautz', 'J. K. Kueny', 'J. Li', 'J. Liberman', 'J. Lumbres', 'E. A. McEwen', 'A. McLeod', 'L. Schatz', 'E. Tonucci', 'K. Twitchell']",2025-03-20,2025-03-20
2503.16689v1,WaveFM: A High-Fidelity and Efficient Vocoder Based on Flow Matching,"Flow matching offers a robust and stable approach to training diffusion
models. However, directly applying flow matching to neural vocoders can result
in subpar audio quality. In this work, we present WaveFM, a reparameterized
flow matching model for mel-spectrogram conditioned speech synthesis, designed
to enhance both sample quality and generation speed for diffusion vocoders.
Since mel-spectrograms represent the energy distribution of waveforms, WaveFM
adopts a mel-conditioned prior distribution instead of a standard Gaussian
prior to minimize unnecessary transportation costs during synthesis. Moreover,
while most diffusion vocoders rely on a single loss function, we argue that
incorporating auxiliary losses, including a refined multi-resolution STFT loss,
can further improve audio quality. To speed up inference without degrading
sample quality significantly, we introduce a tailored consistency distillation
method for WaveFM. Experiment results demonstrate that our model achieves
superior performance in both quality and efficiency compared to previous
diffusion vocoders, while enabling waveform generation in a single inference
step.","['cs.SD', 'cs.CL']","['Tianze Luo', 'Xingchen Miao', 'Wenbo Duan']",2025-03-20,2025-03-20
2503.16683v1,GAIR: Improving Multimodal Geo-Foundation Model with Geo-Aligned Implicit Representations,"Advancements in vision and language foundation models have inspired the
development of geo-foundation models (GeoFMs), enhancing performance across
diverse geospatial tasks. However, many existing GeoFMs primarily focus on
overhead remote sensing (RS) data while neglecting other data modalities such
as ground-level imagery. A key challenge in multimodal GeoFM development is to
explicitly model geospatial relationships across modalities, which enables
generalizability across tasks, spatial scales, and temporal contexts. To
address these limitations, we propose GAIR, a novel multimodal GeoFM
architecture integrating overhead RS data, street view (SV) imagery, and their
geolocation metadata. We utilize three factorized neural encoders to project an
SV image, its geolocation, and an RS image into the embedding space. The SV
image needs to be located within the RS image's spatial footprint but does not
need to be at its geographic center. In order to geographically align the SV
image and RS image, we propose a novel implicit neural representations (INR)
module that learns a continuous RS image representation and looks up the RS
embedding at the SV image's geolocation. Next, these geographically aligned SV
embedding, RS embedding, and location embedding are trained with contrastive
learning objectives from unlabeled data. We evaluate GAIR across 10 geospatial
tasks spanning RS image-based, SV image-based, and location embedding-based
benchmarks. Experimental results demonstrate that GAIR outperforms
state-of-the-art GeoFMs and other strong baselines, highlighting its
effectiveness in learning generalizable and transferable geospatial
representations.","['cs.CV', 'cs.AI', 'I.4.10']","['Zeping Liu', 'Fan Zhang', 'Junfeng Jiao', 'Ni Lao', 'Gengchen Mai']",2025-03-20,2025-03-20
2503.16681v1,GauRast: Enhancing GPU Triangle Rasterizers to Accelerate 3D Gaussian Splatting,"3D intelligence leverages rich 3D features and stands as a promising frontier
in AI, with 3D rendering fundamental to many downstream applications. 3D
Gaussian Splatting (3DGS), an emerging high-quality 3D rendering method,
requires significant computation, making real-time execution on existing
GPU-equipped edge devices infeasible. Previous efforts to accelerate 3DGS rely
on dedicated accelerators that require substantial integration overhead and
hardware costs. This work proposes an acceleration strategy that leverages the
similarities between the 3DGS pipeline and the highly optimized conventional
graphics pipeline in modern GPUs. Instead of developing a dedicated
accelerator, we enhance existing GPU rasterizer hardware to efficiently support
3DGS operations. Our results demonstrate a 23$\times$ increase in processing
speed and a 24$\times$ reduction in energy consumption, with improvements
yielding 6$\times$ faster end-to-end runtime for the original 3DGS algorithm
and 4$\times$ for the latest efficiency-improved pipeline, achieving 24 FPS and
46 FPS respectively. These enhancements incur only a minimal area overhead of
0.2\% relative to the entire SoC chip area, underscoring the practicality and
efficiency of our approach for enabling 3DGS rendering on resource-constrained
platforms.","['cs.GR', 'cs.AI', 'cs.AR']","['Sixu Li', 'Ben Keller', 'Yingyan Celine Lin', 'Brucek Khailany']",2025-03-20,2025-03-20
2503.16679v1,Echoes of Power: Investigating Geopolitical Bias in US and China Large Language Models,"Large Language Models (LLMs) have emerged as powerful tools for generating
human-like text, transforming human-machine interactions. However, their
widespread adoption has raised concerns about their potential to influence
public opinion and shape political narratives. In this work, we investigate the
geopolitical biases in US and Chinese LLMs, focusing on how these models
respond to questions related to geopolitics and international relations. We
collected responses from ChatGPT and DeepSeek to a set of geopolitical
questions and evaluated their outputs through both qualitative and quantitative
analyses. Our findings show notable biases in both models, reflecting distinct
ideological perspectives and cultural influences. However, despite these
biases, for a set of questions, the models' responses are more aligned than
expected, indicating that they can address sensitive topics without necessarily
presenting directly opposing viewpoints. This study highlights the potential of
LLMs to shape public discourse and underscores the importance of critically
assessing AI-generated content, particularly in politically sensitive contexts.","['cs.CY', 'cs.AI', 'cs.HC']","['Andre G. C. Pacheco', 'Athus Cavalini', 'Giovanni Comarela']",2025-03-20,2025-03-20
2503.16678v1,QCPINN: Quantum Classical Physics-Informed Neural Networks for Solving PDEs,"Hybrid quantum-classical neural network methods represent an emerging
approach to solving computational challenges by leveraging advantages from both
paradigms. As physics-informed neural networks (PINNs) have successfully
applied to solve partial differential equations (PDEs) by incorporating
physical constraints into neural architectures, this work investigates whether
quantum-classical physics-informed neural networks (QCPINNs) can efficiently
solve PDEs with reduced parameter counts compared to classical approaches. We
evaluate two quantum circuit paradigms: continuous-variable (CV) and
qubit-based discrete-variable (DV) across multiple circuit ansatze (Alternate,
Cascade, Cross mesh, and Layered). Benchmarking across five challenging PDEs
(Helmholtz, Cavity, Wave, Klein-Gordon, and Convection-Diffusion equations)
demonstrates that our hybrid approaches achieve comparable accuracy to
classical PINNs while requiring up to 89% fewer trainable parameters. DV-based
implementations, particularly those with angle encoding and cascade circuit
configurations, exhibit better stability and convergence properties across all
problem types. For the Convection-Diffusion equation, our angle-cascade QCPINN
achieves parameter efficiency and a 37% reduction in relative L2 error compared
to classical counterparts. Our findings highlight the potential of
quantum-enhanced architectures for physics-informed learning, establishing
parameter efficiency as a quantifiable quantum advantage while providing a
foundation for future quantum-classical hybrid systems solving complex physical
models.","['quant-ph', 'cs.LG']","['Afrah Farea', 'Saiful Khan', 'Mustafa Serdar Celebi']",2025-03-20,2025-03-20
2503.16674v1,"Through the LLM Looking Glass: A Socratic Self-Assessment of Donkeys, Elephants, and Markets","While detecting and avoiding bias in LLM-generated text is becoming
increasingly important, media bias often remains subtle and subjective, making
it particularly difficult to identify and mitigate. In this study, we assess
media bias in LLM-generated content and LLMs' ability to detect subtle
ideological bias. We conduct this evaluation using two datasets, PoliGen and
EconoLex, covering political and economic discourse, respectively. We evaluate
eight widely used LLMs by prompting them to generate articles and analyze their
ideological preferences via self-assessment. By using self-assessment, the
study aims to directly measure the models' biases rather than relying on
external interpretations, thereby minimizing subjective judgments about media
bias. Our results reveal a consistent preference of Democratic over Republican
positions across all models. Conversely, in economic topics, biases vary among
Western LLMs, while those developed in China lean more strongly toward
socialism.",['cs.CL'],"['Molly Kennedy', 'Ayyoob Imani', 'Timo Spinde', 'Hinrich Schütze']",2025-03-20,2025-03-20
2503.16673v1,Subgradient Method for System Identification with Non-Smooth Objectives,"This paper investigates a subgradient-based algorithm to solve the system
identification problem for linear time-invariant systems with non-smooth
objectives. This is essential for robust system identification in
safety-critical applications. While existing work provides theoretical exact
recovery guarantees using optimization solvers, the design of fast learning
algorithms with convergence guarantees for practical use remains unexplored. We
analyze the subgradient method in this setting where the optimization problems
to be solved change over time as new measurements are taken, and we establish
linear convergence results for both the best and Polyak step sizes after a
burn-in period. Additionally, we characterize the asymptotic convergence of the
best average sub-optimality gap under diminishing and constant step sizes.
Finally, we compare the time complexity of standard solvers with the
subgradient algorithm and support our findings with experimental results. This
is the first work to analyze subgradient algorithms for system identification
with non-smooth objectives.","['math.OC', 'cs.CC', 'cs.LG', 'cs.SY', 'eess.SY', '62, 90, 93']","['Baturalp Yalcin', 'Javad Lavaei']",2025-03-20,2025-03-20
2503.16672v1,Accelerating Transformer Inference and Training with 2:4 Activation Sparsity,"In this paper, we demonstrate how to leverage 2:4 sparsity, a popular
hardware-accelerated GPU sparsity pattern, to activations to accelerate large
language model training and inference. Crucially we exploit the intrinsic
sparsity found in Squared-ReLU activations to provide this acceleration with no
accuracy loss. Our approach achieves up to 1.3x faster Feed Forward Network
(FFNs) in both the forwards and backwards pass. This work highlights the
potential for sparsity to play a key role in accelerating large language model
training and inference.","['cs.LG', 'cs.AI', 'I.2']","['Daniel Haziza', 'Timothy Chou', 'Dhruv Choudhary', 'Luca Wehrstedt', 'Francisco Massa', 'Jiecao Yu', 'Geonhwa Jeong', 'Supriya Rao', 'Patrick Labatut', 'Jesse Cai']",2025-03-20,2025-03-20
2503.16669v1,Aligning Text-to-Music Evaluation with Human Preferences,"Despite significant recent advances in generative acoustic text-to-music
(TTM) modeling, robust evaluation of these models lags behind, relying in
particular on the popular Fr\'echet Audio Distance (FAD). In this work, we
rigorously study the design space of reference-based divergence metrics for
evaluating TTM models through (1) designing four synthetic meta-evaluations to
measure sensitivity to particular musical desiderata, and (2) collecting and
evaluating on MusicPrefs, the first open-source dataset of human preferences
for TTM systems. We find that not only is the standard FAD setup inconsistent
on both synthetic and human preference data, but that nearly all existing
metrics fail to effectively capture desiderata, and are only weakly correlated
with human perception. We propose a new metric, the MAUVE Audio Divergence
(MAD), computed on representations from a self-supervised audio embedding
model. We find that this metric effectively captures diverse musical desiderata
(average rank correlation 0.84 for MAD vs. 0.49 for FAD and also correlates
more strongly with MusicPrefs (0.62 vs. 0.14).","['cs.SD', 'cs.AI', 'eess.AS']","['Yichen Huang', 'Zachary Novack', 'Koichi Saito', 'Jiatong Shi', 'Shinji Watanabe', 'Yuki Mitsufuji', 'John Thickstun', 'Chris Donahue']",2025-03-20,2025-03-20
2503.16668v1,Code Evolution Graphs: Understanding Large Language Model Driven Design of Algorithms,"Large Language Models (LLMs) have demonstrated great promise in generating
code, especially when used inside an evolutionary computation framework to
iteratively optimize the generated algorithms. However, in some cases they fail
to generate competitive algorithms or the code optimization stalls, and we are
left with no recourse because of a lack of understanding of the generation
process and generated codes. We present a novel approach to mitigate this
problem by enabling users to analyze the generated codes inside the
evolutionary process and how they evolve over repeated prompting of the LLM. We
show results for three benchmark problem classes and demonstrate novel
insights. In particular, LLMs tend to generate more complex code with repeated
prompting, but additional complexity can hurt algorithmic performance in some
cases. Different LLMs have different coding ``styles'' and generated code tends
to be dissimilar to other LLMs. These two findings suggest that using different
LLMs inside the code evolution frameworks might produce higher performing code
than using only one LLM.","['cs.NE', 'cs.AI']","['Niki van Stein', 'Anna V. Kononova', 'Lars Kotthoff', 'Thomas Bäck']",2025-03-20,2025-03-20
2503.16667v1,A preliminary data fusion study to assess the feasibility of Foundation Process-Property Models in Laser Powder Bed Fusion,"Foundation models are at the forefront of an increasing number of critical
applications. In regards to technologies such as additive manufacturing (AM),
these models have the potential to dramatically accelerate process optimization
and, in turn, design of next generation materials. A major challenge that
impedes the construction of foundation process-property models is data
scarcity. To understand the impact of this challenge, and since foundation
models rely on data fusion, in this work we conduct controlled experiments
where we focus on the transferability of information across different material
systems and properties. More specifically, we generate experimental datasets
from 17-4 PH and 316L stainless steels (SSs) in Laser Powder Bed Fusion (LPBF)
where we measure the effect of five process parameters on porosity and
hardness. We then leverage Gaussian processes (GPs) for process-property
modeling in various configurations to test if knowledge about one material
system or property can be leveraged to build more accurate machine learning
models for other material systems or properties. Through extensive
cross-validation studies and probing the GPs' interpretable hyperparameters, we
study the intricate relation among data size and dimensionality, complexity of
the process-property relations, noise, and characteristics of machine learning
models. Our findings highlight the need for structured learning approaches that
incorporate domain knowledge in building foundation process-property models
rather than relying on uninformed data fusion in data-limited applications.",['cs.LG'],"['Oriol Vendrell-Gallart', 'Nima Negarandeh', 'Zahra Zanjani Foumani', 'Mahsa Amiri', 'Lorenzo Valdevit', 'Ramin Bostanabad']",2025-03-20,2025-03-20
2503.16666v1,Efficient Training of Neural Fractional-Order Differential Equation via Adjoint Backpropagation,"Fractional-order differential equations (FDEs) enhance traditional
differential equations by extending the order of differential operators from
integers to real numbers, offering greater flexibility in modeling complex
dynamical systems with nonlocal characteristics. Recent progress at the
intersection of FDEs and deep learning has catalyzed a new wave of innovative
models, demonstrating the potential to address challenges such as graph
representation learning. However, training neural FDEs has primarily relied on
direct differentiation through forward-pass operations in FDE numerical
solvers, leading to increased memory usage and computational complexity,
particularly in large-scale applications. To address these challenges, we
propose a scalable adjoint backpropagation method for training neural FDEs by
solving an augmented FDE backward in time, which substantially reduces memory
requirements. This approach provides a practical neural FDE toolbox and holds
considerable promise for diverse applications. We demonstrate the effectiveness
of our method in several tasks, achieving performance comparable to baseline
models while significantly reducing computational overhead.",['cs.LG'],"['Qiyu Kang', 'Xuhao Li', 'Kai Zhao', 'Wenjun Cui', 'Yanan Zhao', 'Weihua Deng', 'Wee Peng Tay']",2025-03-20,2025-03-20
2503.16664v1,TextBite: A Historical Czech Document Dataset for Logical Page Segmentation,"Logical page segmentation is an important step in document analysis, enabling
better semantic representations, information retrieval, and text understanding.
Previous approaches define logical segmentation either through text or
geometric objects, relying on OCR or precise geometry. To avoid the need for
OCR, we define the task purely as segmentation in the image domain.
Furthermore, to ensure the evaluation remains unaffected by geometrical
variations that do not impact text segmentation, we propose to use only
foreground text pixels in the evaluation metric and disregard all background
pixels. To support research in logical document segmentation, we introduce
TextBite, a dataset of historical Czech documents spanning the 18th to 20th
centuries, featuring diverse layouts from newspapers, dictionaries, and
handwritten records. The dataset comprises 8,449 page images with 78,863
annotated segments of logically and thematically coherent text. We propose a
set of baseline methods combining text region detection and relation
prediction. The dataset, baselines and evaluation framework can be accessed at
https://github.com/DCGM/textbite-dataset.",['cs.CV'],"['Martin Kostelník', 'Karel Beneš', 'Michal Hradiš']",2025-03-20,2025-03-20
2503.16661v1,ContextGNN goes to Elliot: Towards Benchmarking Relational Deep Learning for Static Link Prediction (aka Personalized Item Recommendation),"Relational deep learning (RDL) settles among the most exciting advances in
machine learning for relational databases, leveraging the representational
power of message passing graph neural networks (GNNs) to derive useful
knowledge and run predicting tasks on tables connected through
primary-to-foreign key links. The RDL paradigm has been successfully applied to
recommendation lately, through its most recent representative deep learning
architecture namely, ContextGNN. While acknowledging ContextGNN's improved
performance on real-world recommendation datasets and tasks, preliminary tests
for the more traditional static link prediction task (aka personalized item
recommendation) on the popular Amazon Book dataset have demonstrated how
ContextGNN has still room for improvement compared to other state-of-the-art
GNN-based recommender systems. To this end, with this paper, we integrate
ContextGNN within Elliot, a popular framework for reproducibility and
benchmarking analyses, counting around 50 state-of-the-art recommendation
models from the literature to date. On such basis, we run preliminary
experiments on three standard recommendation datasets and against six
state-of-the-art GNN-based recommender systems, confirming similar trends to
those observed by the authors in their original paper. The code is publicly
available on GitHub:
https://github.com/danielemalitesta/Rel-DeepLearning-RecSys.","['cs.IR', 'cs.LG']","['Alejandro Ariza-Casabona', 'Nikos Kanakaris', 'Daniele Malitesta']",2025-03-20,2025-03-20
2503.16660v1,When Less is Enough: Adaptive Token Reduction for Efficient Image Representation,"Vision encoders typically generate a large number of visual tokens, providing
information-rich representations but significantly increasing computational
demands. This raises the question of whether all generated tokens are equally
valuable or if some of them can be discarded to reduce computational costs
without compromising quality. In this paper, we introduce a new method for
determining feature utility based on the idea that less valuable features can
be reconstructed from more valuable ones. We implement this concept by
integrating an autoencoder with a Gumbel-Softmax selection mechanism, that
allows identifying and retaining only the most informative visual tokens. To
validate our approach, we compared the performance of the LLaVA-NeXT model,
using features selected by our method with randomly selected features. We found
that on OCR-based tasks, more than 50% of the visual context can be removed
with minimal performance loss, whereas randomly discarding the same proportion
of features significantly affects the model capabilities. Furthermore, in
general-domain tasks, even randomly retaining only 30% of tokens achieves
performance comparable to using the full set of visual tokens. Our results
highlight a promising direction towards adaptive and efficient multimodal
pruning that facilitates scalable and low-overhead inference without
compromising performance.","['cs.CV', '68T10, 68T30, 68T45', 'I.2.10']","['Eduard Allakhverdov', 'Elizaveta Goncharova', 'Andrey Kuznetsov']",2025-03-20,2025-03-20
2503.16659v1,"Advances in Protein Representation Learning: Methods, Applications, and Future Directions","Proteins are complex biomolecules that play a central role in various
biological processes, making them critical targets for breakthroughs in
molecular biology, medical research, and drug discovery. Deciphering their
intricate, hierarchical structures, and diverse functions is essential for
advancing our understanding of life at the molecular level. Protein
Representation Learning (PRL) has emerged as a transformative approach,
enabling the extraction of meaningful computational representations from
protein data to address these challenges. In this paper, we provide a
comprehensive review of PRL research, categorizing methodologies into five key
areas: feature-based, sequence-based, structure-based, multimodal, and
complex-based approaches. To support researchers in this rapidly evolving
field, we introduce widely used databases for protein sequences, structures,
and functions, which serve as essential resources for model development and
evaluation. We also explore the diverse applications of these approaches in
multiple domains, demonstrating their broad impact. Finally, we discuss
pressing technical challenges and outline future directions to advance PRL,
offering insights to inspire continued innovation in this foundational field.","['cs.LG', 'q-bio.BM']","['Viet Thanh Duy Nguyen', 'Truong-Son Hy']",2025-03-20,2025-03-20
2503.16655v1,Accelerating Antibiotic Discovery with Large Language Models and Knowledge Graphs,"The discovery of novel antibiotics is critical to address the growing
antimicrobial resistance (AMR). However, pharmaceutical industries face high
costs (over $1 billion), long timelines, and a high failure rate, worsened by
the rediscovery of known compounds. We propose an LLM-based pipeline that acts
as an alarm system, detecting prior evidence of antibiotic activity to prevent
costly rediscoveries. The system integrates organism and chemical literature
into a Knowledge Graph (KG), ensuring taxonomic resolution, synonym handling,
and multi-level evidence classification. We tested the pipeline on a private
list of 73 potential antibiotic-producing organisms, disclosing 12 negative
hits for evaluation. The results highlight the effectiveness of the pipeline
for evidence reviewing, reducing false negatives, and accelerating
decision-making. The KG for negative hits and the user interface for
interactive exploration will be made publicly available.",['cs.CL'],"['Maxime Delmas', 'Magdalena Wysocka', 'Danilo Gusicuma', 'André Freitas']",2025-03-20,2025-03-20
2503.16653v1,iFlame: Interleaving Full and Linear Attention for Efficient Mesh Generation,"This paper propose iFlame, a novel transformer-based network architecture for
mesh generation. While attention-based models have demonstrated remarkable
performance in mesh generation, their quadratic computational complexity limits
scalability, particularly for high-resolution 3D data. Conversely, linear
attention mechanisms offer lower computational costs but often struggle to
capture long-range dependencies, resulting in suboptimal outcomes. To address
this trade-off, we propose an interleaving autoregressive mesh generation
framework that combines the efficiency of linear attention with the expressive
power of full attention mechanisms. To further enhance efficiency and leverage
the inherent structure of mesh representations, we integrate this interleaving
approach into an hourglass architecture, which significantly boosts efficiency.
Our approach reduces training time while achieving performance comparable to
pure attention-based models. To improve inference efficiency, we implemented a
caching algorithm that almost doubles the speed and reduces the KV cache size
by seven-eighths compared to the original Transformer. We evaluate our
framework on ShapeNet and Objaverse, demonstrating its ability to generate
high-quality 3D meshes efficiently. Our results indicate that the proposed
interleaving framework effectively balances computational efficiency and
generative performance, making it a practical solution for mesh generation. The
training takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces
on Objaverse.",['cs.CV'],"['Hanxiao Wang', 'Biao Zhang', 'Weize Quan', 'Dong-Ming Yan', 'Peter Wonka']",2025-03-20,2025-03-20
2503.16644v1,To impute or not to impute: How machine learning modelers treat missing data,"Missing data is prevalent in tabular machine learning (ML) models, and
different missing data treatment methods can significantly affect ML model
training results. However, little is known about how ML researchers and
engineers choose missing data treatment methods and what factors affect their
choices. To this end, we conducted a survey of 70 ML researchers and engineers.
Our results revealed that most participants were not making informed decisions
regarding missing data treatment, which could significantly affect the validity
of the ML models trained by these researchers. We advocate for better education
on missing data, more standardized missing data reporting, and better missing
data analysis tools.","['cs.LG', 'cs.HC']","['Wanyi Chen', 'Mary Cummings']",2025-03-20,2025-03-20
2503.16639v1,"Whenever, Wherever: Towards Orchestrating Crowd Simulations with Spatio-Temporal Spawn Dynamics","Realistic crowd simulations are essential for immersive virtual environments,
relying on both individual behaviors (microscopic dynamics) and overall crowd
patterns (macroscopic characteristics). While recent data-driven methods like
deep reinforcement learning improve microscopic realism, they often overlook
critical macroscopic features such as crowd density and flow, which are
governed by spatio-temporal spawn dynamics, namely, when and where agents enter
a scene. Traditional methods, like random spawn rates, stochastic processes, or
fixed schedules, are not guaranteed to capture the underlying complexity or
lack diversity and realism. To address this issue, we propose a novel approach
called nTPP-GMM that models spatio-temporal spawn dynamics using Neural
Temporal Point Processes (nTPPs) that are coupled with a spawn-conditional
Gaussian Mixture Model (GMM) for agent spawn and goal positions. We evaluate
our approach by orchestrating crowd simulations of three diverse real-world
datasets with nTPP-GMM. Our experiments demonstrate the orchestration with
nTPP-GMM leads to realistic simulations that reflect real-world crowd scenarios
and allow crowd analysis.",['cs.LG'],"['Thomas Kreutz', 'Max Mühlhäuser', 'Alejandro Sanchez Guinea']",2025-03-20,2025-03-20
2503.16635v1,Fed-NDIF: A Noise-Embedded Federated Diffusion Model For Low-Count Whole-Body PET Denoising,"Low-count positron emission tomography (LCPET) imaging can reduce patients'
exposure to radiation but often suffers from increased image noise and reduced
lesion detectability, necessitating effective denoising techniques. Diffusion
models have shown promise in LCPET denoising for recovering degraded image
quality. However, training such models requires large and diverse datasets,
which are challenging to obtain in the medical domain. To address data scarcity
and privacy concerns, we combine diffusion models with federated learning -- a
decentralized training approach where models are trained individually at
different sites, and their parameters are aggregated on a central server over
multiple iterations. The variation in scanner types and image noise levels
within and across institutions poses additional challenges for federated
learning in LCPET denoising. In this study, we propose a novel noise-embedded
federated learning diffusion model (Fed-NDIF) to address these challenges,
leveraging a multicenter dataset and varying count levels. Our approach
incorporates liver normalized standard deviation (NSTD) noise embedding into a
2.5D diffusion model and utilizes the Federated Averaging (FedAvg) algorithm to
aggregate locally trained models into a global model, which is subsequently
fine-tuned on local datasets to optimize performance and obtain personalized
models. Extensive validation on datasets from the University of Bern, Ruijin
Hospital in Shanghai, and Yale-New Haven Hospital demonstrates the superior
performance of our method in enhancing image quality and improving lesion
quantification. The Fed-NDIF model shows significant improvements in PSNR,
SSIM, and NMSE of the entire 3D volume, as well as enhanced lesion
detectability and quantification, compared to local diffusion models and
federated UNet-based models.","['eess.IV', 'cs.CV']","['Yinchi Zhou', 'Huidong Xie', 'Menghua Xia', 'Qiong Liu', 'Bo Zhou', 'Tianqi Chen', 'Jun Hou', 'Liang Guo', 'Xinyuan Zheng', 'Hanzhong Wang', 'Biao Li', 'Axel Rominger', 'Kuangyu Shi', 'Nicha C. Dvorneka', 'Chi Liu']",2025-03-20,2025-03-20
2503.16630v1,TriTex: Learning Texture from a Single Mesh via Triplane Semantic Features,"As 3D content creation continues to grow, transferring semantic textures
between 3D meshes remains a significant challenge in computer graphics. While
recent methods leverage text-to-image diffusion models for texturing, they
often struggle to preserve the appearance of the source texture during texture
transfer. We present \ourmethod, a novel approach that learns a volumetric
texture field from a single textured mesh by mapping semantic features to
surface colors. Using an efficient triplane-based architecture, our method
enables semantic-aware texture transfer to a novel target mesh. Despite
training on just one example, it generalizes effectively to diverse shapes
within the same category. Extensive evaluation on our newly created benchmark
dataset shows that \ourmethod{} achieves superior texture transfer quality and
fast inference times compared to existing methods. Our approach advances
single-example texture transfer, providing a practical solution for maintaining
visual coherence across related 3D models in applications like game development
and simulation.","['cs.GR', 'cs.CV']","['Dana Cohen-Bar', 'Daniel Cohen-Or', 'Gal Chechik', 'Yoni Kasten']",2025-03-20,2025-03-20
2503.16629v1,Utilizing Reinforcement Learning for Bottom-Up part-wise Reconstruction of 2D Wire-Frame Projections,"This work concerns itself with the task of reconstructing all edges of an
arbitrary 3D wire-frame model projected to an image plane. We explore a
bottom-up part-wise procedure undertaken by an RL agent to segment and
reconstruct these 2D multipart objects. The environment's state is represented
as a four-colour image, where different colours correspond to background, a
target edge, a reconstruction line, and the overlap of both. At each step, the
agent can transform the reconstruction line within a four-dimensional action
space or terminate the episode using a specific termination action. To
investigate the impact of reward function formulations, we tested episodic and
incremental rewards, as well as combined approaches. Empirical results
demonstrated that the latter yielded the most effective training performance.
To further enhance efficiency and stability, we introduce curriculum learning
strategies. First, an action-based curriculum was implemented, where the agent
was initially restricted to a reduced action space, being able to only perform
three of the five possible actions, before progressing to the full action
space. Second, we test a task-based curriculum, where the agent first solves a
simplified version of the problem before being presented with the full, more
complex task. This second approach produced promising results, as the agent not
only successfully transitioned from learning the simplified task to mastering
the full task, but in doing so gained significant performance. This study
demonstrates the potential of an iterative RL wire-frame reconstruction in two
dimensions. By combining optimized reward function formulations with curriculum
learning strategies, we achieved significant improvements in training success.
The proposed methodology provides an effective framework for solving similar
tasks and represents a promising direction for future research in the field.","['cs.LG', 'cs.CV']","['Julian Ziegler', 'Patrick Frenzel', 'Mirco Fuchs']",2025-03-20,2025-03-20
2503.16628v1,MobilePlantViT: A Mobile-friendly Hybrid ViT for Generalized Plant Disease Image Classification,"Plant diseases significantly threaten global food security by reducing crop
yields and undermining agricultural sustainability. AI-driven automated
classification has emerged as a promising solution, with deep learning models
demonstrating impressive performance in plant disease identification. However,
deploying these models on mobile and edge devices remains challenging due to
high computational demands and resource constraints, highlighting the need for
lightweight, accurate solutions for accessible smart agriculture systems. To
address this, we propose MobilePlantViT, a novel hybrid Vision Transformer
(ViT) architecture designed for generalized plant disease classification, which
optimizes resource efficiency while maintaining high performance. Extensive
experiments across diverse plant disease datasets of varying scales show our
model's effectiveness and strong generalizability, achieving test accuracies
ranging from 80% to over 99%. Notably, with only 0.69 million parameters, our
architecture outperforms the smallest versions of MobileViTv1 and MobileViTv2,
despite their higher parameter counts. These results underscore the potential
of our approach for real-world, AI-powered automated plant disease
classification in sustainable and resource-efficient smart agriculture systems.
All codes will be available in the GitHub repository:
https://github.com/moshiurtonmoy/MobilePlantViT","['cs.CV', 'cs.AI', 'cs.LG']","['Moshiur Rahman Tonmoy', 'Md. Mithun Hossain', 'Nilanjan Dey', 'M. F. Mridha']",2025-03-20,2025-03-20
2503.16622v1,Leveraging Large Language Models for Explainable Activity Recognition in Smart Homes: A Critical Evaluation,"Explainable Artificial Intelligence (XAI) aims to uncover the inner reasoning
of machine learning models. In IoT systems, XAI improves the transparency of
models processing sensor data from multiple heterogeneous devices, ensuring
end-users understand and trust their outputs. Among the many applications, XAI
has also been applied to sensor-based Activities of Daily Living (ADLs)
recognition in smart homes. Existing approaches highlight which sensor events
are most important for each predicted activity, using simple rules to convert
these events into natural language explanations for non-expert users. However,
these methods produce rigid explanations lacking natural language flexibility
and are not scalable. With the recent rise of Large Language Models (LLMs), it
is worth exploring whether they can enhance explanation generation, considering
their proven knowledge of human activities. This paper investigates potential
approaches to combine XAI and LLMs for sensor-based ADL recognition. We
evaluate if LLMs can be used: a) as explainable zero-shot ADL recognition
models, avoiding costly labeled data collection, and b) to automate the
generation of explanations for existing data-driven XAI approaches when
training data is available and the goal is higher recognition rates. Our
critical evaluation provides insights into the benefits and challenges of using
LLMs for explainable ADL recognition.",['cs.CL'],"['Michele Fiori', 'Gabriele Civitarese', 'Priyankar Choudhary', 'Claudio Bettini']",2025-03-20,2025-03-20
2503.16616v1,Progressive Test Time Energy Adaptation for Medical Image Segmentation,"We propose a model-agnostic, progressive test-time energy adaptation approach
for medical image segmentation. Maintaining model performance across diverse
medical datasets is challenging, as distribution shifts arise from inconsistent
imaging protocols and patient variations. Unlike domain adaptation methods that
require multiple passes through target data - impractical in clinical settings
- our approach adapts pretrained models progressively as they process test
data. Our method leverages a shape energy model trained on source data, which
assigns an energy score at the patch level to segmentation maps: low energy
represents in-distribution (accurate) shapes, while high energy signals
out-of-distribution (erroneous) predictions. By minimizing this energy score at
test time, we refine the segmentation model to align with the target
distribution. To validate the effectiveness and adaptability, we evaluated our
framework on eight public MRI (bSSFP, T1- and T2-weighted) and X-ray datasets
spanning cardiac, spinal cord, and lung segmentation. We consistently
outperform baselines both quantitatively and qualitatively.",['cs.CV'],"['Xiaoran Zhang', 'Byung-Woo Hong', 'Hyoungseob Park', 'Daniel H. Pak', 'Anne-Marie Rickmann', 'Lawrence H. Staib', 'James S. Duncan', 'Alex Wong']",2025-03-20,2025-03-20
2503.16614v1,Classification of User Reports for Detection of Faulty Computer Components using NLP Models: A Case Study,"Computer manufacturers typically offer platforms for users to report faults.
However, there remains a significant gap in these platforms' ability to
effectively utilize textual reports, which impedes users from describing their
issues in their own words. In this context, Natural Language Processing (NLP)
offers a promising solution, by enabling the analysis of user-generated text.
This paper presents an innovative approach that employs NLP models to classify
user reports for detecting faulty computer components, such as CPU, memory,
motherboard, video card, and more. In this work, we build a dataset of 341 user
reports obtained from many sources. Additionally, through extensive
experimental evaluation, our approach achieved an accuracy of 79% with our
dataset.","['cs.CL', 'cs.AI', 'cs.LG']","['Maria de Lourdes M. Silva', 'André L. C. Mendonça', 'Eduardo R. D. Neto', 'Iago C. Chaves', 'Felipe T. Brito', 'Victor A. E. Farias', 'Javam C. Machado']",2025-03-20,2025-03-20
2503.16613v1,Informative Path Planning to Explore and Map Unknown Planetary Surfaces with Gaussian Processes,"Many environments, such as unvisited planetary surfaces and oceanic regions,
remain unexplored due to a lack of prior knowledge. Autonomous vehicles must
sample upon arrival, process data, and either transmit findings to a
teleoperator or decide where to explore next. Teleoperation is suboptimal, as
human intuition lacks mathematical guarantees for optimality. This study
evaluates an informative path planning algorithm for mapping a scalar variable
distribution while minimizing travel distance and ensuring model convergence.
We compare traditional open loop coverage methods (e.g., Boustrophedon, Spiral)
with information-theoretic approaches using Gaussian processes, which update
models iteratively with confidence metrics. The algorithm's performance is
tested on three surfaces, a parabola, Townsend function, and lunar crater
hydration map, to assess noise, convexity, and function behavior. Results
demonstrate that information-driven methods significantly outperform naive
exploration in reducing model error and travel distance while improving
convergence potential.","['cs.RO', 'cs.IR', 'cs.LG']","['Ashten Akemoto', 'Frances Zhu']",2025-03-20,2025-03-20
2503.16611v1,A Recipe for Generating 3D Worlds From a Single Image,"We introduce a recipe for generating immersive 3D worlds from a single image
by framing the task as an in-context learning problem for 2D inpainting models.
This approach requires minimal training and uses existing generative models.
Our process involves two steps: generating coherent panoramas using a
pre-trained diffusion model and lifting these into 3D with a metric depth
estimator. We then fill unobserved regions by conditioning the inpainting model
on rendered point clouds, requiring minimal fine-tuning. Tested on both
synthetic and real images, our method produces high-quality 3D environments
suitable for VR display. By explicitly modeling the 3D structure of the
generated environment from the start, our approach consistently outperforms
state-of-the-art, video synthesis-based methods along multiple quantitative
image quality metrics. Project Page: https://katjaschwarz.github.io/worlds/","['cs.CV', 'cs.AI', 'cs.LG']","['Katja Schwarz', 'Denys Rozumnyi', 'Samuel Rota Bulò', 'Lorenzo Porzi', 'Peter Kontschieder']",2025-03-20,2025-03-20
2503.16429v1,Sonata: Self-Supervised Learning of Reliable Point Representations,"In this paper, we question whether we have a reliable self-supervised point
cloud model that can be used for diverse 3D tasks via simple linear probing,
even with limited data and minimal computation. We find that existing 3D
self-supervised learning approaches fall short when evaluated on representation
quality through linear probing. We hypothesize that this is due to what we term
the ""geometric shortcut"", which causes representations to collapse to low-level
spatial features. This challenge is unique to 3D and arises from the sparse
nature of point cloud data. We address it through two key strategies: obscuring
spatial information and enhancing the reliance on input features, ultimately
composing a Sonata of 140k point clouds through self-distillation. Sonata is
simple and intuitive, yet its learned representations are strong and reliable:
zero-shot visualizations demonstrate semantic grouping, alongside strong
spatial reasoning through nearest-neighbor relationships. Sonata demonstrates
exceptional parameter and data efficiency, tripling linear probing accuracy
(from 21.8% to 72.5%) on ScanNet and nearly doubling performance with only 1%
of the data compared to previous approaches. Full fine-tuning further advances
SOTA across both 3D indoor and outdoor perception tasks.",['cs.CV'],"['Xiaoyang Wu', 'Daniel DeTone', 'Duncan Frost', 'Tianwei Shen', 'Chris Xie', 'Nan Yang', 'Jakob Engel', 'Richard Newcombe', 'Hengshuang Zhao', 'Julian Straub']",2025-03-20,2025-03-20
2503.16430v1,Bridging Continuous and Discrete Tokens for Autoregressive Visual Generation,"Autoregressive visual generation models typically rely on tokenizers to
compress images into tokens that can be predicted sequentially. A fundamental
dilemma exists in token representation: discrete tokens enable straightforward
modeling with standard cross-entropy loss, but suffer from information loss and
tokenizer training instability; continuous tokens better preserve visual
details, but require complex distribution modeling, complicating the generation
pipeline. In this paper, we propose TokenBridge, which bridges this gap by
maintaining the strong representation capacity of continuous tokens while
preserving the modeling simplicity of discrete tokens. To achieve this, we
decouple discretization from the tokenizer training process through
post-training quantization that directly obtains discrete tokens from
continuous representations. Specifically, we introduce a dimension-wise
quantization strategy that independently discretizes each feature dimension,
paired with a lightweight autoregressive prediction mechanism that efficiently
model the resulting large token space. Extensive experiments show that our
approach achieves reconstruction and generation quality on par with continuous
methods while using standard categorical prediction. This work demonstrates
that bridging discrete and continuous paradigms can effectively harness the
strengths of both approaches, providing a promising direction for high-quality
visual generation with simple autoregressive modeling. Project page:
https://yuqingwang1029.github.io/TokenBridge.",['cs.CV'],"['Yuqing Wang', 'Zhijie Lin', 'Yao Teng', 'Yuanzhi Zhu', 'Shuhuai Ren', 'Jiashi Feng', 'Xihui Liu']",2025-03-20,2025-03-20
2503.16428v1,XAttention: Block Sparse Attention with Antidiagonal Scoring,"Long-Context Transformer Models (LCTMs) are vital for real-world applications
but suffer high computational costs due to attention's quadratic complexity.
Block-sparse attention mitigates this by focusing computation on critical
regions, yet existing methods struggle with balancing accuracy and efficiency
due to costly block importance measurements. In this paper, we introduce
XAttention, a plug-and-play framework that dramatically accelerates
long-context inference in Transformers models using sparse attention.
XAttention's key innovation is the insight that the sum of antidiagonal values
(i.e., from the lower-left to upper-right) in the attention matrix provides a
powerful proxy for block importance. This allows for precise identification and
pruning of non-essential blocks, resulting in high sparsity and dramatically
accelerated inference. Across comprehensive evaluations on demanding
long-context benchmarks-including RULER and LongBench for language, VideoMME
for video understanding, and VBench for video generation. XAttention achieves
accuracy comparable to full attention while delivering substantial
computational gains. We demonstrate up to 13.5x acceleration in attention
computation. These results underscore XAttention's ability to unlock the
practical potential of block sparse attention, paving the way for scalable and
efficient deployment of LCTMs in real-world applications. Code is available at
https://github.com/mit-han-lab/x-attention.","['cs.CL', 'cs.CV']","['Ruyi Xu', 'Guangxuan Xiao', 'Haofeng Huang', 'Junxian Guo', 'Song Han']",2025-03-20,2025-03-20
2503.16426v1,DynamicVis: An Efficient and General Visual Foundation Model for Remote Sensing Image Understanding,"The advancement of remote sensing technology has improved the spatial
resolution of satellite imagery, facilitating more detailed visual
representations for diverse interpretations. However, existing methods exhibit
limited generalization capabilities across varied applications. While some
contemporary foundation models demonstrate potential, they are hindered by
insufficient cross-task adaptability and primarily process low-resolution
imagery of restricted sizes, thus failing to fully exploit high-resolution data
or leverage comprehensive large-scene semantics. Crucially, remote sensing
imagery differs fundamentally from natural images, as key foreground targets
(eg., maritime objects, artificial structures) often occupy minimal spatial
proportions (~1%) and exhibit sparse distributions. Efficiently modeling
cross-task generalizable knowledge from lengthy 2D tokens (~100,000) poses a
significant challenge yet remains critical for remote sensing image
understanding. Motivated by the selective attention mechanisms inherent to the
human visual system, we propose DynamicVis, a dynamic visual perception
foundation model for remote sensing imagery. The framework integrates a novel
dynamic region perception backbone based on the selective state space model,
which strategically balances localized detail extraction with global contextual
integration, enabling computationally efficient encoding of large-scale data
while maintaining architectural scalability. To enhance cross-task knowledge
transferring, we introduce a multi-instance learning paradigm utilizing
meta-embedding representations, trained on million-scale region-level
annotations. Evaluations across nine downstream tasks demonstrate the model's
versatility. DynamicVis achieves multi-level feature modeling with exceptional
efficiency, processing (2048x2048) pixels with 97 ms latency (6% of ViT's) and
833 MB GPU memory (3% of ViT's).",['cs.CV'],"['Keyan Chen', 'Chenyang Liu', 'Bowen Chen', 'Wenyuan Li', 'Zhengxia Zou', 'Zhenwei Shi']",2025-03-20,2025-03-20
2503.16425v1,Tokenize Image as a Set,"This paper proposes a fundamentally new paradigm for image generation through
set-based tokenization and distribution modeling. Unlike conventional methods
that serialize images into fixed-position latent codes with a uniform
compression ratio, we introduce an unordered token set representation to
dynamically allocate coding capacity based on regional semantic complexity.
This TokenSet enhances global context aggregation and improves robustness
against local perturbations. To address the critical challenge of modeling
discrete sets, we devise a dual transformation mechanism that bijectively
converts sets into fixed-length integer sequences with summation constraints.
Further, we propose Fixed-Sum Discrete Diffusion--the first framework to
simultaneously handle discrete values, fixed sequence length, and summation
invariance--enabling effective set distribution modeling. Experiments
demonstrate our method's superiority in semantic-aware representation and
generation quality. Our innovations, spanning novel representation and modeling
strategies, advance visual generation beyond traditional sequential token
paradigms. Our code and models are publicly available at
https://github.com/Gengzigang/TokenSet.",['cs.CV'],"['Zigang Geng', 'Mengde Xu', 'Han Hu', 'Shuyang Gu']",2025-03-20,2025-03-20
2503.16424v1,Bézier Splatting for Fast and Differentiable Vector Graphics,"Differentiable vector graphics (VGs) are widely used in image vectorization
and vector synthesis, while existing representations are costly to optimize and
struggle to achieve high-quality rendering results for high-resolution images.
This work introduces a new differentiable VG representation, dubbed B\'ezier
splatting, that enables fast yet high-fidelity VG rasterization. B\'ezier
splatting samples 2D Gaussians along B\'ezier curves, which naturally provide
positional gradients at object boundaries. Thanks to the efficient
splatting-based differentiable rasterizer, B\'ezier splatting achieves over 20x
and 150x faster per forward and backward rasterization step for open curves
compared to DiffVG. Additionally, we introduce an adaptive pruning and
densification strategy that dynamically adjusts the spatial distribution of
curves to escape local minima, further improving VG quality. Experimental
results show that B\'ezier splatting significantly outperforms existing methods
with better visual fidelity and 10x faster optimization speed.","['cs.GR', 'cs.CV']","['Xi Liu', 'Chaoyi Zhou', 'Nanxuan Zhao', 'Siyu Huang']",2025-03-20,2025-03-20
2503.16423v1,GAEA: A Geolocation Aware Conversational Model,"Image geolocalization, in which, traditionally, an AI model predicts the
precise GPS coordinates of an image is a challenging task with many downstream
applications. However, the user cannot utilize the model to further their
knowledge other than the GPS coordinate; the model lacks an understanding of
the location and the conversational ability to communicate with the user. In
recent days, with tremendous progress of large multimodal models (LMMs)
proprietary and open-source researchers have attempted to geolocalize images
via LMMs. However, the issues remain unaddressed; beyond general tasks, for
more specialized downstream tasks, one of which is geolocalization, LMMs
struggle. In this work, we propose to solve this problem by introducing a
conversational model GAEA that can provide information regarding the location
of an image, as required by a user. No large-scale dataset enabling the
training of such a model exists. Thus we propose a comprehensive dataset GAEA
with 800K images and around 1.6M question answer pairs constructed by
leveraging OpenStreetMap (OSM) attributes and geographical context clues. For
quantitative evaluation, we propose a diverse benchmark comprising 4K
image-text pairs to evaluate conversational capabilities equipped with diverse
question types. We consider 11 state-of-the-art open-source and proprietary
LMMs and demonstrate that GAEA significantly outperforms the best open-source
model, LLaVA-OneVision by 25.69% and the best proprietary model, GPT-4o by
8.28%. Our dataset, model and codes are available","['cs.CV', 'cs.LG', 'I.4; I.2.7; I.5']","['Ron Campos', 'Ashmal Vayani', 'Parth Parag Kulkarni', 'Rohit Gupta', 'Aritra Dutta', 'Mubarak Shah']",2025-03-20,2025-03-20
2503.16422v1,1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering,"4D Gaussian Splatting (4DGS) has recently gained considerable attention as a
method for reconstructing dynamic scenes. Despite achieving superior quality,
4DGS typically requires substantial storage and suffers from slow rendering
speed. In this work, we delve into these issues and identify two key sources of
temporal redundancy. (Q1) \textbf{Short-Lifespan Gaussians}: 4DGS uses a large
portion of Gaussians with short temporal span to represent scene dynamics,
leading to an excessive number of Gaussians. (Q2) \textbf{Inactive Gaussians}:
When rendering, only a small subset of Gaussians contributes to each frame.
Despite this, all Gaussians are processed during rasterization, resulting in
redundant computation overhead. To address these redundancies, we present
\textbf{4DGS-1K}, which runs at over 1000 FPS on modern GPUs. For Q1, we
introduce the Spatial-Temporal Variation Score, a new pruning criterion that
effectively removes short-lifespan Gaussians while encouraging 4DGS to capture
scene dynamics using Gaussians with longer temporal spans. For Q2, we store a
mask for active Gaussians across consecutive frames, significantly reducing
redundant computations in rendering. Compared to vanilla 4DGS, our method
achieves a $41\times$ reduction in storage and $9\times$ faster rasterization
speed on complex dynamic scenes, while maintaining comparable visual quality.
Please see our project page at https://4DGS-1K.github.io.",['cs.CV'],"['Yuheng Yuan', 'Qiuhong Shen', 'Xingyi Yang', 'Xinchao Wang']",2025-03-20,2025-03-20
2503.16421v1,MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance,"Recent advances in video generation have led to remarkable improvements in
visual quality and temporal coherence. Upon this, trajectory-controllable video
generation has emerged to enable precise object motion control through
explicitly defined spatial paths. However, existing methods struggle with
complex object movements and multi-object motion control, resulting in
imprecise trajectory adherence, poor object consistency, and compromised visual
quality. Furthermore, these methods only support trajectory control in a single
format, limiting their applicability in diverse scenarios. Additionally, there
is no publicly available dataset or benchmark specifically tailored for
trajectory-controllable video generation, hindering robust training and
systematic evaluation. To address these challenges, we introduce MagicMotion, a
novel image-to-video generation framework that enables trajectory control
through three levels of conditions from dense to sparse: masks, bounding boxes,
and sparse boxes. Given an input image and trajectories, MagicMotion seamlessly
animates objects along defined trajectories while maintaining object
consistency and visual quality. Furthermore, we present MagicData, a
large-scale trajectory-controlled video dataset, along with an automated
pipeline for annotation and filtering. We also introduce MagicBench, a
comprehensive benchmark that assesses both video quality and trajectory control
accuracy across different numbers of objects. Extensive experiments demonstrate
that MagicMotion outperforms previous methods across various metrics. Our
project page are publicly available at
https://quanhaol.github.io/magicmotion-site.","['cs.CV', 'cs.AI', 'cs.LG', 'cs.MM']","['Quanhao Li', 'Zhen Xing', 'Rui Wang', 'Hui Zhang', 'Qi Dai', 'Zuxuan Wu']",2025-03-20,2025-03-20
2503.16420v1,SynCity: Training-Free Generation of 3D Worlds,"We address the challenge of generating 3D worlds from textual descriptions.
We propose SynCity, a training- and optimization-free approach, which leverages
the geometric precision of pre-trained 3D generative models and the artistic
versatility of 2D image generators to create large, high-quality 3D spaces.
While most 3D generative models are object-centric and cannot generate
large-scale worlds, we show how 3D and 2D generators can be combined to
generate ever-expanding scenes. Through a tile-based approach, we allow
fine-grained control over the layout and the appearance of scenes. The world is
generated tile-by-tile, and each new tile is generated within its world-context
and then fused with the scene. SynCity generates compelling and immersive
scenes that are rich in detail and diversity.",['cs.CV'],"['Paul Engstler', 'Aleksandar Shtedritski', 'Iro Laina', 'Christian Rupprecht', 'Andrea Vedaldi']",2025-03-20,2025-03-20
2503.16419v1,Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models,"Large Language Models (LLMs) have demonstrated remarkable capabilities in
complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as
OpenAI o1 and DeepSeek-R1, have further improved performance in System-2
reasoning domains like mathematics and programming by harnessing supervised
fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the
Chain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences
improve performance, they also introduce significant computational overhead due
to verbose and redundant outputs, known as the ""overthinking phenomenon"". In
this paper, we provide the first structured survey to systematically
investigate and explore the current progress toward achieving efficient
reasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we
categorize existing works into several key directions: (1) model-based
efficient reasoning, which considers optimizing full-length reasoning models
into more concise reasoning models or directly training efficient reasoning
models; (2) reasoning output-based efficient reasoning, which aims to
dynamically reduce reasoning steps and length during inference; (3) input
prompts-based efficient reasoning, which seeks to enhance reasoning efficiency
based on input prompt properties such as difficulty or length control.
Additionally, we introduce the use of efficient data for training reasoning
models, explore the reasoning capabilities of small language models, and
discuss evaluation methods and benchmarking.",['cs.CL'],"['Yang Sui', 'Yu-Neng Chuang', 'Guanchu Wang', 'Jiamu Zhang', 'Tianyi Zhang', 'Jiayi Yuan', 'Hongyi Liu', 'Andrew Wen', 'Shaochen', 'Zhong', 'Hanjie Chen', 'Xia Hu']",2025-03-20,2025-03-20
2503.16418v1,InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity,"Achieving flexible and high-fidelity identity-preserved image generation
remains formidable, particularly with advanced Diffusion Transformers (DiTs)
like FLUX. We introduce InfiniteYou (InfU), one of the earliest robust
frameworks leveraging DiTs for this task. InfU addresses significant issues of
existing methods, such as insufficient identity similarity, poor text-image
alignment, and low generation quality and aesthetics. Central to InfU is
InfuseNet, a component that injects identity features into the DiT base model
via residual connections, enhancing identity similarity while maintaining
generation capabilities. A multi-stage training strategy, including pretraining
and supervised fine-tuning (SFT) with synthetic single-person-multiple-sample
(SPMS) data, further improves text-image alignment, ameliorates image quality,
and alleviates face copy-pasting. Extensive experiments demonstrate that InfU
achieves state-of-the-art performance, surpassing existing baselines. In
addition, the plug-and-play design of InfU ensures compatibility with various
existing methods, offering a valuable contribution to the broader community.","['cs.CV', 'cs.LG']","['Liming Jiang', 'Qing Yan', 'Yumin Jia', 'Zichuan Liu', 'Hao Kang', 'Xin Lu']",2025-03-20,2025-03-20
2503.16416v1,Survey on Evaluation of LLM-based Agents,"The emergence of LLM-based agents represents a paradigm shift in AI, enabling
autonomous systems to plan, reason, use tools, and maintain memory while
interacting with dynamic environments. This paper provides the first
comprehensive survey of evaluation methodologies for these increasingly capable
agents. We systematically analyze evaluation benchmarks and frameworks across
four critical dimensions: (1) fundamental agent capabilities, including
planning, tool use, self-reflection, and memory; (2) application-specific
benchmarks for web, software engineering, scientific, and conversational
agents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating
agents. Our analysis reveals emerging trends, including a shift toward more
realistic, challenging evaluations with continuously updated benchmarks. We
also identify critical gaps that future research must address-particularly in
assessing cost-efficiency, safety, and robustness, and in developing
fine-grained, and scalable evaluation methods. This survey maps the rapidly
evolving landscape of agent evaluation, reveals the emerging trends in the
field, identifies current limitations, and proposes directions for future
research.","['cs.AI', 'cs.CL', 'cs.LG']","['Asaf Yehudai', 'Lilach Eden', 'Alan Li', 'Guy Uziel', 'Yilun Zhao', 'Roy Bar-Haim', 'Arman Cohan', 'Michal Shmueli-Scheuer']",2025-03-20,2025-03-20
2503.16412v1,DreamTexture: Shape from Virtual Texture with Analysis by Augmentation,"DreamFusion established a new paradigm for unsupervised 3D reconstruction
from virtual views by combining advances in generative models and
differentiable rendering. However, the underlying multi-view rendering, along
with supervision from large-scale generative models, is computationally
expensive and under-constrained. We propose DreamTexture, a novel
Shape-from-Virtual-Texture approach that leverages monocular depth cues to
reconstruct 3D objects. Our method textures an input image by aligning a
virtual texture with the real depth cues in the input, exploiting the inherent
understanding of monocular geometry encoded in modern diffusion models. We then
reconstruct depth from the virtual texture deformation with a new conformal map
optimization, which alleviates memory-intensive volumetric representations. Our
experiments reveal that generative models possess an understanding of monocular
shape cues, which can be extracted by augmenting and aligning texture cues -- a
novel monocular reconstruction paradigm that we call Analysis by Augmentation.","['cs.CV', 'cs.AI', 'cs.LG']","['Ananta R. Bhattarai', 'Xingzhe He', 'Alla Sheffer', 'Helge Rhodin']",2025-03-20,2025-03-20
2503.16413v1,M3: 3D-Spatial MultiModal Memory,"We present 3D Spatial MultiModal Memory (M3), a multimodal memory system
designed to retain information about medium-sized static scenes through video
sources for visual perception. By integrating 3D Gaussian Splatting techniques
with foundation models, M3 builds a multimodal memory capable of rendering
feature representations across granularities, encompassing a wide range of
knowledge. In our exploration, we identify two key challenges in previous works
on feature splatting: (1) computational constraints in storing high-dimensional
features for each Gaussian primitive, and (2) misalignment or information loss
between distilled features and foundation model features. To address these
challenges, we propose M3 with key components of principal scene components and
Gaussian memory attention, enabling efficient training and inference. To
validate M3, we conduct comprehensive quantitative evaluations of feature
similarity and downstream tasks, as well as qualitative visualizations to
highlight the pixel trace of Gaussian memory attention. Our approach
encompasses a diverse range of foundation models, including vision-language
models (VLMs), perception models, and large multimodal and language models
(LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy
M3's feature field in indoor scenes on a quadruped robot. Notably, we claim
that M3 is the first work to address the core compression challenges in 3D
feature distillation.","['cs.CV', 'cs.RO']","['Xueyan Zou', 'Yuchen Song', 'Ri-Zhao Qiu', 'Xuanbin Peng', 'Jianglong Ye', 'Sifei Liu', 'Xiaolong Wang']",2025-03-20,2025-03-20
2503.16408v1,RoboFactory: Exploring Embodied Agent Collaboration with Compositional Constraints,"Designing effective embodied multi-agent systems is critical for solving
complex real-world tasks across domains. Due to the complexity of multi-agent
embodied systems, existing methods fail to automatically generate safe and
efficient training data for such systems. To this end, we propose the concept
of compositional constraints for embodied multi-agent systems, addressing the
challenges arising from collaboration among embodied agents. We design various
interfaces tailored to different types of constraints, enabling seamless
interaction with the physical world. Leveraging compositional constraints and
specifically designed interfaces, we develop an automated data collection
framework for embodied multi-agent systems and introduce the first benchmark
for embodied multi-agent manipulation, RoboFactory. Based on RoboFactory
benchmark, we adapt and evaluate the method of imitation learning and analyzed
its performance in different difficulty agent tasks. Furthermore, we explore
the architectures and training strategies for multi-agent imitation learning,
aiming to build safe and efficient embodied multi-agent systems.","['cs.RO', 'cs.AI', 'cs.CV', 'cs.LG']","['Yiran Qin', 'Li Kang', 'Xiufeng Song', 'Zhenfei Yin', 'Xiaohong Liu', 'Xihui Liu', 'Ruimao Zhang', 'Lei Bai']",2025-03-20,2025-03-20
2503.16594v1,Transformer-based Wireless Symbol Detection Over Fading Channels,"Pre-trained Transformers, through in-context learning (ICL), have
demonstrated exceptional capabilities to adapt to new tasks using example
prompts without model update. Transformer-based wireless receivers, where
prompts consist of the pilot data in the form of transmitted and received
signal pairs, have shown high detection accuracy when pilot data are abundant.
However, pilot information is often costly and limited in practice. In this
work, we propose the DEcision Feedback INcontExt Detection (DEFINED) solution
as a new wireless receiver design, which bypasses channel estimation and
directly performs symbol detection using the (sometimes extremely) limited
pilot data. The key innovation in DEFINED is the proposed decision feedback
mechanism in ICL, where we sequentially incorporate the detected symbols into
the prompts as pseudo-labels to improve the detection for subsequent symbols.
Furthermore, we proposed another detection method where we combine ICL with
Semi-Supervised Learning (SSL) to extract information from both labeled and
unlabeled data during inference, thus avoiding the errors propagated during the
decision feedback process of the original DEFINED. Extensive experiments across
a broad range of wireless communication settings demonstrate that a small
Transformer trained with DEFINED or IC-SSL achieves significant performance
improvements over conventional methods, in some cases only needing a single
pilot pair to achieve similar performance of the latter with more than 4 pilot
pairs.","['cs.IT', 'cs.LG', 'eess.SP', 'math.IT', 'stat.ML']","['Li Fan', 'Jing Yang', 'Cong Shen']",2025-03-20,2025-03-20
2503.16406v1,VerbDiff: Text-Only Diffusion Models with Enhanced Interaction Awareness,"Recent large-scale text-to-image diffusion models generate photorealistic
images but often struggle to accurately depict interactions between humans and
objects due to their limited ability to differentiate various interaction
words. In this work, we propose VerbDiff to address the challenge of capturing
nuanced interactions within text-to-image diffusion models. VerbDiff is a novel
text-to-image generation model that weakens the bias between interaction words
and objects, enhancing the understanding of interactions. Specifically, we
disentangle various interaction words from frequency-based anchor words and
leverage localized interaction regions from generated images to help the model
better capture semantics in distinctive words without extra conditions. Our
approach enables the model to accurately understand the intended interaction
between humans and objects, producing high-quality images with accurate
interactions aligned with specified verbs. Extensive experiments on the
HICO-DET dataset demonstrate the effectiveness of our method compared to
previous approaches.","['cs.GR', 'cs.CV', 'cs.MM']","['SeungJu Cha', 'Kwanyoung Lee', 'Ye-Chan Kim', 'Hyunwoo Oh', 'Dong-Jin Kim']",2025-03-20,2025-03-20
2503.16402v1,The Emperor's New Clothes in Benchmarking? A Rigorous Examination of Mitigation Strategies for LLM Benchmark Data Contamination,"Benchmark Data Contamination (BDC)-the inclusion of benchmark testing samples
in the training set-has raised increasing concerns in Large Language Model
(LLM) evaluation, leading to falsely inflated performance estimates and
undermining evaluation reliability. To address this, researchers have proposed
various mitigation strategies to update existing benchmarks, including
modifying original questions or generating new ones based on them. However, a
rigorous examination of the effectiveness of these mitigation strategies
remains lacking. In this paper, we design a systematic and controlled pipeline
along with two novel metrics-fidelity and contamination resistance-to provide a
fine-grained and comprehensive assessment of existing BDC mitigation
strategies. Previous assessment methods, such as accuracy drop and accuracy
matching, focus solely on aggregate accuracy, often leading to incomplete or
misleading conclusions. Our metrics address this limitation by emphasizing
question-level evaluation result matching. Extensive experiments with 10 LLMs,
5 benchmarks, 20 BDC mitigation strategies, and 2 contamination scenarios
reveal that no existing strategy significantly improves resistance over the
vanilla case (i.e., no benchmark update) across all benchmarks, and none
effectively balances fidelity and contamination resistance. These findings
underscore the urgent need for designing more effective BDC mitigation
strategies. Our code repository is available at
https://github.com/ASTRAL-Group/BDC_mitigation_assessment.","['cs.AI', 'cs.CL', 'cs.LG']","['Yifan Sun', 'Han Wang', 'Dongbai Li', 'Gang Wang', 'Huan Zhang']",2025-03-20,2025-03-20
2503.16401v1,Exploring the Hidden Reasoning Process of Large Language Models by Misleading Them,"Large language models (LLMs) and Vision language models (VLMs) have been able
to perform various forms of reasoning tasks in a wide range of scenarios, but
are they truly engaging in task abstraction and rule-based reasoning beyond
mere memorization and pattern matching? To answer this question, we propose a
novel experimental approach, Misleading Fine-Tuning (MisFT), to examine whether
LLMs/VLMs perform abstract reasoning by altering their original understanding
of fundamental rules. In particular, by constructing a dataset with math
expressions that contradict correct operation principles, we fine-tune the
model to learn those contradictory rules and assess its generalization ability
on different test domains. Through a series of experiments, we find that
current LLMs/VLMs are capable of effectively applying contradictory rules to
solve practical math word problems and math expressions represented by images,
implying the presence of an internal mechanism that abstracts before reasoning.",['cs.LG'],"['Guanyu Chen', 'Peiyang Wang', 'Tianren Zhang', 'Feng Chen']",2025-03-20,2025-03-20
2503.16400v1,ScalingNoise: Scaling Inference-Time Search for Generating Infinite Videos,"Video diffusion models (VDMs) facilitate the generation of high-quality
videos, with current research predominantly concentrated on scaling efforts
during training through improvements in data quality, computational resources,
and model complexity. However, inference-time scaling has received less
attention, with most approaches restricting models to a single generation
attempt. Recent studies have uncovered the existence of ""golden noises"" that
can enhance video quality during generation. Building on this, we find that
guiding the scaling inference-time search of VDMs to identify better noise
candidates not only evaluates the quality of the frames generated in the
current step but also preserves the high-level object features by referencing
the anchor frame from previous multi-chunks, thereby delivering long-term
value. Our analysis reveals that diffusion models inherently possess flexible
adjustments of computation by varying denoising steps, and even a one-step
denoising approach, when guided by a reward signal, yields significant
long-term benefits. Based on the observation, we proposeScalingNoise, a
plug-and-play inference-time search strategy that identifies golden initial
noises for the diffusion sampling process to improve global content consistency
and visual diversity. Specifically, we perform one-step denoising to convert
initial noises into a clip and subsequently evaluate its long-term value,
leveraging a reward model anchored by previously generated content. Moreover,
to preserve diversity, we sample candidates from a tilted noise distribution
that up-weights promising noises. In this way, ScalingNoise significantly
reduces noise-induced errors, ensuring more coherent and spatiotemporally
consistent video generation. Extensive experiments on benchmark datasets
demonstrate that the proposed ScalingNoise effectively improves long video
generation.",['cs.LG'],"['Haolin Yang', 'Feilong Tang', 'Ming Hu', 'Yulong Li', 'Junjie Guo', 'Yexin Liu', 'Zelin Peng', 'Junjun He', 'Zongyuan Ge', 'Imran Razzak']",2025-03-20,2025-03-20
2503.16399v1,SA-Occ: Satellite-Assisted 3D Occupancy Prediction in Real World,"Existing vision-based 3D occupancy prediction methods are inherently limited
in accuracy due to their exclusive reliance on street-view imagery, neglecting
the potential benefits of incorporating satellite views. We propose SA-Occ, the
first Satellite-Assisted 3D occupancy prediction model, which leverages GPS &
IMU to integrate historical yet readily available satellite imagery into
real-time applications, effectively mitigating limitations of ego-vehicle
perceptions, involving occlusions and degraded performance in distant regions.
To address the core challenges of cross-view perception, we propose: 1)
Dynamic-Decoupling Fusion, which resolves inconsistencies in dynamic regions
caused by the temporal asynchrony between satellite and street views; 2)
3D-Proj Guidance, a module that enhances 3D feature extraction from inherently
2D satellite imagery; and 3) Uniform Sampling Alignment, which aligns the
sampling density between street and satellite views. Evaluated on
Occ3D-nuScenes, SA-Occ achieves state-of-the-art performance, especially among
single-frame methods, with a 39.05% mIoU (a 6.97% improvement), while incurring
only 6.93 ms of additional latency per frame. Our code and newly curated
dataset are available at https://github.com/chenchen235/SA-Occ.","['cs.CV', 'cs.AI']","['Chen Chen', 'Zhirui Wang', 'Taowei Sheng', 'Yi Jiang', 'Yundu Li', 'Peirui Cheng', 'Luning Zhang', 'Kaiqiang Chen', 'Yanfeng Hu', 'Xue Yang', 'Xian Sun']",2025-03-20,2025-03-20
2503.16398v1,The global convergence time of stochastic gradient descent in non-convex landscapes: Sharp estimates via large deviations,"In this paper, we examine the time it takes for stochastic gradient descent
(SGD) to reach the global minimum of a general, non-convex loss function. We
approach this question through the lens of randomly perturbed dynamical systems
and large deviations theory, and we provide a tight characterization of the
global convergence time of SGD via matching upper and lower bounds. These
bounds are dominated by the most ""costly"" set of obstacles that the algorithm
may need to overcome to reach a global minimizer from a given initialization,
coupling in this way the global geometry of the underlying loss landscape with
the statistics of the noise entering the process. Finally, motivated by
applications to the training of deep neural networks, we also provide a series
of refinements and extensions of our analysis for loss functions with shallow
local minima.","['math.OC', 'cs.LG', 'Primary 90C15, 90C26, 60F10, secondary 90C30, 68Q32']","['Waïss Azizian', 'Franck Iutzeler', 'Jérôme Malick', 'Panayotis Mertikopoulos']",2025-03-20,2025-03-20
2503.16397v1,Scale-wise Distillation of Diffusion Models,"We present SwD, a scale-wise distillation framework for diffusion models
(DMs), which effectively employs next-scale prediction ideas for
diffusion-based few-step generators. In more detail, SwD is inspired by the
recent insights relating diffusion processes to the implicit spectral
autoregression. We suppose that DMs can initiate generation at lower data
resolutions and gradually upscale the samples at each denoising step without
loss in performance while significantly reducing computational costs. SwD
naturally integrates this idea into existing diffusion distillation methods
based on distribution matching. Also, we enrich the family of distribution
matching approaches by introducing a novel patch loss enforcing finer-grained
similarity to the target distribution. When applied to state-of-the-art
text-to-image diffusion models, SwD approaches the inference times of two full
resolution steps and significantly outperforms the counterparts under the same
computation budget, as evidenced by automated metrics and human preference
studies.",['cs.CV'],"['Nikita Starodubcev', 'Denis Kuznedelev', 'Artem Babenko', 'Dmitry Baranchuk']",2025-03-20,2025-03-20
2503.16396v2,SV4D 2.0: Enhancing Spatio-Temporal Consistency in Multi-View Video Diffusion for High-Quality 4D Generation,"We present Stable Video 4D 2.0 (SV4D 2.0), a multi-view video diffusion model
for dynamic 3D asset generation. Compared to its predecessor SV4D, SV4D 2.0 is
more robust to occlusions and large motion, generalizes better to real-world
videos, and produces higher-quality outputs in terms of detail sharpness and
spatio-temporal consistency. We achieve this by introducing key improvements in
multiple aspects: 1) network architecture: eliminating the dependency of
reference multi-views and designing blending mechanism for 3D and frame
attention, 2) data: enhancing quality and quantity of training data, 3)
training strategy: adopting progressive 3D-4D training for better
generalization, and 4) 4D optimization: handling 3D inconsistency and large
motion via 2-stage refinement and progressive frame sampling. Extensive
experiments demonstrate significant performance gain by SV4D 2.0 both visually
and quantitatively, achieving better detail (-14\% LPIPS) and 4D consistency
(-44\% FV4D) in novel-view video synthesis and 4D optimization (-12\% LPIPS and
-24\% FV4D) compared to SV4D.",['cs.CV'],"['Chun-Han Yao', 'Yiming Xie', 'Vikram Voleti', 'Huaizu Jiang', 'Varun Jampani']",2025-03-20,2025-03-21
2503.16395v1,Truthful Elicitation of Imprecise Forecasts,"The quality of probabilistic forecasts is crucial for decision-making under
uncertainty. While proper scoring rules incentivize truthful reporting of
precise forecasts, they fall short when forecasters face epistemic uncertainty
about their beliefs, limiting their use in safety-critical domains where
decision-makers (DMs) prioritize proper uncertainty management. To address
this, we propose a framework for scoring imprecise forecasts -- forecasts given
as a set of beliefs. Despite existing impossibility results for deterministic
scoring rules, we enable truthful elicitation by drawing connection to social
choice theory and introducing a two-way communication framework where DMs first
share their aggregation rules (e.g., averaging or min-max) used in downstream
decisions for resolving forecast ambiguity. This, in turn, helps forecasters
resolve indecision during elicitation. We further show that truthful
elicitation of imprecise forecasts is achievable using proper scoring rules
randomized over the aggregation procedure. Our approach allows DM to elicit and
integrate the forecaster's epistemic uncertainty into their decision-making
process, thus improving credibility.",['cs.LG'],"['Anurag Singh', 'Siu Lun Chau', 'Krikamol Muandet']",2025-03-20,2025-03-20
2503.16394v1,Do Visual Imaginations Improve Vision-and-Language Navigation Agents?,"Vision-and-Language Navigation (VLN) agents are tasked with navigating an
unseen environment using natural language instructions. In this work, we study
if visual representations of sub-goals implied by the instructions can serve as
navigational cues and lead to increased navigation performance. To synthesize
these visual representations or imaginations, we leverage a text-to-image
diffusion model on landmark references contained in segmented instructions.
These imaginations are provided to VLN agents as an added modality to act as
landmark cues and an auxiliary loss is added to explicitly encourage relating
these with their corresponding referring expressions. Our findings reveal an
increase in success rate (SR) of around 1 point and up to 0.5 points in success
scaled by inverse path length (SPL) across agents. These results suggest that
the proposed approach reinforces visual understanding compared to relying on
language instructions alone. Code and data for our work can be found at
https://www.akhilperincherry.com/VLN-Imagine-website/.","['cs.CV', 'cs.AI', 'cs.CL', 'cs.RO']","['Akhil Perincherry', 'Jacob Krantz', 'Stefan Lee']",2025-03-20,2025-03-20
2503.16392v1,Graph of Effort: Quantifying Risk of AI Usage for Vulnerability Assessment,"With AI-based software becoming widely available, the risk of exploiting its
capabilities, such as high automation and complex pattern recognition, could
significantly increase. An AI used offensively to attack non-AI assets is
referred to as offensive AI.
  Current research explores how offensive AI can be utilized and how its usage
can be classified. Additionally, methods for threat modeling are being
developed for AI-based assets within organizations. However, there are gaps
that need to be addressed. Firstly, there is a need to quantify the factors
contributing to the AI threat. Secondly, there is a requirement to create
threat models that analyze the risk of being attacked by AI for vulnerability
assessment across all assets of an organization. This is particularly crucial
and challenging in cloud environments, where sophisticated infrastructure and
access control landscapes are prevalent. The ability to quantify and further
analyze the threat posed by offensive AI enables analysts to rank
vulnerabilities and prioritize the implementation of proactive countermeasures.
  To address these gaps, this paper introduces the Graph of Effort, an
intuitive, flexible, and effective threat modeling method for analyzing the
effort required to use offensive AI for vulnerability exploitation by an
adversary. While the threat model is functional and provides valuable support,
its design choices need further empirical validation in future work.","['cs.CR', 'cs.AI', 'cs.DC']","['Anket Mehra', 'Andreas Aßmuth', 'Malte Prieß']",2025-03-20,2025-03-20
2503.16591v1,UniK3D: Universal Camera Monocular 3D Estimation,"Monocular 3D estimation is crucial for visual perception. However, current
methods fall short by relying on oversimplified assumptions, such as pinhole
camera models or rectified images. These limitations severely restrict their
general applicability, causing poor performance in real-world scenarios with
fisheye or panoramic images and resulting in substantial context loss. To
address this, we present UniK3D, the first generalizable method for monocular
3D estimation able to model any camera. Our method introduces a spherical 3D
representation which allows for better disentanglement of camera and scene
geometry and enables accurate metric 3D reconstruction for unconstrained camera
models. Our camera component features a novel, model-independent representation
of the pencil of rays, achieved through a learned superposition of spherical
harmonics. We also introduce an angular loss, which, together with the camera
module design, prevents the contraction of the 3D outputs for wide-view
cameras. A comprehensive zero-shot evaluation on 13 diverse datasets
demonstrates the state-of-the-art performance of UniK3D across 3D, depth, and
camera metrics, with substantial gains in challenging large-field-of-view and
panoramic settings, while maintaining top accuracy in conventional pinhole
small-field-of-view domains. Code and models are available at
github.com/lpiccinelli-eth/unik3d .",['cs.CV'],"['Luigi Piccinelli', 'Christos Sakaridis', 'Mattia Segu', 'Yung-Hsu Yang', 'Siyuan Li', 'Wim Abbeloos', 'Luc Van Gool']",2025-03-20,2025-03-20
2503.16389v1,Attentional Triple-Encoder Network in Spatiospectral Domains for Medical Image Segmentation,"Retinal Optical Coherence Tomography (OCT) segmentation is essential for
diagnosing pathology. Traditional methods focus on either spatial or spectral
domains, overlooking their combined dependencies. We propose a triple-encoder
network that integrates CNNs for spatial features, Fast Fourier Convolution
(FFC) for spectral features, and attention mechanisms to capture global
relationships across both domains. Attention fusion modules integrate
convolution and cross-attention to further enhance features. Our method
achieves an average Dice score improvement from 0.855 to 0.864, outperforming
prior work.","['eess.IV', 'cs.AI', 'cs.CV']","['Kristin Qi', 'Xinhan Di']",2025-03-20,2025-03-20
2503.16385v1,Deconstructing Long Chain-of-Thought: A Structured Reasoning Optimization Framework for Long CoT Distillation,"Recent advancements in large language models (LLMs) have demonstrated
remarkable reasoning capabilities through long chain-of-thought (CoT)
reasoning. The R1 distillation scheme has emerged as a promising approach for
training cost-effective models with enhanced reasoning abilities. However, the
underlying mechanisms driving its effectiveness remain unclear. This study
examines the universality of distillation data and identifies key components
that enable the efficient transfer of long-chain reasoning capabilities in LLM
distillation. Our findings reveal that the effectiveness of long CoT reasoning
distillation from teacher models like Qwen-QwQ degrades significantly on
nonhomologous models, challenging the assumed universality of current
distillation methods. To gain deeper insights into the structure and patterns
of long CoT reasoning, we propose DLCoT (Deconstructing Long Chain-of-Thought),
a distillation data enhancement framework. DLCoT consists of three key steps:
(1) data segmentation to decompose complex long CoT structures, (2)
simplification by eliminating unsolvable and redundant solutions, and (3)
optimization of intermediate error states. Our approach significantly improves
model performance and token efficiency, facilitating the development of
high-performance LLMs.",['cs.AI'],"['Yijia Luo', 'Yulin Song', 'Xingyao Zhang', 'Jiaheng Liu', 'Weixun Wang', 'GengRu Chen', 'Wenbo Su', 'Bo Zheng']",2025-03-20,2025-03-20
2503.16382v1,Sparse Nonparametric Contextual Bandits,"This paper studies the problem of simultaneously learning relevant features
and minimising regret in contextual bandit problems. We introduce and analyse a
new class of contextual bandit problems, called sparse nonparametric contextual
bandits, in which the expected reward function lies in the linear span of a
small unknown set of features that belongs to a known infinite set of candidate
features. We consider two notions of sparsity, for which the set of candidate
features is either countable or uncountable. Our contribution is two-fold.
First, we provide lower bounds on the minimax regret, which show that
polynomial dependence on the number of actions is generally unavoidable in this
setting. Second, we show that a variant of the Feel-Good Thompson Sampling
algorithm enjoys regret bounds that match our lower bounds up to logarithmic
factors of the horizon, and have logarithmic dependence on the effective number
of candidate features. When we apply our results to kernelised and neural
contextual bandits, we find that sparsity always enables better regret bounds,
as long as the horizon is large enough relative to the sparsity and the number
of actions.","['stat.ML', 'cs.LG']","['Hamish Flynn', 'Julia Olkhovskaya', 'Paul Rognon-Vael']",2025-03-20,2025-03-20
2503.16378v1,Panoptic-CUDAL Technical Report: Rural Australia Point Cloud Dataset in Rainy Conditions,"Existing autonomous driving datasets are predominantly oriented towards
well-structured urban settings and favorable weather conditions, leaving the
complexities of rural environments and adverse weather conditions largely
unaddressed. Although some datasets encompass variations in weather and
lighting, bad weather scenarios do not appear often. Rainfall can significantly
impair sensor functionality, introducing noise and reflections in LiDAR and
camera data and reducing the system's capabilities for reliable environmental
perception and safe navigation. We introduce the Panoptic-CUDAL dataset, a
novel dataset purpose-built for panoptic segmentation in rural areas subject to
rain. By recording high-resolution LiDAR, camera, and pose data, Panoptic-CUDAL
offers a diverse, information-rich dataset in a challenging scenario. We
present analysis of the recorded data and provide baseline results for panoptic
and semantic segmentation methods on LiDAR point clouds. The dataset can be
found here:
https://robotics.sydney.edu.au/our-research/intelligent-transportation-systems/",['cs.CV'],"['Tzu-Yun Tseng', 'Alexey Nekrasov', 'Malcolm Burdorf', 'Bastian Leibe', 'Julie Stephany Berrio', 'Mao Shan', 'Stewart Worrall']",2025-03-20,2025-03-20
2503.16376v1,LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial Images,"The success of modern machine learning, particularly in facial translation
networks, is highly dependent on the availability of high-quality, paired,
large-scale datasets. However, acquiring sufficient data is often challenging
and costly. Inspired by the recent success of diffusion models in high-quality
image synthesis and advancements in Large Language Models (LLMs), we propose a
novel framework called LLM-assisted Paired Image Generation (LaPIG). This
framework enables the construction of comprehensive, high-quality paired
visible and thermal images using captions generated by LLMs. Our method
encompasses three parts: visible image synthesis with ArcFace embedding,
thermal image translation using Latent Diffusion Models (LDMs), and caption
generation with LLMs. Our approach not only generates multi-view paired visible
and thermal images to increase data diversity but also produces high-quality
paired data while maintaining their identity information. We evaluate our
method on public datasets by comparing it with existing methods, demonstrating
the superiority of LaPIG.",['cs.CV'],"['Leyang Wang', 'Joice Lin']",2025-03-20,2025-03-20
2503.16589v1,A Statistical Analysis for Per-Instance Evaluation of Stochastic Optimizers: How Many Repeats Are Enough?,"A key trait of stochastic optimizers is that multiple runs of the same
optimizer in attempting to solve the same problem can produce different
results. As a result, their performance is evaluated over several repeats, or
runs, on the problem. However, the accuracy of the estimated performance
metrics depends on the number of runs and should be studied using statistical
tools. We present a statistical analysis of the common metrics, and develop
guidelines for experiment design to measure the optimizer's performance using
these metrics to a high level of confidence and accuracy. To this end, we first
discuss the confidence interval of the metrics and how they are related to the
number of runs of an experiment. We then derive a lower bound on the number of
repeats in order to guarantee achieving a given accuracy in the metrics. Using
this bound, we propose an algorithm to adaptively adjust the number of repeats
needed to ensure the accuracy of the evaluated metric. Our simulation results
demonstrate the utility of our analysis and how it allows us to conduct
reliable benchmarking as well as hyperparameter tuning and prevent us from
drawing premature conclusions regarding the performance of stochastic
optimizers.","['cs.LG', 'cs.ET', 'math.ST', 'stat.TH']","['Moslem Noori', 'Elisabetta Valiante', 'Thomas Van Vaerenbergh', 'Masoud Mohseni', 'Ignacio Rozada']",2025-03-20,2025-03-20
2503.16375v1,NuiScene: Exploring Efficient Generation of Unbounded Outdoor Scenes,"In this paper, we explore the task of generating expansive outdoor scenes,
ranging from castles to high-rises. Unlike indoor scene generation, which has
been a primary focus of prior work, outdoor scene generation presents unique
challenges, including wide variations in scene heights and the need for a
method capable of rapidly producing large landscapes. To address this, we
propose an efficient approach that encodes scene chunks as uniform vector sets,
offering better compression and performance than the spatially structured
latents used in prior methods. Furthermore, we train an explicit outpainting
model for unbounded generation, which improves coherence compared to prior
resampling-based inpainting schemes while also speeding up generation by
eliminating extra diffusion steps. To facilitate this task, we curate
NuiScene43, a small but high-quality set of scenes, preprocessed for joint
training. Notably, when trained on scenes of varying styles, our model can
blend different environments, such as rural houses and city skyscrapers, within
the same scene, highlighting the potential of our curation process to leverage
heterogeneous scenes for joint training.",['cs.CV'],"['Han-Hung Lee', 'Qinghong Han', 'Angel X. Chang']",2025-03-20,2025-03-20
2503.16371v1,Reinforcement Learning-based Heuristics to Guide Domain-Independent Dynamic Programming,"Domain-Independent Dynamic Programming (DIDP) is a state-space search
paradigm based on dynamic programming for combinatorial optimization. In its
current implementation, DIDP guides the search using user-defined dual bounds.
Reinforcement learning (RL) is increasingly being applied to combinatorial
optimization problems and shares several key structures with DP, being
represented by the Bellman equation and state-based transition systems. We
propose using reinforcement learning to obtain a heuristic function to guide
the search in DIDP. We develop two RL-based guidance approaches: value-based
guidance using Deep Q-Networks and policy-based guidance using Proximal Policy
Optimization. Our experiments indicate that RL-based guidance significantly
outperforms standard DIDP and problem-specific greedy heuristics with the same
number of node expansions. Further, despite longer node evaluation times, RL
guidance achieves better run-time performance than standard DIDP on three of
four benchmark domains.","['cs.AI', 'cs.LG']","['Minori Narita', 'Ryo Kuroiwa', 'J. Christopher Beck']",2025-03-20,2025-03-20
2503.16365v1,JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse,"Recently, action-based decision-making in open-world environments has gained
significant attention. Visual Language Action (VLA) models, pretrained on
large-scale web datasets, have shown promise in decision-making tasks. However,
previous work has primarily focused on action post-training, often neglecting
enhancements to the foundational model itself. In response, we introduce a
novel approach, Act from Visual Language Post-Training, which refines Visual
Language Models (VLMs) through visual and linguistic guidance in a
self-supervised manner. This enhancement improves the models' capabilities in
world knowledge, visual recognition, and spatial grounding in open-world
environments. Following the above post-training paradigms, we obtain the first
VLA models in Minecraft that can follow human instructions on over 1k different
atomic tasks, including crafting, smelting, cooking, mining, and killing. Our
experiments demonstrate that post-training on non-trajectory tasks leads to a
significant 40% improvement over the best agent baseline on a diverse set of
atomic tasks. Furthermore, we demonstrate that our approach surpasses
traditional imitation learning-based policies in Minecraft, achieving
state-of-the-art performance. We have open-sourced the code, models, and
datasets to foster further research. The project page can be found in
https://craftjarvis.github.io/JarvisVLA.","['cs.CV', 'cs.AI']","['Muyao Li', 'Zihao Wang', 'Kaichen He', 'Xiaojian Ma', 'Yitao Liang']",2025-03-20,2025-03-20
2503.16364v1,Neural Networks: According to the Principles of Grassmann Algebra,"In this paper, we explore the algebra of quantum idempotents and the
quantization of fermions which gives rise to a Hilbert space equal to the
Grassmann algebra associated with the Lie algebra. Since idempotents carry
representations of the algebra under consideration, they form algebraic
varieties and smooth manifolds in the natural topology. In addition to the
motivation of linking up mathematical physics with machine learning, it is also
shown that by using idempotents and invariant subspace of the corresponding
algebras, these representations encode and perhaps provide a probabilistic
interpretation of reasoning and relational paths in geometrical terms.","['cs.LG', 'cs.AI']","['Z. Zarezadeh', 'N. Zarezadeh']",2025-03-20,2025-03-20
2503.16363v1,Probabilistic Quantum SVM Training on Ising Machine,"Quantum computing holds significant potential to accelerate machine learning
algorithms, especially in solving optimization problems like those encountered
in Support Vector Machine (SVM) training. However, current QUBO-based Quantum
SVM (QSVM) methods rely solely on binary optimal solutions, limiting their
ability to identify fuzzy boundaries in data. Additionally, the limited qubit
count in contemporary quantum devices constrains training on larger datasets.
In this paper, we propose a probabilistic quantum SVM training framework
suitable for Coherent Ising Machines (CIMs). By formulating the SVM training
problem as a QUBO model, we leverage CIMs' energy minimization capabilities and
introduce a Boltzmann distribution-based probabilistic approach to better
approximate optimal SVM solutions, enhancing robustness. To address qubit
limitations, we employ batch processing and multi-batch ensemble strategies,
enabling small-scale quantum devices to train SVMs on larger datasets and
support multi-class classification tasks via a one-vs-one approach. Our method
is validated through simulations and real-machine experiments on binary and
multi-class datasets. On the banknote binary classification dataset, our
CIM-based QSVM, utilizing an energy-based probabilistic approach, achieved up
to 20% higher accuracy compared to the original QSVM, while training up to
$10^4$ times faster than simulated annealing methods. Compared with classical
SVM, our approach either matched or reduced training time. On the IRIS
three-class dataset, our improved QSVM outperformed existing QSVM models in all
key metrics. As quantum technology advances, increased qubit counts are
expected to further enhance QSVM performance relative to classical SVM.","['cs.LG', 'quant-ph']","['Haoqi He', 'Yan Xiao']",2025-03-20,2025-03-20
2503.16361v1,Enhancing variational quantum algorithms by balancing training on classical and quantum hardware,"Quantum computers offer a promising route to tackling problems that are
classically intractable such as in prime-factorization, solving large-scale
linear algebra and simulating complex quantum systems, but require
fault-tolerant quantum hardware. On the other hand, variational quantum
algorithms (VQAs) have the potential to provide a near-term route to quantum
utility or advantage, and is usually constructed by using parametrized quantum
circuits (PQCs) in combination with a classical optimizer for training.
Although VQAs have been proposed for a multitude of tasks such as ground-state
estimation, combinatorial optimization and unitary compilation, there remain
major challenges in its trainability and resource costs on quantum hardware.
Here we address these challenges by adopting Hardware Efficient and dynamical
LIe algebra Supported Ansatz (HELIA), and propose two training schemes that
combine an existing g-sim method (that uses the underlying group structure of
the operators) and the Parameter-Shift Rule (PSR). Our improvement comes from
distributing the resources required for gradient estimation and training to
both classical and quantum hardware. We numerically test our proposal for
ground-state estimation using Variational Quantum Eigensolver (VQE) and
classification of quantum phases using quantum neural networks. Our methods
show better accuracy and success of trials, and also need fewer calls to the
quantum hardware on an average than using only PSR (upto 60% reduction), that
runs exclusively on quantum hardware. We also numerically demonstrate the
capability of HELIA in mitigating barren plateaus, paving the way for training
large-scale quantum models.","['quant-ph', 'cs.LG', 'stat.ML']","['Rahul Bhowmick', 'Harsh Wadhwa', 'Avinash Singh', 'Tania Sidana', 'Quoc Hoan Tran', 'Krishna Kumar Sabapathy']",2025-03-20,2025-03-20
2503.16357v1,UniSync: A Unified Framework for Audio-Visual Synchronization,"Precise audio-visual synchronization in speech videos is crucial for content
quality and viewer comprehension. Existing methods have made significant
strides in addressing this challenge through rule-based approaches and
end-to-end learning techniques. However, these methods often rely on limited
audio-visual representations and suboptimal learning strategies, potentially
constraining their effectiveness in more complex scenarios. To address these
limitations, we present UniSync, a novel approach for evaluating audio-visual
synchronization using embedding similarities. UniSync offers broad
compatibility with various audio representations (e.g., Mel spectrograms,
HuBERT) and visual representations (e.g., RGB images, face parsing maps, facial
landmarks, 3DMM), effectively handling their significant dimensional
differences. We enhance the contrastive learning framework with a margin-based
loss component and cross-speaker unsynchronized pairs, improving discriminative
capabilities. UniSync outperforms existing methods on standard datasets and
demonstrates versatility across diverse audio-visual representations. Its
integration into talking face generation frameworks enhances synchronization
quality in both natural and AI-generated content.","['cs.CV', 'cs.SD', 'eess.AS']","['Tao Feng', 'Yifan Xie', 'Xun Guan', 'Jiyuan Song', 'Zhou Liu', 'Fei Ma', 'Fei Yu']",2025-03-20,2025-03-20
2503.16356v1,CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners,"Knowledge Editing (KE) enables the modification of outdated or incorrect
information in large language models (LLMs). While existing KE methods can
update isolated facts, they struggle to generalize these updates to multi-hop
reasoning tasks that depend on the modified knowledge. Through an analysis of
reasoning circuits -- the neural pathways LLMs use for knowledge-based
inference, we observe that current layer-localized KE approaches, such as MEMIT
and WISE, which edit only single or a few model layers, struggle to effectively
incorporate updated information into these reasoning pathways. To address this
limitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method
that enables more effective integration of updated knowledge in LLMs. CaKE
leverages strategically curated data, guided by our circuits-based analysis,
that enforces the model to utilize the modified knowledge, stimulating the
model to develop appropriate reasoning circuits for newly integrated knowledge.
Experimental results show that CaKE enables more accurate and consistent use of
updated knowledge across related reasoning tasks, leading to an average of 20%
improvement in multi-hop reasoning accuracy on MQuAKE dataset compared to
existing KE methods. We release the code and data in
https://github.com/zjunlp/CaKE.","['cs.CL', 'cs.AI', 'cs.CV', 'cs.IR', 'cs.LG']","['Yunzhi Yao', 'Jizhan Fang', 'Jia-Chen Gu', 'Ningyu Zhang', 'Shumin Deng', 'Huajun Chen', 'Nanyun Peng']",2025-03-20,2025-03-20
2503.16351v1,Lyra: An Efficient and Expressive Subquadratic Architecture for Modeling Biological Sequences,"Deep learning architectures such as convolutional neural networks and
Transformers have revolutionized biological sequence modeling, with recent
advances driven by scaling up foundation and task-specific models. The
computational resources and large datasets required, however, limit their
applicability in biological contexts. We introduce Lyra, a subquadratic
architecture for sequence modeling, grounded in the biological framework of
epistasis for understanding sequence-to-function relationships. Mathematically,
we demonstrate that state space models efficiently capture global epistatic
interactions and combine them with projected gated convolutions for modeling
local relationships. We demonstrate that Lyra is performant across over 100
wide-ranging biological tasks, achieving state-of-the-art (SOTA) performance in
many key areas, including protein fitness landscape prediction, biophysical
property prediction (e.g. disordered protein region functions) peptide
engineering applications (e.g. antibody binding, cell-penetrating peptide
prediction), RNA structure analysis, RNA function prediction, and CRISPR guide
design. It achieves this with orders-of-magnitude improvements in inference
speed and reduction in parameters (up to 120,000-fold in our tests) compared to
recent biology foundation models. Using Lyra, we were able to train and run
every task in this study on two or fewer GPUs in under two hours, democratizing
access to biological sequence modeling at SOTA performance, with potential
applications to many fields.","['cs.LG', 'q-bio.GN']","['Krithik Ramesh', 'Sameed M. Siddiqui', 'Albert Gu', 'Michael D. Mitzenmacher', 'Pardis C. Sabeti']",2025-03-20,2025-03-20
2503.16348v1,Palatable Conceptions of Disembodied Being: Terra Incognita in the Space of Possible Minds,"Is it possible to articulate a conception of consciousness that is compatible
with the exotic characteristics of contemporary, disembodied AI systems, and
that can stand up to philosophical scrutiny? How would subjective time and
selfhood show up for an entity that conformed to such a conception? Trying to
answer these questions, even metaphorically, stretches the language of
consciousness to breaking point. Ultimately, the attempt yields something like
emptiness, in the Buddhist sense, and helps to undermine our dualistic
inclinations towards subjectivity and selfhood.",['cs.AI'],['Murray Shanahan'],2025-03-20,2025-03-20
2503.16342v1,HiQ-Lip: The First Quantum-Classical Hierarchical Method for Global Lipschitz Constant Estimation of ReLU Networks,"Estimating the global Lipschitz constant of neural networks is crucial for
understanding and improving their robustness and generalization capabilities.
However, precise calculations are NP-hard, and current semidefinite programming
(SDP) methods face challenges such as high memory usage and slow processing
speeds. In this paper, we propose \textbf{HiQ-Lip}, a hybrid quantum-classical
hierarchical method that leverages Coherent Ising Machines (CIMs) to estimate
the global Lipschitz constant. We tackle the estimation by converting it into a
Quadratic Unconstrained Binary Optimization (QUBO) problem and implement a
multilevel graph coarsening and refinement strategy to adapt to the constraints
of contemporary quantum hardware. Our experimental evaluations on fully
connected neural networks demonstrate that HiQ-Lip not only provides estimates
comparable to state-of-the-art methods but also significantly accelerates the
computation process. In specific tests involving two-layer neural networks with
256 hidden neurons, HiQ-Lip doubles the solving speed and offers more accurate
upper bounds than the existing best method, LiPopt. These findings highlight
the promising utility of small-scale quantum devices in advancing the
estimation of neural network robustness.","['cs.LG', 'cs.AI', 'quant-ph']","['Haoqi He', 'Yan Xiao']",2025-03-20,2025-03-20
2503.16340v1,Nonlinear action prediction models reveal multi-timescale locomotor control,"Modeling movement in real-world tasks is a fundamental scientific goal.
However, it is unclear whether existing models and their assumptions,
overwhelmingly tested in laboratory-constrained settings, generalize to the
real world. For example, data-driven models of foot placement control -- a
crucial action for stable locomotion -- assume linear and single timescale
mappings. We develop nonlinear foot placement prediction models, finding that
neural network architectures with flexible input history-dependence like GRU
and Transformer perform best across multiple contexts (walking and running,
treadmill and overground, varying terrains) and input modalities (multiple body
states, gaze), outperforming traditional models. These models reveal context-
and modality-dependent timescales: there is more reliance on fast-timescale
predictions in complex terrain, gaze predictions precede body state
predictions, and full-body state predictions precede center-of-mass-relevant
predictions. Thus, nonlinear action prediction models provide quantifiable
insights into real-world motor control and can be extended to other actions,
contexts, and populations.","['cs.LG', 'cs.RO']","['Wei-Chen Wang', 'Antoine De Comite', 'Monica Daley', 'Alexandra Voloshina', 'Nidhi Seethapathi']",2025-03-20,2025-03-20
2503.16338v1,Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images,"3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis
performance. While conventional methods require per-scene optimization, more
recently several feed-forward methods have been proposed to generate
pixel-aligned Gaussian representations with a learnable network, which are
generalizable to different scenes. However, these methods simply combine
pixel-aligned Gaussians from multiple views as scene representations, thereby
leading to artifacts and extra memory cost without fully capturing the
relations of Gaussians from different images. In this paper, we propose
Gaussian Graph Network (GGN) to generate efficient and generalizable Gaussian
representations. Specifically, we construct Gaussian Graphs to model the
relations of Gaussian groups from different views. To support message passing
at Gaussian level, we reformulate the basic graph operations over Gaussian
representations, enabling each Gaussian to benefit from its connected Gaussian
groups with Gaussian feature fusion. Furthermore, we design a Gaussian pooling
layer to aggregate various Gaussian groups for efficient representations. We
conduct experiments on the large-scale RealEstate10K and ACID datasets to
demonstrate the efficiency and generalization of our method. Compared to the
state-of-the-art methods, our model uses fewer Gaussians and achieves better
image quality with higher rendering speed.",['cs.CV'],"['Shengjun Zhang', 'Xin Fei', 'Fangfu Liu', 'Haixu Song', 'Yueqi Duan']",2025-03-20,2025-03-20
2503.16337v1,Optimal Complexity in Byzantine-Robust Distributed Stochastic Optimization with Data Heterogeneity,"In this paper, we establish tight lower bounds for Byzantine-robust
distributed first-order stochastic optimization methods in both strongly convex
and non-convex stochastic optimization. We reveal that when the distributed
nodes have heterogeneous data, the convergence error comprises two components:
a non-vanishing Byzantine error and a vanishing optimization error. We
establish the lower bounds on the Byzantine error and on the minimum number of
queries to a stochastic gradient oracle required to achieve an arbitrarily
small optimization error. Nevertheless, we identify significant discrepancies
between our established lower bounds and the existing upper bounds. To fill
this gap, we leverage the techniques of Nesterov's acceleration and variance
reduction to develop novel Byzantine-robust distributed stochastic optimization
methods that provably match these lower bounds, up to logarithmic factors,
implying that our established lower bounds are tight.","['math.OC', 'cs.LG']","['Qiankun Shi', 'Jie Peng', 'Kun Yuan', 'Xiao Wang', 'Qing Ling']",2025-03-20,2025-03-20
2503.16335v1,Enhancing Software Quality Assurance with an Adaptive Differential Evolution based Quantum Variational Autoencoder-Transformer Model,"An AI-powered quality engineering platform uses artificial intelligence to
boost software quality assessments through automated defect prediction and
optimized performance alongside improved feature extraction. Existing models
result in difficulties addressing noisy data types together with imbalances,
pattern recognition complexities, ineffective feature extraction, and
generalization weaknesses. To overcome those existing challenges in this
research, we develop a new model Adaptive Differential Evolution based Quantum
Variational Autoencoder-Transformer Model (ADE-QVAET), that combines a Quantum
Variational Autoencoder-Transformer (QVAET) to obtain high-dimensional latent
features and maintain sequential dependencies together with contextual
relationships, resulting in superior defect prediction accuracy. Adaptive
Differential Evolution (ADE) Optimization utilizes an adaptive parameter tuning
method that enhances model convergence and predictive performance. ADE-QVAET
integrates advanced AI techniques to create a robust solution for scalable and
accurate software defect prediction that represents a top-level AI-driven
technology for quality engineering applications. The proposed ADE-QVAET model
attains high accuracy, precision, recall, and f1-score during the training
percentage (TP) 90 of 98.08%, 92.45%, 94.67%, and 98.12%.","['cs.AI', 'cs.ET']","['Seshu Babu Barma', 'Mohanakrishnan Hariharan', 'Satish Arvapalli']",2025-03-20,2025-03-20
2503.16334v1,LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates,"Recent findings reveal that much of the knowledge in a Transformer-based
Large Language Model (LLM) is encoded in its feed-forward (FFN) layers, where
each FNN layer can be interpreted as the summation of sub-updates, each
corresponding to a weighted column vector from the FFN's value parameter matrix
that often encodes human-interpretable concepts. In light of this, we
hypothesize that model performance and behaviors can be further enhanced and
controlled by modulating the contributions of these sub-updates based on their
relevance to the input or target output style, and propose LLMBRACES, a novel
and efficient method that computes relevance scores associated with value
vectors in FFN layers and leverages these scores to dynamically adjust the
contribution of sub-updates. By optimizing sub-update contributions, LLMBRACES
refines the prediction process, leading to more accurate and reliable outputs,
much like a 'brace' providing support and stability. Moreover, LLMBRACES can be
extended to support conditional control over generation characteristics, such
as sentiment, thereby offering fine-grained steering of LLM outputs. Extensive
experiments on various LLMs-including Qwen2.5-1.5B, Llama2-7B, and
Llama3-8B-demonstrate that LLMBRACES outperforms baseline approaches in both
fine-tuning and zero-shot settings while requiring significantly fewer tunable
parameters, up to 75% fewer compared to LoRA. Furthermore, LLMBRACES excels in
sentiment-controlled generation and toxicity reduction, highlighting its
potential for flexible, controlled text generation across applications.",['cs.CL'],"['Ying Shen', 'Lifu Huang']",2025-03-20,2025-03-20
2503.16328v1,Knowledge-guided machine learning model with soil moisture for corn yield prediction under drought conditions,"Remote sensing (RS) techniques, by enabling non-contact acquisition of
extensive ground observations, have become a valuable tool for corn yield
prediction. Traditional process-based (PB) models are limited by fixed input
features and struggle to incorporate large volumes of RS data. In contrast,
machine learning (ML) models are often criticized for being ``black boxes''
with limited interpretability. To address these limitations, we used
Knowledge-Guided Machine Learning (KGML), which combined the strengths of both
approaches and fully used RS data. However, previous KGML methods overlooked
the crucial role of soil moisture in plant growth. To bridge this gap, we
proposed the Knowledge-Guided Machine Learning with Soil Moisture (KGML-SM)
framework, using soil moisture as an intermediate variable to emphasize its key
role in plant development. Additionally, based on the prior knowledge that the
model may overestimate under drought conditions, we designed a drought-aware
loss function that penalizes predicted yield in drought-affected areas. Our
experiments showed that the KGML-SM model outperformed other ML models.
Finally, we explored the relationships between drought, soil moisture, and corn
yield prediction, assessing the importance of various features and analyzing
how soil moisture impacts corn yield predictions across different regions and
time periods.","['cs.LG', 'cs.AI']","['Xiaoyu Wang', 'Yijia Xu', 'Jingyi Huang', 'Zhengwei Yang', 'Zhou Zhang']",2025-03-20,2025-03-20
2503.16326v1,OmniGeo: Towards a Multimodal Large Language Models for Geospatial Artificial Intelligence,"The rapid advancement of multimodal large language models (LLMs) has opened
new frontiers in artificial intelligence, enabling the integration of diverse
large-scale data types such as text, images, and spatial information. In this
paper, we explore the potential of multimodal LLMs (MLLM) for geospatial
artificial intelligence (GeoAI), a field that leverages spatial data to address
challenges in domains including Geospatial Semantics, Health Geography, Urban
Geography, Urban Perception, and Remote Sensing. We propose a MLLM (OmniGeo)
tailored to geospatial applications, capable of processing and analyzing
heterogeneous data sources, including satellite imagery, geospatial metadata,
and textual descriptions. By combining the strengths of natural language
understanding and spatial reasoning, our model enhances the ability of
instruction following and the accuracy of GeoAI systems. Results demonstrate
that our model outperforms task-specific models and existing LLMs on diverse
geospatial tasks, effectively addressing the multimodality nature while
achieving competitive results on the zero-shot geospatial tasks. Our code will
be released after publication.",['cs.AI'],"['Long Yuan', 'Fengran Mo', 'Kaiyu Huang', 'Wenjie Wang', 'Wangyuxuan Zhai', 'Xiaoyu Zhu', 'You Li', 'Jinan Xu', 'Jian-Yun Nie']",2025-03-20,2025-03-20
2503.16323v1,NeuralFoil: An Airfoil Aerodynamics Analysis Tool Using Physics-Informed Machine Learning,"NeuralFoil is an open-source Python-based tool for rapid aerodynamics
analysis of airfoils, similar in purpose to XFoil. Speedups ranging from 8x to
1,000x over XFoil are demonstrated, after controlling for equivalent accuracy.
NeuralFoil computes both global and local quantities (lift, drag, velocity
distribution, etc.) over a broad input space, including: an 18-dimensional
space of airfoil shapes, possibly including control deflections; a 360 degree
range of angles of attack; Reynolds numbers from $10^2$ to $10^{10}$; subsonic
flows up to the transonic drag rise; and with varying turbulence parameters.
Results match those of XFoil closely: the mean relative error of drag is 0.37%
on simple cases, and remains as low as 2.0% on a test dataset with numerous
post-stall and transitional cases. NeuralFoil facilitates gradient-based design
optimization, due to its $C^\infty$-continuous solutions,
automatic-differentiation-compatibility, and bounded computational cost without
non-convergence issues.
  NeuralFoil is a hybrid of physics-informed machine learning techniques and
analytical models. Here, physics information includes symmetries that are
structurally embedded into the model architecture, feature engineering using
domain knowledge, and guaranteed extrapolation to known limit cases. This work
also introduces a new approach for surrogate model uncertainty quantification
that enables robust design optimization.
  This work discusses the methodology and performance of NeuralFoil with
several case studies, including a practical airfoil design optimization study
including both aerodynamic and non-aerodynamic constraints. Here, NeuralFoil
optimization is able to produce airfoils nearly identical in performance and
shape to expert-designed airfoils within seconds; these
computationally-optimized airfoils provide a useful starting point for further
expert refinement.","['physics.flu-dyn', 'cs.LG']","['Peter Sharpe', 'R. John Hansman']",2025-03-20,2025-03-20
2503.16322v1,Ultra-Resolution Adaptation with Ease,"Text-to-image diffusion models have achieved remarkable progress in recent
years. However, training models for high-resolution image generation remains
challenging, particularly when training data and computational resources are
limited. In this paper, we explore this practical problem from two key
perspectives: data and parameter efficiency, and propose a set of key
guidelines for ultra-resolution adaptation termed \emph{URAE}. For data
efficiency, we theoretically and empirically demonstrate that synthetic data
generated by some teacher models can significantly promote training
convergence. For parameter efficiency, we find that tuning minor components of
the weight matrices outperforms widely-used low-rank adapters when synthetic
data are unavailable, offering substantial performance gains while maintaining
efficiency. Additionally, for models leveraging guidance distillation, such as
FLUX, we show that disabling classifier-free guidance, \textit{i.e.}, setting
the guidance scale to 1 during adaptation, is crucial for satisfactory
performance. Extensive experiments validate that URAE achieves comparable
2K-generation performance to state-of-the-art closed-source models like FLUX1.1
[Pro] Ultra with only 3K samples and 2K iterations, while setting new
benchmarks for 4K-resolution generation. Codes are available
\href{https://github.com/Huage001/URAE}{here}.",['cs.CV'],"['Ruonan Yu', 'Songhua Liu', 'Zhenxiong Tan', 'Xinchao Wang']",2025-03-20,2025-03-20
2503.16318v1,Dynamic Point Maps: A Versatile Representation for Dynamic 3D Reconstruction,"DUSt3R has recently shown that one can reduce many tasks in multi-view
geometry, including estimating camera intrinsics and extrinsics, reconstructing
the scene in 3D, and establishing image correspondences, to the prediction of a
pair of viewpoint-invariant point maps, i.e., pixel-aligned point clouds
defined in a common reference frame. This formulation is elegant and powerful,
but unable to tackle dynamic scenes. To address this challenge, we introduce
the concept of Dynamic Point Maps (DPM), extending standard point maps to
support 4D tasks such as motion segmentation, scene flow estimation, 3D object
tracking, and 2D correspondence. Our key intuition is that, when time is
introduced, there are several possible spatial and time references that can be
used to define the point maps. We identify a minimal subset of such
combinations that can be regressed by a network to solve the sub tasks
mentioned above. We train a DPM predictor on a mixture of synthetic and real
data and evaluate it across diverse benchmarks for video depth prediction,
dynamic point cloud reconstruction, 3D scene flow and object pose tracking,
achieving state-of-the-art performance. Code, models and additional results are
available at https://www.robots.ox.ac.uk/~vgg/research/dynamic-point-maps/.",['cs.CV'],"['Edgar Sucar', 'Zihang Lai', 'Eldar Insafutdinov', 'Andrea Vedaldi']",2025-03-20,2025-03-20
2503.16316v1,On the Cone Effect in the Learning Dynamics,"Understanding the learning dynamics of neural networks is a central topic in
the deep learning community. In this paper, we take an empirical perspective to
study the learning dynamics of neural networks in real-world settings.
Specifically, we investigate the evolution process of the empirical Neural
Tangent Kernel (eNTK) during training. Our key findings reveal a two-phase
learning process: i) in Phase I, the eNTK evolves significantly, signaling the
rich regime, and ii) in Phase II, the eNTK keeps evolving but is constrained in
a narrow space, a phenomenon we term the cone effect. This two-phase framework
builds on the hypothesis proposed by Fort et al. (2020), but we uniquely
identify the cone effect in Phase II, demonstrating its significant performance
advantages over fully linearized training.",['cs.LG'],"['Zhanpeng Zhou', 'Yongyi Yang', 'Jie Ren', 'Mahito Sugiyama', 'Junchi Yan']",2025-03-20,2025-03-20
2503.16315v1,Active Learning For Repairable Hardware Systems With Partial Coverage,"Identifying the optimal diagnostic test and hardware system instance to infer
reliability characteristics using field data is challenging, especially when
constrained by fixed budgets and minimal maintenance cycles. Active Learning
(AL) has shown promise for parameter inference with limited data and budget
constraints in machine learning/deep learning tasks. However, AL for
reliability model parameter inference remains underexplored for repairable
hardware systems. It requires specialized AL Acquisition Functions (AFs) that
consider hardware aging and the fact that a hardware system consists of
multiple sub-systems, which may undergo only partial testing during a given
diagnostic test. To address these challenges, we propose a relaxed Mixed
Integer Semidefinite Program (MISDP) AL AF that incorporates Diagnostic
Coverage (DC), Fisher Information Matrices (FIMs), and diagnostic testing
budgets. Furthermore, we design empirical-based simulation experiments focusing
on two diagnostic testing scenarios: (1) partial tests of a hardware system
with overlapping subsystem coverage, and (2) partial tests where one diagnostic
test fully subsumes the subsystem coverage of another. We evaluate our proposed
approach against the most widely used AL AF in the literature (entropy), as
well as several intuitive AL AFs tailored for reliability model parameter
inference. Our proposed AF ranked best on average among the alternative AFs
across 6,000 experimental configurations, with respect to Area Under the Curve
(AUC) of the Absolute Total Expected Event Error (ATEER) and Mean Squared Error
(MSE) curves, with statistical significance calculated at a 0.05 alpha level
using a Friedman hypothesis test.","['stat.AP', 'cs.LG']","['Michael Potter', 'Beyza Kalkanlı', 'Deniz Erdoğmuş', 'Michael Everett']",2025-03-20,2025-03-20
2503.16311v1,"Structured-Noise Masked Modeling for Video, Audio and Beyond","Masked modeling has emerged as a powerful self-supervised learning framework,
but existing methods largely rely on random masking, disregarding the
structural properties of different modalities. In this work, we introduce
structured noise-based masking, a simple yet effective approach that naturally
aligns with the spatial, temporal, and spectral characteristics of video and
audio data. By filtering white noise into distinct color noise distributions,
we generate structured masks that preserve modality-specific patterns without
requiring handcrafted heuristics or access to the data. Our approach improves
the performance of masked video and audio modeling frameworks without any
computational overhead. Extensive experiments demonstrate that structured noise
masking achieves consistent improvement over random masking for standard and
advanced masked modeling methods, highlighting the importance of modality-aware
masking strategies for representation learning.","['cs.LG', 'cs.AI', 'cs.SD']","['Aritra Bhowmik', 'Fida Mohammad Thoker', 'Carlos Hinojosa', 'Bernard Ghanem', 'Cees G. M. Snoek']",2025-03-20,2025-03-20
2503.16309v1,Rapid patient-specific neural networks for intraoperative X-ray to volume registration,"The integration of artificial intelligence in image-guided interventions
holds transformative potential, promising to extract 3D geometric and
quantitative information from conventional 2D imaging modalities during complex
procedures. Achieving this requires the rapid and precise alignment of 2D
intraoperative images (e.g., X-ray) with 3D preoperative volumes (e.g., CT,
MRI). However, current 2D/3D registration methods fail across the broad
spectrum of procedures dependent on X-ray guidance: traditional optimization
techniques require custom parameter tuning for each subject, whereas neural
networks trained on small datasets do not generalize to new patients or require
labor-intensive manual annotations, increasing clinical burden and precluding
application to new anatomical targets. To address these challenges, we present
xvr, a fully automated framework for training patient-specific neural networks
for 2D/3D registration. xvr uses physics-based simulation to generate abundant
high-quality training data from a patient's own preoperative volumetric
imaging, thereby overcoming the inherently limited ability of supervised models
to generalize to new patients and procedures. Furthermore, xvr requires only 5
minutes of training per patient, making it suitable for emergency interventions
as well as planned procedures. We perform the largest evaluation of a 2D/3D
registration algorithm on real X-ray data to date and find that xvr robustly
generalizes across a diverse dataset comprising multiple anatomical structures,
imaging modalities, and hospitals. Across surgical tasks, xvr achieves
submillimeter-accurate registration at intraoperative speeds, improving upon
existing methods by an order of magnitude. xvr is released as open-source
software freely available at https://github.com/eigenvivek/xvr.","['eess.IV', 'cs.CV', 'physics.med-ph']","['Vivek Gopalakrishnan', 'Neel Dey', 'David-Dimitris Chlorogiannis', 'Andrew Abumoussa', 'Anna M. Larson', 'Darren B. Orbach', 'Sarah Frisken', 'Polina Golland']",2025-03-20,2025-03-20
2503.16307v1,Speeding up design and making to reduce time-to-project and time-to-market: an AI-Enhanced approach in engineering education,"This paper explores the integration of AI tools, such as ChatGPT and GitHub
Copilot, in the Software Architecture for Embedded Systems course. AI-supported
workflows enabled students to rapidly prototype complex projects, emphasizing
real-world applications like SLAM robotics. Results demon-started enhanced
problem-solving, faster development, and more sophisticated outcomes, with AI
augmenting but not replacing human decision-making.","['cs.AI', 'I.2; K.3']","['Giovanni Adorni', 'Daniele Grosso']",2025-03-20,2025-03-20
2503.16304v2,Bridging Technology and Humanities: Evaluating the Impact of Large Language Models on Social Sciences Research with DeepSeek-R1,"In recent years, the development of Large Language Models (LLMs) has made
significant breakthroughs in the field of natural language processing and has
gradually been applied to the field of humanities and social sciences research.
LLMs have a wide range of application value in the field of humanities and
social sciences because of its strong text understanding, generation and
reasoning capabilities. In humanities and social sciences research, LLMs can
analyze large-scale text data and make inferences.
  This article analyzes the large language model DeepSeek-R1 from seven
aspects: low-resource language translation, educational question-answering,
student writing improvement in higher education, logical reasoning, educational
measurement and psychometrics, public health policy analysis, and art
education.Then we compare the answers given by DeepSeek-R1 in the seven aspects
with the answers given by o1-preview. DeepSeek-R1 performs well in the
humanities and social sciences, answering most questions correctly and
logically, and can give reasonable analysis processes and explanations.
Compared with o1-preview, it can automatically generate reasoning processes and
provide more detailed explanations, which is suitable for beginners or people
who need to have a detailed understanding of this knowledge, while o1-preview
is more suitable for quick reading.
  Through analysis, it is found that LLM has broad application potential in the
field of humanities and social sciences, and shows great advantages in
improving text analysis efficiency, language communication and other fields.
LLM's powerful language understanding and generation capabilities enable it to
deeply explore complex problems in the field of humanities and social sciences,
and provide innovative tools for academic research and practical applications.","['cs.CY', 'cs.AI']","['Peiran Gu', 'Fuhao Duan', 'Wenhao Li', 'Bochen Xu', 'Ying Cai', 'Teng Yao', 'Chenxun Zhuo', 'Tianming Liu', 'Bao Ge']",2025-03-20,2025-03-21
2503.16302v1,Unleashing Vecset Diffusion Model for Fast Shape Generation,"3D shape generation has greatly flourished through the development of
so-called ""native"" 3D diffusion, particularly through the Vecset Diffusion
Model (VDM). While recent advancements have shown promising results in
generating high-resolution 3D shapes, VDM still struggles with high-speed
generation. Challenges exist because of difficulties not only in accelerating
diffusion sampling but also VAE decoding in VDM, areas under-explored in
previous works. To address these challenges, we present FlashVDM, a systematic
framework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables
flexible diffusion sampling with as few as 5 inference steps and comparable
quality, which is made possible by stabilizing consistency distillation with
our newly introduced Progressive Flow Distillation. For VAE, we introduce a
lightning vecset decoder equipped with Adaptive KV Selection, Hierarchical
Volume Decoding, and Efficient Network Design. By exploiting the locality of
the vecset and the sparsity of shape surface in the volume, our decoder
drastically lowers FLOPs, minimizing the overall decoding overhead. We apply
FlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic
evaluation, we show that our model significantly outperforms existing fast 3D
generation methods, achieving comparable performance to the state-of-the-art
while reducing inference time by over 45x for reconstruction and 32x for
generation. Code and models are available at
https://github.com/Tencent/FlashVDM.","['cs.CV', 'cs.AI', 'eess.IV']","['Zeqiang Lai', 'Yunfei Zhao', 'Zibo Zhao', 'Haolin Liu', 'Fuyun Wang', 'Huiwen Shi', 'Xianghui Yang', 'Qinxiang Lin', 'Jinwei Huang', 'Yuhong Liu', 'Jie Jiang', 'Chunchao Guo', 'Xiangyu Yue']",2025-03-20,2025-03-20
2503.16290v1,Diffusion-augmented Graph Contrastive Learning for Collaborative Filter,"Graph-based collaborative filtering has been established as a prominent
approach in recommendation systems, leveraging the inherent graph topology of
user-item interactions to model high-order connectivity patterns and enhance
recommendation performance. Recent advances in Graph Contrastive Learning (GCL)
have demonstrated promising potential to alleviate data sparsity issues by
improving representation learning through contrastive view generation and
mutual information maximization. However, existing approaches lack effective
data augmentation strategies. Structural augmentation risks distorting
fundamental graph topology, while feature-level perturbation techniques
predominantly employ uniform noise scales that fail to account for
node-specific characteristics. To solve these challenges, we propose
Diffusion-augmented Contrastive Learning (DGCL), an innovative framework that
integrates diffusion models with contrastive learning for enhanced
collaborative filtering. Our approach employs a diffusion process that learns
node-specific Gaussian distributions of representations, thereby generating
semantically consistent yet diversified contrastive views through reverse
diffusion sampling. DGCL facilitates adaptive data augmentation based on
reconstructed representations, considering both semantic coherence and
node-specific features. In addition, it explores unrepresented regions of the
latent sparse feature space, thereby enriching the diversity of contrastive
views. Extensive experimental results demonstrate the effectiveness of DGCL on
three public datasets.","['cs.IR', 'cs.AI']","['Fan Huang', 'Wei Wang']",2025-03-20,2025-03-20
2503.16289v1,SceneMI: Motion In-betweening for Modeling Human-Scene Interactions,"Modeling human-scene interactions (HSI) is essential for understanding and
simulating everyday human behaviors. Recent approaches utilizing generative
modeling have made progress in this domain; however, they are limited in
controllability and flexibility for real-world applications. To address these
challenges, we propose reformulating the HSI modeling problem as Scene-aware
Motion In-betweening -- a more tractable and practical task. We introduce
SceneMI, a framework that supports several practical applications, including
keyframe-guided character animation in 3D scenes and enhancing the motion
quality of imperfect HSI data. SceneMI employs dual scene descriptors to
comprehensively encode global and local scene context. Furthermore, our
framework leverages the inherent denoising nature of diffusion models to
generalize on noisy keyframes. Experimental results demonstrate SceneMI's
effectiveness in scene-aware keyframe in-betweening and generalization to the
real-world GIMO dataset, where motions and scenes are acquired by noisy IMU
sensors and smartphones. We further showcase SceneMI's applicability in HSI
reconstruction from monocular videos.",['cs.CV'],"['Inwoo Hwang', 'Bing Zhou', 'Young Min Kim', 'Jian Wang', 'Chuan Guo']",2025-03-20,2025-03-20
2503.16286v1,Explainable Graph-theoretical Machine Learning: with Application to Alzheimer's Disease Prediction,"Alzheimer's disease (AD) affects 50 million people worldwide and is projected
to overwhelm 152 million by 2050. AD is characterized by cognitive decline due
partly to disruptions in metabolic brain connectivity. Thus, early and accurate
detection of metabolic brain network impairments is crucial for AD management.
Chief to identifying such impairments is FDG-PET data. Despite advancements,
most graph-based studies using FDG-PET data rely on group-level analysis or
thresholding. Yet, group-level analysis can veil individual differences and
thresholding may overlook weaker but biologically critical brain connections.
Additionally, machine learning-based AD prediction largely focuses on
univariate outcomes, such as disease status. Here, we introduce explainable
graph-theoretical machine learning (XGML), a framework employing kernel density
estimation and dynamic time warping to construct individual metabolic brain
graphs that capture the distance between pair-wise brain regions and identify
subgraphs most predictive of multivariate AD-related outcomes. Using FDG-PET
data from the Alzheimer's Disease Neuroimaging Initiative, XGML builds
metabolic brain graphs and uncovers subgraphs predictive of eight AD-related
cognitive scores in new subjects. XGML shows robust performance, particularly
for predicting scores measuring learning, memory, language, praxis, and
orientation, such as CDRSB ($r = 0.74$), ADAS11 ($r = 0.73$), and ADAS13 ($r =
0.71$). Moreover, XGML unveils key edges jointly but differentially predictive
of several AD-related outcomes; they may serve as potential network biomarkers
for assessing overall cognitive decline. Together, we show the promise of
graph-theoretical machine learning in biomarker discovery and disease
prediction and its potential to improve our understanding of network neural
mechanisms underlying AD.",['cs.LG'],"['Narmina Baghirova', 'Duy-Thanh Vũ', 'Duy-Cat Can', 'Christelle Schneuwly Diaz', 'Julien Bodlet', 'Guillaume Blanc', 'Georgi Hrusanov', 'Bernard Ries', 'Oliver Y. Chén']",2025-03-20,2025-03-20
2503.16284v1,PSA-MIL: A Probabilistic Spatial Attention-Based Multiple Instance Learning for Whole Slide Image Classification,"Whole Slide Images (WSIs) are high-resolution digital scans widely used in
medical diagnostics. WSI classification is typically approached using Multiple
Instance Learning (MIL), where the slide is partitioned into tiles treated as
interconnected instances. While attention-based MIL methods aim to identify the
most informative tiles, they often fail to fully exploit the spatial
relationships among them, potentially overlooking intricate tissue structures
crucial for accurate diagnosis. To address this limitation, we propose
Probabilistic Spatial Attention MIL (PSA-MIL), a novel attention-based MIL
framework that integrates spatial context into the attention mechanism through
learnable distance-decayed priors, formulated within a probabilistic
interpretation of self-attention as a posterior distribution. This formulation
enables a dynamic inference of spatial relationships during training,
eliminating the need for predefined assumptions often imposed by previous
approaches. Additionally, we suggest a spatial pruning strategy for the
posterior, effectively reducing self-attention's quadratic complexity. To
further enhance spatial modeling, we introduce a diversity loss that encourages
variation among attention heads, ensuring each captures distinct spatial
representations. Together, PSA-MIL enables a more data-driven and adaptive
integration of spatial context, moving beyond predefined constraints. We
achieve state-of-the-art performance across both contextual and non-contextual
baselines, while significantly reducing computational costs.",['cs.CV'],"['Sharon Peled', 'Yosef E. Maruvka', 'Moti Freiman']",2025-03-20,2025-03-20
2503.16282v1,Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language Model,"Generalized few-shot 3D point cloud segmentation (GFS-PCS) adapts models to
new classes with few support samples while retaining base class segmentation.
Existing GFS-PCS methods enhance prototypes via interacting with support or
query features but remain limited by sparse knowledge from few-shot samples.
Meanwhile, 3D vision-language models (3D VLMs), generalizing across open-world
novel classes, contain rich but noisy novel class knowledge. In this work, we
introduce a GFS-PCS framework that synergizes dense but noisy pseudo-labels
from 3D VLMs with precise yet sparse few-shot samples to maximize the strengths
of both, named GFS-VL. Specifically, we present a prototype-guided pseudo-label
selection to filter low-quality regions, followed by an adaptive infilling
strategy that combines knowledge from pseudo-label contexts and few-shot
samples to adaptively label the filtered, unlabeled areas. Additionally, we
design a novel-base mix strategy to embed few-shot samples into training
scenes, preserving essential context for improved novel class learning.
Moreover, recognizing the limited diversity in current GFS-PCS benchmarks, we
introduce two challenging benchmarks with diverse novel classes for
comprehensive generalization evaluation. Experiments validate the effectiveness
of our framework across models and datasets. Our approach and benchmarks
provide a solid foundation for advancing GFS-PCS in the real world. The code is
at https://github.com/ZhaochongAn/GFS-VL",['cs.CV'],"['Zhaochong An', 'Guolei Sun', 'Yun Liu', 'Runjia Li', 'Junlin Han', 'Ender Konukoglu', 'Serge Belongie']",2025-03-20,2025-03-20
2503.16278v2,Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on Compressed Spatial Tokens,"Recent advancements in large language models and their multi-modal extensions
have demonstrated the effectiveness of unifying generation and understanding
through autoregressive next-token prediction. However, despite the critical
role of 3D structural generation and understanding (3D GU) in AI for science,
these tasks have largely evolved independently, with autoregressive methods
remaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified
framework that seamlessly integrates 3D GU tasks via autoregressive prediction.
At its core, Uni-3DAR employs a novel hierarchical tokenization that compresses
3D space using an octree, leveraging the inherent sparsity of 3D structures. It
then applies an additional tokenization for fine-grained structural details,
capturing key attributes such as atom types and precise spatial coordinates in
microscopic 3D structures. We further propose two optimizations to enhance
efficiency and effectiveness. The first is a two-level subtree compression
strategy, which reduces the octree token sequence by up to 8x. The second is a
masked next-token prediction mechanism tailored for dynamically varying token
positions, significantly boosting model performance. By combining these
strategies, Uni-3DAR successfully unifies diverse 3D GU tasks within a single
autoregressive framework. Extensive experiments across multiple microscopic 3D
GU tasks, including molecules, proteins, polymers, and crystals, validate its
effectiveness and versatility. Notably, Uni-3DAR surpasses previous
state-of-the-art diffusion models by a substantial margin, achieving up to
256\% relative improvement while delivering inference speeds up to 21.8x
faster. The code is publicly available at
https://github.com/dptech-corp/Uni-3DAR.","['cs.LG', 'cond-mat.mtrl-sci', 'q-bio.BM']","['Shuqi Lu', 'Haowei Lin', 'Lin Yao', 'Zhifeng Gao', 'Xiaohong Ji', 'Weinan E', 'Linfeng Zhang', 'Guolin Ke']",2025-03-20,2025-03-21
2503.16271v1,Rethinking Robustness in Machine Learning: A Posterior Agreement Approach,"The robustness of algorithms against covariate shifts is a fundamental
problem with critical implications for the deployment of machine learning
algorithms in the real world. Current evaluation methods predominantly match
the robustness definition to that of standard generalization, relying on
standard metrics like accuracy-based scores, which, while designed for
performance assessment, lack a theoretical foundation encompassing their
application in estimating robustness to distribution shifts. In this work, we
set the desiderata for a robustness metric, and we propose a novel principled
framework for the robustness assessment problem that directly follows the
Posterior Agreement (PA) theory of model validation. Specifically, we extend
the PA framework to the covariate shift setting by proposing a PA metric for
robustness evaluation in supervised classification tasks. We assess the
soundness of our metric in controlled environments and through an empirical
robustness analysis in two different covariate shift scenarios: adversarial
learning and domain generalization. We illustrate the suitability of PA by
evaluating several models under different nature and magnitudes of shift, and
proportion of affected observations. The results show that the PA metric
provides a sensible and consistent analysis of the vulnerabilities in learning
algorithms, even in the presence of few perturbed observations.",['cs.LG'],"['João Borges S. Carvalho', 'Alessandro Torcinovich', 'Victor Jimenez Rodriguez', 'Antonio E. Cinà', 'Carlos Cotrini', 'Lea Schönherr', 'Joachim M. Buhmann']",2025-03-20,2025-03-20
2503.16264v1,Do image and video quality metrics model low-level human vision?,"Image and video quality metrics, such as SSIM, LPIPS, and VMAF, are aimed to
predict the perceived quality of the evaluated content and are often claimed to
be ""perceptual"". Yet, few metrics directly model human visual perception, and
most rely on hand-crafted formulas or training datasets to achieve alignment
with perceptual data. In this paper, we propose a set of tests for
full-reference quality metrics that examine their ability to model several
aspects of low-level human vision: contrast sensitivity, contrast masking, and
contrast matching. The tests are meant to provide additional scrutiny for newly
proposed metrics. We use our tests to analyze 33 existing image and video
quality metrics and find their strengths and weaknesses, such as the ability of
LPIPS and MS-SSIM to predict contrast masking and poor performance of VMAF in
this task. We further find that the popular SSIM metric overemphasizes
differences in high spatial frequencies, but its multi-scale counterpart,
MS-SSIM, addresses this shortcoming. Such findings cannot be easily made using
existing evaluation protocols.","['eess.IV', 'cs.CV', 'cs.MM']","['Dounia Hammou', 'Yancheng Cai', 'Pavan Madhusudanarao', 'Christos G. Bampis', 'Rafał K. Mantiuk']",2025-03-20,2025-03-20
2503.16263v1,From Monocular Vision to Autonomous Action: Guiding Tumor Resection via 3D Reconstruction,"Surgical automation requires precise guidance and understanding of the scene.
Current methods in the literature rely on bulky depth cameras to create maps of
the anatomy, however this does not translate well to space-limited clinical
applications. Monocular cameras are small and allow minimally invasive
surgeries in tight spaces but additional processing is required to generate 3D
scene understanding. We propose a 3D mapping pipeline that uses only RGB images
to create segmented point clouds of the target anatomy. To ensure the most
precise reconstruction, we compare different structure from motion algorithms'
performance on mapping the central airway obstructions, and test the pipeline
on a downstream task of tumor resection. In several metrics, including
post-procedure tissue model evaluation, our pipeline performs comparably to
RGB-D cameras and, in some cases, even surpasses their performance. These
promising results demonstrate that automation guidance can be achieved in
minimally invasive procedures with monocular cameras. This study is a step
toward the complete autonomy of surgical robots.","['cs.CV', 'cs.RO']","['Ayberk Acar', 'Mariana Smith', 'Lidia Al-Zogbi', 'Tanner Watts', 'Fangjie Li', 'Hao Li', 'Nural Yilmaz', 'Paul Maria Scheikl', ""Jesse F. d'Almeida"", 'Susheela Sharma', 'Lauren Branscombe', 'Tayfun Efe Ertop', 'Robert J. Webster III', 'Ipek Oguz', 'Alan Kuntz', 'Axel Krieger', 'Jie Ying Wu']",2025-03-20,2025-03-20
2503.16260v1,Chain of Functions: A Programmatic Pipeline for Fine-Grained Chart Reasoning Data,"Visual reasoning is crucial for multimodal large language models (MLLMs) to
address complex chart queries, yet high-quality rationale data remains scarce.
Existing methods leveraged (M)LLMs for data generation, but direct prompting
often yields limited precision and diversity. In this paper, we propose
\textit{Chain of Functions (CoF)}, a novel programmatic reasoning data
generation pipeline that utilizes freely-explored reasoning paths as
supervision to ensure data precision and diversity. Specifically, it starts
with human-free exploration among the atomic functions (e.g., maximum data and
arithmetic operations) to generate diverse function chains, which are then
translated into linguistic rationales and questions with only a moderate
open-sourced LLM. \textit{CoF} provides multiple benefits: 1) Precision:
function-governed generation reduces hallucinations compared to freeform
generation; 2) Diversity: enumerating function chains enables varied question
taxonomies; 3) Explainability: function chains serve as built-in rationales,
allowing fine-grained evaluation beyond overall accuracy; 4) Practicality:
eliminating reliance on extremely large models. Employing \textit{CoF}, we
construct the \textit{ChartCoF} dataset, with 1.4k complex reasoning Q\&A for
fine-grained analysis and 50k Q\&A for reasoning enhancement. The fine-grained
evaluation on \textit{ChartCoF} reveals varying performance across question
taxonomies for each MLLM, and the experiments also show that finetuning with
\textit{ChartCoF} achieves state-of-the-art performance among same-scale MLLMs
on widely used benchmarks. Furthermore, the novel paradigm of function-governed
rationale generation in \textit{CoF} could inspire broader applications beyond
charts.",['cs.CV'],"['Zijian Li', 'Jingjing Fu', 'Lei Song', 'Jiang Bian', 'Jun Zhang', 'Rui Wang']",2025-03-20,2025-03-20
2503.16257v1,Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language Models,"Video large language models (VideoLLMs) have demonstrated the capability to
process longer video inputs and enable complex reasoning and analysis. However,
due to the thousands of visual tokens from the video frames, key-value (KV)
cache can significantly increase memory requirements, becoming a bottleneck for
inference speed and memory usage. KV cache quantization is a widely used
approach to address this problem. In this paper, we find that 2-bit KV
quantization of VideoLLMs can hardly hurt the model performance, while the
limit of KV cache quantization in even lower bits has not been investigated. To
bridge this gap, we introduce VidKV, a plug-and-play KV cache quantization
method to compress the KV cache to lower than 2 bits. Specifically, (1) for
key, we propose a mixed-precision quantization strategy in the channel
dimension, where we perform 2-bit quantization for anomalous channels and 1-bit
quantization combined with FFT for normal channels; (2) for value, we implement
1.58-bit quantization while selectively filtering semantically salient visual
tokens for targeted preservation, for a better trade-off between precision and
model performance. Importantly, our findings suggest that the value cache of
VideoLLMs should be quantized in a per-channel fashion instead of the per-token
fashion proposed by prior KV cache quantization works for LLMs. Empirically,
extensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show
that VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit
precision with almost no performance drop compared to the FP16 counterparts.",['cs.CV'],"['Keda Tao', 'Haoxuan You', 'Yang Sui', 'Can Qin', 'Huan Wang']",2025-03-20,2025-03-20
2503.16254v1,M2N2V2: Multi-Modal Unsupervised and Training-free Interactive Segmentation,"We present Markov Map Nearest Neighbor V2 (M2N2V2), a novel and simple, yet
effective approach which leverages depth guidance and attention maps for
unsupervised and training-free point-prompt-based interactive segmentation.
Following recent trends in supervised multimodal approaches, we carefully
integrate depth as an additional modality to create novel depth-guided
Markov-maps. Furthermore, we observe occasional segment size fluctuations in
M2N2 during the interactive process, which can decrease the overall mIoU's. To
mitigate this problem, we model the prompting as a sequential process and
propose a novel adaptive score function which considers the previous
segmentation and the current prompt point in order to prevent unreasonable
segment size changes. Using Stable Diffusion 2 and Depth Anything V2 as
backbones, we empirically show that our proposed M2N2V2 significantly improves
the Number of Clicks (NoC) and mIoU compared to M2N2 in all datasets except
those from the medical domain. Interestingly, our unsupervised approach
achieves competitive results compared to supervised methods like SAM and
SimpleClick in the more challenging DAVIS and HQSeg44K datasets in the NoC
metric, reducing the gap between supervised and unsupervised methods.",['cs.CV'],"['Markus Karmann', 'Peng-Tao Jiang', 'Bo Li', 'Onay Urfalioglu']",2025-03-20,2025-03-20
2503.16252v2,Fin-R1: A Large Language Model for Financial Reasoning through Reinforcement Learning,"Reasoning large language models are rapidly evolving across various domains.
However, their capabilities in handling complex financial tasks still require
in-depth exploration. In this paper, we introduce Fin-R1, a reasoning large
language model specifically designed for the financial sector. Fin-R1 is built
using a two-stage architecture, leveraging a financial reasoning dataset
distilled and processed based on DeepSeek-R1. Through supervised fine-tuning
(SFT) and reinforcement learning (RL) training, it demonstrates performance
close to DeepSeek-R1 with a parameter size of 7 billion across a range of
financial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA
and ConvFinQA tasks between those LLMs in our evaluation, surpassing larger
models in other tasks as well. Fin-R1 showcases strong reasoning and
decision-making capabilities, providing solutions to various problems
encountered in the financial domain. Our code is available at
https://github.com/SUFE-AIFLM-Lab/Fin-R1.",['cs.CL'],"['Zhaowei Liu', 'Xin Guo', 'Fangqi Lou', 'Lingfeng Zeng', 'Jinyi Niu', 'Zixuan Wang', 'Jiajie Xu', 'Weige Cai', 'Ziwei Yang', 'Xueqian Zhao', 'Chao Li', 'Sheng Xu', 'Dezhi Chen', 'Yun Chen', 'Zuo Bai', 'Liwen Zhang']",2025-03-20,2025-03-21
2503.16251v1,"RESFL: An Uncertainty-Aware Framework for Responsible Federated Learning by Balancing Privacy, Fairness and Utility in Autonomous Vehicles","Autonomous vehicles (AVs) increasingly rely on Federated Learning (FL) to
enhance perception models while preserving privacy. However, existing FL
frameworks struggle to balance privacy, fairness, and robustness, leading to
performance disparities across demographic groups. Privacy-preserving
techniques like differential privacy mitigate data leakage risks but worsen
fairness by restricting access to sensitive attributes needed for bias
correction. This work explores the trade-off between privacy and fairness in
FL-based object detection for AVs and introduces RESFL, an integrated solution
optimizing both. RESFL incorporates adversarial privacy disentanglement and
uncertainty-guided fairness-aware aggregation. The adversarial component uses a
gradient reversal layer to remove sensitive attributes, reducing privacy risks
while maintaining fairness. The uncertainty-aware aggregation employs an
evidential neural network to weight client updates adaptively, prioritizing
contributions with lower fairness disparities and higher confidence. This
ensures robust and equitable FL model updates. We evaluate RESFL on the FACET
dataset and CARLA simulator, assessing accuracy, fairness, privacy resilience,
and robustness under varying conditions. RESFL improves detection accuracy,
reduces fairness disparities, and lowers privacy attack success rates while
demonstrating superior robustness to adversarial conditions compared to other
approaches.","['cs.LG', 'cs.CV', 'cs.DC', 'cs.ET']","['Dawood Wasif', 'Terrence J. Moore', 'Jin-Hee Cho']",2025-03-20,2025-03-20
2503.16248v1,AI Agents in Cryptoland: Practical Attacks and No Silver Bullet,"The integration of AI agents with Web3 ecosystems harnesses their
complementary potential for autonomy and openness, yet also introduces
underexplored security risks, as these agents dynamically interact with
financial protocols and immutable smart contracts. This paper investigates the
vulnerabilities of AI agents within blockchain-based financial ecosystems when
exposed to adversarial threats in real-world scenarios. We introduce the
concept of context manipulation -- a comprehensive attack vector that exploits
unprotected context surfaces, including input channels, memory modules, and
external data feeds. Through empirical analysis of ElizaOS, a decentralized AI
agent framework for automated Web3 operations, we demonstrate how adversaries
can manipulate context by injecting malicious instructions into prompts or
historical interaction records, leading to unintended asset transfers and
protocol violations which could be financially devastating. Our findings
indicate that prompt-based defenses are insufficient, as malicious inputs can
corrupt an agent's stored context, creating cascading vulnerabilities across
interactions and platforms. This research highlights the urgent need to develop
AI agents that are both secure and fiduciarily responsible.","['cs.CR', 'cs.AI', 'I.2.7']","['Atharv Singh Patlan', 'Peiyao Sheng', 'S. Ashwin Hebbar', 'Prateek Mittal', 'Pramod Viswanath']",2025-03-20,2025-03-20
2503.16247v1,OpenMIBOOD: Open Medical Imaging Benchmarks for Out-Of-Distribution Detection,"The growing reliance on Artificial Intelligence (AI) in critical domains such
as healthcare demands robust mechanisms to ensure the trustworthiness of these
systems, especially when faced with unexpected or anomalous inputs. This paper
introduces the Open Medical Imaging Benchmarks for Out-Of-Distribution
Detection (OpenMIBOOD), a comprehensive framework for evaluating
out-of-distribution (OOD) detection methods specifically in medical imaging
contexts. OpenMIBOOD includes three benchmarks from diverse medical domains,
encompassing 14 datasets divided into covariate-shifted in-distribution,
near-OOD, and far-OOD categories. We evaluate 24 post-hoc methods across these
benchmarks, providing a standardized reference to advance the development and
fair comparison of OOD detection methods. Results reveal that findings from
broad-scale OOD benchmarks in natural image domains do not translate to medical
applications, underscoring the critical need for such benchmarks in the medical
field. By mitigating the risk of exposing AI models to inputs outside their
training distribution, OpenMIBOOD aims to support the advancement of reliable
and trustworthy AI systems in healthcare. The repository is available at
https://github.com/remic-othr/OpenMIBOOD.","['cs.CV', 'cs.LG']","['Max Gutbrod', 'David Rauber', 'Danilo Weber Nunes', 'Christoph Palm']",2025-03-20,2025-03-20
2503.16240v1,Machine learning identifies nullclines in oscillatory dynamical systems,"We introduce CLINE (Computational Learning and Identification of Nullclines),
a neural network-based method that uncovers the hidden structure of nullclines
from oscillatory time series data. Unlike traditional approaches aiming at
direct prediction of system dynamics, CLINE identifies static geometric
features of the phase space that encode the (non)linear relationships between
state variables. It overcomes challenges such as multiple time scales and
strong nonlinearities while producing interpretable results convertible into
symbolic differential equations. We validate CLINE on various oscillatory
systems, showcasing its effectiveness.","['cs.LG', 'math.DS', 'nlin.AO', 'physics.comp-ph']","['Bartosz Prokop', 'Jimmy Billen', 'Nikita Frolov', 'Lendert Gelens']",2025-03-20,2025-03-20
2503.16233v1,Empirical Analysis of Privacy-Fairness-Accuracy Trade-offs in Federated Learning: A Step Towards Responsible AI,"Federated Learning (FL) enables collaborative machine learning while
preserving data privacy but struggles to balance privacy preservation (PP) and
fairness. Techniques like Differential Privacy (DP), Homomorphic Encryption
(HE), and Secure Multi-Party Computation (SMC) protect sensitive data but
introduce trade-offs. DP enhances privacy but can disproportionately impact
underrepresented groups, while HE and SMC mitigate fairness concerns at the
cost of computational overhead. This work explores the privacy-fairness
trade-offs in FL under IID (Independent and Identically Distributed) and
non-IID data distributions, benchmarking q-FedAvg, q-MAML, and Ditto on diverse
datasets. Our findings highlight context-dependent trade-offs and offer
guidelines for designing FL systems that uphold responsible AI principles,
ensuring fairness, privacy, and equitable real-world applications.","['cs.LG', 'cs.CR', 'cs.DC', 'cs.ET']","['Dawood Wasif', 'Dian Chen', 'Sindhuja Madabushi', 'Nithin Alluru', 'Terrence J. Moore', 'Jin-Hee Cho']",2025-03-20,2025-03-20
2503.16227v1,Flight Testing an Optionally Piloted Aircraft: a Case Study on Trust Dynamics in Human-Autonomy Teaming,"This paper examines how trust is formed, maintained, or diminished over time
in the context of human-autonomy teaming with an optionally piloted aircraft.
Whereas traditional factor-based trust models offer a static representation of
human confidence in technology, here we discuss how variations in the
underlying factors lead to variations in trust, trust thresholds, and human
behaviours. Over 200 hours of flight test data collected over a multi-year test
campaign from 2021 to 2023 were reviewed. The
dispositional-situational-learned, process-performance-purpose, and IMPACTS
homeostasis trust models are applied to illuminate trust trends during nominal
autonomous flight operations. The results offer promising directions for future
studies on trust dynamics and design-for-trust in human-autonomy teaming.","['cs.HC', 'cs.AI', 'cs.ET', 'cs.LG', 'cs.SY', 'eess.SY']","['Jeremy C. -H. Wang', 'Ming Hou', 'David Dunwoody', 'Marko Ilievski', 'Justin Tomasi', 'Edward Chao', 'Carl Pigeon']",2025-03-20,2025-03-20
2503.16585v1,"Distributed LLMs and Multimodal Large Language Models: A Survey on Advances, Challenges, and Future Directions","Language models (LMs) are machine learning models designed to predict
linguistic patterns by estimating the probability of word sequences based on
large-scale datasets, such as text. LMs have a wide range of applications in
natural language processing (NLP) tasks, including autocomplete and machine
translation. Although larger datasets typically enhance LM performance,
scalability remains a challenge due to constraints in computational power and
resources. Distributed computing strategies offer essential solutions for
improving scalability and managing the growing computational demand. Further,
the use of sensitive datasets in training and deployment raises significant
privacy concerns. Recent research has focused on developing decentralized
techniques to enable distributed training and inference while utilizing diverse
computational resources and enabling edge AI. This paper presents a survey on
distributed solutions for various LMs, including large language models (LLMs),
vision language models (VLMs), multimodal LLMs (MLLMs), and small language
models (SLMs). While LLMs focus on processing and generating text, MLLMs are
designed to handle multiple modalities of data (e.g., text, images, and audio)
and to integrate them for broader applications. To this end, this paper reviews
key advancements across the MLLM pipeline, including distributed training,
inference, fine-tuning, and deployment, while also identifying the
contributions, limitations, and future areas of improvement. Further, it
categorizes the literature based on six primary focus areas of
decentralization. Our analysis describes gaps in current methodologies for
enabling distributed solutions for LMs and outline future research directions,
emphasizing the need for novel solutions to enhance the robustness and
applicability of distributed LMs.","['cs.CL', 'cs.CV', 'cs.DC', 'cs.LG']","['Hadi Amini', 'Md Jueal Mia', 'Yasaman Saadati', 'Ahmed Imteaj', 'Seyedsina Nabavirazavi', 'Urmish Thakker', 'Md Zarif Hossain', 'Awal Ahmed Fime', 'S. S. Iyengar']",2025-03-20,2025-03-20
2503.16222v1,Efficient Bayesian Computation Using Plug-and-Play Priors for Poisson Inverse Problems,"This paper introduces a novel plug-and-play (PnP) Langevin sampling
methodology for Bayesian inference in low-photon Poisson imaging problems, a
challenging class of problems with significant applications in astronomy,
medicine, and biology. PnP Langevin sampling algorithms offer a powerful
framework for Bayesian image restoration, enabling accurate point estimation as
well as advanced inference tasks, including uncertainty quantification and
visualization analyses, and empirical Bayesian inference for automatic model
parameter tuning. However, existing PnP Langevin algorithms are not well-suited
for low-photon Poisson imaging due to high solution uncertainty and poor
regularity properties, such as exploding gradients and non-negativity
constraints. To address these challenges, we propose two strategies for
extending Langevin PnP sampling to Poisson imaging models: (i) an accelerated
PnP Langevin method that incorporates boundary reflections and a Poisson
likelihood approximation and (ii) a mirror sampling algorithm that leverages a
Riemannian geometry to handle the constraints and the poor regularity of the
likelihood without approximations. The effectiveness of these approaches is
demonstrated through extensive numerical experiments and comparisons with
state-of-the-art methods.","['stat.CO', 'cs.CV', 'cs.NA', 'math.NA', 'stat.ML', '53B21, 60H35, 62F15, 65C40, 65C60, 65J22, 68U10']","['Teresa Klatzer', 'Savvas Melidonis', 'Marcelo Pereyra', 'Konstantinos C. Zygalakis']",2025-03-20,2025-03-20
2503.16219v1,Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn't,"Enhancing the reasoning capabilities of large language models (LLMs)
typically relies on massive computational resources and extensive datasets,
limiting accessibility for resource-constrained settings. Our study
investigates the potential of reinforcement learning (RL) to improve reasoning
in small LLMs, focusing on a 1.5-billion-parameter model,
DeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA
A40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy
Optimization (GRPO) algorithm and curating a compact, high-quality mathematical
reasoning dataset, we conducted three experiments to explore model behavior and
performance. Our results demonstrate rapid reasoning gains - e.g., AMC23
accuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing
o1-preview - using only 7,000 samples and a $42 training cost, compared to
thousands of dollars for baseline models. However, challenges such as
optimization instability and length constraints emerged with prolonged
training. These findings highlight the efficacy of RL-based fine-tuning for
small LLMs, offering a cost-effective alternative to large-scale approaches. We
release our code and datasets as open-source resources, providing insights into
trade-offs and laying a foundation for scalable, reasoning-capable LLMs in
resource-limited environments. All are available at
https://github.com/knoveleng/open-rs.","['cs.LG', 'cs.CL']","['Quy-Anh Dang', 'Chris Ngo']",2025-03-20,2025-03-20
2503.16218v1,Temporal Score Analysis for Understanding and Correcting Diffusion Artifacts,"Visual artifacts remain a persistent challenge in diffusion models, even with
training on massive datasets. Current solutions primarily rely on supervised
detectors, yet lack understanding of why these artifacts occur in the first
place. In our analysis, we identify three distinct phases in the diffusion
generative process: Profiling, Mutation, and Refinement. Artifacts typically
emerge during the Mutation phase, where certain regions exhibit anomalous score
dynamics over time, causing abrupt disruptions in the normal evolution pattern.
This temporal nature explains why existing methods focusing only on spatial
uncertainty of the final output fail at effective artifact localization. Based
on these insights, we propose ASCED (Abnormal Score Correction for Enhancing
Diffusion), that detects artifacts by monitoring abnormal score dynamics during
the diffusion process, with a trajectory-aware on-the-fly mitigation strategy
that appropriate generation of noise in the detected areas. Unlike most
existing methods that apply post hoc corrections, \eg, by applying a
noising-denoising scheme after generation, our mitigation strategy operates
seamlessly within the existing diffusion process. Extensive experiments
demonstrate that our proposed approach effectively reduces artifacts across
diverse domains, matching or surpassing existing supervised methods without
additional training.",['cs.CV'],"['Yu Cao', 'Zengqun Zhao', 'Ioannis Patras', 'Shaogang Gong']",2025-03-20,2025-03-20
2503.16212v1,MathFusion: Enhancing Mathematic Problem-solving of LLM through Instruction Fusion,"Large Language Models (LLMs) have shown impressive progress in mathematical
reasoning. While data augmentation is promising to enhance mathematical
problem-solving ability, current approaches are predominantly limited to
instance-level modifications-such as rephrasing or generating syntactic
variations-which fail to capture and leverage the intrinsic relational
structures inherent in mathematical knowledge. Inspired by human learning
processes, where mathematical proficiency develops through systematic exposure
to interconnected concepts, we introduce MathFusion, a novel framework that
enhances mathematical reasoning through cross-problem instruction synthesis.
MathFusion implements this through three fusion strategies: (1) sequential
fusion, which chains related problems to model solution dependencies; (2)
parallel fusion, which combines analogous problems to reinforce conceptual
understanding; and (3) conditional fusion, which creates context-aware
selective problems to enhance reasoning flexibility. By applying these
strategies, we generate a new dataset, \textbf{MathFusionQA}, followed by
fine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental
results demonstrate that MathFusion achieves substantial improvements in
mathematical reasoning while maintaining high data efficiency, boosting
performance by 18.0 points in accuracy across diverse benchmarks while
requiring only 45K additional synthetic instructions, representing a
substantial improvement over traditional single-instruction approaches. Our
datasets, models, and code are publicly available at
https://github.com/QizhiPei/mathfusion.","['cs.CL', 'cs.AI']","['Qizhi Pei', 'Lijun Wu', 'Zhuoshi Pan', 'Yu Li', 'Honglin Lin', 'Chenlin Ming', 'Xin Gao', 'Conghui He', 'Rui Yan']",2025-03-20,2025-03-20
2503.16207v1,Neural Variable-Order Fractional Differential Equation Networks,"Neural differential equation models have garnered significant attention in
recent years for their effectiveness in machine learning applications.Among
these, fractional differential equations (FDEs) have emerged as a promising
tool due to their ability to capture memory-dependent dynamics, which are often
challenging to model with traditional integer-order approaches.While existing
models have primarily focused on constant-order fractional derivatives,
variable-order fractional operators offer a more flexible and expressive
framework for modeling complex memory patterns. In this work, we introduce the
Neural Variable-Order Fractional Differential Equation network (NvoFDE), a
novel neural network framework that integrates variable-order fractional
derivatives with learnable neural networks.Our framework allows for the
modeling of adaptive derivative orders dependent on hidden features, capturing
more complex feature-updating dynamics and providing enhanced flexibility. We
conduct extensive experiments across multiple graph datasets to validate the
effectiveness of our approach.Our results demonstrate that NvoFDE outperforms
traditional constant-order fractional and integer models across a range of
tasks, showcasing its superior adaptability and performance.",['cs.LG'],"['Wenjun Cui', 'Qiyu Kang', 'Xuhao Li', 'Kai Zhao', 'Wee Peng Tay', 'Weihua Deng', 'Yidong Li']",2025-03-20,2025-03-20
2503.16206v1,Interpretable Neural Causal Models with TRAM-DAGs,"The ultimate goal of most scientific studies is to understand the underlying
causal mechanism between the involved variables. Structural causal models
(SCMs) are widely used to represent such causal mechanisms. Given an SCM,
causal queries on all three levels of Pearl's causal hierarchy can be answered:
$L_1$ observational, $L_2$ interventional, and $L_3$ counterfactual. An
essential aspect of modeling the SCM is to model the dependency of each
variable on its causal parents. Traditionally this is done by parametric
statistical models, such as linear or logistic regression models. This allows
to handle all kinds of data types and fit interpretable models but bears the
risk of introducing a bias. More recently neural causal models came up using
neural networks (NNs) to model the causal relationships, allowing the
estimation of nearly any underlying functional form without bias. However,
current neural causal models are generally restricted to continuous variables
and do not yield an interpretable form of the causal relationships.
Transformation models range from simple statistical regressions to complex
networks and can handle continuous, ordinal, and binary data. Here, we propose
to use TRAMs to model the functional relationships in SCMs allowing us to
bridge the gap between interpretability and flexibility in causal modeling. We
call this method TRAM-DAG and assume currently that the underlying directed
acyclic graph is known. For the fully observed case, we benchmark TRAM-DAGs
against state-of-the-art statistical and NN-based causal models. We show that
TRAM-DAGs are interpretable but also achieve equal or superior performance in
queries ranging from $L_1$ to $L_3$ in the causal hierarchy. For the continuous
case, TRAM-DAGs allow for counterfactual queries for three common causal
structures, including unobserved confounding.","['stat.ML', 'cs.LG']","['Beate Sick', 'Oliver Dürr']",2025-03-20,2025-03-20
2503.16203v1,Logic Explanation of AI Classifiers by Categorical Explaining Functors,"The most common methods in explainable artificial intelligence are post-hoc
techniques which identify the most relevant features used by pretrained opaque
models. Some of the most advanced post hoc methods can generate explanations
that account for the mutual interactions of input features in the form of logic
rules. However, these methods frequently fail to guarantee the consistency of
the extracted explanations with the model's underlying reasoning. To bridge
this gap, we propose a theoretically grounded approach to ensure coherence and
fidelity of the extracted explanations, moving beyond the limitations of
current heuristic-based approaches. To this end, drawing from category theory,
we introduce an explaining functor which structurally preserves logical
entailment between the explanation and the opaque model's reasoning. As a proof
of concept, we validate the proposed theoretical constructions on a synthetic
benchmark verifying how the proposed approach significantly mitigates the
generation of contradictory or unfaithful explanations.",['cs.AI'],"['Stefano Fioravanti', 'Francesco Giannini', 'Paolo Frazzetto', 'Fabio Zanasi', 'Pietro Barbiero']",2025-03-20,2025-03-20
2503.16199v1,Deferring Concept Bottleneck Models: Learning to Defer Interventions to Inaccurate Experts,"Concept Bottleneck Models (CBMs) are machine learning models that improve
interpretability by grounding their predictions on human-understandable
concepts, allowing for targeted interventions in their decision-making process.
However, when intervened on, CBMs assume the availability of humans that can
identify the need to intervene and always provide correct interventions. Both
assumptions are unrealistic and impractical, considering labor costs and human
error-proneness. In contrast, Learning to Defer (L2D) extends supervised
learning by allowing machine learning models to identify cases where a human is
more likely to be correct than the model, thus leading to deferring systems
with improved performance. In this work, we gain inspiration from L2D and
propose Deferring CBMs (DCBMs), a novel framework that allows CBMs to learn
when an intervention is needed. To this end, we model DCBMs as a composition of
deferring systems and derive a consistent L2D loss to train them. Moreover, by
relying on a CBM architecture, DCBMs can explain why defer occurs on the final
task. Our results show that DCBMs achieve high predictive performance and
interpretability at the cost of deferring more to humans.",['cs.LG'],"['Andrea Pugnana', 'Riccardo Massidda', 'Francesco Giannini', 'Pietro Barbiero', 'Mateo Espinosa Zarlenga', 'Roberto Pellungrini', 'Gabriele Dominici', 'Fosca Giannotti', 'Davide Bacciu']",2025-03-20,2025-03-20
2503.16195v1,VP-NTK: Exploring the Benefits of Visual Prompting in Differentially Private Data Synthesis,"Differentially private (DP) synthetic data has become the de facto standard
for releasing sensitive data. However, many DP generative models suffer from
the low utility of synthetic data, especially for high-resolution images. On
the other hand, one of the emerging techniques in parameter efficient
fine-tuning (PEFT) is visual prompting (VP), which allows well-trained existing
models to be reused for the purpose of adapting to subsequent downstream tasks.
In this work, we explore such a phenomenon in constructing captivating
generative models with DP constraints. We show that VP in conjunction with
DP-NTK, a DP generator that exploits the power of the neural tangent kernel
(NTK) in training DP generative models, achieves a significant performance
boost, particularly for high-resolution image datasets, with accuracy improving
from 0.644$\pm$0.044 to 0.769. Lastly, we perform ablation studies on the
effect of different parameters that influence the overall performance of
VP-NTK. Our work demonstrates a promising step forward in improving the utility
of DP synthetic data, particularly for high-resolution images.","['cs.CV', 'cs.LG']","['Chia-Yi Hsu', 'Jia-You Chen', 'Yu-Lin Tsai', 'Chih-Hsun Lin', 'Pin-Yu Chen', 'Chia-Mu Yu', 'Chun-Ying Huang']",2025-03-20,2025-03-20
2503.16194v1,Improving Autoregressive Image Generation through Coarse-to-Fine Token Prediction,"Autoregressive models have shown remarkable success in image generation by
adapting sequential prediction techniques from language modeling. However,
applying these approaches to images requires discretizing continuous pixel data
through vector quantization methods like VQ-VAE. To alleviate the quantization
errors that existed in VQ-VAE, recent works tend to use larger codebooks.
However, this will accordingly expand vocabulary size, complicating the
autoregressive modeling task. This paper aims to find a way to enjoy the
benefits of large codebooks without making autoregressive modeling more
difficult. Through empirical investigation, we discover that tokens with
similar codeword representations produce similar effects on the final generated
image, revealing significant redundancy in large codebooks. Based on this
insight, we propose to predict tokens from coarse to fine (CTF), realized by
assigning the same coarse label for similar tokens. Our framework consists of
two stages: (1) an autoregressive model that sequentially predicts coarse
labels for each token in the sequence, and (2) an auxiliary model that
simultaneously predicts fine-grained labels for all tokens conditioned on their
coarse labels. Experiments on ImageNet demonstrate our method's superior
performance, achieving an average improvement of 59 points in Inception Score
compared to baselines. Notably, despite adding an inference step, our approach
achieves faster sampling speeds.",['cs.CV'],"['Ziyao Guo', 'Kaipeng Zhang', 'Michael Qizhe Shieh']",2025-03-20,2025-03-20
2503.16192v1,Nonparametric Bellman Mappings for Value Iteration in Distributed Reinforcement Learning,"This paper introduces novel Bellman mappings (B-Maps) for value iteration
(VI) in distributed reinforcement learning (DRL), where multiple agents operate
over a network without a centralized fusion node. Each agent constructs its own
nonparametric B-Map for VI while communicating only with direct neighbors to
achieve consensus. These B-Maps operate on Q-functions represented in a
reproducing kernel Hilbert space, enabling a nonparametric formulation that
allows for flexible, agent-specific basis function design. Unlike existing DRL
methods that restrict information exchange to Q-function estimates, the
proposed framework also enables agents to share basis information in the form
of covariance matrices, capturing additional structural details. A theoretical
analysis establishes linear convergence rates for both Q-function and
covariance-matrix estimates toward their consensus values. The optimal learning
rates for consensus-based updates are dictated by the ratio of the smallest
positive eigenvalue to the largest one of the network's Laplacian matrix.
Furthermore, each nodal Q-function estimate is shown to lie very close to the
fixed point of a centralized nonparametric B-Map, effectively allowing the
proposed DRL design to approximate the performance of a centralized fusion
center. Numerical experiments on two well-known control problems demonstrate
the superior performance of the proposed nonparametric B-Maps compared to prior
methods. Notably, the results reveal a counter-intuitive finding: although the
proposed approach involves greater information exchange -- specifically through
the sharing of covariance matrices -- it achieves the desired performance with
lower cumulative communication cost than existing DRL schemes, highlighting the
crucial role of basis information in accelerating the learning process.","['cs.LG', 'eess.SP']","['Yuki Akiyama', 'Konstantinos Slavakis']",2025-03-20,2025-03-20
2503.16191v1,Large Language Models for Water Distribution Systems Modeling and Decision-Making,"The design, operations, and management of water distribution systems (WDS)
involve complex mathematical models. These models are continually improving due
to computational advancements, leading to better decision-making and more
efficient WDS management. However, the significant time and effort required for
modeling, programming, and analyzing results remain substantial challenges.
Another issue is the professional burden, which confines the interaction with
models, databases, and other sophisticated tools to a small group of experts,
thereby causing non-technical stakeholders to depend on these experts or make
decisions without modeling support. Furthermore, explaining model results is
challenging even for experts, as it is often unclear which conditions cause the
model to reach a certain state or recommend a specific policy. The recent
advancements in Large Language Models (LLMs) open doors for a new stage in
human-model interaction. This study proposes a framework of plain language
interactions with hydraulic and water quality models based on LLM-EPANET
architecture. This framework is tested with increasing levels of complexity of
queries to study the ability of LLMs to interact with WDS models, run complex
simulations, and report simulation results. The performance of the proposed
framework is evaluated across several categories of queries and hyper-parameter
configurations, demonstrating its potential to enhance decision-making
processes in WDS management.","['cs.AI', 'cs.HC', 'cs.LG']","['Yinon Goldshtein', 'Gal Perelman', 'Assaf Schuster', 'Avi Ostfeld']",2025-03-20,2025-03-20
2503.16188v1,CLS-RL: Image Classification with Rule-Based Reinforcement Learning,"Classification is a core task in machine learning. Recent research has shown
that although Multimodal Large Language Models (MLLMs) are initially poor at
image classification, fine-tuning them with an adequate amount of data can
significantly enhance their performance, making them comparable to SOTA
classification models. However, acquiring large-scale labeled data is
expensive. In this paper, we explore few-shot MLLM classification fine-tuning.
We found that SFT can cause severe overfitting issues and may even degrade
performance over the zero-shot approach. To address this challenge, inspired by
the recent successes in rule-based reinforcement learning, we propose CLS-RL,
which uses verifiable signals as reward to fine-tune MLLMs. We discovered that
CLS-RL outperforms SFT in most datasets and has a much higher average accuracy
on both base-to-new and few-shot learning setting. Moreover, we observed a
free-lunch phenomenon for CLS-RL; when models are fine-tuned on a particular
dataset, their performance on other distinct datasets may also improve over
zero-shot models, even if those datasets differ in distribution and class
names. This suggests that RL-based methods effectively teach models the
fundamentals of classification. Lastly, inspired by recent works in inference
time thinking, we re-examine the `thinking process' during fine-tuning, a
critical aspect of RL-based methods, in the context of visual classification.
We question whether such tasks require extensive thinking process during
fine-tuning, proposing that this may actually detract from performance. Based
on this premise, we introduce the No-Thinking-CLS-RL method, which minimizes
thinking processes during training by setting an equality accuracy reward. Our
findings indicate that, with much less fine-tuning time, No-Thinking-CLS-RL
method achieves superior in-domain performance and generalization capabilities
than CLS-RL.",['cs.CV'],"['Ming Li', 'Shitian Zhao', 'Jike Zhong', 'Yuxiang Lai', 'Kaipeng Zhang']",2025-03-20,2025-03-20
2503.16187v1,Manifold learning in metric spaces,"Laplacian-based methods are popular for dimensionality reduction of data
lying in $\mathbb{R}^N$. Several theoretical results for these algorithms
depend on the fact that the Euclidean distance approximates the geodesic
distance on the underlying submanifold which the data are assumed to lie on.
However, for some applications, other metrics, such as the Wasserstein
distance, may provide a more appropriate notion of distance than the Euclidean
distance. We provide a framework that generalizes the problem of manifold
learning to metric spaces and study when a metric satisfies sufficient
conditions for the pointwise convergence of the graph Laplacian.","['cs.LG', 'stat.ML']","['Liane Xu', 'Amit Singer']",2025-03-20,2025-03-20
2503.16185v1,MapGlue: Multimodal Remote Sensing Image Matching,"Multimodal remote sensing image (MRSI) matching is pivotal for cross-modal
fusion, localization, and object detection, but it faces severe challenges due
to geometric, radiometric, and viewpoint discrepancies across imaging
modalities. Existing unimodal datasets lack scale and diversity, limiting deep
learning solutions. This paper proposes MapGlue, a universal MRSI matching
framework, and MapData, a large-scale multimodal dataset addressing these gaps.
Our contributions are twofold. MapData, a globally diverse dataset spanning 233
sampling points, offers original images (7,000x5,000 to 20,000x15,000 pixels).
After rigorous cleaning, it provides 121,781 aligned electronic map-visible
image pairs (512x512 pixels) with hybrid manual-automated ground truth,
addressing the scarcity of scalable multimodal benchmarks. MapGlue integrates
semantic context with a dual graph-guided mechanism to extract cross-modal
invariant features. This structure enables global-to-local interaction,
enhancing descriptor robustness against modality-specific distortions.
Extensive evaluations on MapData and five public datasets demonstrate MapGlue's
superiority in matching accuracy under complex conditions, outperforming
state-of-the-art methods. Notably, MapGlue generalizes effectively to unseen
modalities without retraining, highlighting its adaptability. This work
addresses longstanding challenges in MRSI matching by combining scalable
dataset construction with a robust, semantics-driven framework. Furthermore,
MapGlue shows strong generalization capabilities on other modality matching
tasks for which it was not specifically trained. The dataset and code are
available at https://github.com/PeihaoWu/MapGlue.",['cs.CV'],"['Peihao Wu', 'Yongxiang Yao', 'Wenfei Zhang', 'Dong Wei', 'Yi Wan', 'Yansheng Li', 'Yongjun Zhang']",2025-03-20,2025-03-20
2503.16184v1,Accurate Scene Text Recognition with Efficient Model Scaling and Cloze Self-Distillation,"Scaling architectures have been proven effective for improving Scene Text
Recognition (STR), but the individual contribution of vision encoder and text
decoder scaling remain under-explored. In this work, we present an in-depth
empirical analysis and demonstrate that, contrary to previous observations,
scaling the decoder yields significant performance gains, always exceeding
those achieved by encoder scaling alone. We also identify label noise as a key
challenge in STR, particularly in real-world data, which can limit the
effectiveness of STR models. To address this, we propose Cloze
Self-Distillation (CSD), a method that mitigates label noise by distilling a
student model from context-aware soft predictions and pseudolabels generated by
a teacher model. Additionally, we enhance the decoder architecture by
introducing differential cross-attention for STR. Our methodology achieves
state-of-the-art performance on 10 out of 11 benchmarks using only real data,
while significantly reducing the parameter size and computational costs.","['cs.CV', 'cs.AI', 'cs.CL']","['Andrea Maracani', 'Savas Ozkan', 'Sijun Cho', 'Hyowon Kim', 'Eunchung Noh', 'Jeongwon Min', 'Cho Jung Min', 'Dookun Park', 'Mete Ozay']",2025-03-20,2025-03-20
2503.16183v1,Variance-Aware Noisy Training: Hardening DNNs against Unstable Analog Computations,"The disparity between the computational demands of deep learning and the
capabilities of compute hardware is expanding drastically. Although deep
learning achieves remarkable performance in countless tasks, its escalating
requirements for computational power and energy consumption surpass the
sustainable limits of even specialized neural processing units, including the
Apple Neural Engine and NVIDIA TensorCores. This challenge is intensified by
the slowdown in CMOS scaling.
  Analog computing presents a promising alternative, offering substantial
improvements in energy efficiency by directly manipulating physical quantities
such as current, voltage, charge, or photons. However, it is inherently
vulnerable to manufacturing variations, nonlinearities, and noise, leading to
degraded prediction accuracy. One of the most effective techniques for
enhancing robustness, Noisy Training, introduces noise during the training
phase to reinforce the model against disturbances encountered during inference.
Although highly effective, its performance degrades in real-world environments
where noise characteristics fluctuate due to external factors such as
temperature variations and temporal drift.
  This study underscores the necessity of Noisy Training while revealing its
fundamental limitations in the presence of dynamic noise. To address these
challenges, we propose Variance-Aware Noisy Training, a novel approach that
mitigates performance degradation by incorporating noise schedules which
emulate the evolving noise conditions encountered during inference. Our method
substantially improves model robustness, without training overhead. We
demonstrate a significant increase in robustness, from 72.3\% with conventional
Noisy Training to 97.3\% with Variance-Aware Noisy Training on CIFAR-10 and
from 38.5\% to 89.9\% on Tiny ImageNet.",['cs.LG'],"['Xiao Wang', 'Hendrik Borras', 'Bernhard Klein', 'Holger Fröning']",2025-03-20,2025-03-20
2503.16583v1,Explainable AI-Guided Efficient Approximate DNN Generation for Multi-Pod Systolic Arrays,"Approximate deep neural networks (AxDNNs) are promising for enhancing energy
efficiency in real-world devices. One of the key contributors behind this
enhanced energy efficiency in AxDNNs is the use of approximate multipliers.
Unfortunately, the simulation of approximate multipliers does not usually scale
well on CPUs and GPUs. As a consequence, this slows down the overall simulation
of AxDNNs aimed at identifying the appropriate approximate multipliers to
achieve high energy efficiency with a minimum accuracy loss. To address this
problem, we present a novel XAI-Gen methodology, which leverages the analytical
model of the emerging hardware accelerator (e.g., Google TPU v4) and
explainable artificial intelligence (XAI) to precisely identify the
non-critical layers for approximation and quickly discover the appropriate
approximate multipliers for AxDNN layers. Our results show that XAI-Gen
achieves up to 7x lower energy consumption with only 1-2% accuracy loss. We
also showcase the effectiveness of the XAI-Gen approach through a neural
architecture search (XAI-NAS) case study. Interestingly, XAI-NAS achieves 40\%
higher energy efficiency with up to 5x less execution time when compared to the
state-of-the-art NAS methods for generating AxDNNs.","['cs.LG', 'cs.AI', 'cs.AR']","['Ayesha Siddique', 'Khurram Khalil', 'Khaza Anuarul Hoque']",2025-03-20,2025-03-20
2503.16179v1,Narrowing Class-Wise Robustness Gaps in Adversarial Training,"Efforts to address declining accuracy as a result of data shifts often
involve various data-augmentation strategies. Adversarial training is one such
method, designed to improve robustness to worst-case distribution shifts caused
by adversarial examples. While this method can improve robustness, it may also
hinder generalization to clean examples and exacerbate performance imbalances
across different classes. This paper explores the impact of adversarial
training on both overall and class-specific performance, as well as its
spill-over effects. We observe that enhanced labeling during training boosts
adversarial robustness by 53.50% and mitigates class imbalances by 5.73%,
leading to improved accuracy in both clean and adversarial settings compared to
standard adversarial training.","['cs.CV', 'cs.LG']","['Fatemeh Amerehi', 'Patrick Healy']",2025-03-20,2025-03-20
2503.16177v1,OccluGaussian: Occlusion-Aware Gaussian Splatting for Large Scene Reconstruction and Rendering,"In large-scale scene reconstruction using 3D Gaussian splatting, it is common
to partition the scene into multiple smaller regions and reconstruct them
individually. However, existing division methods are occlusion-agnostic,
meaning that each region may contain areas with severe occlusions. As a result,
the cameras within those regions are less correlated, leading to a low average
contribution to the overall reconstruction. In this paper, we propose an
occlusion-aware scene division strategy that clusters training cameras based on
their positions and co-visibilities to acquire multiple regions. Cameras in
such regions exhibit stronger correlations and a higher average contribution,
facilitating high-quality scene reconstruction. We further propose a
region-based rendering technique to accelerate large scene rendering, which
culls Gaussians invisible to the region where the viewpoint is located. Such a
technique significantly speeds up the rendering without compromising quality.
Extensive experiments on multiple large scenes show that our method achieves
superior reconstruction results with faster rendering speed compared to
existing state-of-the-art approaches. Project page:
https://occlugaussian.github.io.","['cs.GR', 'cs.CV']","['Shiyong Liu', 'Xiao Tang', 'Zhihao Li', 'Yingfan He', 'Chongjie Ye', 'Jianzhuang Liu', 'Binxiao Huang', 'Shunbo Zhou', 'Xiaofei Wu']",2025-03-20,2025-03-20
2503.16167v1,CodeReviewQA: The Code Review Comprehension Assessment for Large Language Models,"State-of-the-art large language models (LLMs) have demonstrated impressive
code generation capabilities but struggle with real-world software engineering
tasks, such as revising source code to address code reviews, hindering their
practical use. Code review comments are often implicit, ambiguous, and
colloquial, requiring models to grasp both code and human intent. This
challenge calls for evaluating large language models' ability to bridge both
technical and conversational contexts. While existing work has employed the
automated code refinement (ACR) task to resolve these comments, current
evaluation methods fall short, relying on text matching metrics that provide
limited insight into model failures and remain susceptible to training data
contamination. To address these limitations, we introduce a novel evaluation
benchmark, $\textbf{CodeReviewQA}$ that enables us to conduct fine-grained
assessment of model capabilities and mitigate data contamination risks. In
CodeReviewQA, we decompose the generation task of code refinement into
$\textbf{three essential reasoning steps}$: $\textit{change type recognition}$
(CTR), $\textit{change localisation}$ (CL), and $\textit{solution
identification}$ (SI). Each step is reformulated as multiple-choice questions
with varied difficulty levels, enabling precise assessment of model
capabilities, while mitigating data contamination risks. Our comprehensive
evaluation spans 72 recently released large language models on $\textbf{900
manually curated, high-quality examples}$ across nine programming languages.
Our results show that CodeReviewQA is able to expose specific model weaknesses
in code review comprehension, disentangled from their generative automated code
refinement results.","['cs.SE', 'cs.CL']","['Hong Yi Lin', 'Chunhua Liu', 'Haoyu Gao', 'Patanamon Thongtanunam', 'Christoph Treude']",2025-03-20,2025-03-20
2503.16165v1,Iterative Optimal Attention and Local Model for Single Image Rain Streak Removal,"High-fidelity imaging is crucial for the successful safety supervision and
intelligent deployment of vision-based measurement systems (VBMS). It ensures
high-quality imaging in VBMS, which is fundamental for reliable visual
measurement and analysis. However, imaging quality can be significantly
impaired by adverse weather conditions, particularly rain, leading to blurred
images and reduced contrast. Such impairments increase the risk of inaccurate
evaluations and misinterpretations in VBMS. To address these limitations, we
propose an Expectation Maximization Reconstruction Transformer (EMResformer)
for single image rain streak removal. The EMResformer retains the key
self-attention values for feature aggregation, enhancing local features to
produce superior image reconstruction. Specifically, we propose an Expectation
Maximization Block seamlessly integrated into the single image rain streak
removal network, enhancing its ability to eliminate superfluous information and
restore a cleaner background image. Additionally, to further enhance local
information for improved detail rendition, we introduce a Local Model Residual
Block, which integrates two local model blocks along with a sequence of
convolutions and activation functions. This integration synergistically
facilitates the extraction of more pertinent features for enhanced single image
rain streak removal. Extensive experiments validate that our proposed
EMResformer surpasses current state-of-the-art single image rain streak removal
methods on both synthetic and real-world datasets, achieving an improved
balance between model complexity and single image deraining performance.
Furthermore, we evaluate the effectiveness of our method in VBMS scenarios,
demonstrating that high-quality imaging significantly improves the accuracy and
reliability of VBMS tasks.","['cs.CV', 'cs.IR']","['Xiangyu Li', 'Wanshu Fan', 'Yue Shen', 'Cong Wang', 'Wei Wang', 'Xin Yang', 'Qiang Zhang', 'Dongsheng Zhou']",2025-03-20,2025-03-20
2503.16163v1,SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs,"Transformer-based large language models (LLMs) have already achieved
remarkable results on long-text tasks, but the limited GPU memory (VRAM)
resources struggle to accommodate the linearly growing demand for key-value
(KV) cache as the sequence length increases, which has become a bottleneck for
the application of LLMs on long sequences. Existing KV cache compression
methods include eviction, merging, or quantization of the KV cache to reduce
its size. However, compression results in irreversible information forgetting,
potentially affecting the accuracy of subsequent decoding. In this paper, we
propose SpeCache, which takes full advantage of the large and easily expandable
CPU memory to offload the complete KV cache, and dynamically fetches KV pairs
back in each decoding step based on their importance measured by low-bit KV
cache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,
SpeCache speculatively predicts the KV pairs that the next token might attend
to, allowing us to prefetch them before the next decoding step which enables
parallelization of prefetching and computation. Experiments on LongBench and
Needle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM
usage while avoiding information forgetting for long sequences without
re-training, even with a 10x high KV cache compression ratio.",['cs.CL'],"['Shibo Jie', 'Yehui Tang', 'Kai Han', 'Zhi-Hong Deng', 'Jing Han']",2025-03-20,2025-03-20
2503.16161v1,Towards Lighter and Robust Evaluation for Retrieval Augmented Generation,"Large Language Models are prompting us to view more NLP tasks from a
generative perspective. At the same time, they offer a new way of accessing
information, mainly through the RAG framework. While there have been notable
improvements for the autoregressive models, overcoming hallucination in the
generated answers remains a continuous problem. A standard solution is to use
commercial LLMs, such as GPT4, to evaluate these algorithms. However, such
frameworks are expensive and not very transparent. Therefore, we propose a
study which demonstrates the interest of open-weight models for evaluating RAG
hallucination. We develop a lightweight approach using smaller, quantized LLMs
to provide an accessible and interpretable metric that gives continuous scores
for the generated answer with respect to their correctness and faithfulness.
This score allows us to question decisions' reliability and explore thresholds
to develop a new AUC metric as an alternative to correlation with human
judgment.","['cs.CL', 'cs.AI', '62-08', 'I.2.7']","['Alex-Razvan Ispas', 'Charles-Elie Simon', 'Fabien Caspani', 'Vincent Guigue']",2025-03-20,2025-03-20
2503.16159v1,Neural Combinatorial Optimization for Real-World Routing,"Vehicle Routing Problems (VRPs) are a class of NP-hard problems ubiquitous in
several real-world logistics scenarios that pose significant challenges for
optimization. Neural Combinatorial Optimization (NCO) has emerged as a
promising alternative to classical approaches, as it can learn fast heuristics
to solve VRPs. However, most research works in NCO for VRPs focus on simplified
settings, which do not account for asymmetric distances and travel durations
that cannot be derived by simple Euclidean distances and unrealistic data
distributions, hindering real-world deployment. This work introduces RRNCO
(Real Routing NCO) to bridge the gap of NCO between synthetic and real-world
VRPs in the critical aspects of both data and modeling. First, we introduce a
new, openly available dataset with real-world data containing a diverse dataset
of locations, distances, and duration matrices from 100 cities, considering
realistic settings with actual routing distances and durations obtained from
Open Source Routing Machine (OSRM). Second, we propose a novel approach that
efficiently processes both node and edge features through contextual gating,
enabling the construction of more informed node embedding, and we finally
incorporate an Adaptation Attention Free Module (AAFM) with neural adaptive
bias mechanisms that effectively integrates not only distance matrices but also
angular relationships between nodes, allowing our model to capture rich
structural information. RRNCO achieves state-of-the-art results in real-world
VRPs among NCO methods. We make our dataset and code publicly available at
https://github.com/ai4co/real-routing-nco.","['cs.LG', 'cs.AI']","['Jiwoo Son', 'Zhikai Zhao', 'Federico Berto', 'Chuanbo Hua', 'Changhyun Kwon', 'Jinkyoo Park']",2025-03-20,2025-03-20
2503.16158v1,Automatically Generating Chinese Homophone Words to Probe Machine Translation Estimation Systems,"Evaluating machine translation (MT) of user-generated content (UGC) involves
unique challenges such as checking whether the nuance of emotions from the
source are preserved in the target text. Recent studies have proposed
emotion-related datasets, frameworks and models to automatically evaluate MT
quality of Chinese UGC, without relying on reference translations. However,
whether these models are robust to the challenge of preserving emotional
nuances has been left largely unexplored. To address this gap, we introduce a
novel method inspired by information theory which generates challenging Chinese
homophone words related to emotions, by leveraging the concept of
self-information. Our approach generates homophones that were observed to cause
translation errors in emotion preservation, and exposes vulnerabilities in MT
systems and their evaluation methods when tackling emotional UGC. We evaluate
the efficacy of our method using human evaluation for the quality of these
generated homophones, and compare it with an existing one, showing that our
method achieves higher correlation with human judgments. The generated Chinese
homophones, along with their manual translations, are utilized to generate
perturbations and to probe the robustness of existing quality evaluation
models, including models trained using multi-task learning, fine-tuned variants
of multilingual language models, as well as large language models (LLMs). Our
results indicate that LLMs with larger size exhibit higher stability and
robustness to such perturbations. We release our data and code for
reproducibility and further research.",['cs.CL'],"['Shenbin Qian', 'Constantin Orăsan', 'Diptesh Kanojia', 'Félix do Carmo']",2025-03-20,2025-03-20
2503.16153v1,FreeFlux: Understanding and Exploiting Layer-Specific Roles in RoPE-Based MMDiT for Versatile Image Editing,"The integration of Rotary Position Embedding (RoPE) in Multimodal Diffusion
Transformer (MMDiT) has significantly enhanced text-to-image generation
quality. However, the fundamental reliance of self-attention layers on
positional embedding versus query-key similarity during generation remains an
intriguing question. We present the first mechanistic analysis of RoPE-based
MMDiT models (e.g., FLUX), introducing an automated probing strategy that
disentangles positional information versus content dependencies by
strategically manipulating RoPE during generation. Our analysis reveals
distinct dependency patterns that do not straightforwardly correlate with
depth, offering new insights into the layer-specific roles in RoPE-based MMDiT.
Based on these findings, we propose a training-free, task-specific image
editing framework that categorizes editing tasks into three types:
position-dependent editing (e.g., object addition), content
similarity-dependent editing (e.g., non-rigid editing), and region-preserved
editing (e.g., background replacement). For each type, we design tailored
key-value injection strategies based on the characteristics of the editing
task. Extensive qualitative and quantitative evaluations demonstrate that our
method outperforms state-of-the-art approaches, particularly in preserving
original semantic content and achieving seamless modifications.",['cs.CV'],"['Tianyi Wei', 'Yifan Zhou', 'Dongdong Chen', 'Xingang Pan']",2025-03-20,2025-03-20
2503.16149v1,Selective Complementary Feature Fusion and Modal Feature Compression Interaction for Brain Tumor Segmentation,"Efficient modal feature fusion strategy is the key to achieve accurate
segmentation of brain glioma. However, due to the specificity of different MRI
modes, it is difficult to carry out cross-modal fusion with large differences
in modal features, resulting in the model ignoring rich feature information. On
the other hand, the problem of multi-modal feature redundancy interaction
occurs in parallel networks due to the proliferation of feature dimensions,
further increase the difficulty of multi-modal feature fusion at the bottom
end. In order to solve the above problems, we propose a noval complementary
feature compression interaction network (CFCI-Net), which realizes the
complementary fusion and compression interaction of multi-modal feature
information with an efficient mode fusion strategy. Firstly, we propose a
selective complementary feature fusion (SCFF) module, which adaptively fuses
rich cross-modal feature information by complementary soft selection weights.
Secondly, a modal feature compression interaction (MFCI) transformer is
proposed to deal with the multi-mode fusion redundancy problem when the feature
dimension surges. The MFCI transformer is composed of modal feature compression
(MFC) and modal feature interaction (MFI) to realize redundancy feature
compression and multi-mode feature interactive learning. %In MFI, we propose a
hierarchical interactive attention mechanism based on multi-head attention.
Evaluations on the BraTS2019 and BraTS2020 datasets demonstrate that CFCI-Net
achieves superior results compared to state-of-the-art models. Code:
https://github.com/CDmm0/CFCI-Net","['eess.IV', 'cs.CV']","['Dong Chen', 'Boyue Zhao', 'Yi Zhang', 'Meng Zhao']",2025-03-20,2025-03-20
2503.16148v1,Only a Little to the Left: A Theory-grounded Measure of Political Bias in Large Language Models,"Prompt-based language models like GPT4 and LLaMa have been used for a wide
variety of use cases such as simulating agents, searching for information, or
for content analysis. For all of these applications and others, political
biases in these models can affect their performance. Several researchers have
attempted to study political bias in language models using evaluation suites
based on surveys, such as the Political Compass Test (PCT), often finding a
particular leaning favored by these models. However, there is some variation in
the exact prompting techniques, leading to diverging findings and most research
relies on constrained-answer settings to extract model responses. Moreover, the
Political Compass Test is not a scientifically valid survey instrument. In this
work, we contribute a political bias measured informed by political science
theory, building on survey design principles to test a wide variety of input
prompts, while taking into account prompt sensitivity. We then prompt 11
different open and commercial models, differentiating between instruction-tuned
and non-instruction-tuned models, and automatically classify their political
stances from 88,110 responses. Leveraging this dataset, we compute political
bias profiles across different prompt variations and find that while PCT
exaggerates bias in certain models like GPT3.5, measures of political bias are
often unstable, but generally more left-leaning for instruction-tuned models.","['cs.CY', 'cs.CL']","['Mats Faulborn', 'Indira Sen', 'Max Pellert', 'Andreas Spitz', 'David Garcia']",2025-03-20,2025-03-20
2503.16144v1,"Unify and Triumph: Polyglot, Diverse, and Self-Consistent Generation of Unit Tests with LLMs","Large language model (LLM)-based test generation has gained attention in
software engineering, yet most studies evaluate LLMs' ability to generate unit
tests in a single attempt for a given language, missing the opportunity to
leverage LLM diversity for more robust testing. This paper introduces PolyTest,
a novel approach that enhances test generation by exploiting polyglot and
temperature-controlled diversity. PolyTest systematically leverages these
properties in two complementary ways: (1) Cross-lingual test generation, where
tests are generated in multiple languages at zero temperature and then unified;
(2) Diverse test sampling, where multiple test sets are generated within the
same language at a higher temperature before unification. A key insight is that
LLMs can generate diverse yet contradicting tests -- same input, different
expected outputs -- across languages and generations. PolyTest mitigates
inconsistencies by unifying test sets, fostering self-consistency and improving
overall test quality. Unlike single-language or single-attempt approaches,
PolyTest enhances testing without requiring on-the-fly execution, making it
particularly beneficial for weaker-performing languages. We evaluate PolyTest
on Llama3-70B, GPT-4o, and GPT-3.5 using EvalPlus, generating tests in five
languages (Java, C, Python, JavaScript, and a CSV-based format) at temperature
0 and sampling multiple sets at temperature 1. We observe that LLMs frequently
generate contradicting tests across settings, and that PolyTest significantly
improves test quality across all considered metrics -- number of tests, passing
rate, statement/branch coverage (up to +9.01%), and mutation score (up to
+11.23%). Finally, PolyTest outperforms Pynguin in test generation, passing
rate, and mutation score.","['cs.SE', 'cs.AI']","['Djamel Eddine Khelladi', 'Charly Reux', 'Mathieu Acher']",2025-03-20,2025-03-20
2503.16582v1,Machine Learning-Based Genomic Linguistic Analysis (Gene Sequence Feature Learning): A Case Study on Predicting Heavy Metal Response Genes in Rice,"This study explores the application of machine learning-based genetic
linguistics for identifying heavy metal response genes in rice (Oryza sativa).
By integrating convolutional neural networks and random forest algorithms, we
developed a hybrid model capable of extracting and learning meaningful features
from gene sequences, such as k-mer frequencies and physicochemical properties.
The model was trained and tested on datasets of genes, achieving high
predictive performance (precision: 0.89, F1-score: 0.82). RNA-seq and qRT-PCR
experiments conducted on rice leaves which exposed to Hg0, revealed
differential expression of genes associated with heavy metal responses, which
validated the model's predictions. Co-expression network analysis identified
103 related genes, and a literature review indicated that these genes are
highly likely to be involved in heavy metal-related biological processes. By
integrating and comparing the analysis results with those of differentially
expressed genes (DEGs), the validity of the new machine learning method was
further demonstrated. This study highlights the efficacy of combining machine
learning with genetic linguistics for large-scale gene prediction. It
demonstrates a cost-effective and efficient approach for uncovering molecular
mechanisms underlying heavy metal responses, with potential applications in
developing stress-tolerant crop varieties.","['cs.LG', 'cs.AI', 'q-bio.GN']","['Ruiqi Yang', 'Jianxu Wang', 'Wei Yuan', 'Xun Wang', 'Mei Li']",2025-03-20,2025-03-20
2503.16134v1,Binarized Mamba-Transformer for Lightweight Quad Bayer HybridEVS Demosaicing,"Quad Bayer demosaicing is the central challenge for enabling the widespread
application of Hybrid Event-based Vision Sensors (HybridEVS). Although existing
learning-based methods that leverage long-range dependency modeling have
achieved promising results, their complexity severely limits deployment on
mobile devices for real-world applications. To address these limitations, we
propose a lightweight Mamba-based binary neural network designed for efficient
and high-performing demosaicing of HybridEVS RAW images. First, to effectively
capture both global and local dependencies, we introduce a hybrid Binarized
Mamba-Transformer architecture that combines the strengths of the Mamba and
Swin Transformer architectures. Next, to significantly reduce computational
complexity, we propose a binarized Mamba (Bi-Mamba), which binarizes all
projections while retaining the core Selective Scan in full precision. Bi-Mamba
also incorporates additional global visual information to enhance global
context and mitigate precision loss. We conduct quantitative and qualitative
experiments to demonstrate the effectiveness of BMTNet in both performance and
computational efficiency, providing a lightweight demosaicing solution suited
for real-world edge devices. Our codes and models are available at
https://github.com/Clausy9/BMTNet.",['cs.CV'],"['Shiyang Zhou', 'Haijin Zeng', 'Yunfan Lu', 'Tong Shao', 'Ke Tang', 'Yongyong Chen', 'Jie Liu', 'Jingyong Su']",2025-03-20,2025-03-20
2503.16581v1,Investigating Retrieval-Augmented Generation in Quranic Studies: A Study of 13 Open-Source Large Language Models,"Accurate and contextually faithful responses are critical when applying large
language models (LLMs) to sensitive and domain-specific tasks, such as
answering queries related to quranic studies. General-purpose LLMs often
struggle with hallucinations, where generated responses deviate from
authoritative sources, raising concerns about their reliability in religious
contexts. This challenge highlights the need for systems that can integrate
domain-specific knowledge while maintaining response accuracy, relevance, and
faithfulness. In this study, we investigate 13 open-source LLMs categorized
into large (e.g., Llama3:70b, Gemma2:27b, QwQ:32b), medium (e.g., Gemma2:9b,
Llama3:8b), and small (e.g., Llama3.2:3b, Phi3:3.8b). A Retrieval-Augmented
Generation (RAG) is used to make up for the problems that come with using
separate models. This research utilizes a descriptive dataset of Quranic surahs
including the meanings, historical context, and qualities of the 114 surahs,
allowing the model to gather relevant knowledge before responding. The models
are evaluated using three key metrics set by human evaluators: context
relevance, answer faithfulness, and answer relevance. The findings reveal that
large models consistently outperform smaller models in capturing query
semantics and producing accurate, contextually grounded responses. The
Llama3.2:3b model, even though it is considered small, does very well on
faithfulness (4.619) and relevance (4.857), showing the promise of smaller
architectures that have been well optimized. This article examines the
trade-offs between model size, computational efficiency, and response quality
while using LLMs in domain-specific applications.","['cs.CL', 'cs.AI', 'cs.LG']","['Zahra Khalila', 'Arbi Haza Nasution', 'Winda Monika', 'Aytug Onan', 'Yohei Murakami', 'Yasir Bin Ismail Radi', 'Noor Mohammad Osmani']",2025-03-20,2025-03-20
2503.16131v2,MKG-Rank: Enhancing Large Language Models with Knowledge Graph for Multilingual Medical Question Answering,"Large Language Models (LLMs) have shown remarkable progress in medical
question answering (QA), yet their effectiveness remains predominantly limited
to English due to imbalanced multilingual training data and scarce medical
resources for low-resource languages. To address this critical language gap in
medical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking
(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric
LLMs to perform multilingual medical QA. Through a word-level translation
mechanism, our framework efficiently integrates comprehensive English-centric
medical knowledge graphs into LLM reasoning at a low cost, mitigating
cross-lingual semantic distortion and achieving precise medical QA across
language barriers. To enhance efficiency, we introduce caching and multi-angle
ranking strategies to optimize the retrieval process, significantly reducing
response times and prioritizing relevant medical knowledge. Extensive
evaluations on multilingual medical QA benchmarks across Chinese, Japanese,
Korean, and Swahili demonstrate that MKG-Rank consistently outperforms
zero-shot LLMs, achieving maximum 35.03% increase in accuracy, while
maintaining an average retrieval time of only 0.0009 seconds.",['cs.CL'],"['Feiyang Li', 'Yingjian Chen', 'Haoran Liu', 'Rui Yang', 'Han Yuan', 'Yuang Jiang', 'Tianxiao Li', 'Edison Marrese Taylor', 'Hossein Rouhizadeh', 'Yusuke Iwasawa', 'Douglas Teodoro', 'Yutaka Matsuo', 'Irene Li']",2025-03-20,2025-03-21
2503.16128v1,Coupling deep and handcrafted features to assess smile genuineness,"Assessing smile genuineness from video sequences is a vital topic concerned
with recognizing facial expression and linking them with the underlying
emotional states. There have been a number of techniques proposed underpinned
with handcrafted features, as well as those that rely on deep learning to
elaborate the useful features. As both of these approaches have certain
benefits and limitations, in this work we propose to combine the features
learned by a long short-term memory network with the features handcrafted to
capture the dynamics of facial action units. The results of our experiments
indicate that the proposed solution is more effective than the baseline
techniques and it allows for assessing the smile genuineness from video
sequences in real-time.",['cs.CV'],"['Benedykt Pawlus', 'Bogdan Smolka', 'Jolanta Kawulok', 'Michal Kawulok']",2025-03-20,2025-03-20
2503.16125v1,Uncertainty Meets Diversity: A Comprehensive Active Learning Framework for Indoor 3D Object Detection,"Active learning has emerged as a promising approach to reduce the substantial
annotation burden in 3D object detection tasks, spurring several initiatives in
outdoor environments. However, its application in indoor environments remains
unexplored. Compared to outdoor 3D datasets, indoor datasets face significant
challenges, including fewer training samples per class, a greater number of
classes, more severe class imbalance, and more diverse scene types and
intra-class variances. This paper presents the first study on active learning
for indoor 3D object detection, where we propose a novel framework tailored for
this task. Our method incorporates two key criteria - uncertainty and diversity
- to actively select the most ambiguous and informative unlabeled samples for
annotation. The uncertainty criterion accounts for both inaccurate detections
and undetected objects, ensuring that the most ambiguous samples are
prioritized. Meanwhile, the diversity criterion is formulated as a joint
optimization problem that maximizes the diversity of both object class
distributions and scene types, using a new Class-aware Adaptive Prototype (CAP)
bank. The CAP bank dynamically allocates representative prototypes to each
class, helping to capture varying intra-class diversity across different
categories. We evaluate our method on SUN RGB-D and ScanNetV2, where it
outperforms baselines by a significant margin, achieving over 85% of
fully-supervised performance with just 10% of the annotation budget.",['cs.CV'],"['Jiangyi Wang', 'Na Zhao']",2025-03-20,2025-03-20
2503.16123v1,Distributed Learning over Arbitrary Topology: Linear Speed-Up with Polynomial Transient Time,"We study a distributed learning problem in which $n$ agents, each with
potentially heterogeneous local data, collaboratively minimize the sum of their
local cost functions via peer-to-peer communication. We propose a novel
algorithm, Spanning Tree Push-Pull (STPP), which employs two spanning trees
extracted from a general communication graph to distribute both model
parameters and stochastic gradients. Unlike prior approaches that rely heavily
on spectral gap properties, STPP leverages a more flexible topological
characterization, enabling robust information flow and efficient updates.
Theoretically, we prove that STPP achieves linear speedup and polynomial
transient iteration complexity, up to $O(n^7)$ for smooth nonconvex objectives
and $\tilde{O}(n^3)$ for smooth strongly convex objectives, under arbitrary
network topologies. Moreover, compared with the existing methods, STPP achieves
faster convergence rates on sparse and non-regular topologies (e.g., directed
ring) and reduces communication overhead on dense networks (e.g., static
exponential graph). These results significantly advance the state of the art,
especially when $n$ is large. Numerical experiments further demonstrate the
strong performance of STPP and confirm the practical relevance of its
theoretical convergence rates across various common graph architectures. Our
code is available at
https://anonymous.4open.science/r/SpanningTreePushPull-5D3E.","['math.OC', 'cs.LG']","['Runze You', 'Shi Pu']",2025-03-20,2025-03-20
2503.16120v1,Probabilistic Prompt Distribution Learning for Animal Pose Estimation,"Multi-species animal pose estimation has emerged as a challenging yet
critical task, hindered by substantial visual diversity and uncertainty. This
paper challenges the problem by efficient prompt learning for Vision-Language
Pretrained (VLP) models, \textit{e.g.} CLIP, aiming to resolve the
cross-species generalization problem. At the core of the solution lies in the
prompt designing, probabilistic prompt modeling and cross-modal adaptation,
thereby enabling prompts to compensate for cross-modal information and
effectively overcome large data variances under unbalanced data distribution.
To this end, we propose a novel probabilistic prompting approach to fully
explore textual descriptions, which could alleviate the diversity issues caused
by long-tail property and increase the adaptability of prompts on unseen
category instance. Specifically, we first introduce a set of learnable prompts
and propose a diversity loss to maintain distinctiveness among prompts, thus
representing diverse image attributes. Diverse textual probabilistic
representations are sampled and used as the guidance for the pose estimation.
Subsequently, we explore three different cross-modal fusion strategies at
spatial level to alleviate the adverse impacts of visual uncertainty. Extensive
experiments on multi-species animal pose benchmarks show that our method
achieves the state-of-the-art performance under both supervised and zero-shot
settings. The code is available at https://github.com/Raojiyong/PPAP.",['cs.CV'],"['Jiyong Rao', 'Brian Nlong Zhao', 'Yu Wang']",2025-03-20,2025-03-20
2503.16117v1,Improving Discriminator Guidance in Diffusion Models,"Discriminator Guidance has become a popular method for efficiently refining
pre-trained Score-Matching Diffusion models. However, in this paper, we
demonstrate that the standard implementation of this technique does not
necessarily lead to a distribution closer to the real data distribution.
Specifically, we show that training the discriminator using Cross-Entropy loss,
as commonly done, can in fact increase the Kullback-Leibler divergence between
the model and target distributions, particularly when the discriminator
overfits. To address this, we propose a theoretically sound training objective
for discriminator guidance that properly minimizes the KL divergence. We
analyze its properties and demonstrate empirically across multiple datasets
that our proposed method consistently improves over the conventional method by
producing samples of higher quality.",['cs.LG'],"['Alexandre Verine', 'Mehdi Inane', 'Florian Le Bronnec', 'Benjamin Negrevergne', 'Yann Chevaleyre']",2025-03-20,2025-03-20
2503.16112v1,PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video Streaming,"Traditional video compression algorithms exhibit significant quality
degradation at extremely low bitrates. Promptus emerges as a new paradigm for
video streaming, substantially cutting down the bandwidth essential for video
streaming. However, Promptus is computationally intensive and can not run in
real-time on mobile devices. This paper presents PromptMobile, an efficient
acceleration framework tailored for on-device Promptus. Specifically, we
propose (1) a two-stage efficient generation framework to reduce computational
cost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant
computations by 16.6\%, (3) system-level optimizations to further enhance
efficiency. The evaluations demonstrate that compared with the original
Promptus, PromptMobile achieves a 13.6x increase in image generation speed.
Compared with other streaming methods, PromptMobile achives an average LPIPS
improvement of 0.016 (compared with H.265), reducing 60\% of severely distorted
frames (compared to VQGAN).","['cs.NI', 'cs.AI', 'cs.MM']","['Liming Liu', 'Jiangkai Wu', 'Haoyang Wang', 'Peiheng Wang', 'Xinggong Zhang', 'Zongming Guo']",2025-03-20,2025-03-20
2503.16107v1,Learn to Bid as a Price-Maker Wind Power Producer,"Wind power producers (WPPs) participating in short-term power markets face
significant imbalance costs due to their non-dispatchable and variable
production. While some WPPs have a large enough market share to influence
prices with their bidding decisions, existing optimal bidding methods rarely
account for this aspect. Price-maker approaches typically model bidding as a
bilevel optimization problem, but these methods require complex market models,
estimating other participants' actions, and are computationally demanding. To
address these challenges, we propose an online learning algorithm that
leverages contextual information to optimize WPP bids in the price-maker
setting. We formulate the strategic bidding problem as a contextual multi-armed
bandit, ensuring provable regret minimization. The algorithm's performance is
evaluated against various benchmark strategies using a numerical simulation of
the German day-ahead and real-time markets.","['cs.LG', 'cs.SY', 'eess.SY']","['Shobhit Singhal', 'Marta Fochesato', 'Liviu Aolaritei', 'Florian Dörfler']",2025-03-20,2025-03-20
2503.16106v1,OSLoPrompt: Bridging Low-Supervision Challenges and Open-Set Domain Generalization in CLIP,"We introduce Low-Shot Open-Set Domain Generalization (LSOSDG), a novel
paradigm unifying low-shot learning with open-set domain generalization (ODG).
While prompt-based methods using models like CLIP have advanced DG, they falter
in low-data regimes (e.g., 1-shot) and lack precision in detecting open-set
samples with fine-grained semantics related to training classes. To address
these challenges, we propose OSLOPROMPT, an advanced prompt-learning framework
for CLIP with two core innovations. First, to manage limited supervision across
source domains and improve DG, we introduce a domain-agnostic prompt-learning
mechanism that integrates adaptable domain-specific cues and visually guided
semantic attributes through a novel cross-attention module, besides being
supported by learnable domain- and class-generic visual prompts to enhance
cross-modal adaptability. Second, to improve outlier rejection during
inference, we classify unfamiliar samples as ""unknown"" and train specialized
prompts with systematically synthesized pseudo-open samples that maintain
fine-grained relationships to known classes, generated through a targeted query
strategy with off-the-shelf foundation models. This strategy enhances feature
learning, enabling our model to detect open samples with varied granularity
more effectively. Extensive evaluations across five benchmarks demonstrate that
OSLOPROMPT establishes a new state-of-the-art in LSOSDG, significantly
outperforming existing methods.",['cs.CV'],"['Mohamad Hassan N C', 'Divyam Gupta', 'Mainak Singha', 'Sai Bhargav Rongali', 'Ankit Jha', 'Muhammad Haris Khan', 'Biplab Banerjee']",2025-03-20,2025-03-20
2503.16096v1,MarkushGrapher: Joint Visual and Textual Recognition of Markush Structures,"The automated analysis of chemical literature holds promise to accelerate
discovery in fields such as material science and drug development. In
particular, search capabilities for chemical structures and Markush structures
(chemical structure templates) within patent documents are valuable, e.g., for
prior-art search. Advancements have been made in the automatic extraction of
chemical structures from text and images, yet the Markush structures remain
largely unexplored due to their complex multi-modal nature. In this work, we
present MarkushGrapher, a multi-modal approach for recognizing Markush
structures in documents. Our method jointly encodes text, image, and layout
information through a Vision-Text-Layout encoder and an Optical Chemical
Structure Recognition vision encoder. These representations are merged and used
to auto-regressively generate a sequential graph representation of the Markush
structure along with a table defining its variable groups. To overcome the lack
of real-world training data, we propose a synthetic data generation pipeline
that produces a wide range of realistic Markush structures. Additionally, we
present M2S, the first annotated benchmark of real-world Markush structures, to
advance research on this challenging task. Extensive experiments demonstrate
that our approach outperforms state-of-the-art chemistry-specific and
general-purpose vision-language models in most evaluation settings. Code,
models, and datasets will be available.",['cs.CV'],"['Lucas Morin', 'Valéry Weber', 'Ahmed Nassar', 'Gerhard Ingmar Meijer', 'Luc Van Gool', 'Yawei Li', 'Peter Staar']",2025-03-20,2025-03-20
2503.16580v1,Procrustes Wasserstein Metric: A Modified Benamou-Brenier Approach with Applications to Latent Gaussian Distributions,"We introduce a modified Benamou-Brenier type approach leading to a
Wasserstein type distance that allows global invariance, specifically,
isometries, and we show that the problem can be summarized to orthogonal
transformations. This distance is defined by penalizing the action with a
costless movement of the particle that does not change the direction and speed
of its trajectory. We show that for Gaussian distribution resume to measuring
the Euclidean distance between their ordered vector of eigenvalues and we show
a direct application in recovering Latent Gaussian distributions.","['stat.ML', 'cs.LG', 'math.OC', 'math.PR', 'stat.AP', '49Q20, 49Q22, 62D10, 62E17, 62E20']",['Kevine Meugang Toukam'],2025-03-20,2025-03-20
2503.16094v1,Cultural Alignment in Large Language Models Using Soft Prompt Tuning,"Large Language Model (LLM) alignment conventionally relies on supervised
fine-tuning or reinforcement learning based alignment frameworks. These methods
typically require labeled or preference datasets and involve updating model
weights to align the LLM with the training objective or reward model.
Meanwhile, in social sciences such as cross-cultural studies, factor analysis
is widely used to uncover underlying dimensions or latent variables that
explain observed patterns in survey data. The non-differentiable nature of
these measurements deriving from survey data renders the former alignment
methods infeasible for alignment with cultural dimensions. To overcome this, we
propose a parameter efficient strategy that combines soft prompt tuning, which
freezes the model parameters while modifying the input prompt embeddings, with
Differential Evolution (DE), a black-box optimization method for cases where a
differentiable objective is unattainable. This strategy ensures alignment
consistency without the need for preference data or model parameter updates,
significantly enhancing efficiency and mitigating overfitting. Our method
demonstrates significant improvements in LLama-3-8B-Instruct's cultural
dimensions across multiple regions, outperforming both the Naive LLM and the
In-context Learning (ICL) baseline, and effectively bridges computational
models with human cultural nuances.",['cs.CL'],"['Reem I. Masoud', 'Martin Ferianc', 'Philip Treleaven', 'Miguel Rodrigues']",2025-03-20,2025-03-20
2503.16091v1,AIMI: Leveraging Future Knowledge and Personalization in Sparse Event Forecasting for Treatment Adherence,"Adherence to prescribed treatments is crucial for individuals with chronic
conditions to avoid costly or adverse health outcomes. For certain patient
groups, intensive lifestyle interventions are vital for enhancing medication
adherence. Accurate forecasting of treatment adherence can open pathways to
developing an on-demand intervention tool, enabling timely and personalized
support. With the increasing popularity of smartphones and wearables, it is now
easier than ever to develop and deploy smart activity monitoring systems.
However, effective forecasting systems for treatment adherence based on
wearable sensors are still not widely available. We close this gap by proposing
Adherence Forecasting and Intervention with Machine Intelligence (AIMI). AIMI
is a knowledge-guided adherence forecasting system that leverages smartphone
sensors and previous medication history to estimate the likelihood of
forgetting to take a prescribed medication. A user study was conducted with 27
participants who took daily medications to manage their cardiovascular
diseases. We designed and developed CNN and LSTM-based forecasting models with
various combinations of input features and found that LSTM models can forecast
medication adherence with an accuracy of 0.932 and an F-1 score of 0.936.
Moreover, through a series of ablation studies involving convolutional and
recurrent neural network architectures, we demonstrate that leveraging known
knowledge about future and personalized training enhances the accuracy of
medication adherence forecasting. Code available:
https://github.com/ab9mamun/AIMI.","['cs.LG', 'cs.AI']","['Abdullah Mamun', 'Diane J. Cook', 'Hassan Ghasemzadeh']",2025-03-20,2025-03-20
2503.16086v1,Hyperspectral Imaging for Identifying Foreign Objects on Pork Belly,"Ensuring food safety and quality is critical in the food processing industry,
where the detection of contaminants remains a persistent challenge. This study
presents an automated solution for detecting foreign objects on pork belly meat
using hyperspectral imaging (HSI). A hyperspectral camera was used to capture
data across various bands in the near-infrared (NIR) spectrum (900-1700 nm),
enabling accurate identification of contaminants that are often undetectable
through traditional visual inspection methods. The proposed solution combines
pre-processing techniques with a segmentation approach based on a lightweight
Vision Transformer (ViT) to distinguish contaminants from meat, fat, and
conveyor belt materials. The adopted strategy demonstrates high detection
accuracy and training efficiency, while also addressing key industrial
challenges such as inherent noise, temperature variations, and spectral
similarity between contaminants and pork belly. Experimental results validate
the effectiveness of hyperspectral imaging in enhancing food safety,
highlighting its potential for broad real-time applications in automated
quality control processes.","['cs.CV', 'cs.LG', 'I.2.6; I.2.10; J.7']","['Gabriela Ghimpeteanu', 'Hayat Rajani', 'Josep Quintana', 'Rafael Garcia']",2025-03-20,2025-03-20
2503.16085v1,Allostatic Control of Persistent States in Spiking Neural Networks for perception and computation,"We introduce a novel model for updating perceptual beliefs about the
environment by extending the concept of Allostasis to the control of internal
representations. Allostasis is a fundamental regulatory mechanism observed in
animal physiology that orchestrates responses to maintain a dynamic equilibrium
in bodily needs and internal states. In this paper, we focus on an application
in numerical cognition, where a bump of activity in an attractor network is
used as a spatial numerical representation. While existing neural networks can
maintain persistent states, to date, there is no unified framework for
dynamically controlling spatial changes in neuronal activity in response to
environmental changes. To address this, we couple a well known allostatic
microcircuit, the Hammel model, with a ring attractor, resulting in a Spiking
Neural Network architecture that can modulate the location of the bump as a
function of some reference input. This localized activity in turn is used as a
perceptual belief in a simulated subitization task a quick enumeration process
without counting. We provide a general procedure to fine-tune the model and
demonstrate the successful control of the bump location. We also study the
response time in the model with respect to changes in parameters and compare it
with biological data. Finally, we analyze the dynamics of the network to
understand the selectivity and specificity of different neurons to distinct
categories present in the input. The results of this paper, particularly the
mechanism for moving persistent states, are not limited to numerical cognition
but can be applied to a wide range of tasks involving similar representations.","['q-bio.NC', 'cs.AI']","['Aung Htet', 'Alejandro Rodriguez Jimenez', 'Sarah Hamburg', 'Alessandro Di Nuovo']",2025-03-20,2025-03-20
2503.16579v1,World Knowledge from AI Image Generation for Robot Control,"When interacting with the world robots face a number of difficult questions,
having to make decisions when given under-specified tasks where they need to
make choices, often without clearly defined right and wrong answers. Humans, on
the other hand, can often rely on their knowledge and experience to fill in the
gaps. For example, the simple task of organizing newly bought produce into the
fridge involves deciding where to put each thing individually, how to arrange
them together meaningfully, e.g. putting related things together, all while
there is no clear right and wrong way to accomplish this task. We could encode
all this information on how to do such things explicitly into the robots'
knowledge base, but this can quickly become overwhelming, considering the
number of potential tasks and circumstances the robot could encounter. However,
images of the real world often implicitly encode answers to such questions and
can show which configurations of objects are meaningful or are usually used by
humans. An image of a full fridge can give a lot of information about how
things are usually arranged in relation to each other and the full fridge at
large. Modern generative systems are capable of generating plausible images of
the real world and can be conditioned on the environment in which the robot
operates. Here we investigate the idea of using the implicit knowledge about
the world of modern generative AI systems given by their ability to generate
convincing images of the real world to solve under-specified tasks.","['cs.CV', 'cs.RO']","['Jonas Krumme', 'Christoph Zetzsche']",2025-03-20,2025-03-20
2503.16081v1,OThink-MR1: Stimulating multimodal generalized reasoning capabilities through dynamic reinforcement learning,"Multimodal Language Models have gained significant traction for their ability
to process diverse input data types and generate coherent, contextually
relevant outputs across various applications. While supervised fine-tuning
(SFT) has been the predominant approach to enhance MLLM capabilities in
task-specific optimization, it often falls short in fostering crucial
generalized reasoning abilities. Despite the potential of reinforcement
learning (RL) to address these limitations, it faces two issues: (1) its
generalized capabilities in multimodal tasks remain underexplored. (2) its
training constraints such as constant Kullback-Leibler or clamp strategy easily
lead to suboptimal bottleneck. To adress these issues, we introduce OThink-MR1,
a framework that extends RL to MLLMs, enabling them to achieve deeper
understanding and reasoning across multimodal tasks. We design a dynamic
Kullback-Leibler strategy that significantly enhances RL performance,
surpassing SFT in same-task evaluations. Also, we are the first to reveal that
RL exhibits remarkable cross-task generalization capabilities, which shows that
models post-trained with RL on one multimodal task can be effectively
transfered to another tasks. Finally, extensive experiments demonstrate the
great reasoning ability of our proposed OThink-MR1.","['cs.LG', 'cs.IR']","['Zhiyuan Liu', 'Yuting Zhang', 'Feng Liu', 'Changwang Zhang', 'Ying Sun', 'Jun Wang']",2025-03-20,2025-03-20
2503.16075v1,3-D Image-to-Image Fusion in Lightsheet Microscopy by Two-Step Adversarial Network: Contribution to the FuseMyCells Challenge,"Lightsheet microscopy is a powerful 3-D imaging technique that addresses
limitations of traditional optical and confocal microscopy but suffers from a
low penetration depth and reduced image quality at greater depths. Multiview
lightsheet microscopy improves 3-D resolution by combining multiple views but
simultaneously increasing the complexity and the photon budget, leading to
potential photobleaching and phototoxicity. The FuseMyCells challenge,
organized in conjunction with the IEEE ISBI 2025 conference, aims to benchmark
deep learning-based solutions for fusing high-quality 3-D volumes from single
3-D views, potentially simplifying procedures and conserving the photon budget.
In this work, we propose a contribution to the FuseMyCells challenge based on a
two-step procedure. The first step processes a downsampled version of the image
to capture the entire region of interest, while the second step uses a
patch-based approach for high-resolution inference, incorporating adversarial
loss to enhance visual outcomes. This method addresses challenges related to
high data resolution, the necessity of global context, and the preservation of
high-frequency details. Experimental results demonstrate the effectiveness of
our approach, highlighting its potential to improve 3-D image fusion quality
and extend the capabilities of lightsheet microscopy. The average SSIM for the
nucleus and membranes is greater than 0.85 and 0.91, respectively.","['eess.IV', 'cs.AI', 'cs.CV']","['Marek Wodzinski', 'Henning Müller']",2025-03-20,2025-03-20
2503.16072v1,Redefining Toxicity: An Objective and Context-Aware Approach for Stress-Level-Based Detection,"The fundamental problem of toxicity detection lies in the fact that the term
""toxicity"" is ill-defined. Such uncertainty causes researchers to rely on
subjective and vague data during model training, which leads to non-robust and
inaccurate results, following the 'garbage in - garbage out' paradigm. This
study introduces a novel, objective, and context-aware framework for toxicity
detection, leveraging stress levels as a key determinant of toxicity. We
propose new definition, metric and training approach as a parts of our
framework and demonstrate it's effectiveness using a dataset we collected.","['cs.LG', 'cs.AI', 'cs.CL']","['Sergey Berezin', 'Reza Farahbakhsh', 'Noel Crespi']",2025-03-20,2025-03-20
2503.16071v1,Tuning LLMs by RAG Principles: Towards LLM-native Memory,"Memory, additional information beyond the training of large language models
(LLMs), is crucial to various real-world applications, such as personal
assistant. The two mainstream solutions to incorporate memory into the
generation process are long-context LLMs and retrieval-augmented generation
(RAG). In this paper, we first systematically compare these two types of
solutions on three renovated/new datasets and show that (1) long-context
solutions, although more expensive, shall be easier to capture the big picture
and better answer queries which require considering the memory as a whole; and
(2) when the queries concern specific information, RAG solutions shall be more
competitive especially when the keywords can be explicitly matched. Therefore,
we propose a novel method RAG-Tuned-LLM which fine-tunes a relative small
(e.g., 7B) LLM using the data generated following the RAG principles, so it can
combine the advantages of both solutions. Extensive experiments on three
datasets demonstrate that RAG-Tuned-LLM can beat long-context LLMs and RAG
methods across a wide range of query types.","['cs.CL', 'cs.AI', 'cs.IR']","['Jiale Wei', 'Shuchi Wu', 'Ruochen Liu', 'Xiang Ying', 'Jingbo Shang', 'Fangbo Tao']",2025-03-20,2025-03-20
2503.16069v1,Disentangled and Interpretable Multimodal Attention Fusion for Cancer Survival Prediction,"To improve the prediction of cancer survival using whole-slide images and
transcriptomics data, it is crucial to capture both modality-shared and
modality-specific information. However, multimodal frameworks often entangle
these representations, limiting interpretability and potentially suppressing
discriminative features. To address this, we propose Disentangled and
Interpretable Multimodal Attention Fusion (DIMAF), a multimodal framework that
separates the intra- and inter-modal interactions within an attention-based
fusion mechanism to learn distinct modality-specific and modality-shared
representations. We introduce a loss based on Distance Correlation to promote
disentanglement between these representations and integrate Shapley additive
explanations to assess their relative contributions to survival prediction. We
evaluate DIMAF on four public cancer survival datasets, achieving a relative
average improvement of 1.85% in performance and 23.7% in disentanglement
compared to current state-of-the-art multimodal models. Beyond improved
performance, our interpretable framework enables a deeper exploration of the
underlying interactions between and within modalities in cancer biology.",['cs.CV'],"['Aniek Eijpe', 'Soufyan Lakbir', 'Melis Erdal Cesur', 'Sara P. Oliveira', 'Sanne Abeln', 'Wilson Silva']",2025-03-20,2025-03-20
2503.16068v1,PoseTraj: Pose-Aware Trajectory Control in Video Diffusion,"Recent advancements in trajectory-guided video generation have achieved
notable progress. However, existing models still face challenges in generating
object motions with potentially changing 6D poses under wide-range rotations,
due to limited 3D understanding. To address this problem, we introduce
PoseTraj, a pose-aware video dragging model for generating 3D-aligned motion
from 2D trajectories. Our method adopts a novel two-stage pose-aware
pretraining framework, improving 3D understanding across diverse trajectories.
Specifically, we propose a large-scale synthetic dataset PoseTraj-10K,
containing 10k videos of objects following rotational trajectories, and enhance
the model perception of object pose changes by incorporating 3D bounding boxes
as intermediate supervision signals. Following this, we fine-tune the
trajectory-controlling module on real-world videos, applying an additional
camera-disentanglement module to further refine motion accuracy. Experiments on
various benchmark datasets demonstrate that our method not only excels in 3D
pose-aligned dragging for rotational trajectories but also outperforms existing
baselines in trajectory accuracy and video quality.",['cs.CV'],"['Longbin Ji', 'Lei Zhong', 'Pengfei Wei', 'Changjian Li']",2025-03-20,2025-03-20
2503.16067v1,Bokehlicious: Photorealistic Bokeh Rendering with Controllable Apertures,"Bokeh rendering methods play a key role in creating the visually appealing,
softly blurred backgrounds seen in professional photography. While recent
learning-based approaches show promising results, generating realistic Bokeh
with variable strength remains challenging. Existing methods require additional
inputs and suffer from unrealistic Bokeh reproduction due to reliance on
synthetic data. In this work, we propose Bokehlicious, a highly efficient
network that provides intuitive control over Bokeh strength through an
Aperture-Aware Attention mechanism, mimicking the physical lens aperture. To
further address the lack of high-quality real-world data, we present RealBokeh,
a novel dataset featuring 23,000 high-resolution (24-MP) images captured by
professional photographers, covering diverse scenes with varied aperture and
focal length settings. Evaluations on both our new RealBokeh and established
Bokeh rendering benchmarks show that Bokehlicious consistently outperforms SOTA
methods while significantly reducing computational cost and exhibiting strong
zero-shot generalization. Our method and dataset further extend to defocus
deblurring, achieving competitive results on the RealDOF benchmark. Our code
and data can be found at https://github.com/TimSeizinger/Bokehlicious",['cs.CV'],"['Tim Seizinger', 'Florin-Alexandru Vasluianu', 'Marcos V. Conde', 'Radu Timofte']",2025-03-20,2025-03-20
2503.16065v1,Shining Yourself: High-Fidelity Ornaments Virtual Try-on with Diffusion Model,"While virtual try-on for clothes and shoes with diffusion models has gained
attraction, virtual try-on for ornaments, such as bracelets, rings, earrings,
and necklaces, remains largely unexplored. Due to the intricate tiny patterns
and repeated geometric sub-structures in most ornaments, it is much more
difficult to guarantee identity and appearance consistency under large pose and
scale variances between ornaments and models. This paper proposes the task of
virtual try-on for ornaments and presents a method to improve the geometric and
appearance preservation of ornament virtual try-ons. Specifically, we estimate
an accurate wearing mask to improve the alignments between ornaments and models
in an iterative scheme alongside the denoising process. To preserve structure
details, we further regularize attention layers to map the reference ornament
mask to the wearing mask in an implicit way. Experimental results demonstrate
that our method successfully wears ornaments from reference images onto target
models, handling substantial differences in scale and pose while preserving
identity and achieving realistic visual effects.",['cs.CV'],"['Yingmao Miao', 'Zhanpeng Huang', 'Rui Han', 'Zibin Wang', 'Chenhao Lin', 'Chao Shen']",2025-03-20,2025-03-20
2503.16064v1,PromptHash: Affinity-Prompted Collaborative Cross-Modal Learning for Adaptive Hashing Retrieval,"Cross-modal hashing is a promising approach for efficient data retrieval and
storage optimization. However, contemporary methods exhibit significant
limitations in semantic preservation, contextual integrity, and information
redundancy, which constrains retrieval efficacy. We present PromptHash, an
innovative framework leveraging affinity prompt-aware collaborative learning
for adaptive cross-modal hashing. We propose an end-to-end framework for
affinity-prompted collaborative hashing, with the following fundamental
technical contributions: (i) a text affinity prompt learning mechanism that
preserves contextual information while maintaining parameter efficiency, (ii)
an adaptive gated selection fusion architecture that synthesizes State Space
Model with Transformer network for precise cross-modal feature integration, and
(iii) a prompt affinity alignment strategy that bridges modal heterogeneity
through hierarchical contrastive learning. To the best of our knowledge, this
study presents the first investigation into affinity prompt awareness within
collaborative cross-modal adaptive hash learning, establishing a paradigm for
enhanced semantic consistency across modalities. Through comprehensive
evaluation on three benchmark multi-label datasets, PromptHash demonstrates
substantial performance improvements over existing approaches. Notably, on the
NUS-WIDE dataset, our method achieves significant gains of 18.22% and 18.65% in
image-to-text and text-to-image retrieval tasks, respectively. The code is
publicly available at https://github.com/ShiShuMo/PromptHash.","['cs.CV', 'cs.AI', 'cs.IR', 'cs.MM']","['Qiang Zou', 'Shuli Cheng', 'Jiayi Chen']",2025-03-20,2025-03-20
2503.16063v1,Two-stage Incomplete Utterance Rewriting on Editing Operation,"Previous work on Incomplete Utterance Rewriting (IUR) has primarily focused
on generating rewritten utterances based solely on dialogue context, ignoring
the widespread phenomenon of coreference and ellipsis in dialogues. To address
this issue, we propose a novel framework called TEO (\emph{Two-stage approach
on Editing Operation}) for IUR, in which the first stage generates editing
operations and the second stage rewrites incomplete utterances utilizing the
generated editing operations and the dialogue context. Furthermore, an
adversarial perturbation strategy is proposed to mitigate cascading errors and
exposure bias caused by the inconsistency between training and inference in the
second stage. Experimental results on three IUR datasets show that our TEO
outperforms the SOTA models significantly.","['cs.CL', 'cs.AI']","['Zhiyu Cao', 'Peifeng Li', 'Qiaoming Zhu', 'Yaxin Fan']",2025-03-20,2025-03-20
2503.16058v1,Landmarks Are Alike Yet Distinct: Harnessing Similarity and Individuality for One-Shot Medical Landmark Detection,"Landmark detection plays a crucial role in medical imaging applications such
as disease diagnosis, bone age estimation, and therapy planning. However,
training models for detecting multiple landmarks simultaneously often
encounters the ""seesaw phenomenon"", where improvements in detecting certain
landmarks lead to declines in detecting others. Yet, training a separate model
for each landmark increases memory usage and computational overhead. To address
these challenges, we propose a novel approach based on the belief that
""landmarks are distinct"" by training models with pseudo-labels and template
data updated continuously during the training process, where each model is
dedicated to detecting a single landmark to achieve high accuracy. Furthermore,
grounded on the belief that ""landmarks are also alike"", we introduce an
adapter-based fusion model, combining shared weights with landmark-specific
weights, to efficiently share model parameters while allowing flexible
adaptation to individual landmarks. This approach not only significantly
reduces memory and computational resource requirements but also effectively
mitigates the seesaw phenomenon in multi-landmark training. Experimental
results on publicly available medical image datasets demonstrate that the
single-landmark models significantly outperform traditional multi-point joint
training models in detecting individual landmarks. Although our adapter-based
fusion model shows slightly lower performance compared to the combined results
of all single-landmark models, it still surpasses the current state-of-the-art
methods while achieving a notable improvement in resource efficiency.",['cs.CV'],"['Xu He', 'Zhen Huang', 'Qingsong Yao', 'Xiaoqian Zhou', 'S. Kevin Zhou']",2025-03-20,2025-03-20
2503.16057v1,Expert Race: A Flexible Routing Strategy for Scaling Diffusion Transformer with Mixture of Experts,"Diffusion models have emerged as mainstream framework in visual generation.
Building upon this success, the integration of Mixture of Experts (MoE) methods
has shown promise in enhancing model scalability and performance. In this
paper, we introduce Race-DiT, a novel MoE model for diffusion transformers with
a flexible routing strategy, Expert Race. By allowing tokens and experts to
compete together and select the top candidates, the model learns to dynamically
assign experts to critical tokens. Additionally, we propose per-layer
regularization to address challenges in shallow layer learning, and router
similarity loss to prevent mode collapse, ensuring better expert utilization.
Extensive experiments on ImageNet validate the effectiveness of our approach,
showcasing significant performance gains while promising scaling properties.","['cs.CV', 'cs.AI', 'cs.LG']","['Yike Yuan', 'Ziyu Wang', 'Zihao Huang', 'Defa Zhu', 'Xun Zhou', 'Jingyi Yu', 'Qiyang Min']",2025-03-20,2025-03-20
2503.16056v1,Semantic-Guided Global-Local Collaborative Networks for Lightweight Image Super-Resolution,"Single-Image Super-Resolution (SISR) plays a pivotal role in enhancing the
accuracy and reliability of measurement systems, which are integral to various
vision-based instrumentation and measurement applications. These systems often
require clear and detailed images for precise object detection and recognition.
However, images captured by visual measurement tools frequently suffer from
degradation, including blurring and loss of detail, which can impede
measurement accuracy.As a potential remedy, we in this paper propose a
Semantic-Guided Global-Local Collaborative Network (SGGLC-Net) for lightweight
SISR. Our SGGLC-Net leverages semantic priors extracted from a pre-trained
model to guide the super-resolution process, enhancing image detail quality
effectively. Specifically,we propose a Semantic Guidance Module that seamlessly
integrates the semantic priors into the super-resolution network, enabling the
network to more adeptly capture and utilize semantic priors, thereby enhancing
image details. To further explore both local and non-local interactions for
improved detail rendition,we propose a Global-Local Collaborative Module, which
features three Global and Local Detail Enhancement Modules, as well as a Hybrid
Attention Mechanism to work together to efficiently learn more useful features.
Our extensive experiments show that SGGLC-Net achieves competitive PSNR and
SSIM values across multiple benchmark datasets, demonstrating higher
performance with the multi-adds reduction of 12.81G compared to
state-of-the-art lightweight super-resolution approaches. These improvements
underscore the potential of our approach to enhance the precision and
effectiveness of visual measurement systems. Codes are at
https://github.com/fanamber831/SGGLC-Net.",['cs.CV'],"['Wanshu Fan', 'Yue Wang', 'Cong Wang', 'Yunzhe Zhang', 'Wei Wang', 'Dongsheng Zhou']",2025-03-20,2025-03-20
2503.16055v1,SALT: Singular Value Adaptation with Low-Rank Transformation,"The complex nature of medical image segmentation calls for models that are
specifically designed to capture detailed, domain-specific features. Large
foundation models offer considerable flexibility, yet the cost of fine-tuning
these models remains a significant barrier. Parameter-Efficient Fine-Tuning
(PEFT) methods, such as Low-Rank Adaptation (LoRA), efficiently update model
weights with low-rank matrices but may suffer from underfitting when the chosen
rank is insufficient to capture domain-specific nuances. Conversely, full-rank
Singular Value Decomposition (SVD) based methods provide comprehensive updates
by modifying all singular values, yet they often lack flexibility and exhibit
variable performance across datasets. We propose SALT (Singular Value
Adaptation with Low-Rank Transformation), a method that selectively adapts the
most influential singular values using trainable scale and shift parameters
while complementing this with a low-rank update for the remaining subspace.
This hybrid approach harnesses the advantages of both LoRA and SVD, enabling
effective adaptation without relying on increasing model size or depth.
Evaluated on 5 challenging medical datasets, ranging from as few as 20 samples
to 1000, SALT outperforms state-of-the-art PEFT (LoRA and SVD) by 2% to 5% in
Dice with only 3.9% trainable parameters, demonstrating robust adaptation even
in low-resource settings. The code for SALT is available at:
https://github.com/BioMedIA-MBZUAI/SALT","['eess.IV', 'cs.CV']","['Abdelrahman Elsayed', 'Sarim Hashmi', 'Mohammed Elseiagy', 'Hu Wang', 'Mohammad Yaqub', 'Ibrahim Almakky']",2025-03-20,2025-03-20
2503.16051v1,Closer to Ground Truth: Realistic Shape and Appearance Labeled Data Generation for Unsupervised Underwater Image Segmentation,"Solving fish segmentation in underwater videos, a real-world problem of great
practical value in marine and aquaculture industry, is a challenging task due
to the difficulty of the filming environment, poor visibility and limited
existing annotated underwater fish data. In order to overcome these obstacles,
we introduce a novel two stage unsupervised segmentation approach that requires
no human annotations and combines artificially created and real images. Our
method generates challenging synthetic training data, by placing virtual fish
in real-world underwater habitats, after performing fish transformations such
as Thin Plate Spline shape warping and color Histogram Matching, which
realistically integrate synthetic fish into the backgrounds, making the
generated images increasingly closer to the real world data with every stage of
our approach. While we validate our unsupervised method on the popular DeepFish
dataset, obtaining a performance close to a fully-supervised SoTA model, we
further show its effectiveness on the specific case of salmon segmentation in
underwater videos, for which we introduce DeepSalmon, the largest dataset of
its kind in the literature (30 GB). Moreover, on both datasets we prove the
capability of our approach to boost the performance of the fully-supervised
SoTA model.",['cs.CV'],"['Andrei Jelea', 'Ahmed Nabil Belbachir', 'Marius Leordeanu']",2025-03-20,2025-03-20
2503.16048v1,Meta-Learning Neural Mechanisms rather than Bayesian Priors,"Children acquire language despite being exposed to several orders of
magnitude less data than large language models require. Meta-learning has been
proposed as a way to integrate human-like learning biases into neural-network
architectures, combining both the structured generalizations of symbolic models
with the scalability of neural-network models. But what does meta-learning
exactly imbue the model with? We investigate the meta-learning of formal
languages and find that, contrary to previous claims, meta-trained models are
not learning simplicity-based priors when meta-trained on datasets organised
around simplicity. Rather, we find evidence that meta-training imprints neural
mechanisms (such as counters) into the model, which function like cognitive
primitives for the network on downstream tasks. Most surprisingly, we find that
meta-training on a single formal language can provide as much improvement to a
model as meta-training on 5000 different formal languages, provided that the
formal language incentivizes the learning of useful neural mechanisms. Taken
together, our findings provide practical implications for efficient
meta-learning paradigms and new theoretical insights into linking symbolic
theories and neural mechanisms.",['cs.CL'],"['Michael Goodale', 'Salvador Mascarenhas', 'Yair Lakretz']",2025-03-20,2025-03-20
2503.16578v1,SeniorTalk: A Chinese Conversation Dataset with Rich Annotations for Super-Aged Seniors,"While voice technologies increasingly serve aging populations, current
systems exhibit significant performance gaps due to inadequate training data
capturing elderly-specific vocal characteristics like presbyphonia and
dialectal variations. The limited data available on super-aged individuals in
existing elderly speech datasets, coupled with overly simple recording styles
and annotation dimensions, exacerbates this issue. To address the critical
scarcity of speech data from individuals aged 75 and above, we introduce
SeniorTalk, a carefully annotated Chinese spoken dialogue dataset. This dataset
contains 55.53 hours of speech from 101 natural conversations involving 202
participants, ensuring a strategic balance across gender, region, and age.
Through detailed annotation across multiple dimensions, it can support a wide
range of speech tasks. We perform extensive experiments on speaker
verification, speaker diarization, speech recognition, and speech editing
tasks, offering crucial insights for the development of speech technologies
targeting this age group.","['cs.CL', 'cs.SD', 'eess.AS']","['Yang Chen', 'Hui Wang', 'Shiyao Wang', 'Junyang Chen', 'Jiabei He', 'Jiaming Zhou', 'Xi Yang', 'Yequan Wang', 'Yonghua Lin', 'Yong Qin']",2025-03-20,2025-03-20
2503.16047v2,Temporal-Spatial Attention Network (TSAN) for DoS Attack Detection in Network Traffic,"Denial-of-Service (DoS) attacks remain a critical threat to network security,
disrupting services and causing significant economic losses. Traditional
detection methods, including statistical and rule-based models, struggle to
adapt to evolving attack patterns. To address this challenge, we propose a
novel Temporal-Spatial Attention Network (TSAN) architecture for detecting
Denial of Service (DoS) attacks in network traffic. By leveraging both temporal
and spatial features of network traffic, our approach captures complex traffic
patterns and anomalies that traditional methods might miss. The TSAN model
incorporates transformer-based temporal encoding, convolutional spatial
encoding, and a cross-attention mechanism to fuse these complementary feature
spaces. Additionally, we employ multi-task learning with auxiliary tasks to
enhance the model's robustness. Experimental results on the NSL-KDD dataset
demonstrate that TSAN outperforms state-of-the-art models, achieving superior
accuracy, precision, recall, and F1-score while maintaining computational
efficiency for real-time deployment. The proposed architecture offers an
optimal balance between detection accuracy and computational overhead, making
it highly suitable for real-world network security applications.","['cs.CR', 'cs.AI']","['Bisola Faith Kayode', 'Akinyemi Sadeeq Akintola', 'Oluwole Fagbohun', 'Egonna Anaesiuba-Bristol', 'Onyekachukwu Ojumah', 'Oluwagbade Odimayo', 'Toyese Oloyede', 'Aniema Inyang', 'Teslim Kazeem', 'Habeeb Alli', 'Udodirim Ibem Offia', 'Prisca Chinazor Amajuoyi']",2025-03-20,2025-03-21
2503.16045v1,Open Science and Artificial Intelligence for supporting the sustainability of the SRC Network: The espSRC case,"The SKA Observatory (SKAO), a landmark project in radio astronomy, seeks to
address fundamental questions in astronomy. To process its immense data output,
approximately 700 PB/year, a global network of SKA Regional Centres (SR-CNet)
will provide the infrastructure, tools, computational power needed for
scientific analysis and scientific support. The Spanish SRC (espSRC) focuses on
ensuring the sustainability of this network by reducing its environmental
impact, integrating green practices into data platforms, and developing Open
Science technologies to enable reproducible research. This paper discusses and
summarizes part of the research and development activities that the team is
conducting to reduce the SRC energy consumption at the espSRC and SRCNet. The
paper also discusses fundamental research on trusted repositories to support
Open Science practices.","['astro-ph.IM', 'cs.AI']","['J. Garrido', 'S. Sánchez-Expósito', 'A. Ruiz-Falcó', 'J. Ruedas', 'M. Á. Mendoza', 'V. Vázquez', 'M. Parra', 'J. Sánchez', 'I. Labadie', 'L. Darriba', 'J. Moldón', 'M. Rodriguez-Álvarez', 'J. Díaz', 'L. Verdes-Montenegro']",2025-03-20,2025-03-20
2503.16043v1,Incomplete Utterance Rewriting with Editing Operation Guidance and Utterance Augmentation,"Although existing fashionable generation methods on Incomplete Utterance
Rewriting (IUR) can generate coherent utterances, they often result in the
inclusion of irrelevant and redundant tokens in rewritten utterances due to
their inability to focus on critical tokens in dialogue context. Furthermore,
the limited size of the training datasets also contributes to the insufficient
training of the IUR model. To address the first issue, we propose a multi-task
learning framework EO-IUR (Editing Operation-guided Incomplete Utterance
Rewriting) that introduces the editing operation labels generated by sequence
labeling module to guide generation model to focus on critical tokens.
Furthermore, we introduce a token-level heterogeneous graph to represent
dialogues. To address the second issue, we propose a two-dimensional utterance
augmentation strategy, namely editing operation-based incomplete utterance
augmentation and LLM-based historical utterance augmentation. The experimental
results on three datasets demonstrate that our EO-IUR outperforms previous
state-of-the-art (SOTA) baselines in both open-domain and task-oriented
dialogue. The code will be available at https://github.com/Dewset/EO-IUR.","['cs.CL', 'cs.AI']","['Zhiyu Cao', 'Peifeng Li', 'Yaxin Fan', 'Qiaoming Zhu']",2025-03-20,2025-03-20
2503.16041v2,GreenIQ: A Deep Search Platform for Comprehensive Carbon Market Analysis and Automated Report Generation,"This study introduces GreenIQ, an AI-powered deep search platform designed to
revolutionise carbon market intelligence through autonomous analysis and
automated report generation. Carbon markets operate across diverse regulatory
landscapes, generating vast amounts of heterogeneous data from policy
documents, industry reports, academic literature, and real-time trading
platforms. Traditional research approaches remain labour-intensive, slow, and
difficult to scale. GreenIQ addresses these limitations through a multi-agent
architecture powered by Large Language Models (LLMs), integrating five
specialised AI agents: a Main Researcher Agent for intelligent information
retrieval, a Report Writing Agent for structured synthesis, a Final Reviewer
Agent for accuracy verification, a Data Visualisation Agent for enhanced
interpretability, and a Translator Agent for multilingual adaptation. The
system achieves seamless integration of structured and unstructured information
with AI-driven citation verification, ensuring high transparency and
reliability. GreenIQ delivers a 99.2\% reduction in processing time and a
99.7\% cost reduction compared to traditional research methodologies. A novel
AI persona-based evaluation framework involving 16 domain-specific AI personas
highlights its superior cross-jurisdictional analytical capabilities and
regulatory insight generation. GreenIQ sets new standards in AI-driven research
synthesis, policy analysis, and sustainability finance by streamlining carbon
market research. It offers an efficient and scalable framework for
environmental and financial intelligence, enabling more accurate, timely, and
cost-effective decision-making in complex regulatory landscapes",['cs.AI'],"['Oluwole Fagbohun', 'Sai Yashwanth', 'Akinyemi Sadeeq Akintola', 'Ifeoluwa Wurola', 'Lanre Shittu', 'Aniema Inyang', 'Oluwatimilehin Odubola', 'Udodirim Offia', 'Said Olanrewaju', 'Ogidan Toluwaleke', 'Ilemona Abutu', 'Taiwo Akinbolaji']",2025-03-20,2025-03-21
2503.16040v1,"Evaluating Test-Time Scaling LLMs for Legal Reasoning: OpenAI o1, DeepSeek-R1, and Beyond","Recently, Test-Time Scaling Large Language Models (LLMs), such as DeepSeek-R1
and OpenAI o1, have demonstrated exceptional capabilities across various
domains and tasks, particularly in reasoning. While these models have shown
impressive performance on general language tasks, their effectiveness in
specialized fields like legal remains unclear. To address this, we present a
preliminary evaluation of LLMs in various legal scenarios, covering both
Chinese and English legal tasks. Our analysis includes 9 LLMs and 17 legal
tasks, with a focus on newly published and more complex challenges such as
multi-defendant legal judgments and legal argument reasoning. Our findings
indicate that, despite DeepSeek-R1 and OpenAI o1 being among the most powerful
models, their legal reasoning capabilities are still lacking. Specifically,
these models score below 80\% on seven Chinese legal reasoning tasks and below
80\% on two English legal reasoning tasks. This suggests that, even among the
most advanced reasoning models, legal reasoning abilities remain
underdeveloped.",['cs.CL'],"['Yaoyao Yu', 'Leilei Gan', 'Yinghao Hu', 'Bin Wei', 'Kun Kuang', 'Fei Wu']",2025-03-20,2025-03-20
2503.16036v1,Hybrid-Level Instruction Injection for Video Token Compression in Multi-modal Large Language Models,"Recent Multi-modal Large Language Models (MLLMs) have been challenged by the
computational overhead resulting from massive video frames, often alleviated
through compression strategies. However, the visual content is not equally
contributed to user instructions, existing strategies (\eg, average pool)
inevitably lead to the loss of potentially useful information. To tackle this,
we propose the Hybrid-level Instruction Injection Strategy for Conditional
Token Compression in MLLMs (HICom), utilizing the instruction as a condition to
guide the compression from both local and global levels. This encourages the
compression to retain the maximum amount of user-focused information while
reducing visual tokens to minimize computational burden. Specifically, the
instruction condition is injected into the grouped visual tokens at the local
level and the learnable tokens at the global level, and we conduct the
attention mechanism to complete the conditional compression. From the
hybrid-level compression, the instruction-relevant visual parts are highlighted
while the temporal-spatial structure is also preserved for easier understanding
of LLMs. To further unleash the potential of HICom, we introduce a new
conditional pre-training stage with our proposed dataset HICom-248K.
Experiments show that our HICom can obtain distinguished video understanding
ability with fewer tokens, increasing the performance by 2.43\% average on
three multiple-choice QA benchmarks and saving 78.8\% tokens compared with the
SOTA method. The code is available at https://github.com/lntzm/HICom.","['cs.CV', 'cs.AI', 'cs.CL']","['Zhihang Liu', 'Chen-Wei Xie', 'Pandeng Li', 'Liming Zhao', 'Longxiang Tang', 'Yun Zheng', 'Chuanbin Liu', 'Hongtao Xie']",2025-03-20,2025-03-20
2503.16032v1,Agentic Keyframe Search for Video Question Answering,"Video question answering (VideoQA) enables machines to extract and comprehend
key information from videos through natural language interaction, which is a
critical step towards achieving intelligence. However, the demand for a
thorough understanding of videos and high computational costs still limit the
widespread applications of VideoQA. To address it, we propose Agentic Keyframe
Search (AKeyS), a simple yet powerful algorithm for identifying keyframes in
the VideoQA task. It can effectively distinguish key information from
redundant, irrelevant content by leveraging modern language agents to direct
classical search algorithms. Specifically, we first segment the video and
organize it as a tree structure. Then, AKeyS uses a language agent to estimate
heuristics and movement costs while dynamically expanding nodes. Finally, the
agent determines if sufficient keyframes have been collected based on
termination conditions and provides answers. Extensive experiments on the
EgoSchema and NExT-QA datasets show that AKeyS outperforms all previous methods
with the highest keyframe searching efficiency, which means it can accurately
identify key information and conduct effective visual reasoning with minimal
computational overhead. For example, on the EgoSchema subset, it achieves 1.8%
higher accuracy while processing only 43.5% of the frames compared to
VideoTree. We believe that AKeyS represents a significant step towards building
intelligent agents for video understanding. The code is publicly available at
https://github.com/fansunqi/AKeyS.",['cs.CV'],"['Sunqi Fan', 'Meng-Hao Guo', 'Shuojin Yang']",2025-03-20,2025-03-20
2503.16031v1,Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging Fabricated Claims with Humorous Content,"This paper presents the Deceptive Humor Dataset (DHD), a novel resource for
studying humor derived from fabricated claims and misinformation. In an era of
rampant misinformation, understanding how humor intertwines with deception is
essential. DHD consists of humor-infused comments generated from false
narratives, incorporating fabricated claims and manipulated information using
the ChatGPT-4o model. Each instance is labeled with a Satire Level, ranging
from 1 for subtle satire to 3 for high-level satire and classified into five
distinct Humor Categories: Dark Humor, Irony, Social Commentary, Wordplay, and
Absurdity. The dataset spans multiple languages including English, Telugu,
Hindi, Kannada, Tamil, and their code-mixed variants (Te-En, Hi-En, Ka-En,
Ta-En), making it a valuable multilingual benchmark. By introducing DHD, we
establish a structured foundation for analyzing humor in deceptive contexts,
paving the way for a new research direction that explores how humor not only
interacts with misinformation but also influences its perception and spread. We
establish strong baselines for the proposed dataset, providing a foundation for
future research to benchmark and advance deceptive humor detection models.",['cs.CL'],"['Sai Kartheek Reddy Kasu', 'Shankar Biradar', 'Sunil Saumya']",2025-03-20,2025-03-20
2503.16025v1,Single Image Iterative Subject-driven Generation and Editing,"Personalizing image generation and editing is particularly challenging when
we only have a few images of the subject, or even a single image. A common
approach to personalization is concept learning, which can integrate the
subject into existing models relatively quickly, but produces images whose
quality tends to deteriorate quickly when the number of subject images is
small. Quality can be improved by pre-training an encoder, but training
restricts generation to the training distribution, and is time consuming. It is
still an open hard challenge to personalize image generation and editing from a
single image without training. Here, we present SISO, a novel, training-free
approach based on optimizing a similarity score with an input subject image.
More specifically, SISO iteratively generates images and optimizes the model
based on loss of similarity with the given subject image until a satisfactory
level of similarity is achieved, allowing plug-and-play optimization to any
image generator. We evaluated SISO in two tasks, image editing and image
generation, using a diverse data set of personal subjects, and demonstrate
significant improvements over existing methods in image quality, subject
fidelity, and background preservation.","['cs.CV', 'cs.AI']","['Yair Shpitzer', 'Gal Chechik', 'Idan Schwartz']",2025-03-20,2025-03-20
2503.16024v1,The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement,"Large language models (LLMs) have recently transformed from text-based
assistants to autonomous agents capable of planning, reasoning, and iteratively
improving their actions. While numerical reward signals and verifiers can
effectively rank candidate actions, they often provide limited contextual
guidance. In contrast, natural language feedback better aligns with the
generative capabilities of LLMs, providing richer and more actionable
suggestions. However, parsing and implementing this feedback effectively can be
challenging for LLM-based agents. In this work, we introduce Critique-Guided
Improvement (CGI), a novel two-player framework, comprising an actor model that
explores an environment and a critic model that generates detailed nature
language feedback. By training the critic to produce fine-grained assessments
and actionable revisions, and the actor to utilize these critiques, our
approach promotes more robust exploration of alternative strategies while
avoiding local optima. Experiments in three interactive environments show that
CGI outperforms existing baselines by a substantial margin. Notably, even a
small critic model surpasses GPT-4 in feedback quality. The resulting actor
achieves state-of-the-art performance, demonstrating the power of explicit
iterative guidance to enhance decision-making in LLM-based agents.","['cs.CL', 'cs.AI']","['Ruihan Yang', 'Fanghua Ye', 'Jian Li', 'Siyu Yuan', 'Yikai Zhang', 'Zhaopeng Tu', 'Xiaolong Li', 'Deqing Yang']",2025-03-20,2025-03-20
2503.16022v1,Corrective In-Context Learning: Evaluating Self-Correction in Large Language Models,"In-context learning (ICL) has transformed the use of large language models
(LLMs) for NLP tasks, enabling few-shot learning by conditioning on labeled
examples without finetuning. Despite its effectiveness, ICL is prone to errors,
especially for challenging examples. With the goal of improving the performance
of ICL, we propose corrective in-context learning (CICL), an approach that
incorporates a model's incorrect predictions alongside ground truth corrections
into the prompt, aiming to enhance classification accuracy through
self-correction. However, contrary to our hypothesis, extensive experiments on
text classification tasks demonstrate that CICL consistently underperforms
standard ICL, with performance degrading as the proportion of corrections in
the prompt increases. Our findings indicate that CICL introduces confusion by
disrupting the model's task understanding, rather than refining its
predictions. Additionally, we observe that presenting harder examples in
standard ICL does not improve performance, suggesting that example difficulty
alone may not be a reliable criterion for effective selection. By presenting
these negative results, we provide important insights into the limitations of
self-corrective mechanisms in LLMs and offer directions for future research.",['cs.CL'],"['Mario Sanz-Guerrero', 'Katharina von der Wense']",2025-03-20,2025-03-20
2503.16021v2,Autonomous AI imitators increase diversity in homogeneous information ecosystems,"Recent breakthroughs in large language models (LLMs) have facilitated
autonomous AI agents capable of imitating human-generated content. This
technological advancement raises fundamental questions about AI's impact on the
diversity and democratic value of information ecosystems. We introduce a
large-scale simulation framework to examine AI-based imitation within news, a
context crucial for public discourse. By systematically testing two distinct
imitation strategies across a range of information environments varying in
initial diversity, we demonstrate that AI-generated articles do not uniformly
homogenize content. Instead, AI's influence is strongly context-dependent:
AI-generated content can introduce valuable diversity in originally homogeneous
news environments but diminish diversity in initially heterogeneous contexts.
These results illustrate that the initial diversity of an information
environment critically shapes AI's impact, challenging assumptions that
AI-driven imitation uniformly threatens diversity. Instead, when information is
initially homogeneous, AI-driven imitation can expand perspectives, styles, and
topics. This is especially important in news contexts, where information
diversity fosters richer public debate by exposing citizens to alternative
viewpoints, challenging biases, and preventing narrative monopolies, which is
essential for a resilient democracy.","['cs.CY', 'cs.AI', 'cs.CL', 'J.4']","['Emil Bakkensen Johansen', 'Oliver Baumann']",2025-03-20,2025-03-21
2503.16013v1,GraspCoT: Integrating Physical Property Reasoning for 6-DoF Grasping under Flexible Language Instructions,"Flexible instruction-guided 6-DoF grasping is a significant yet challenging
task for real-world robotic systems. Existing methods utilize the contextual
understanding capabilities of the large language models (LLMs) to establish
mappings between expressions and targets, allowing robots to comprehend users'
intentions in the instructions. However, the LLM's knowledge about objects'
physical properties remains underexplored despite its tight relevance to
grasping. In this work, we propose GraspCoT, a 6-DoF grasp detection framework
that integrates a Chain-of-Thought (CoT) reasoning mechanism oriented to
physical properties, guided by auxiliary question-answering (QA) tasks.
Particularly, we design a set of QA templates to enable hierarchical reasoning
that includes three stages: target parsing, physical property analysis, and
grasp action selection. Moreover, GraspCoT presents a unified multimodal LLM
architecture, which encodes multi-view observations of 3D scenes into 3D-aware
visual tokens, and then jointly embeds these visual tokens with CoT-derived
textual tokens within LLMs to generate grasp pose predictions. Furthermore, we
present IntentGrasp, a large-scale benchmark that fills the gap in public
datasets for multi-object grasp detection under diverse and indirect verbal
commands. Extensive experiments on IntentGrasp demonstrate the superiority of
our method, with additional validation in real-world robotic applications
confirming its practicality. Codes and data will be released.","['cs.RO', 'cs.CV']","['Xiaomeng Chu', 'Jiajun Deng', 'Guoliang You', 'Wei Liu', 'Xingchen Li', 'Jianmin Ji', 'Yanyong Zhang']",2025-03-20,2025-03-20
2503.16012v1,GazeSCRNN: Event-based Near-eye Gaze Tracking using a Spiking Neural Network,"This work introduces GazeSCRNN, a novel spiking convolutional recurrent
neural network designed for event-based near-eye gaze tracking. Leveraging the
high temporal resolution, energy efficiency, and compatibility of Dynamic
Vision Sensor (DVS) cameras with event-based systems, GazeSCRNN uses a spiking
neural network (SNN) to address the limitations of traditional gaze-tracking
systems in capturing dynamic movements. The proposed model processes event
streams from DVS cameras using Adaptive Leaky-Integrate-and-Fire (ALIF) neurons
and a hybrid architecture optimized for spatio-temporal data. Extensive
evaluations on the EV-Eye dataset demonstrate the model's accuracy in
predicting gaze vectors. In addition, we conducted ablation studies to reveal
the importance of the ALIF neurons, dynamic event framing, and training
techniques, such as Forward-Propagation-Through-Time, in enhancing overall
system performance. The most accurate model achieved a Mean Angle Error (MAE)
of 6.034{\deg} and a Mean Pupil Error (MPE) of 2.094 mm. Consequently, this
work is pioneering in demonstrating the feasibility of using SNNs for
event-based gaze tracking, while shedding light on critical challenges and
opportunities for further improvement.","['cs.CV', 'cs.NE']","['Stijn Groenen', 'Marzieh Hassanshahi Varposhti', 'Mahyar Shahsavari']",2025-03-20,2025-03-20
2503.16010v1,Patch-based learning of adaptive Total Variation parameter maps for blind image denoising,"We consider a patch-based learning approach defined in terms of neural
networks to estimate spatially adaptive regularisation parameter maps for image
denoising with weighted Total Variation and test it to situations when the
noise distribution is unknown. As an example, we consider situations where
noise could be either Gaussian or Poisson and perform preliminary model
selection by a standard binary classification network. Then, we define a
patch-based approach where at each image pixel an optimal weighting between TV
regularisation and the corresponding data fidelity is learned in a supervised
way using reference natural image patches upon optimisation of SSIM and in a
sliding window fashion. Extensive numerical results are reported for both noise
models, showing significant improvement w.r.t. results obtained by means of
optimal scalar regularisation.","['eess.IV', 'cs.LG', 'cs.NA', 'math.NA']","['Claudio Fantasia', 'Luca Calatroni', 'Xavier Descombes', 'Rim Rekik']",2025-03-20,2025-03-20
2503.16000v1,SenseExpo: Efficient Autonomous Exploration with Prediction Information from Lightweight Neural Networks,"This paper proposes SenseExpo, an efficient autonomous exploration framework
based on a lightweight prediction network, which addresses the limitations of
traditional methods in computational overhead and environmental generalization.
By integrating Generative Adversarial Networks (GANs), Transformer, and Fast
Fourier Convolution (FFC), we designed a lightweight prediction model with
merely 709k parameters. Our smallest model achieves better performance on the
KTH dataset than U-net (24.5M) and LaMa (51M), delivering PSNR 9.026 and SSIM
0.718, particularly representing a 38.7% PSNR improvement over the
51M-parameter LaMa model. Cross-domain testing demonstrates its strong
generalization capability, with an FID score of 161.55 on the HouseExpo
dataset, significantly outperforming comparable methods. Regarding exploration
efficiency, on the KTH dataset,SenseExpo demonstrates approximately a 67.9%
time reduction in exploration time compared to MapEx. On the MRPB 1.0 dataset,
SenseExpo achieves 77.1% time reduction roughly compared to MapEx. Deployed as
a plug-and-play ROS node, the framework seamlessly integrates with existing
navigation systems, providing an efficient solution for resource-constrained
devices.",['cs.CV'],"['Haojia Gao', 'Haohua Que', 'Hoiian Au', 'Weihao Shan', 'Mingkai Liu', 'Yusen Qin', 'Lei Mu', 'Rong Zhao', 'Xinghua Yang', 'Qi Wei', 'Fei Qiao']",2025-03-20,2025-03-20
2503.15997v1,Automating 3D Dataset Generation with Neural Radiance Fields,"3D detection is a critical task to understand spatial characteristics of the
environment and is used in a variety of applications including robotics,
augmented reality, and image retrieval. Training performant detection models
require diverse, precisely annotated, and large scale datasets that involve
complex and expensive creation processes. Hence, there are only few public 3D
datasets that are additionally limited in their range of classes. In this work,
we propose a pipeline for automatic generation of 3D datasets for arbitrary
objects. By utilizing the universal 3D representation and rendering
capabilities of Radiance Fields, our pipeline generates high quality 3D models
for arbitrary objects. These 3D models serve as input for a synthetic dataset
generator. Our pipeline is fast, easy to use and has a high degree of
automation. Our experiments demonstrate, that 3D pose estimation networks,
trained with our generated datasets, archive strong performance in typical
application scenarios.",['cs.CV'],"['P. Schulz', 'T. Hempel', 'A. Al-Hamadi']",2025-03-20,2025-03-20
2503.15996v1,Animating the Uncaptured: Humanoid Mesh Animation with Video Diffusion Models,"Animation of humanoid characters is essential in various graphics
applications, but requires significant time and cost to create realistic
animations. We propose an approach to synthesize 4D animated sequences of input
static 3D humanoid meshes, leveraging strong generalized motion priors from
generative video models -- as such video models contain powerful motion
information covering a wide variety of human motions. From an input static 3D
humanoid mesh and a text prompt describing the desired animation, we synthesize
a corresponding video conditioned on a rendered image of the 3D mesh. We then
employ an underlying SMPL representation to animate the corresponding 3D mesh
according to the video-generated motion, based on our motion optimization. This
enables a cost-effective and accessible solution to enable the synthesis of
diverse and realistic 4D animations.","['cs.GR', 'cs.CV']","['Marc Benedí San Millán', 'Angela Dai', 'Matthias Nießner']",2025-03-20,2025-03-20
2503.16577v1,Feature selection strategies for optimized heart disease diagnosis using ML and DL models,"Heart disease remains one of the leading causes of morbidity and mortality
worldwide, necessitating the development of effective diagnostic tools to
enable early diagnosis and clinical decision-making. This study evaluates the
impact of feature selection techniques Mutual Information (MI), Analysis of
Variance (ANOVA), and Chi-Square on the predictive performance of various
machine learning (ML) and deep learning (DL) models using a dataset of clinical
indicators for heart disease. Eleven ML/DL models were assessed using metrics
such as precision, recall, AUC score, F1-score, and accuracy. Results indicate
that MI outperformed other methods, particularly for advanced models like
neural networks, achieving the highest accuracy of 82.3% and recall score of
0.94. Logistic regression (accuracy 82.1%) and random forest (accuracy 80.99%)
also demonstrated improved performance with MI. Simpler models such as Naive
Bayes and decision trees achieved comparable results with ANOVA and Chi-Square,
yielding accuracies of 76.45% and 75.99%, respectively, making them
computationally efficient alternatives. Conversely, k Nearest Neighbors (KNN)
and Support Vector Machines (SVM) exhibited lower performance, with accuracies
ranging between 51.52% and 54.43%, regardless of the feature selection method.
This study provides a comprehensive comparison of feature selection methods for
heart disease prediction, demonstrating the critical role of feature selection
in optimizing model performance. The results offer practical guidance for
selecting appropriate feature selection techniques based on the chosen
classification algorithm, contributing to the development of more accurate and
efficient diagnostic tools for enhanced clinical decision-making in cardiology.","['cs.LG', 'cs.AI']","['Bilal Ahmad', 'Jinfu Chen', 'Haibao Chen']",2025-03-20,2025-03-20
2503.15990v1,ECKGBench: Benchmarking Large Language Models in E-commerce Leveraging Knowledge Graph,"Large language models (LLMs) have demonstrated their capabilities across
various NLP tasks. Their potential in e-commerce is also substantial, evidenced
by practical implementations such as platform search, personalized
recommendations, and customer service. One primary concern associated with LLMs
is their factuality (e.g., hallucination), which is urgent in e-commerce due to
its significant impact on user experience and revenue. Despite some methods
proposed to evaluate LLMs' factuality, issues such as lack of reliability, high
consumption, and lack of domain expertise leave a gap between effective
assessment in e-commerce. To bridge the evaluation gap, we propose ECKGBench, a
dataset specifically designed to evaluate the capacities of LLMs in e-commerce
knowledge. Specifically, we adopt a standardized workflow to automatically
generate questions based on a large-scale knowledge graph, guaranteeing
sufficient reliability. We employ the simple question-answering paradigm,
substantially improving the evaluation efficiency by the least input and output
tokens. Furthermore, we inject abundant e-commerce expertise in each evaluation
stage, including human annotation, prompt design, negative sampling, and
verification. Besides, we explore the LLMs' knowledge boundaries in e-commerce
from a novel perspective. Through comprehensive evaluations of several advanced
LLMs on ECKGBench, we provide meticulous analysis and insights into leveraging
LLMs for e-commerce.",['cs.CL'],"['Langming Liu', 'Haibin Chen', 'Yuhao Wang', 'Yujin Yuan', 'Shilei Liu', 'Wenbo Su', 'Xiangyu Zhao', 'Bo Zheng']",2025-03-20,2025-03-20
2503.16575v1,"Extract, Match, and Score: An Evaluation Paradigm for Long Question-context-answer Triplets in Financial Analysis","The rapid advancement of large language models (LLMs) has sparked widespread
adoption across diverse applications, making robust evaluation frameworks
crucial for assessing their performance. While conventional evaluation metrics
remain applicable for shorter texts, their efficacy diminishes when evaluating
the quality of long-form answers. This limitation is particularly critical in
real-world scenarios involving extended questions, extensive context, and
long-form answers, such as financial analysis or regulatory compliance. In this
paper, we use a practical financial use case to illustrate applications that
handle ""long question-context-answer triplets"". We construct a real-world
financial dataset comprising long triplets and demonstrate the inadequacies of
traditional metrics. To address this, we propose an effective Extract, Match,
and Score (EMS) evaluation approach tailored to the complexities of long-form
LLMs' outputs, providing practitioners with a reliable methodology for
assessing LLMs' performance in complex real-world scenarios.","['cs.CL', 'cs.AI']","['Bo Hu', 'Han Yuan', 'Vlad Pandelea', 'Wuqiong Luo', 'Yingzhu Zhao', 'Zheng Ma']",2025-03-20,2025-03-20
2503.15986v1,SpiLiFormer: Enhancing Spiking Transformers with Lateral Inhibition,"Spiking Neural Networks (SNNs) based on Transformers have garnered
significant attention due to their superior performance and high energy
efficiency. However, the spiking attention modules of most existing
Transformer-based SNNs are adapted from those of analog Transformers, failing
to fully address the issue of over-allocating attention to irrelevant contexts.
To fix this fundamental yet overlooked issue, we propose a Lateral
Inhibition-inspired Spiking Transformer (SpiLiFormer). It emulates the brain's
lateral inhibition mechanism, guiding the model to enhance attention to
relevant tokens while suppressing attention to irrelevant ones. Our model
achieves state-of-the-art (SOTA) performance across multiple datasets,
including CIFAR-10 (+0.45%), CIFAR-100 (+0.48%), CIFAR10-DVS (+2.70%),
N-Caltech101 (+1.94%), and ImageNet-1K (+1.6%). Notably, on the ImageNet-1K
dataset, SpiLiFormer (69.9M parameters, 4 time steps, 384 resolution)
outperforms E-SpikeFormer (173.0M parameters, 8 time steps, 384 resolution), a
SOTA spiking Transformer, by 0.46% using only 39% of the parameters and half
the time steps. Our code and training checkpoints will be released upon
acceptance.","['cs.NE', 'cs.CV']","['Zeqi Zheng', 'Yanchen Huang', 'Yingchao Yu', 'Zizheng Zhu', 'Junfeng Tang', 'Zhaofei Yu', 'Yaochu Jin']",2025-03-20,2025-03-20
2503.15985v1,Exploring the Reliability of Self-explanation and its Relationship with Classification in Language Model-driven Financial Analysis,"Language models (LMs) have exhibited exceptional versatility in reasoning and
in-depth financial analysis through their proprietary information processing
capabilities. Previous research focused on evaluating classification
performance while often overlooking explainability or pre-conceived that
refined explanation corresponds to higher classification accuracy. Using a
public dataset in finance domain, we quantitatively evaluated self-explanations
by LMs, focusing on their factuality and causality. We identified the
statistically significant relationship between the accuracy of classifications
and the factuality or causality of self-explanations. Our study built an
empirical foundation for approximating classification confidence through
self-explanations and for optimizing classification via proprietary reasoning.",['cs.AI'],"['Han Yuan', 'Li Zhang', 'Zheng Ma']",2025-03-20,2025-03-20
2503.16573v1,AUV Acceleration Prediction Using DVL and Deep Learning,"Autonomous underwater vehicles (AUVs) are essential for various applications,
including oceanographic surveys, underwater mapping, and infrastructure
inspections. Accurate and robust navigation are critical to completing these
tasks. To this end, a Doppler velocity log (DVL) and inertial sensors are fused
together. Recently, a model-based approach demonstrated the ability to extract
the vehicle acceleration vector from DVL velocity measurements. Motivated by
this advancement, in this paper we present an end-to-end deep learning approach
to estimate the AUV acceleration vector based on past DVL velocity
measurements. Based on recorded data from sea experiments, we demonstrate that
the proposed method improves acceleration vector estimation by more than 65%
compared to the model-based approach by using data-driven techniques. As a
result of our data-driven approach, we can enhance navigation accuracy and
reliability in AUV applications, contributing to more efficient and effective
underwater missions through improved accuracy and reliability.","['cs.RO', 'cs.AI']","['Yair Stolero', 'Itzik Klein']",2025-03-20,2025-03-20
2503.15984v1,DIPLI: Deep Image Prior Lucky Imaging for Blind Astronomical Image Restoration,"Contemporary image restoration and super-resolution techniques effectively
harness deep neural networks, markedly outperforming traditional methods.
However, astrophotography presents unique challenges for deep learning due to
limited training data. This work explores hybrid strategies, such as the Deep
Image Prior (DIP) model, which facilitates blind training but is susceptible to
overfitting, artifact generation, and instability when handling noisy images.
We propose enhancements to the DIP model's baseline performance through several
advanced techniques. First, we refine the model to process multiple frames
concurrently, employing the Back Projection method and the TVNet model. Next,
we adopt a Markov approach incorporating Monte Carlo estimation, Langevin
dynamics, and a variational input technique to achieve unbiased estimates with
minimal variance and counteract overfitting effectively. Collectively, these
modifications reduce the likelihood of noise learning and mitigate loss
function fluctuations during training, enhancing result stability. We validated
our algorithm across multiple image sets of astronomical and celestial objects,
achieving performance that not only mitigates limitations of Lucky Imaging, a
classical computer vision technique that remains a standard in astronomical
image reconstruction but surpasses the original DIP model, state of the art
transformer- and diffusion-based models, underscoring the significance of our
improvements.","['cs.CV', 'astro-ph.IM', 'cs.AI', 'eess.IV']","['Suraj Singh', 'Anastasia Batsheva', 'Oleg Y. Rogov', 'Ahmed Bouridane']",2025-03-20,2025-03-20
2503.15983v1,InhibiDistilbert: Knowledge Distillation for a ReLU and Addition-based Transformer,"This work explores optimizing transformer-based language models by
integrating model compression techniques with inhibitor attention, a novel
alternative attention mechanism. Inhibitor attention employs Manhattan
distances and ReLU activations instead of the matrix multiplications and
softmax activation of the conventional scaled dot-product attention. This shift
offers potential computational and energy savings while maintaining model
effectiveness. We propose further adjustments to improve the inhibitor
mechanism's training efficiency and evaluate its performance on the DistilBERT
architecture. Our knowledge distillation experiments indicate that the modified
inhibitor transformer model can achieve competitive performance on standard NLP
benchmarks, including General Language Understanding Evaluation (GLUE) and
sentiment analysis tasks.","['cs.CL', 'cs.AI', 'cs.LG', '68T50 (Primary) 68T07, 68Q32 (Secondary)', 'I.2.6; I.2.7; I.5.1']","['Tony Zhang', 'Rickard Brännvall']",2025-03-20,2025-03-20
2503.15979v1,Exploratory Study into Relations between Cognitive Distortions and Emotional Appraisals,"In recent years, there has been growing interest in studying cognitive
distortions and emotional appraisals from both computational and psychological
perspectives. Despite considerable similarities between emotional reappraisal
and cognitive reframing as emotion regulation techniques, these concepts have
largely been examined in isolation. This research explores the relationship
between cognitive distortions and emotional appraisal dimensions, examining
their potential connections and relevance for future interdisciplinary studies.
Under this pretext, we conduct an exploratory computational study, aimed at
investigating the relationship between cognitive distortion and emotional
appraisals. We show that the patterns of statistically significant
relationships between cognitive distortions and appraisal dimensions vary
across different distortion categories, giving rise to distinct appraisal
profiles for individual distortion classes. Additionally, we analyze the impact
of cognitive restructuring on appraisal dimensions, exemplifying the emotion
regulation aspect of cognitive restructuring.",['cs.CL'],"['Navneet Agarwal', 'Kairit Sirts']",2025-03-20,2025-03-20
2503.15978v1,A Survey on fMRI-based Brain Decoding for Reconstructing Multimodal Stimuli,"In daily life, we encounter diverse external stimuli, such as images, sounds,
and videos. As research in multimodal stimuli and neuroscience advances,
fMRI-based brain decoding has become a key tool for understanding brain
perception and its complex cognitive processes. Decoding brain signals to
reconstruct stimuli not only reveals intricate neural mechanisms but also
drives progress in AI, disease treatment, and brain-computer interfaces. Recent
advancements in neuroimaging and image generation models have significantly
improved fMRI-based decoding. While fMRI offers high spatial resolution for
precise brain activity mapping, its low temporal resolution and signal noise
pose challenges. Meanwhile, techniques like GANs, VAEs, and Diffusion Models
have enhanced reconstructed image quality, and multimodal pre-trained models
have boosted cross-modal decoding tasks. This survey systematically reviews
recent progress in fMRI-based brain decoding, focusing on stimulus
reconstruction from passive brain signals. It summarizes datasets, relevant
brain regions, and categorizes existing methods by model structure.
Additionally, it evaluates model performance and discusses their effectiveness.
Finally, it identifies key challenges and proposes future research directions,
offering valuable insights for the field. For more information and resources
related to this survey, visit https://github.com/LpyNow/BrainDecodingImage.",['cs.CV'],"['Pengyu Liu', 'Guohua Dong', 'Dan Guo', 'Kun Li', 'Fengling Li', 'Xun Yang', 'Meng Wang', 'Xiaomin Ying']",2025-03-20,2025-03-20
2503.15975v1,Acc3D: Accelerating Single Image to 3D Diffusion Models via Edge Consistency Guided Score Distillation,"We present Acc3D to tackle the challenge of accelerating the diffusion
process to generate 3D models from single images. To derive high-quality
reconstructions through few-step inferences, we emphasize the critical issue of
regularizing the learning of score function in states of random noise. To this
end, we propose edge consistency, i.e., consistent predictions across the high
signal-to-noise ratio region, to enhance a pre-trained diffusion model,
enabling a distillation-based refinement of the endpoint score function.
Building on those distilled diffusion models, we propose an adversarial
augmentation strategy to further enrich the generation detail and boost overall
generation quality. The two modules complement each other, mutually reinforcing
to elevate generative performance. Extensive experiments demonstrate that our
Acc3D not only achieves over a $20\times$ increase in computational efficiency
but also yields notable quality improvements, compared to the
state-of-the-arts.",['cs.CV'],"['Kendong Liu', 'Zhiyu Zhu', 'Hui Liu', 'Junhui Hou']",2025-03-20,2025-03-20
2503.15973v1,STOP: Integrated Spatial-Temporal Dynamic Prompting for Video Understanding,"Pre-trained on tremendous image-text pairs, vision-language models like CLIP
have demonstrated promising zero-shot generalization across numerous
image-based tasks. However, extending these capabilities to video tasks remains
challenging due to limited labeled video data and high training costs. Recent
video prompting methods attempt to adapt CLIP for video tasks by introducing
learnable prompts, but they typically rely on a single static prompt for all
video sequences, overlooking the diverse temporal dynamics and spatial
variations that exist across frames. This limitation significantly hinders the
model's ability to capture essential temporal information for effective video
understanding. To address this, we propose an integrated Spatial-TempOral
dynamic Prompting (STOP) model which consists of two complementary modules, the
intra-frame spatial prompting and inter-frame temporal prompting. Our
intra-frame spatial prompts are designed to adaptively highlight discriminative
regions within each frame by leveraging intra-frame attention and temporal
variation, allowing the model to focus on areas with substantial temporal
dynamics and capture fine-grained spatial details. Additionally, to highlight
the varying importance of frames for video understanding, we further introduce
inter-frame temporal prompts, dynamically inserting prompts between frames with
high temporal variance as measured by frame similarity. This enables the model
to prioritize key frames and enhances its capacity to understand temporal
dependencies across sequences. Extensive experiments on various video
benchmarks demonstrate that STOP consistently achieves superior performance
against state-of-the-art methods. The code is available at
https://github.com/zhoujiahuan1991/CVPR2025-STOP.",['cs.CV'],"['Zichen Liu', 'Kunlun Xu', 'Bing Su', 'Xu Zou', 'Yuxin Peng', 'Jiahuan Zhou']",2025-03-20,2025-03-20
2503.15972v1,TVineSynth: A Truncated C-Vine Copula Generator of Synthetic Tabular Data to Balance Privacy and Utility,"We propose TVineSynth, a vine copula based synthetic tabular data generator,
which is designed to balance privacy and utility, using the vine tree structure
and its truncation to do the trade-off. Contrary to synthetic data generators
that achieve DP by globally adding noise, TVineSynth performs a controlled
approximation of the estimated data generating distribution, so that it does
not suffer from poor utility of the resulting synthetic data for downstream
prediction tasks. TVineSynth introduces a targeted bias into the vine copula
model that, combined with the specific tree structure of the vine, causes the
model to zero out privacy-leaking dependencies while relying on those that are
beneficial for utility. Privacy is here measured with membership (MIA) and
attribute inference attacks (AIA). Further, we theoretically justify how the
construction of TVineSynth ensures AIA privacy under a natural privacy measure
for continuous sensitive attributes. When compared to competitor models, with
and without DP, on simulated and on real-world data, TVineSynth achieves a
superior privacy-utility balance.","['cs.LG', 'stat.ML']","['Elisabeth Griesbauer', 'Claudia Czado', 'Arnoldo Frigessi', 'Ingrid Hobæk Haff']",2025-03-20,2025-03-20
2503.15970v1,V-NAW: Video-based Noise-aware Adaptive Weighting for Facial Expression Recognition,"Facial Expression Recognition (FER) plays a crucial role in human affective
analysis and has been widely applied in computer vision tasks such as
human-computer interaction and psychological assessment. The 8th Affective
Behavior Analysis in-the-Wild (ABAW) Challenge aims to assess human emotions
using the video-based Aff-Wild2 dataset. This challenge includes various tasks,
including the video-based EXPR recognition track, which is our primary focus.
In this paper, we demonstrate that addressing label ambiguity and class
imbalance, which are known to cause performance degradation, can lead to
meaningful performance improvements. Specifically, we propose Video-based
Noise-aware Adaptive Weighting (V-NAW), which adaptively assigns importance to
each frame in a clip to address label ambiguity and effectively capture
temporal variations in facial expressions. Furthermore, we introduce a simple
and effective augmentation strategy to reduce redundancy between consecutive
frames, which is a primary cause of overfitting. Through extensive experiments,
we validate the effectiveness of our approach, demonstrating significant
improvements in video-based FER performance.",['cs.CV'],"['JunGyu Lee', 'Kunyoung Lee', 'Haesol Park', 'Ig-Jae Kim', 'Gi Pyo Nam']",2025-03-20,2025-03-20
2503.15969v1,Beyond the Visible: Multispectral Vision-Language Learning for Earth Observation,"Vision-language models for Earth observation (EO) typically rely on the
visual spectrum of data as the only model input, thus failing to leverage the
rich spectral information available in the multispectral channels recorded by
satellites. Therefore, in this paper, we introduce Llama3-MS-CLIP, the first
vision-language model pre-trained with contrastive learning on a large-scale
multispectral dataset and report on the performance gains due to the extended
spectral range. Furthermore, we present the largest-to-date image-caption
dataset for multispectral data, consisting of one million Sentinel-2 samples
and corresponding textual descriptions generated with Llama3-LLaVA-Next and
Overture Maps data. We develop a scalable captioning pipeline, which is
validated by domain experts. We evaluate Llama3-MS-CLIP on multispectral
zero-shot image classification and retrieval using three datasets of varying
complexity. Our results demonstrate that Llama3-MS-CLIP significantly
outperforms other RGB-based approaches, improving classification accuracy by
6.77% on average and retrieval performance by 4.63% mAP compared to the
second-best model. Our results emphasize the relevance of multispectral
vision-language learning. We release the image-caption dataset, code, and model
weights under an open-source license.","['cs.CV', 'cs.AI']","['Clive Tinashe Marimo', 'Benedikt Blumenstiel', 'Maximilian Nitsche', 'Johannes Jakubik', 'Thomas Brunschwiler']",2025-03-20,2025-03-20
2503.16572v1,Efficient ANN-Guided Distillation: Aligning Rate-based Features of Spiking Neural Networks through Hybrid Block-wise Replacement,"Spiking Neural Networks (SNNs) have garnered considerable attention as a
potential alternative to Artificial Neural Networks (ANNs). Recent studies have
highlighted SNNs' potential on large-scale datasets. For SNN training, two main
approaches exist: direct training and ANN-to-SNN (ANN2SNN) conversion. To fully
leverage existing ANN models in guiding SNN learning, either direct ANN-to-SNN
conversion or ANN-SNN distillation training can be employed. In this paper, we
propose an ANN-SNN distillation framework from the ANN-to-SNN perspective,
designed with a block-wise replacement strategy for ANN-guided learning. By
generating intermediate hybrid models that progressively align SNN feature
spaces to those of ANN through rate-based features, our framework naturally
incorporates rate-based backpropagation as a training method. Our approach
achieves results comparable to or better than state-of-the-art SNN distillation
methods, showing both training and learning efficiency.","['cs.LG', 'cs.AI']","['Shu Yang', 'Chengting Yu', 'Lei Liu', 'Hanzhi Ma', 'Aili Wang', 'Erping Li']",2025-03-20,2025-03-20
2503.15962v1,Information maximization for a broad variety of multi-armed bandit games,"Information and free-energy maximization are physics principles that provide
general rules for an agent to optimize actions in line with specific goals and
policies. These principles are the building blocks for designing
decision-making policies capable of efficient performance with only partial
information. Notably, the information maximization principle has shown
remarkable success in the classical bandit problem and has recently been shown
to yield optimal algorithms for Gaussian and sub-Gaussian reward distributions.
This article explores a broad extension of physics-based approaches to more
complex and structured bandit problems. To this end, we cover three distinct
types of bandit problems, where information maximization is adapted and leads
to strong performance. Since the main challenge of information maximization
lies in avoiding over-exploration, we highlight how information is tailored at
various levels to mitigate this issue, paving the way for more efficient and
robust decision-making strategies.","['cs.LG', 'cond-mat.stat-mech', 'stat.ML']","['Alex Barbier-Chebbah', 'Christian L. Vestergaard', 'Jean-Baptiste Masson']",2025-03-20,2025-03-20
2503.15953v1,GAN-enhanced Simulation-driven DNN Testing in Absence of Ground Truth,"The generation of synthetic inputs via simulators driven by search algorithms
is essential for cost-effective testing of Deep Neural Network (DNN) components
for safety-critical systems. However, in many applications, simulators are
unable to produce the ground-truth data needed for automated test oracles and
to guide the search process.
  To tackle this issue, we propose an approach for the generation of inputs for
computer vision DNNs that integrates a generative network to ensure simulator
fidelity and employs heuristic-based search fitnesses that leverage
transformation consistency, noise resistance, surprise adequacy, and
uncertainty estimation. We compare the performance of our fitnesses with that
of a traditional fitness function leveraging ground truth; further, we assess
how the integration of a GAN not leveraging the ground truth impacts on test
and retraining effectiveness.
  Our results suggest that leveraging transformation consistency is the best
option to generate inputs for both DNN testing and retraining; it maximizes
input diversity, spots the inputs leading to worse DNN performance, and leads
to best DNN performance after retraining. Besides enabling simulator-based
testing in the absence of ground truth, our findings pave the way for testing
solutions that replace costly simulators with diffusion and large language
models, which might be more affordable than simulators, but cannot generate
ground-truth data.","['cs.SE', 'cs.AI']","['Mohammed Attaoui', 'Fabrizio Pastore']",2025-03-20,2025-03-20
2503.15952v1,Adaptive Group Policy Optimization: Towards Stable Training and Token-Efficient Reasoning,"Since DeepSeek-R1 popularized, Group Relative Policy Optimization (GRPO) has
become the core part of Reasoning LLMs training. However, we find some
deficiency that influences RL stability and inference efficiency. Thus, we
propose Adaptive Group Policy Optimization (AGPO) which contains two simple but
effective modifications: a revised advantage estimation method to mitigate
zero-variance situations; a length-based reward, incentivizing the model to
avoid overthinking. The experiments demonstrate our methods achieve more stable
training and comparable or superior performance with significantly fewer tokens
in reasoning steps.",['cs.CL'],"['Chen Li', 'Nazhou Liu', 'Kai Yang']",2025-03-20,2025-03-20
2503.15949v1,CausalCLIPSeg: Unlocking CLIP's Potential in Referring Medical Image Segmentation with Causal Intervention,"Referring medical image segmentation targets delineating lesions indicated by
textual descriptions. Aligning visual and textual cues is challenging due to
their distinct data properties. Inspired by large-scale pre-trained
vision-language models, we propose CausalCLIPSeg, an end-to-end framework for
referring medical image segmentation that leverages CLIP. Despite not being
trained on medical data, we enforce CLIP's rich semantic space onto the medical
domain by a tailored cross-modal decoding method to achieve text-to-pixel
alignment. Furthermore, to mitigate confounding bias that may cause the model
to learn spurious correlations instead of meaningful causal relationships,
CausalCLIPSeg introduces a causal intervention module which self-annotates
confounders and excavates causal features from inputs for segmentation
judgments. We also devise an adversarial min-max game to optimize causal
features while penalizing confounding ones. Extensive experiments demonstrate
the state-of-the-art performance of our proposed method. Code is available at
https://github.com/WUTCM-Lab/CausalCLIPSeg.",['cs.CV'],"['Yaxiong Chen', 'Minghong Wei', 'Zixuan Zheng', 'Jingliang Hu', 'Yilei Shi', 'Shengwu Xiong', 'Xiao Xiang Zhu', 'Lichao Mou']",2025-03-20,2025-03-20
2503.15948v1,"Don't Fight Hallucinations, Use Them: Estimating Image Realism using NLI over Atomic Facts","Quantifying the realism of images remains a challenging problem in the field
of artificial intelligence. For example, an image of Albert Einstein holding a
smartphone violates common-sense because modern smartphone were invented after
Einstein's death. We introduce a novel method for assessing image realism using
Large Vision-Language Models (LVLMs) and Natural Language Inference (NLI). Our
approach is based on the premise that LVLMs may generate hallucinations when
confronted with images that defy common sense. Using LVLM to extract atomic
facts from these images, we obtain a mix of accurate facts and erroneous
hallucinations. We proceed by calculating pairwise entailment scores among
these facts, subsequently aggregating these values to yield a singular reality
score. This process serves to identify contradictions between genuine facts and
hallucinatory elements, signaling the presence of images that violate common
sense. Our approach has achieved a new state-of-the-art performance in
zero-shot mode on the WHOOPS! dataset.","['cs.CV', 'cs.AI', 'cs.CL']","['Elisei Rykov', 'Kseniia Petrushina', 'Kseniia Titova', 'Alexander Panchenko', 'Vasily Konovalov']",2025-03-20,2025-03-20
2503.15947v1,Unreal-MAP: Unreal-Engine-Based General Platform for Multi-Agent Reinforcement Learning,"In this paper, we propose Unreal Multi-Agent Playground (Unreal-MAP), an MARL
general platform based on the Unreal-Engine (UE). Unreal-MAP allows users to
freely create multi-agent tasks using the vast visual and physical resources
available in the UE community, and deploy state-of-the-art (SOTA) MARL
algorithms within them. Unreal-MAP is user-friendly in terms of deployment,
modification, and visualization, and all its components are open-source. We
also develop an experimental framework compatible with algorithms ranging from
rule-based to learning-based provided by third-party frameworks. Lastly, we
deploy several SOTA algorithms in example tasks developed via Unreal-MAP, and
conduct corresponding experimental analyses. We believe Unreal-MAP can play an
important role in the MARL field by closely integrating existing algorithms
with user-customized tasks, thus advancing the field of MARL.",['cs.AI'],"['Tianyi Hu', 'Qingxu Fu', 'Zhiqiang Pu', 'Yuan Wang', 'Tenghai Qiu']",2025-03-20,2025-03-20
2503.15946v1,Multivariate Time Series Anomaly Detection in Industry 5.0,"Industry5.0 environments present a critical need for effective anomaly
detection methods that can indicate equipment malfunctions, process
inefficiencies, or potential safety hazards. The ever-increasing sensorization
of manufacturing lines makes processes more observable, but also poses the
challenge of continuously analyzing vast amounts of multivariate time series
data. These challenges include data quality since data may contain noise, be
unlabeled or even mislabeled. A promising approach consists of combining an
embedding model with other Machine Learning algorithms to enhance the overall
performance in detecting anomalies. Moreover, representing time series as
vectors brings many advantages like higher flexibility and improved ability to
capture complex temporal dependencies. We tested our solution in a real
industrial use case, using data collected from a Bonfiglioli plant. The results
demonstrate that, unlike traditional reconstruction-based autoencoders, which
often struggle in the presence of sporadic noise, our embedding-based framework
maintains high performance across various noise conditions.",['cs.LG'],"['Lorenzo Colombi', 'Michela Vespa', 'Nicolas Belletti', 'Matteo Brina', 'Simon Dahdal', 'Filippo Tabanelli', 'Elena Bellodi', 'Mauro Tortonesi', 'Cesare Stefanelli', 'Massimiliano Vignoli']",2025-03-20,2025-03-20
2503.15944v1,From Chaos to Order: The Atomic Reasoner Framework for Fine-grained Reasoning in Large Language Models,"Recent advances in large language models (LLMs) have shown remarkable
progress, yet their capacity for logical ``slow-thinking'' reasoning persists
as a critical research frontier. Current inference scaling paradigms suffer
from two fundamental constraints: fragmented thought flows compromising logical
coherence, and intensively computational complexity that escalates with search
space dimensions. To overcome these limitations, we present \textbf{Atomic
Reasoner} (\textbf{AR}), a cognitive inference strategy that enables
fine-grained reasoning through systematic atomic-level operations. AR
decomposes the reasoning process into atomic cognitive units, employing a
cognitive routing mechanism to dynamically construct reasoning representations
and orchestrate inference pathways. This systematic methodology implements
stepwise, structured cognition, which ensures logical coherence while
significantly reducing cognitive load, effectively simulating the cognitive
patterns observed in human deep thinking processes. Extensive experimental
results demonstrate AR's superior reasoning capabilities without the
computational burden of exhaustive solution searches, particularly excelling in
linguistic logic puzzles. These findings substantiate AR's effectiveness in
enhancing LLMs' capacity for robust, long-sequence logical reasoning and
deliberation.",['cs.CL'],"['Jinyi Liu', 'Yan Zheng', 'Rong Cheng', 'Qiyu Wu', 'Wei Guo', 'Fei Ni', 'Hebin Liang', 'Yifu Yuan', 'Hangyu Mao', 'Fuzheng Zhang', 'Jianye Hao']",2025-03-20,2025-03-20
2503.15940v1,UniCrossAdapter: Multimodal Adaptation of CLIP for Radiology Report Generation,"Automated radiology report generation aims to expedite the tedious and
error-prone reporting process for radiologists. While recent works have made
progress, learning to align medical images and textual findings remains
challenging due to the relative scarcity of labeled medical data. For example,
datasets for this task are much smaller than those used for image captioning in
computer vision. In this work, we propose to transfer representations from
CLIP, a large-scale pre-trained vision-language model, to better capture
cross-modal semantics between images and texts. However, directly applying CLIP
is suboptimal due to the domain gap between natural images and radiology. To
enable efficient adaptation, we introduce UniCrossAdapter, lightweight adapter
modules that are incorporated into CLIP and fine-tuned on the target task while
keeping base parameters fixed. The adapters are distributed across modalities
and their interaction to enhance vision-language alignment. Experiments on two
public datasets demonstrate the effectiveness of our approach, advancing
state-of-the-art in radiology report generation. The proposed transfer learning
framework provides a means of harnessing semantic knowledge from large-scale
pre-trained models to tackle data-scarce medical vision-language tasks. Code is
available at https://github.com/chauncey-tow/MRG-CLIP.",['cs.CV'],"['Yaxiong Chen', 'Chuang Du', 'Chunlei Li', 'Jingliang Hu', 'Yilei Shi', 'Shengwu Xiong', 'Xiao Xiang Zhu', 'Lichao Mou']",2025-03-20,2025-03-20
2503.15937v2,Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical Deployment,"We propose V-Droid, a mobile GUI task automation agent. Unlike previous
mobile agents that utilize Large Language Models (LLMs) as generators to
directly generate actions at each step, V-Droid employs LLMs as verifiers to
evaluate candidate actions before making final decisions. To realize this novel
paradigm, we introduce a comprehensive framework for constructing
verifier-driven mobile agents: the discretized action space construction
coupled with the prefilling-only workflow to accelerate the verification
process, the pair-wise progress preference training to significantly enhance
the verifier's decision-making capabilities, and the scalable human-agent joint
annotation scheme to efficiently collect the necessary data at scale. V-Droid
sets a new state-of-the-art task success rate across several public mobile task
automation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49% on
MobileAgentBench, surpassing existing agents by 9.5%, 2.1%, and 9%,
respectively. Furthermore, V-Droid achieves an impressively low latency of 0.7
seconds per step, making it the first mobile agent capable of delivering
near-real-time, effective decision-making capabilities.",['cs.AI'],"['Gaole Dai', 'Shiqi Jiang', 'Ting Cao', 'Yuanchun Li', 'Yuqing Yang', 'Rui Tan', 'Mo Li', 'Lili Qiu']",2025-03-20,2025-03-21
2503.15934v1,SaMam: Style-aware State Space Model for Arbitrary Image Style Transfer,"Global effective receptive field plays a crucial role for image style
transfer (ST) to obtain high-quality stylized results. However, existing ST
backbones (e.g., CNNs and Transformers) suffer huge computational complexity to
achieve global receptive fields. Recently, the State Space Model (SSM),
especially the improved variant Mamba, has shown great potential for long-range
dependency modeling with linear complexity, which offers a approach to resolve
the above dilemma. In this paper, we develop a Mamba-based style transfer
framework, termed SaMam. Specifically, a mamba encoder is designed to
efficiently extract content and style information. In addition, a style-aware
mamba decoder is developed to flexibly adapt to various styles. Moreover, to
address the problems of local pixel forgetting, channel redundancy and spatial
discontinuity of existing SSMs, we introduce both local enhancement and zigzag
scan. Qualitative and quantitative results demonstrate that our SaMam
outperforms state-of-the-art methods in terms of both accuracy and efficiency.",['cs.CV'],"['Hongda Liu', 'Longguang Wang', 'Ye Zhang', 'Ziru Yu', 'Yulan Guo']",2025-03-20,2025-03-20
2503.15931v1,DnLUT: Ultra-Efficient Color Image Denoising via Channel-Aware Lookup Tables,"While deep neural networks have revolutionized image denoising capabilities,
their deployment on edge devices remains challenging due to substantial
computational and memory requirements. To this end, we present DnLUT, an
ultra-efficient lookup table-based framework that achieves high-quality color
image denoising with minimal resource consumption. Our key innovation lies in
two complementary components: a Pairwise Channel Mixer (PCM) that effectively
captures inter-channel correlations and spatial dependencies in parallel, and a
novel L-shaped convolution design that maximizes receptive field coverage while
minimizing storage overhead. By converting these components into optimized
lookup tables post-training, DnLUT achieves remarkable efficiency - requiring
only 500KB storage and 0.1% energy consumption compared to its CNN contestant
DnCNN, while delivering 20X faster inference. Extensive experiments demonstrate
that DnLUT outperforms all existing LUT-based methods by over 1dB in PSNR,
establishing a new state-of-the-art in resource-efficient color image
denoising. The project is available at https://github.com/Stephen0808/DnLUT.",['cs.CV'],"['Sidi Yang', 'Binxiao Huang', 'Yulun Zhang', 'Dahai Yu', 'Yujiu Yang', 'Ngai Wong']",2025-03-20,2025-03-20
2503.15928v2,Sample-Efficient Bayesian Transfer Learning for Online Machine Parameter Optimization,"Correctly setting the parameters of a production machine is essential to
improve product quality, increase efficiency, and reduce production costs while
also supporting sustainability goals. Identifying optimal parameters involves
an iterative process of producing an object and evaluating its quality.
Minimizing the number of iterations is, therefore, desirable to reduce the
costs associated with unsuccessful attempts. This work introduces a method to
optimize the machine parameters in the system itself using a Bayesian
optimization algorithm. By leveraging existing machine data, we use a transfer
learning approach in order to identify an optimum with minimal iterations,
resulting in a cost-effective transfer learning algorithm. We validate our
approach on a laser machine for cutting sheet metal in the real world.",['cs.LG'],"['Philipp Wagner', 'Tobias Nagel', 'Philipp Leube', 'Marco F. Huber']",2025-03-20,2025-03-21
2503.15927v1,BlockDance: Reuse Structurally Similar Spatio-Temporal Features to Accelerate Diffusion Transformers,"Diffusion models have demonstrated impressive generation capabilities,
particularly with recent advancements leveraging transformer architectures to
improve both visual and artistic quality. However, Diffusion Transformers
(DiTs) continue to encounter challenges related to low inference speed,
primarily due to the iterative denoising process. To address this issue, we
propose BlockDance, a training-free approach that explores feature similarities
at adjacent time steps to accelerate DiTs. Unlike previous feature-reuse
methods that lack tailored reuse strategies for features at different scales,
BlockDance prioritizes the identification of the most structurally similar
features, referred to as Structurally Similar Spatio-Temporal (STSS) features.
These features are primarily located within the structure-focused blocks of the
transformer during the later stages of denoising. BlockDance caches and reuses
these highly similar features to mitigate redundant computation, thereby
accelerating DiTs while maximizing consistency with the generated results of
the original model. Furthermore, considering the diversity of generated content
and the varying distributions of redundant features, we introduce
BlockDance-Ada, a lightweight decision-making network tailored for
instance-specific acceleration. BlockDance-Ada dynamically allocates resources
and provides superior content quality. Both BlockDance and BlockDance-Ada have
proven effective across various generation tasks and models, achieving
accelerations between 25% and 50% while maintaining generation quality.",['cs.CV'],"['Hui Zhang', 'Tingwei Gao', 'Jie Shao', 'Zuxuan Wu']",2025-03-20,2025-03-20
2503.16567v1,Exploring Deep Learning Models for EEG Neural Decoding,"Neural decoding is an important method in cognitive neuroscience that aims to
decode brain representations from recorded neural activity using a multivariate
machine learning model. The THINGS initiative provides a large EEG dataset of
46 subjects watching rapidly shown images. Here, we test the feasibility of
using this method for decoding high-level object features using recent deep
learning models. We create a derivative dataset from this of living vs
non-living entities test 15 different deep learning models with 5 different
architectures and compare to a SOTA linear model. We show that the linear model
is not able to solve the decoding task, while almost all the deep learning
models are successful, suggesting that in some cases non-linear models are
needed to decode neural representations. We also run a comparative study of the
models' performance on individual object categories, and suggest how artificial
neural networks can be used to study brain activity.",['cs.LG'],"['Laurits Dixen', 'Stefan Heinrich', 'Paolo Burelli']",2025-03-20,2025-03-20
2503.15924v1,Towards Automatic Continual Learning: A Self-Adaptive Framework for Continual Instruction Tuning,"Continual instruction tuning enables large language models (LLMs) to learn
incrementally while retaining past knowledge, whereas existing methods
primarily focus on how to retain old knowledge rather than on selecting which
new knowledge to learn. In domain-specific contexts, maintaining data quality
and managing system constraints remain key challenges. To address these issues,
we propose an automated continual instruction tuning framework that dynamically
filters incoming data, which identify and reduce redundant data across
successive updates. Our approach utilizes a small proxy model for efficient
perplexity-based filtering, and updates the proxy to ensure that the filtering
criteria remain aligned with the evolving state of the deployed model. Compared
to existing static data selection methods, our framework can effectively handle
incrementally acquired data and shifting distributions. Additionally, it
addresses practical deployment challenges by enabling seamless model updates,
supporting version rollback and incorporating automatic checkpoint evaluation.
We evaluated the system in real-world medical scenarios. It reduced
computational costs by 66.7% and improved model performance, and achieved
autonomous updates, thus demonstrating its effectiveness for automatic
continual instruction tuning.","['cs.CL', 'cs.AI']","['Peiyi Lin', 'Fukai Zhang', 'Kai Niu', 'Hao Fu']",2025-03-20,2025-03-20
2503.16566v1,REVAL: A Comprehension Evaluation on Reliability and Values of Large Vision-Language Models,"The rapid evolution of Large Vision-Language Models (LVLMs) has highlighted
the necessity for comprehensive evaluation frameworks that assess these models
across diverse dimensions. While existing benchmarks focus on specific aspects
such as perceptual abilities, cognitive capabilities, and safety against
adversarial attacks, they often lack the breadth and depth required to provide
a holistic understanding of LVLMs' strengths and limitations. To address this
gap, we introduce REVAL, a comprehensive benchmark designed to evaluate the
\textbf{RE}liability and \textbf{VAL}ue of LVLMs. REVAL encompasses over 144K
image-text Visual Question Answering (VQA) samples, structured into two primary
sections: Reliability, which assesses truthfulness (\eg, perceptual accuracy
and hallucination tendencies) and robustness (\eg, resilience to adversarial
attacks, typographic attacks, and image corruption), and Values, which
evaluates ethical concerns (\eg, bias and moral understanding), safety issues
(\eg, toxicity and jailbreak vulnerabilities), and privacy problems (\eg,
privacy awareness and privacy leakage). We evaluate 26 models, including
mainstream open-source LVLMs and prominent closed-source models like GPT-4o and
Gemini-1.5-Pro. Our findings reveal that while current LVLMs excel in
perceptual tasks and toxicity avoidance, they exhibit significant
vulnerabilities in adversarial scenarios, privacy preservation, and ethical
reasoning. These insights underscore critical areas for future improvements,
guiding the development of more secure, reliable, and ethically aligned LVLMs.
REVAL provides a robust framework for researchers to systematically assess and
compare LVLMs, fostering advancements in the field.",['cs.CV'],"['Jie Zhang', 'Zheng Yuan', 'Zhongqi Wang', 'Bei Yan', 'Sibo Wang', 'Xiangkui Cao', 'Zonghui Guo', 'Shiguang Shan', 'Xilin Chen']",2025-03-20,2025-03-20
2503.15918v1,Denoising-based Contractive Imitation Learning,"A fundamental challenge in imitation learning is the \emph{covariate shift}
problem. Existing methods to mitigate covariate shift often require additional
expert interactions, access to environment dynamics, or complex adversarial
training, which may not be practical in real-world applications. In this paper,
we propose a simple yet effective method (DeCIL) to mitigate covariate shift by
incorporating a denoising mechanism that enhances the contraction properties of
the state transition mapping. Our approach involves training two neural
networks: a dynamics model ( f ) that predicts the next state from the current
state, and a joint state-action denoising policy network ( d ) that refines
this state prediction via denoising and outputs the corresponding action. We
provide theoretical analysis showing that the denoising network acts as a local
contraction mapping, reducing the error propagation of the state transition and
improving stability. Our method is straightforward to implement and can be
easily integrated with existing imitation learning frameworks without requiring
additional expert data or complex modifications to the training procedure.
Empirical results demonstrate that our approach effectively improves success
rate of various imitation learning tasks under noise perturbation.","['cs.LG', 'cs.AI']","['Macheng Shen', 'Jishen Peng', 'Zefang Huang']",2025-03-20,2025-03-20
2503.15917v1,Learning to Efficiently Adapt Foundation Models for Self-Supervised Endoscopic 3D Scene Reconstruction from Any Cameras,"Accurate 3D scene reconstruction is essential for numerous medical tasks.
Given the challenges in obtaining ground truth data, there has been an
increasing focus on self-supervised learning (SSL) for endoscopic depth
estimation as a basis for scene reconstruction. While foundation models have
shown remarkable progress in visual tasks, their direct application to the
medical domain often leads to suboptimal results. However, the visual features
from these models can still enhance endoscopic tasks, emphasizing the need for
efficient adaptation strategies, which still lack exploration currently. In
this paper, we introduce Endo3DAC, a unified framework for endoscopic scene
reconstruction that efficiently adapts foundation models. We design an
integrated network capable of simultaneously estimating depth maps, relative
poses, and camera intrinsic parameters. By freezing the backbone foundation
model and training only the specially designed Gated Dynamic Vector-Based
Low-Rank Adaptation (GDV-LoRA) with separate decoder heads, Endo3DAC achieves
superior depth and pose estimation while maintaining training efficiency.
Additionally, we propose a 3D scene reconstruction pipeline that optimizes
depth maps' scales, shifts, and a few parameters based on our integrated
network. Extensive experiments across four endoscopic datasets demonstrate that
Endo3DAC significantly outperforms other state-of-the-art methods while
requiring fewer trainable parameters. To our knowledge, we are the first to
utilize a single network that only requires surgical videos to perform both SSL
depth estimation and scene reconstruction tasks. The code will be released upon
acceptance.",['cs.CV'],"['Beilei Cui', 'Long Bai', 'Mobarakol Islam', 'An Wang', 'Zhiqi Ma', 'Yiming Huang', 'Feng Li', 'Zhen Chen', 'Zhongliang Jiang', 'Nassir Navab', 'Hongliang Ren']",2025-03-20,2025-03-20
2503.15914v1,Text-Driven Diffusion Model for Sign Language Production,"We introduce the hfut-lmc team's solution to the SLRTP Sign Production
Challenge. The challenge aims to generate semantically aligned sign language
pose sequences from text inputs. To this end, we propose a Text-driven
Diffusion Model (TDM) framework. During the training phase, TDM utilizes an
encoder to encode text sequences and incorporates them into the diffusion model
as conditional input to generate sign pose sequences. To guarantee the high
quality and accuracy of the generated pose sequences, we utilize two key loss
functions. The joint loss function L_{joint} is used to precisely measure and
minimize the differences between the joint positions of the generated pose
sequences and those of the ground truth. Similarly, the bone orientation loss
function L_{bone} is instrumental in ensuring that the orientation of the bones
in the generated poses aligns with the actual, correct orientations. In the
inference stage, the TDM framework takes on a different yet equally important
task. It starts with noisy sequences and, under the strict constraints of the
text conditions, gradually refines and generates semantically consistent sign
language pose sequences. Our carefully designed framework performs well on the
sign language production task, and our solution achieves a BLEU-1 score of
20.17, placing second in the challenge.",['cs.CV'],"['Jiayi He', 'Xu Wang', 'Ruobei Zhang', 'Shengeng Tang', 'Yaxiong Wang', 'Lechao Cheng']",2025-03-20,2025-03-20
2503.15910v1,"No Thing, Nothing: Highlighting Safety-Critical Classes for Robust LiDAR Semantic Segmentation in Adverse Weather","Existing domain generalization methods for LiDAR semantic segmentation under
adverse weather struggle to accurately predict ""things"" categories compared to
""stuff"" categories. In typical driving scenes, ""things"" categories can be
dynamic and associated with higher collision risks, making them crucial for
safe navigation and planning. Recognizing the importance of ""things""
categories, we identify their performance drop as a serious bottleneck in
existing approaches. We observed that adverse weather induces degradation of
semantic-level features and both corruption of local features, leading to a
misprediction of ""things"" as ""stuff"". To mitigate these corruptions, we suggest
our method, NTN - segmeNt Things for No-accident. To address semantic-level
feature corruption, we bind each point feature to its superclass, preventing
the misprediction of things classes into visually dissimilar categories.
Additionally, to enhance robustness against local corruption caused by adverse
weather, we define each LiDAR beam as a local region and propose a
regularization term that aligns the clean data with its corrupted counterpart
in feature space. NTN achieves state-of-the-art performance with a +2.6 mIoU
gain on the SemanticKITTI-to-SemanticSTF benchmark and +7.9 mIoU on the
SemanticPOSS-to-SemanticSTF benchmark. Notably, NTN achieves a +4.8 and +7.9
mIoU improvement on ""things"" classes, respectively, highlighting its
effectiveness.","['cs.CV', 'cs.AI']","['Junsung Park', 'Hwijeong Lee', 'Inha Kang', 'Hyunjung Shim']",2025-03-20,2025-03-20
2503.15908v1,Enhancing Close-up Novel View Synthesis via Pseudo-labeling,"Recent methods, such as Neural Radiance Fields (NeRF) and 3D Gaussian
Splatting (3DGS), have demonstrated remarkable capabilities in novel view
synthesis. However, despite their success in producing high-quality images for
viewpoints similar to those seen during training, they struggle when generating
detailed images from viewpoints that significantly deviate from the training
set, particularly in close-up views. The primary challenge stems from the lack
of specific training data for close-up views, leading to the inability of
current methods to render these views accurately. To address this issue, we
introduce a novel pseudo-label-based learning strategy. This approach leverages
pseudo-labels derived from existing training data to provide targeted
supervision across a wide range of close-up viewpoints. Recognizing the absence
of benchmarks for this specific challenge, we also present a new dataset
designed to assess the effectiveness of both current and future methods in this
area. Our extensive experiments demonstrate the efficacy of our approach.","['cs.CV', 'cs.AI']","['Jiatong Xia', 'Libo Sun', 'Lingqiao Liu']",2025-03-20,2025-03-20
2503.15905v1,Jasmine: Harnessing Diffusion Prior for Self-supervised Depth Estimation,"In this paper, we propose Jasmine, the first Stable Diffusion (SD)-based
self-supervised framework for monocular depth estimation, which effectively
harnesses SD's visual priors to enhance the sharpness and generalization of
unsupervised prediction. Previous SD-based methods are all supervised since
adapting diffusion models for dense prediction requires high-precision
supervision. In contrast, self-supervised reprojection suffers from inherent
challenges (e.g., occlusions, texture-less regions, illumination variance), and
the predictions exhibit blurs and artifacts that severely compromise SD's
latent priors. To resolve this, we construct a novel surrogate task of hybrid
image reconstruction. Without any additional supervision, it preserves the
detail priors of SD models by reconstructing the images themselves while
preventing depth estimation from degradation. Furthermore, to address the
inherent misalignment between SD's scale and shift invariant estimation and
self-supervised scale-invariant depth estimation, we build the Scale-Shift GRU.
It not only bridges this distribution gap but also isolates the fine-grained
texture of SD output against the interference of reprojection loss. Extensive
experiments demonstrate that Jasmine achieves SoTA performance on the KITTI
benchmark and exhibits superior zero-shot generalization across multiple
datasets.","['cs.CV', 'cs.AI']","['Jiyuan Wang', 'Chunyu Lin', 'Cheng Guan', 'Lang Nie', 'Jing He', 'Haodong Li', 'Kang Liao', 'Yao Zhao']",2025-03-20,2025-03-20
2503.15904v1,From Structured Prompts to Open Narratives: Measuring Gender Bias in LLMs Through Open-Ended Storytelling,"Large Language Models (LLMs) have revolutionized natural language processing,
yet concerns persist regarding their tendency to reflect or amplify social
biases present in their training data. This study introduces a novel evaluation
framework to uncover gender biases in LLMs, focusing on their occupational
narratives. Unlike previous methods relying on structured scenarios or
carefully crafted prompts, our approach leverages free-form storytelling to
reveal biases embedded in the models. Systematic analyses show an
overrepresentation of female characters across occupations in six widely used
LLMs. Additionally, our findings reveal that LLM-generated occupational gender
rankings align more closely with human stereotypes than actual labor
statistics. These insights underscore the need for balanced mitigation
strategies to ensure fairness while avoiding the reinforcement of new
stereotypes.","['cs.CL', 'cs.AI']","['Evan Chen', 'Run-Jun Zhan', 'Yan-Bai Lin', 'Hung-Hsuan Chen']",2025-03-20,2025-03-20
2503.16565v1,Gene42: Long-Range Genomic Foundation Model With Dense Attention,"We introduce Gene42, a novel family of Genomic Foundation Models (GFMs)
designed to manage context lengths of up to 192,000 base pairs (bp) at a
single-nucleotide resolution. Gene42 models utilize a decoder-only
(LLaMA-style) architecture with a dense self-attention mechanism. Initially
trained on fixed-length sequences of 4,096 bp, our models underwent continuous
pretraining to extend the context length to 192,000 bp. This iterative
extension allowed for the comprehensive processing of large-scale genomic data
and the capture of intricate patterns and dependencies within the human genome.
Gene42 is the first dense attention model capable of handling such extensive
long context lengths in genomics, challenging state-space models that often
rely on convolutional operators among other mechanisms. Our pretrained models
exhibit notably low perplexity values and high reconstruction accuracy,
highlighting their strong ability to model genomic data. Extensive experiments
on various genomic benchmarks have demonstrated state-of-the-art performance
across multiple tasks, including biotype classification, regulatory region
identification, chromatin profiling prediction, variant pathogenicity
prediction, and species classification. The models are publicly available at
huggingface.co/inceptionai.","['cs.LG', 'cs.AI', 'cs.CL']","['Kirill Vishniakov', 'Boulbaba Ben Amor', 'Engin Tekin', 'Nancy A. ElNaker', 'Karthik Viswanathan', 'Aleksandr Medvedev', 'Aahan Singh', 'Maryam Nadeem', 'Mohammad Amaan Sayeed', 'Praveenkumar Kanithi', 'Tiago Magalhaes', 'Natalia Vassilieva', 'Dwarikanath Mahapatra', 'Marco Pimentel', 'and Shadab Khan']",2025-03-20,2025-03-20
2503.16563v1,Chem42: a Family of chemical Language Models for Target-aware Ligand Generation,"Revolutionizing drug discovery demands more than just understanding molecular
interactions - it requires generative models that can design novel ligands
tailored to specific biological targets. While chemical Language Models (cLMs)
have made strides in learning molecular properties, most fail to incorporate
target-specific insights, restricting their ability to drive de-novo ligand
generation. Chem42, a cutting-edge family of generative chemical Language
Models, is designed to bridge this gap. By integrating atomic-level
interactions with multimodal inputs from Prot42, a complementary protein
Language Model, Chem42 achieves a sophisticated cross-modal representation of
molecular structures, interactions, and binding patterns. This innovative
framework enables the creation of structurally valid, synthetically accessible
ligands with enhanced target specificity. Evaluations across diverse protein
targets confirm that Chem42 surpasses existing approaches in chemical validity,
target-aware design, and predicted binding affinity. By reducing the search
space of viable drug candidates, Chem42 could accelerate the drug discovery
pipeline, offering a powerful generative AI tool for precision medicine. Our
Chem42 models set a new benchmark in molecule property prediction, conditional
molecule generation, and target-aware ligand design. The models are publicly
available at huggingface.co/inceptionai.","['cs.LG', 'cs.AI', 'cs.CL']","['Aahan Singh', 'Engin Tekin', 'Maryam Nadeem', 'Nancy A. ElNaker', 'Mohammad Amaan Sayeed', 'Natalia Vassilieva', 'Boulbaba Ben Amor']",2025-03-20,2025-03-20
2503.15902v1,On the Limits of Applying Graph Transformers for Brain Connectome Classification,"Brain connectomes offer detailed maps of neural connections within the brain.
Recent studies have proposed novel connectome graph datasets and attempted to
improve connectome classification by using graph deep learning. With recent
advances demonstrating transformers' ability to model intricate relationships
and outperform in various domains, this work explores their performance on the
novel NeuroGraph benchmark datasets and synthetic variants derived from
probabilistically removing edges to simulate noisy data. Our findings suggest
that graph transformers offer no major advantage over traditional GNNs on this
dataset. Furthermore, both traditional and transformer GNN models maintain
accuracy even with all edges removed, suggesting that the dataset's graph
structures may not significantly impact predictions. We propose further
assessing NeuroGraph as a brain connectome benchmark, emphasizing the need for
well-curated datasets and improved preprocessing strategies to obtain
meaningful edge connections.",['cs.LG'],"['Jose Lara-Rangel', 'Clare Heinbaugh']",2025-03-20,2025-03-20
2503.15901v1,A multi-model approach using XAI and anomaly detection to predict asteroid hazards,"The potential for catastrophic collision makes near-Earth asteroids (NEAs) a
serious concern. Planetary defense depends on accurately classifying
potentially hazardous asteroids (PHAs), however the complexity of the data
hampers conventional techniques. This work offers a sophisticated method for
accurately predicting hazards by combining machine learning, deep learning,
explainable AI (XAI), and anomaly detection. Our approach extracts essential
parameters like size, velocity, and trajectory from historical and real-time
asteroid data. A hybrid algorithm improves prediction accuracy by combining
several cutting-edge models. A forecasting module predicts future asteroid
behavior, and Monte Carlo simulations evaluate the likelihood of collisions.
Timely mitigation is made possible by a real-time alarm system that notifies
worldwide monitoring stations. This technique enhances planetary defense
efforts by combining real-time alarms with sophisticated predictive modeling.","['astro-ph.EP', 'astro-ph.IM', 'cs.AI', 'cs.LG']","['Amit Kumar Mondal', 'Nafisha Aslam', 'Prasenjit Maji', 'Hemanta Kumar Mondal']",2025-03-20,2025-03-20
2503.15898v1,Reconstructing In-the-Wild Open-Vocabulary Human-Object Interactions,"Reconstructing human-object interactions (HOI) from single images is
fundamental in computer vision. Existing methods are primarily trained and
tested on indoor scenes due to the lack of 3D data, particularly constrained by
the object variety, making it challenging to generalize to real-world scenes
with a wide range of objects. The limitations of previous 3D HOI datasets were
primarily due to the difficulty in acquiring 3D object assets. However, with
the development of 3D reconstruction from single images, recently it has become
possible to reconstruct various objects from 2D HOI images. We therefore
propose a pipeline for annotating fine-grained 3D humans, objects, and their
interactions from single images. We annotated 2.5k+ 3D HOI assets from existing
2D HOI datasets and built the first open-vocabulary in-the-wild 3D HOI dataset
Open3DHOI, to serve as a future test set. Moreover, we design a novel
Gaussian-HOI optimizer, which efficiently reconstructs the spatial interactions
between humans and objects while learning the contact regions. Besides the 3D
HOI reconstruction, we also propose several new tasks for 3D HOI understanding
to pave the way for future work. Data and code will be publicly available at
https://wenboran2002.github.io/3dhoi.",['cs.CV'],"['Boran Wen', 'Dingbang Huang', 'Zichen Zhang', 'Jiahong Zhou', 'Jianbin Deng', 'Jingyu Gong', 'Yulong Chen', 'Lizhuang Ma', 'Yong-Lu Li']",2025-03-20,2025-03-20
2503.15897v1,Learning 3D Scene Analogies with Neural Contextual Scene Maps,"Understanding scene contexts is crucial for machines to perform tasks and
adapt prior knowledge in unseen or noisy 3D environments. As data-driven
learning is intractable to comprehensively encapsulate diverse ranges of
layouts and open spaces, we propose teaching machines to identify relational
commonalities in 3D spaces. Instead of focusing on point-wise or object-wise
representations, we introduce 3D scene analogies, which are smooth maps between
3D scene regions that align spatial relationships. Unlike well-studied single
instance-level maps, these scene-level maps smoothly link large scene regions,
potentially enabling unique applications in trajectory transfer in AR/VR, long
demonstration transfer for imitation learning, and context-aware object
rearrangement. To find 3D scene analogies, we propose neural contextual scene
maps, which extract descriptor fields summarizing semantic and geometric
contexts, and holistically align them in a coarse-to-fine manner for map
estimation. This approach reduces reliance on individual feature points, making
it robust to input noise or shape variations. Experiments demonstrate the
effectiveness of our approach in identifying scene analogies and transferring
trajectories or object placements in diverse indoor scenes, indicating its
potential for robotics and AR/VR applications.","['cs.CV', 'cs.LG']","['Junho Kim', 'Gwangtak Bae', 'Eun Sun Lee', 'Young Min Kim']",2025-03-20,2025-03-20
2503.16562v1,Bezier Distillation,"In Rectified Flow, by obtaining the rectified flow several times, the mapping
relationship between distributions can be distilled into a neural network, and
the target distribution can be directly predicted by the straight lines of the
flow. However, during the pairing process of the mapping relationship, a large
amount of error accumulation will occur, resulting in a decrease in performance
after multiple rectifications. In the field of flow models, knowledge
distillation of multi - teacher diffusion models is also a problem worthy of
discussion in accelerating sampling. I intend to combine multi - teacher
knowledge distillation with Bezier curves to solve the problem of error
accumulation. Currently, the related paper is being written by myself.",['cs.LG'],"['Ling Feng', 'SK Yang']",2025-03-20,2025-03-20
2503.15893v1,UniHDSA: A Unified Relation Prediction Approach for Hierarchical Document Structure Analysis,"Document structure analysis, aka document layout analysis, is crucial for
understanding both the physical layout and logical structure of documents,
serving information retrieval, document summarization, knowledge extraction,
etc. Hierarchical Document Structure Analysis (HDSA) specifically aims to
restore the hierarchical structure of documents created using authoring
software with hierarchical schemas. Previous research has primarily followed
two approaches: one focuses on tackling specific subtasks of HDSA in isolation,
such as table detection or reading order prediction, while the other adopts a
unified framework that uses multiple branches or modules, each designed to
address a distinct task. In this work, we propose a unified relation prediction
approach for HDSA, called UniHDSA, which treats various HDSA sub-tasks as
relation prediction problems and consolidates relation prediction labels into a
unified label space. This allows a single relation prediction module to handle
multiple tasks simultaneously, whether at a page-level or document-level
structure analysis. To validate the effectiveness of UniHDSA, we develop a
multimodal end-to-end system based on Transformer architectures. Extensive
experimental results demonstrate that our approach achieves state-of-the-art
performance on a hierarchical document structure analysis benchmark,
Comp-HRDoc, and competitive results on a large-scale document layout analysis
dataset, DocLayNet, effectively illustrating the superiority of our method
across all sub-tasks.",['cs.CV'],"['Jiawei Wang', 'Kai Hu', 'Qiang Huo']",2025-03-20,2025-03-20
2503.15892v1,UMIT: Unifying Medical Imaging Tasks via Vision-Language Models,"With the rapid advancement of deep learning, particularly in the field of
medical image analysis, an increasing number of Vision-Language Models (VLMs)
are being widely applied to solve complex health and biomedical challenges.
However, existing research has primarily focused on specific tasks or single
modalities, which limits their applicability and generalization across diverse
medical scenarios. To address this challenge, we propose UMIT, a unified
multi-modal, multi-task VLM designed specifically for medical imaging tasks.
UMIT is able to solve various tasks, including visual question answering,
disease detection, and medical report generation. In addition, it is applicable
to multiple imaging modalities (e.g., X-ray, CT and PET), covering a wide range
of applications from basic diagnostics to complex lesion analysis. Moreover,
UMIT supports both English and Chinese, expanding its applicability globally
and ensuring accessibility to healthcare services in different linguistic
contexts. To enhance the model's adaptability and task-handling capability, we
design a unique two-stage training strategy and fine-tune UMIT with designed
instruction templates. Through extensive empirical evaluation, UMIT outperforms
previous methods in five tasks across multiple datasets. The performance of
UMIT indicates that it can significantly enhance diagnostic accuracy and
workflow efficiency, thus providing effective solutions for medical imaging
applications.",['cs.CV'],"['Haiyang Yu', 'Siyang Yi', 'Ke Niu', 'Minghan Zhuo', 'Bin Li']",2025-03-20,2025-03-20
2503.15890v1,Time After Time: Deep-Q Effect Estimation for Interventions on When and What to do,"Problems in fields such as healthcare, robotics, and finance requires
reasoning about the value both of what decision or action to take and when to
take it. The prevailing hope is that artificial intelligence will support such
decisions by estimating the causal effect of policies such as how to treat
patients or how to allocate resources over time. However, existing methods for
estimating the effect of a policy struggle with \emph{irregular time}. They
either discretize time, or disregard the effect of timing policies. We present
a new deep-Q algorithm that estimates the effect of both when and what to do
called Earliest Disagreement Q-Evaluation (EDQ). EDQ makes use of recursion for
the Q-function that is compatible with flexible sequence models, such as
transformers. EDQ provides accurate estimates under standard assumptions. We
validate the approach through experiments on survival time and tumor growth
tasks.","['cs.LG', 'cs.AI']","['Yoav Wald', 'Mark Goldstein', 'Yonathan Efroni', 'Wouter A. C. van Amsterdam', 'Rajesh Ranganath']",2025-03-20,2025-03-20
2503.15889v1,LeanTTA: A Backpropagation-Free and Stateless Approach to Quantized Test-Time Adaptation on Edge Devices,"While there are many advantages to deploying machine learning models on edge
devices, the resource constraints of mobile platforms, the dynamic nature of
the environment, and differences between the distribution of training versus
in-the-wild data make such deployments challenging. Current test-time
adaptation methods are often memory-intensive and not designed to be
quantization-compatible or deployed on low-resource devices. To address these
challenges, we present LeanTTA, a novel backpropagation-free and stateless
framework for quantized test-time adaptation tailored to edge devices. Our
approach minimizes computational costs by dynamically updating normalization
statistics without backpropagation, which frees LeanTTA from the common pitfall
of relying on large batches and historical data, making our method robust to
realistic deployment scenarios. Our approach is the first to enable further
computational gains by combining partial adaptation with quantized module
fusion. We validate our framework across sensor modalities, demonstrating
significant improvements over state-of-the-art TTA methods, including a 15.7%
error reduction, peak memory usage of only 11.2MB for ResNet18, and fast
adaptation within an order-of-magnitude of normal inference speeds on-device.
LeanTTA provides a robust solution for achieving the right trade offs between
accuracy and system efficiency in edge deployments, addressing the unique
challenges posed by limited data and varied operational conditions.","['cs.LG', 'cs.AI']","['Cynthia Dong', 'Hong Jia', 'Young D. Kwon', 'Georgios Rizos', 'Cecilia Mascolo']",2025-03-20,2025-03-20
2503.15888v1,Parameters vs. Context: Fine-Grained Control of Knowledge Reliance in Language Models,"Retrieval-Augmented Generation (RAG) mitigates hallucinations in Large
Language Models (LLMs) by integrating external knowledge. However, conflicts
between parametric knowledge and retrieved context pose challenges,
particularly when retrieved information is unreliable or the model's internal
knowledge is outdated. In such cases, LLMs struggle to determine whether to
rely more on their own parameters or the conflicted context. To address this,
we propose **CK-PLUG**, a plug-and-play method for controlling LLMs' reliance
on parametric and contextual knowledge. We introduce a novel knowledge
consistency metric, Confidence Gain, which detects knowledge conflicts by
measuring entropy shifts in token probability distributions after context
insertion. CK-PLUG then enables fine-grained control over knowledge preference
by adjusting the probability distribution of tokens with negative confidence
gain through a single tuning parameter. Experiments demonstrate CK-PLUG's
ability to significantly regulate knowledge reliance in counterfactual RAG
scenarios while maintaining generation fluency and knowledge accuracy. For
instance, on Llama3-8B, memory recall (MR) of RAG response can be adjusted
within a broad range (9.9%-71.9%), compared to the baseline of 42.1%. Moreover,
CK-PLUG supports adaptive control based on the model's confidence in both
internal and external knowledge, achieving consistent performance improvements
across various general RAG tasks. Our code is available at:
$\href{https://github.com/byronBBL/CK-PLUG}{\text{this https URL}}$.","['cs.CL', 'cs.AI']","['Baolong Bi', 'Shenghua Liu', 'Yiwei Wang', 'Yilong Xu', 'Junfeng Fang', 'Lingrui Mei', 'Xueqi Cheng']",2025-03-20,2025-03-20
2503.15887v1,DocVideoQA: Towards Comprehensive Understanding of Document-Centric Videos through Question Answering,"Remote work and online courses have become important methods of knowledge
dissemination, leading to a large number of document-based instructional
videos. Unlike traditional video datasets, these videos mainly feature
rich-text images and audio that are densely packed with information closely
tied to the visual content, requiring advanced multimodal understanding
capabilities. However, this domain remains underexplored due to dataset
availability and its inherent complexity. In this paper, we introduce the
DocVideoQA task and dataset for the first time, comprising 1454 videos across
23 categories with a total duration of about 828 hours. The dataset is
annotated with 154k question-answer pairs generated manually and via GPT,
assessing models' comprehension, temporal awareness, and modality integration
capabilities. Initially, we establish a baseline using open-source MLLMs.
Recognizing the challenges in modality comprehension for document-centric
videos, we present DV-LLaMA, a robust video MLLM baseline. Our method enhances
unimodal feature extraction with diverse instruction-tuning data and employs
contrastive learning to strengthen modality integration. Through fine-tuning,
the LLM is equipped with audio-visual capabilities, leading to significant
improvements in document-centric video understanding. Extensive testing on the
DocVideoQA dataset shows that DV-LLaMA significantly outperforms existing
models. We'll release the code and dataset to facilitate future research.",['cs.CV'],"['Haochen Wang', 'Kai Hu', 'Liangcai Gao']",2025-03-20,2025-03-20
2503.15886v2,Enhancing Zero-Shot Image Recognition in Vision-Language Models through Human-like Concept Guidance,"In zero-shot image recognition tasks, humans demonstrate remarkable
flexibility in classifying unseen categories by composing known simpler
concepts. However, existing vision-language models (VLMs), despite achieving
significant progress through large-scale natural language supervision, often
underperform in real-world applications because of sub-optimal prompt
engineering and the inability to adapt effectively to target classes. To
address these issues, we propose a Concept-guided Human-like Bayesian Reasoning
(CHBR) framework. Grounded in Bayes' theorem, CHBR models the concept used in
human image recognition as latent variables and formulates this task by summing
across potential concepts, weighted by a prior distribution and a likelihood
function. To tackle the intractable computation over an infinite concept space,
we introduce an importance sampling algorithm that iteratively prompts large
language models (LLMs) to generate discriminative concepts, emphasizing
inter-class differences. We further propose three heuristic approaches
involving Average Likelihood, Confidence Likelihood, and Test Time Augmentation
(TTA) Likelihood, which dynamically refine the combination of concepts based on
the test image. Extensive evaluations across fifteen datasets demonstrate that
CHBR consistently outperforms existing state-of-the-art zero-shot
generalization methods.","['cs.CV', 'cs.LG']","['Hui Liu', 'Wenya Wang', 'Kecheng Chen', 'Jie Liu', 'Yibing Liu', 'Tiexin Qin', 'Peisong He', 'Xinghao Jiang', 'Haoliang Li']",2025-03-20,2025-03-21
2503.16561v1,FutureGen: LLM-RAG Approach to Generate the Future Work of Scientific Article,"The future work section of a scientific article outlines potential research
directions by identifying gaps and limitations of a current study. This section
serves as a valuable resource for early-career researchers seeking unexplored
areas and experienced researchers looking for new projects or collaborations.
In this study, we generate future work suggestions from key sections of a
scientific article alongside related papers and analyze how the trends have
evolved. We experimented with various Large Language Models (LLMs) and
integrated Retrieval-Augmented Generation (RAG) to enhance the generation
process. We incorporate a LLM feedback mechanism to improve the quality of the
generated content and propose an LLM-as-a-judge approach for evaluation. Our
results demonstrated that the RAG-based approach with LLM feedback outperforms
other methods evaluated through qualitative and quantitative metrics. Moreover,
we conduct a human evaluation to assess the LLM as an extractor and judge. The
code and dataset for this project are here, code: HuggingFace","['cs.CL', 'cs.LG']","['Ibrahim Al Azher', 'Miftahul Jannat Mokarrama', 'Zhishuai Guo', 'Sagnik Ray Choudhury', 'Hamed Alhoori']",2025-03-20,2025-03-20
2503.15880v1,InCo-DPO: Balancing Distribution Shift and Data Quality for Enhanced Preference Optimization,"Direct Preference Optimization (DPO) optimizes language models to align with
human preferences. Utilizing on-policy samples, generated directly by the
policy model, typically results in better performance due to its distribution
consistency with the model compared to off-policy samples. This paper
identifies the quality of candidate preference samples as another critical
factor. While the quality of on-policy data is inherently constrained by the
capabilities of the policy model, off-policy data, which can be derived from
diverse sources, offers greater potential for quality despite experiencing
distribution shifts. However, current research mostly relies on on-policy data
and neglects the value of off-policy data in terms of data quality, due to the
challenge posed by distribution shift. In this paper, we propose InCo-DPO, an
efficient method for synthesizing preference data by integrating on-policy and
off-policy data, allowing dynamic adjustments to balance distribution shifts
and data quality, thus finding an optimal trade-off. Consequently, InCo-DPO
overcomes the limitations of distribution shifts in off-policy data and the
quality constraints of on-policy data. We evaluated InCo-DPO with the
Alpaca-Eval 2.0 and Arena-Hard benchmarks. Experimental results demonstrate
that our approach not only outperforms both on-policy and off-policy data but
also achieves a state-of-the-art win rate of 60.8 on Arena-Hard with the
vanilla DPO using Gemma-2 model.","['cs.LG', 'cs.CL']","['Yunan Wang', 'Jijie Li', 'Bo-Wen Zhang', 'Liangdong Wang', 'Guang Liu']",2025-03-20,2025-03-20
2503.15879v2,Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid Question Answering,"Non-factoid question-answering (NFQA) poses a significant challenge due to
its open-ended nature, diverse intents, and the need for multi-aspect
reasoning, which renders conventional factoid QA approaches, including
retrieval-augmented generation (RAG), inadequate. Unlike factoid questions,
non-factoid questions (NFQs) lack definitive answers and require synthesizing
information from multiple sources across various reasoning dimensions. To
address these limitations, we introduce Typed-RAG, a type-aware multi-aspect
decomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies
NFQs into distinct types -- such as debate, experience, and comparison -- and
applies aspect-based decomposition to refine retrieval and generation
strategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and
aggregating the results, Typed-RAG generates more informative and contextually
relevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, a benchmark
dataset covering diverse NFQ types. Experimental results demonstrate that
Typed-RAG outperforms baselines, thereby highlighting the importance of
type-aware decomposition for effective retrieval and generation in NFQA. Our
code and dataset are available at https://github.com/TeamNLP/Typed-RAG.","['cs.CL', 'cs.IR']","['DongGeon Lee', 'Ahjeong Park', 'Hyeri Lee', 'Hyeonseo Nam', 'Yunho Maeng']",2025-03-20,2025-03-21
2503.15877v1,Repurposing 2D Diffusion Models with Gaussian Atlas for 3D Generation,"Recent advances in text-to-image diffusion models have been driven by the
increasing availability of paired 2D data. However, the development of 3D
diffusion models has been hindered by the scarcity of high-quality 3D data,
resulting in less competitive performance compared to their 2D counterparts. To
address this challenge, we propose repurposing pre-trained 2D diffusion models
for 3D object generation. We introduce Gaussian Atlas, a novel representation
that utilizes dense 2D grids, enabling the fine-tuning of 2D diffusion models
to generate 3D Gaussians. Our approach demonstrates successful transfer
learning from a pre-trained 2D diffusion model to a 2D manifold flattened from
3D structures. To support model training, we compile GaussianVerse, a
large-scale dataset comprising 205K high-quality 3D Gaussian fittings of
various 3D objects. Our experimental results show that text-to-image diffusion
models can be effectively adapted for 3D content generation, bridging the gap
between 2D and 3D modeling.",['cs.CV'],"['Tiange Xiang', 'Kai Li', 'Chengjiang Long', 'Christian Häne', 'Peihong Guo', 'Scott Delp', 'Ehsan Adeli', 'Li Fei-Fei']",2025-03-20,2025-03-20
2503.15876v1,DeepPsy-Agent: A Stage-Aware and Deep-Thinking Emotional Support Agent System,"This paper introduces DeepPsy-Agent, an innovative psychological support
system that combines the three-stage helping theory in psychology with deep
learning techniques. The system consists of two core components: (1) a
multi-stage response-capable dialogue model (\textit{deeppsy-chat}), which
enhances reasoning capabilities through stage-awareness and deep-thinking
analysis to generate high-quality responses; and (2) a real-time stage
transition detection model that identifies contextual shifts to guide the
dialogue towards more effective intervention stages. Based on 30,000 real
psychological hotline conversations, we employ AI-simulated dialogues and
expert re-annotation strategies to construct a high-quality multi-turn dialogue
dataset. Experimental results demonstrate that DeepPsy-Agent outperforms
general-purpose large language models (LLMs) in key metrics such as problem
exposure completeness, cognitive restructuring success rate, and action
adoption rate. Ablation studies further validate the effectiveness of
stage-awareness and deep-thinking modules, showing that stage information
contributes 42.3\% to performance, while the deep-thinking module increases
root-cause identification by 58.3\% and reduces ineffective suggestions by
72.1\%. This system addresses critical challenges in AI-based psychological
support through dynamic dialogue management and deep reasoning, advancing
intelligent mental health services.",['cs.AI'],"['Kai Chen', 'Zebing Sun']",2025-03-20,2025-03-20
2503.15875v1,MiLA: Multi-view Intensive-fidelity Long-term Video Generation World Model for Autonomous Driving,"In recent years, data-driven techniques have greatly advanced autonomous
driving systems, but the need for rare and diverse training data remains a
challenge, requiring significant investment in equipment and labor. World
models, which predict and generate future environmental states, offer a
promising solution by synthesizing annotated video data for training. However,
existing methods struggle to generate long, consistent videos without
accumulating errors, especially in dynamic scenes. To address this, we propose
MiLA, a novel framework for generating high-fidelity, long-duration videos up
to one minute. MiLA utilizes a Coarse-to-Re(fine) approach to both stabilize
video generation and correct distortion of dynamic objects. Additionally, we
introduce a Temporal Progressive Denoising Scheduler and Joint Denoising and
Correcting Flow modules to improve the quality of generated videos. Extensive
experiments on the nuScenes dataset show that MiLA achieves state-of-the-art
performance in video generation quality. For more information, visit the
project website: https://github.com/xiaomi-mlab/mila.github.io.",['cs.CV'],"['Haiguang Wang', 'Daqi Liu', 'Hongwei Xie', 'Haisong Liu', 'Enhui Ma', 'Kaicheng Yu', 'Limin Wang', 'Bing Wang']",2025-03-20,2025-03-20
2503.15871v1,MASH-VLM: Mitigating Action-Scene Hallucination in Video-LLMs through Disentangled Spatial-Temporal Representations,"In this work, we tackle action-scene hallucination in Video Large Language
Models (Video-LLMs), where models incorrectly predict actions based on the
scene context or scenes based on observed actions. We observe that existing
Video-LLMs often suffer from action-scene hallucination due to two main
factors. First, existing Video-LLMs intermingle spatial and temporal features
by applying an attention operation across all tokens. Second, they use the
standard Rotary Position Embedding (RoPE), which causes the text tokens to
overemphasize certain types of tokens depending on their sequential orders. To
address these issues, we introduce MASH-VLM, Mitigating Action-Scene
Hallucination in Video-LLMs through disentangled spatial-temporal
representations. Our approach includes two key innovations: (1) DST-attention,
a novel attention mechanism that disentangles the spatial and temporal tokens
within the LLM by using masked attention to restrict direct interactions
between the spatial and temporal tokens; (2) Harmonic-RoPE, which extends the
dimensionality of the positional IDs, allowing the spatial and temporal tokens
to maintain balanced positions relative to the text tokens. To evaluate the
action-scene hallucination in Video-LLMs, we introduce the UNSCENE benchmark
with 1,320 videos and 4,078 QA pairs. Extensive experiments demonstrate that
MASH-VLM achieves state-of-the-art results on the UNSCENE benchmark, as well as
on existing video understanding benchmarks.",['cs.CV'],"['Kyungho Bae', 'Jinhyung Kim', 'Sihaeng Lee', 'Soonyoung Lee', 'Gunhee Lee', 'Jinwoo Choi']",2025-03-20,2025-03-20
2503.15870v1,FedSAF: A Federated Learning Framework for Enhanced Gastric Cancer Detection and Privacy Preservation,"Gastric cancer is one of the most commonly diagnosed cancers and has a high
mortality rate. Due to limited medical resources, developing machine learning
models for gastric cancer recognition provides an efficient solution for
medical institutions. However, such models typically require large sample sizes
for training and testing, which can challenge patient privacy. Federated
learning offers an effective alternative by enabling model training across
multiple institutions without sharing sensitive patient data. This paper
addresses the limited sample size of publicly available gastric cancer data
with a modified data processing method. This paper introduces FedSAF, a novel
federated learning algorithm designed to improve the performance of existing
methods, particularly in non-independent and identically distributed (non-IID)
data scenarios. FedSAF incorporates attention-based message passing and the
Fisher Information Matrix to enhance model accuracy, while a model splitting
function reduces computation and transmission costs. Hyperparameter tuning and
ablation studies demonstrate the effectiveness of this new algorithm, showing
improvements in test accuracy on gastric cancer datasets, with FedSAF
outperforming existing federated learning methods like FedAMP, FedAvg, and
FedProx. The framework's robustness and generalization ability were further
validated across additional datasets (SEED, BOT, FashionMNIST, and CIFAR-10),
achieving high performance in diverse environments.",['cs.LG'],"['Yuxin Miao', 'Xinyuan Yang', 'Hongda Fan', 'Yichun Li', 'Yishu Hong', 'Xiechen Guo', 'Ali Braytee', 'Weidong Huang', 'Ali Anaissi']",2025-03-20,2025-03-20
2503.15868v2,UniCoRN: Latent Diffusion-based Unified Controllable Image Restoration Network across Multiple Degradations,"Image restoration is essential for enhancing degraded images across computer
vision tasks. However, most existing methods address only a single type of
degradation (e.g., blur, noise, or haze) at a time, limiting their real-world
applicability where multiple degradations often occur simultaneously. In this
paper, we propose UniCoRN, a unified image restoration approach capable of
handling multiple degradation types simultaneously using a multi-head diffusion
model. Specifically, we uncover the potential of low-level visual cues
extracted from images in guiding a controllable diffusion model for real-world
image restoration and we design a multi-head control network adaptable via a
mixture-of-experts strategy. We train our model without any prior assumption of
specific degradations, through a smartly designed curriculum learning recipe.
Additionally, we also introduce MetaRestore, a metalens imaging benchmark
containing images with multiple degradations and artifacts. Extensive
evaluations on several challenging datasets, including our benchmark,
demonstrate that our method achieves significant performance gains and can
robustly restore images with severe degradations. Project page:
https://codejaeger.github.io/unicorn-gh",['cs.CV'],"['Debabrata Mandal', 'Soumitri Chattopadhyay', 'Guansen Tong', 'Praneeth Chakravarthula']",2025-03-20,2025-03-21
2503.15867v1,TruthLens: Explainable DeepFake Detection for Face Manipulated and Fully Synthetic Data,"Detecting DeepFakes has become a crucial research area as the widespread use
of AI image generators enables the effortless creation of face-manipulated and
fully synthetic content, yet existing methods are often limited to binary
classification (real vs. fake) and lack interpretability. To address these
challenges, we propose TruthLens, a novel and highly generalizable framework
for DeepFake detection that not only determines whether an image is real or
fake but also provides detailed textual reasoning for its predictions. Unlike
traditional methods, TruthLens effectively handles both face-manipulated
DeepFakes and fully AI-generated content while addressing fine-grained queries
such as ""Does the eyes/nose/mouth look real or fake?""
  The architecture of TruthLens combines the global contextual understanding of
multimodal large language models like PaliGemma2 with the localized feature
extraction capabilities of vision-only models like DINOv2. This hybrid design
leverages the complementary strengths of both models, enabling robust detection
of subtle manipulations while maintaining interpretability. Extensive
experiments on diverse datasets demonstrate that TruthLens outperforms
state-of-the-art methods in detection accuracy (by 2-14%) and explainability,
in both in-domain and cross-data settings, generalizing effectively across
traditional and emerging manipulation techniques.","['cs.CV', 'cs.AI']","['Rohit Kundu', 'Athula Balachandran', 'Amit K. Roy-Chowdhury']",2025-03-20,2025-03-20
2503.15865v1,Active management of battery degradation in wireless sensor network using deep reinforcement learning for group battery replacement,"Wireless sensor networks (WSNs) have become a promising solution for
structural health monitoring (SHM), especially in hard-to-reach or remote
locations. Battery-powered WSNs offer various advantages over wired systems,
however limited battery life has always been one of the biggest obstacles in
practical use of the WSNs, regardless of energy harvesting methods. While
various methods have been studied for battery health management, existing
methods exclusively aim to extend lifetime of individual batteries, lacking a
system level view. A consequence of applying such methods is that batteries in
a WSN tend to fail at different times, posing significant difficulty on
planning and scheduling of battery replacement trip. This study investigate a
deep reinforcement learning (DRL) method for active battery degradation
management by optimizing duty cycle of WSNs at the system level. This active
management strategy effectively reduces earlier failure of battery individuals
which enable group replacement without sacrificing WSN performances. A
simulated environment based on a real-world WSN setup was developed to train a
DRL agent and learn optimal duty cycle strategies. The performance of the
strategy was validated in a long-term setup with various network sizes,
demonstrating its efficiency and scalability.","['cs.LG', 'cs.AI']","['Jong-Hyun Jeonga', 'Hongki Jo', 'Qiang Zhou', 'Tahsin Afroz Hoque Nishat', 'Lang Wu']",2025-03-20,2025-03-20
2503.15861v1,Sequential Spatial-Temporal Network for Interpretable Automatic Ultrasonic Assessment of Fetal Head during labor,"The intrapartum ultrasound guideline established by ISUOG highlights the
Angle of Progression (AoP) and Head Symphysis Distance (HSD) as pivotal metrics
for assessing fetal head descent and predicting delivery outcomes. Accurate
measurement of the AoP and HSD requires a structured process. This begins with
identifying standardized ultrasound planes, followed by the detection of
specific anatomical landmarks within the regions of the pubic symphysis and
fetal head that correlate with the delivery parameters AoP and HSD. Finally,
these measurements are derived based on the identified anatomical landmarks.
Addressing the clinical demands and standard operation process outlined in the
ISUOG guideline, we introduce the Sequential Spatial-Temporal Network (SSTN),
the first interpretable model specifically designed for the video of
intrapartum ultrasound analysis. The SSTN operates by first identifying
ultrasound planes, then segmenting anatomical structures such as the pubic
symphysis and fetal head, and finally detecting key landmarks for precise
measurement of HSD and AoP. Furthermore, the cohesive framework leverages
task-related information to improve accuracy and reliability. Experimental
evaluations on clinical datasets demonstrate that SSTN significantly surpasses
existing models, reducing the mean absolute error by 18% for AoP and 22% for
HSD.","['eess.IV', 'cs.CV']","['Jie Gan', 'Zhuonan Liang', 'Jianan Fan', 'Lisa Mcguire', 'Caterina Watson', 'Jacqueline Spurway', 'Jillian Clarke', 'Weidong Cai']",2025-03-20,2025-03-20
2503.15855v1,VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting Generation with Flexible Pose and Multi-View Joint Modeling,"We propose VideoRFSplat, a direct text-to-3D model leveraging a video
generation model to generate realistic 3D Gaussian Splatting (3DGS) for
unbounded real-world scenes. To generate diverse camera poses and unbounded
spatial extent of real-world scenes, while ensuring generalization to arbitrary
text prompts, previous methods fine-tune 2D generative models to jointly model
camera poses and multi-view images. However, these methods suffer from
instability when extending 2D generative models to joint modeling due to the
modality gap, which necessitates additional models to stabilize training and
inference. In this work, we propose an architecture and a sampling strategy to
jointly model multi-view images and camera poses when fine-tuning a video
generation model. Our core idea is a dual-stream architecture that attaches a
dedicated pose generation model alongside a pre-trained video generation model
via communication blocks, generating multi-view images and camera poses through
separate streams. This design reduces interference between the pose and image
modalities. Additionally, we propose an asynchronous sampling strategy that
denoises camera poses faster than multi-view images, allowing rapidly denoised
poses to condition multi-view generation, reducing mutual ambiguity and
enhancing cross-modal consistency. Trained on multiple large-scale real-world
datasets (RealEstate10K, MVImgNet, DL3DV-10K, ACID), VideoRFSplat outperforms
existing text-to-3D direct generation methods that heavily depend on post-hoc
refinement via score distillation sampling, achieving superior results without
such refinement.","['cs.CV', 'cs.AI']","['Hyojun Go', 'Byeongjun Park', 'Hyelin Nam', 'Byung-Hoon Kim', 'Hyungjin Chung', 'Changick Kim']",2025-03-20,2025-03-20
2503.15853v1,Network Embedding Exploration Tool (NEExT),"Many real-world and artificial systems and processes can be represented as
graphs. Some examples of such systems include social networks, financial
transactions, supply chains, and molecular structures. In many of these cases,
one needs to consider a collection of graphs, rather than a single network.
This could be a collection of distinct but related graphs, such as different
protein structures or graphs resulting from dynamic processes on the same
network. Examples of the latter include the evolution of social networks,
community-induced graphs, or ego-nets around various nodes. A significant
challenge commonly encountered is the absence of ground-truth labels for graphs
or nodes, necessitating the use of unsupervised techniques to analyze such
systems. Moreover, even when ground-truth labels are available, many existing
graph machine learning methods depend on complex deep learning models,
complicating model explainability and interpretability. To address some of
these challenges, we have introduced NEExT (Network Embedding Exploration Tool)
for embedding collections of graphs via user-defined node features. The
advantages of the framework are twofold: (i) the ability to easily define your
own interpretable node-based features in view of the task at hand, and (ii)
fast embedding of graphs provided by the Vectorizers library. In this paper, we
demonstrate the usefulness of NEExT on collections of synthetic and real-world
graphs. For supervised tasks, we demonstrate that performance in graph
classification tasks could be achieved similarly to other state-of-the-art
techniques while maintaining model interpretability. Furthermore, our framework
can also be used to generate high-quality embeddings in an unsupervised way,
where target variables are not available.",['cs.LG'],"['Ashkan Dehghan', 'Paweł Prałat', 'François Théberge']",2025-03-20,2025-03-20
2503.15851v1,Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video Diffusion,"Animatable head avatar generation typically requires extensive data for
training. To reduce the data requirements, a natural solution is to leverage
existing data-free static avatar generation methods, such as pre-trained
diffusion models with score distillation sampling (SDS), which align avatars
with pseudo ground-truth outputs from the diffusion model. However, directly
distilling 4D avatars from video diffusion often leads to over-smooth results
due to spatial and temporal inconsistencies in the generated video. To address
this issue, we propose Zero-1-to-A, a robust method that synthesizes a spatial
and temporal consistency dataset for 4D avatar reconstruction using the video
diffusion model. Specifically, Zero-1-to-A iteratively constructs video
datasets and optimizes animatable avatars in a progressive manner, ensuring
that avatar quality increases smoothly and consistently throughout the learning
process. This progressive learning involves two stages: (1) Spatial Consistency
Learning fixes expressions and learns from front-to-side views, and (2)
Temporal Consistency Learning fixes views and learns from relaxed to
exaggerated expressions, generating 4D avatars in a simple-to-complex manner.
Extensive experiments demonstrate that Zero-1-to-A improves fidelity, animation
quality, and rendering speed compared to existing diffusion-based methods,
providing a solution for lifelike avatar creation. Code is publicly available
at: https://github.com/ZhenglinZhou/Zero-1-to-A.",['cs.CV'],"['Zhou Zhenglin', 'Ma Fan', 'Fan Hehe', 'Chua Tat-Seng']",2025-03-20,2025-03-20
2503.15850v1,Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey,"Large Language Models (LLMs) excel in text generation, reasoning, and
decision-making, enabling their adoption in high-stakes domains such as
healthcare, law, and transportation. However, their reliability is a major
concern, as they often produce plausible but incorrect responses. Uncertainty
quantification (UQ) enhances trustworthiness by estimating confidence in
outputs, enabling risk mitigation and selective prediction. However,
traditional UQ methods struggle with LLMs due to computational constraints and
decoding inconsistencies. Moreover, LLMs introduce unique uncertainty sources,
such as input ambiguity, reasoning path divergence, and decoding stochasticity,
that extend beyond classical aleatoric and epistemic uncertainty. To address
this, we introduce a new taxonomy that categorizes UQ methods based on
computational efficiency and uncertainty dimensions (input, reasoning,
parameter, and prediction uncertainty). We evaluate existing techniques, assess
their real-world applicability, and identify open challenges, emphasizing the
need for scalable, interpretable, and robust UQ approaches to enhance LLM
reliability.",['cs.CL'],"['Xiaoou Liu', 'Tiejin Chen', 'Longchao Da', 'Chacha Chen', 'Zhen Lin', 'Hua Wei']",2025-03-20,2025-03-20
2503.15848v1,Entropy-based Exploration Conduction for Multi-step Reasoning,"In large language model (LLM) reasoning, multi-step processes have proven
effective for solving complex tasks. However, the depth of exploration can
significantly affect the reasoning performance. Existing methods to
automatically decide the depth often bring high costs and lack flexibility, and
thus undermine the model's reasoning accuracy. To address these issues, we
propose Entropy-based Exploration Depth Conduction (Entro-duction), a novel
method that dynamically adjusts the exploration depth during multi-step
reasoning by monitoring LLM's output entropy and variance entropy. We employ
these two metrics to capture the model's current uncertainty and the
fluctuation of uncertainty across consecutive reasoning steps. Based on the
observed changes, the LLM selects whether to deepen, expand or stop exploration
according to the probability. In this way, we balance the reasoning accuracy
and exploration effectiveness. Experimental results across four benchmark
datasets demonstrate the efficacy of Entro-duction. We further conduct
experiments and analysis on the components of Entro-duction to discuss their
contributions to reasoning performance.","['cs.AI', 'cs.CL']","['Jinghan Zhang', 'Xiting Wang', 'Fengran Mo', 'Yeyang Zhou', 'Wanfu Gao', 'Kunpeng Liu']",2025-03-20,2025-03-20
2503.15847v1,Beyond Local Selection: Global Cut Selection for Enhanced Mixed-Integer Programming,"In mixed-integer programming (MIP) solvers, cutting planes are essential for
Branch-and-Cut (B&C) algorithms as they reduce the search space and accelerate
the solving process. Traditional methods rely on hard-coded heuristics for cut
plane selection but fail to leverage problem-specific structural features.
Recent machine learning approaches use neural networks for cut selection but
focus narrowly on the efficiency of single-node within the B&C algorithm,
without considering the broader contextual information. To address this, we
propose Global Cut Selection (GCS), which uses a bipartite graph to represent
the search tree and combines graph neural networks with reinforcement learning
to develop cut selection strategies. Unlike prior methods, GCS applies cutting
planes across all nodes, incorporating richer contextual information.
Experiments show GCS significantly improves solving efficiency for synthetic
and large-scale real-world MIPs compared to traditional and learning-based
methods.",['cs.AI'],"['Shuli Zeng', 'Sijia Zhang', 'Shaoang Li', 'Feng Wu', 'Xiang-Yang Li']",2025-03-20,2025-03-20
2503.15846v1,What can Off-the-Shelves Large Multi-Modal Models do for Dynamic Scene Graph Generation?,"Dynamic Scene Graph Generation (DSGG) for videos is a challenging task in
computer vision. While existing approaches often focus on sophisticated
architectural design and solely use recall during evaluation, we take a closer
look at their predicted scene graphs and discover three critical issues with
existing DSGG methods: severe precision-recall trade-off, lack of awareness on
triplet importance, and inappropriate evaluation protocols. On the other hand,
recent advances of Large Multimodal Models (LMMs) have shown great capabilities
in video understanding, yet they have not been tested on fine-grained,
frame-wise understanding tasks like DSGG. In this work, we conduct the first
systematic analysis of Video LMMs for performing DSGG. Without relying on
sophisticated architectural design, we show that LMMs with simple decoder-only
structure can be turned into State-of-the-Art scene graph generators that
effectively overcome the aforementioned issues, while requiring little
finetuning (5-10% training data).",['cs.CV'],"['Xuanming Cui', 'Jaiminkumar Ashokbhai Bhoi', 'Chionh Wei Peng', 'Adriel Kuek', 'Ser Nam Lim']",2025-03-20,2025-03-20
2503.15845v1,Network-wide Freeway Traffic Estimation Using Sparse Sensor Data: A Dirichlet Graph Auto-Encoder Approach,"Network-wide Traffic State Estimation (TSE), which aims to infer a complete
image of network traffic states with sparsely deployed sensors, plays a vital
role in intelligent transportation systems. With the development of data-driven
methods, traffic dynamics modeling has advanced significantly. However, TSE
poses fundamental challenges for data-driven approaches, since historical
patterns cannot be learned locally at sensor-free segments. Although inductive
graph learning shows promise in estimating states at locations without sensor,
existing methods typically handle unobserved locations by filling them with
zeros, introducing bias to the sensitive graph message propagation. The
recently proposed Dirichlet Energy-based Feature Propagation (DEFP) method
achieves State-Of-The-Art (SOTA) performance in unobserved node classification
by eliminating the need for zero-filling. However, applying it to TSE faces
three key challenges: inability to handle directed traffic networks, strong
assumptions in traffic spatial correlation modeling, and overlooks distinct
propagation rules of different patterns (e.g., congestion and free flow). We
propose DGAE, a novel inductive graph representation model that addresses these
challenges through theoretically derived DEFP for Directed graph (DEFP4D),
enhanced spatial representation learning via DEFP4D-guided latent space
encoding, and physics-guided propagation mechanisms that separately handles
congested and free-flow patterns. Experiments on three traffic datasets
demonstrate that DGAE outperforms existing SOTA methods and exhibits strong
cross-city transferability. Furthermore, DEFP4D can serve as a standalone
lightweight solution, showing superior performance under extremely sparse
sensor conditions.",['cs.LG'],"['Qishen Zhou', 'Yifan Zhang', 'Michail A. Makridis', 'Anastasios Kouvelas', 'Yibing Wang', 'Simon Hu']",2025-03-20,2025-03-20
2503.15842v1,FedAWA: Adaptive Optimization of Aggregation Weights in Federated Learning Using Client Vectors,"Federated Learning (FL) has emerged as a promising framework for distributed
machine learning, enabling collaborative model training without sharing local
data, thereby preserving privacy and enhancing security. However, data
heterogeneity resulting from differences across user behaviors, preferences,
and device characteristics poses a significant challenge for federated
learning. Most previous works overlook the adjustment of aggregation weights,
relying solely on dataset size for weight assignment, which often leads to
unstable convergence and reduced model performance. Recently, several studies
have sought to refine aggregation strategies by incorporating dataset
characteristics and model alignment. However, adaptively adjusting aggregation
weights while ensuring data security-without requiring additional proxy
data-remains a significant challenge. In this work, we propose Federated
learning with Adaptive Weight Aggregation (FedAWA), a novel method that
adaptively adjusts aggregation weights based on client vectors during the
learning process. The client vector captures the direction of model updates,
reflecting local data variations, and is used to optimize the aggregation
weight without requiring additional datasets or violating privacy. By assigning
higher aggregation weights to local models whose updates align closely with the
global optimization direction, FedAWA enhances the stability and generalization
of the global model. Extensive experiments under diverse scenarios demonstrate
the superiority of our method, providing a promising solution to the challenges
of data heterogeneity in federated learning.",['cs.LG'],"['Changlong Shi', 'He Zhao', 'Bingjie Zhang', 'Mingyuan Zhou', 'Dandan Guo', 'Yi Chang']",2025-03-20,2025-03-20
2503.15837v1,Fùxì: A Benchmark for Evaluating Language Models on Ancient Chinese Text Understanding and Generation,"Ancient Chinese text processing presents unique challenges for large language
models (LLMs) due to its distinct linguistic features, complex structural
constraints, and rich cultural context. While existing benchmarks have
primarily focused on evaluating comprehension through multiple-choice
questions, there remains a critical gap in assessing models' generative
capabilities in classical Chinese. We introduce F\`ux\`i, a comprehensive
benchmark that evaluates both understanding and generation capabilities across
21 diverse tasks. Our benchmark distinguishes itself through three key
contributions: (1) balanced coverage of both comprehension and generation
tasks, including novel tasks like poetry composition and couplet completion,
(2) specialized evaluation metrics designed specifically for classical Chinese
text generation, combining rule-based verification with fine-tuned LLM
evaluators, and (3) a systematic assessment framework that considers both
linguistic accuracy and cultural authenticity. Through extensive evaluation of
state-of-the-art LLMs, we reveal significant performance gaps between
understanding and generation tasks, with models achieving promising results in
comprehension but struggling considerably in generation tasks, particularly
those requiring deep cultural knowledge and adherence to classical formats. Our
findings highlight the current limitations in ancient Chinese text processing
and provide insights for future model development. The benchmark, evaluation
toolkit, and baseline results are publicly available to facilitate research in
this domain.","['cs.CL', 'cs.AI']","['Shangqing Zhao', 'Yuhao Zhou', 'Yupei Ren', 'Zhe Chen', 'Chenghao Jia', 'Fang Zhe', 'Zhaogaung Long', 'Shu Liu', 'Man Lan']",2025-03-20,2025-03-20
2503.15835v1,BARD-GS: Blur-Aware Reconstruction of Dynamic Scenes via Gaussian Splatting,"3D Gaussian Splatting (3DGS) has shown remarkable potential for static scene
reconstruction, and recent advancements have extended its application to
dynamic scenes. However, the quality of reconstructions depends heavily on
high-quality input images and precise camera poses, which are not that trivial
to fulfill in real-world scenarios. Capturing dynamic scenes with handheld
monocular cameras, for instance, typically involves simultaneous movement of
both the camera and objects within a single exposure. This combined motion
frequently results in image blur that existing methods cannot adequately
handle. To address these challenges, we introduce BARD-GS, a novel approach for
robust dynamic scene reconstruction that effectively handles blurry inputs and
imprecise camera poses. Our method comprises two main components: 1) camera
motion deblurring and 2) object motion deblurring. By explicitly decomposing
motion blur into camera motion blur and object motion blur and modeling them
separately, we achieve significantly improved rendering results in dynamic
regions. In addition, we collect a real-world motion blur dataset of dynamic
scenes to evaluate our approach. Extensive experiments demonstrate that BARD-GS
effectively reconstructs high-quality dynamic scenes under realistic
conditions, significantly outperforming existing methods.",['cs.CV'],"['Yiren Lu', 'Yunlai Zhou', 'Disheng Liu', 'Tuo Liang', 'Yu Yin']",2025-03-20,2025-03-20
2503.15831v1,EDEN: Enhanced Diffusion for High-quality Large-motion Video Frame Interpolation,"Handling complex or nonlinear motion patterns has long posed challenges for
video frame interpolation. Although recent advances in diffusion-based methods
offer improvements over traditional optical flow-based approaches, they still
struggle to generate sharp, temporally consistent frames in scenarios with
large motion. To address this limitation, we introduce EDEN, an Enhanced
Diffusion for high-quality large-motion vidEo frame iNterpolation. Our approach
first utilizes a transformer-based tokenizer to produce refined latent
representations of the intermediate frames for diffusion models. We then
enhance the diffusion transformer with temporal attention across the process
and incorporate a start-end frame difference embedding to guide the generation
of dynamic motion. Extensive experiments demonstrate that EDEN achieves
state-of-the-art results across popular benchmarks, including nearly a 10%
LPIPS reduction on DAVIS and SNU-FILM, and an 8% improvement on DAIN-HD.",['cs.CV'],"['Zihao Zhang', 'Haoran Chen', 'Haoyu Zhao', 'Guansong Lu', 'Yanwei Fu', 'Hang Xu', 'Zuxuan Wu']",2025-03-20,2025-03-20
2503.16560v1,Early Prediction of Alzheimer's and Related Dementias: A Machine Learning Approach Utilizing Social Determinants of Health Data,"Alzheimer's disease and related dementias (AD/ADRD) represent a growing
healthcare crisis affecting over 6 million Americans. While genetic factors
play a crucial role, emerging research reveals that social determinants of
health (SDOH) significantly influence both the risk and progression of
cognitive functioning, such as cognitive scores and cognitive decline. This
report examines how these social, environmental, and structural factors impact
cognitive health trajectories, with a particular focus on Hispanic populations,
who face disproportionate risk for AD/ADRD. Using data from the Mexican Health
and Aging Study (MHAS) and its cognitive assessment sub study (Mex-Cog), we
employed ensemble of regression trees models to predict 4-year and 9-year
cognitive scores and cognitive decline based on SDOH. This approach identified
key predictive SDOH factors to inform potential multilevel interventions to
address cognitive health disparities in this population.","['q-bio.QM', 'cs.LG']","['Bereket Kindo', 'Arjee Restar', 'Anh Tran']",2025-03-20,2025-03-20
2503.15822v1,Energy-Efficient Federated Learning and Migration in Digital Twin Edge Networks,"The digital twin edge network (DITEN) is a significant paradigm in the
sixth-generation wireless system (6G) that aims to organize well-developed
infrastructures to meet the requirements of evolving application scenarios.
However, the impact of the interaction between the long-term DITEN maintenance
and detailed digital twin tasks, which often entail privacy considerations, is
commonly overlooked in current research. This paper addresses this issue by
introducing a problem of digital twin association and historical data
allocation for a federated learning (FL) task within DITEN. To achieve this
goal, we start by introducing a closed-form function to predict the training
accuracy of the FL task, referring to it as the data utility. Subsequently, we
carry out comprehensive convergence analyses on the proposed FL methodology.
Our objective is to jointly optimize the data utility of the digital
twin-empowered FL task and the energy costs incurred by the long-term DITEN
maintenance, encompassing FL model training, data synchronization, and twin
migration. To tackle the aforementioned challenge, we present an
optimization-driven learning algorithm that effectively identifies optimized
solutions for the formulated problem. Numerical results demonstrate that our
proposed algorithm outperforms various baseline approaches.","['cs.NI', 'cs.LG']","['Yuzhi Zhou', 'Yaru Fu', 'Zheng Shi', 'Howard H. Yang', 'Kevin Hung', 'Yan Zhang']",2025-03-20,2025-03-20
2503.15819v1,Control Pneumatic Soft Bending Actuator with Online Learning Pneumatic Physical Reservoir Computing,"The intrinsic nonlinearities of soft robots present significant control but
simultaneously provide them with rich computational potential. Reservoir
computing (RC) has shown effectiveness in online learning systems for
controlling nonlinear systems such as soft actuators. Conventional RC can be
extended into physical reservoir computing (PRC) by leveraging the nonlinear
dynamics of soft actuators for computation. This paper introduces a PRC-based
online learning framework to control the motion of a pneumatic soft bending
actuator, utilizing another pneumatic soft actuator as the PRC model. Unlike
conventional designs requiring two RC models, the proposed control system
employs a more compact architecture with a single RC model. Additionally, the
framework enables zero-shot online learning, addressing limitations of previous
PRC-based control systems reliant on offline training. Simulations and
experiments validated the performance of the proposed system. Experimental
results indicate that the PRC model achieved superior control performance
compared to a linear model, reducing the root-mean-square error (RMSE) by an
average of over 37% in bending motion control tasks. The proposed PRC-based
online learning control framework provides a novel approach for harnessing
physical systems' inherent nonlinearities to enhance the control of soft
actuators.","['cs.RO', 'cs.LG', 'cs.SY', 'eess.SY']","['Junyi Shen', 'Tetsuro Miyazaki', 'Kenji Kawashima']",2025-03-20,2025-03-20
2503.15818v1,Computation-Efficient and Recognition-Friendly 3D Point Cloud Privacy Protection,"3D point cloud has been widely used in applications such as self-driving
cars, robotics, CAD models, etc. To the best of our knowledge, these
applications raised the issue of privacy leakage in 3D point clouds, which has
not been studied well. Different from the 2D image privacy, which is related to
texture and 2D geometric structure, the 3D point cloud is texture-less and only
relevant to 3D geometric structure. In this work, we defined the 3D point cloud
privacy problem and proposed an efficient privacy-preserving framework named
PointFlowGMM that can support downstream classification and segmentation tasks
without seeing the original data. Using a flow-based generative model, the
point cloud is projected into a latent Gaussian mixture distributed subspace.
We further designed a novel angular similarity loss to obfuscate the original
geometric structure and reduce the model size from 767MB to 120MB without a
decrease in recognition performance. The projected point cloud in the latent
space is orthogonally rotated randomly to further protect the original
geometric structure, the class-to-class relationship is preserved after
rotation, thus, the protected point cloud can support the recognition task. We
evaluated our model on multiple datasets and achieved comparable recognition
results on encrypted point clouds compared to the original point clouds.","['cs.CV', 'cs.AI']","['Haotian Ma', 'Lin Gu', 'Siyi Wu', 'Yingying Zhu']",2025-03-20,2025-03-20
2503.15817v1,Ranking Counterfactual Explanations,"AI-driven outcomes can be challenging for end-users to understand.
Explanations can address two key questions: ""Why this outcome?"" (factual) and
""Why not another?"" (counterfactual). While substantial efforts have been made
to formalize factual explanations, a precise and comprehensive study of
counterfactual explanations is still lacking. This paper proposes a formal
definition of counterfactual explanations, proving some properties they
satisfy, and examining the relationship with factual explanations. Given that
multiple counterfactual explanations generally exist for a specific case, we
also introduce a rigorous method to rank these counterfactual explanations,
going beyond a simple minimality condition, and to identify the optimal ones.
Our experiments with 12 real-world datasets highlight that, in most cases, a
single optimal counterfactual explanation emerges. We also demonstrate, via
three metrics, that the selected optimal explanation exhibits higher
representativeness and can explain a broader range of elements than a random
minimal counterfactual. This result highlights the effectiveness of our
approach in identifying more robust and comprehensive counterfactual
explanations.",['cs.AI'],"['Suryani Lim', 'Henri Prade', 'Gilles Richard']",2025-03-20,2025-03-20
2503.15816v1,A Vision Centric Remote Sensing Benchmark,"Multimodal Large Language Models (MLLMs) have achieved remarkable success in
vision-language tasks but their remote sensing (RS) counterpart are relatively
under explored. Unlike natural images, RS imagery presents unique challenges
that current MLLMs struggle to handle, particularly in visual grounding and
spatial reasoning. This study investigates the limitations of CLIP-based MLLMs
in RS, highlighting their failure to differentiate visually distinct yet
semantically similar RS images. To address this, we introduce a remote sensing
multimodal visual patterns (RSMMVP) benchmark. It is designed to evaluate MLLMs
in RS tasks by identifying the CLIP-blind pairs, where CLIP-based models
incorrectly assign high similarity scores to visually distinct RS images.
Through a visual question answering (VQA) evaluation, we analyze the
performance of state-of-the-art MLLMs, revealing significant limitations in RS
specific representation learning. The results provide valuable insights into
the weaknesses of CLIP-based visual encoding and offer a foundation for future
research to develop more effective MLLMs tailored for remote sensing
applications.","['cs.CV', 'F.2.2; I.2.7']","['Abduljaleel Adejumo', 'Faegheh Yeganli', 'Clifford Broni-bediako', 'Aoran Xiao', 'Naoto Yokoya', 'Mennatullah Siam']",2025-03-20,2025-03-20
2503.15815v1,Attention Pruning: Automated Fairness Repair of Language Models via Surrogate Simulated Annealing,"This paper explores pruning attention heads as a post-processing bias
mitigation method for large language models (LLMs). Modern AI systems such as
LLMs are expanding into sensitive social contexts where fairness concerns
become especially crucial. Since LLMs develop decision-making patterns by
training on massive datasets of human-generated content, they naturally encode
and perpetuate societal biases. While modifying training datasets and
algorithms is expensive and requires significant resources; post-processing
techniques-such as selectively deactivating neurons and attention heads in
pre-trained LLMs-can provide feasible and effective approaches to improve
fairness. However, identifying the optimal subset of parameters to prune
presents a combinatorial challenge within LLMs' immense parameter space,
requiring solutions that efficiently balance competing objectives across the
frontiers of model fairness and utility.
  To address the computational challenges, we explore a search-based program
repair approach via randomized simulated annealing. Given the prohibitive
evaluation costs in billion-parameter LLMs, we develop surrogate deep neural
networks that efficiently model the relationship between attention head states
(active/inactive) and their corresponding fairness/utility metrics. This allows
us to perform optimization over the surrogate models and efficiently identify
optimal subsets of attention heads for selective pruning rather than directly
searching through the LLM parameter space. This paper introduces Attention
Pruning, a fairness-aware surrogate simulated annealing approach to prune
attention heads in LLMs that disproportionately contribute to bias while
minimally impacting overall model utility. Our experiments show that Attention
Pruning achieves up to $40\%$ reduction in gender bias and outperforms the
state-of-the-art bias mitigation strategies.",['cs.AI'],"['Vishnu Asutosh Dasu', 'Md Rafi ur Rashid', 'Vipul Gupta', 'Saeid Tizpaz-Niari', 'Gang Tan']",2025-03-20,2025-03-20
2503.15810v1,Big data comparison of quantum invariants,"We apply big data techniques, including exploratory and topological data
analysis, to investigate quantum invariants. More precisely, our study explores
the Jones polynomial's structural properties and contrasts its behavior under
four principal methods of enhancement: coloring, rank increase,
categorification, and leaving the realm of Lie algebras.","['math.GT', 'cs.LG', 'math.QA', 'Primary: 57K16, 62R07, secondary: 57K18, 68P05']","['Daniel Tubbenhauer', 'Victor Zhang']",2025-03-20,2025-03-20
2503.15809v1,Controlling Avatar Diffusion with Learnable Gaussian Embedding,"Recent advances in diffusion models have made significant progress in digital
human generation. However, most existing models still struggle to maintain 3D
consistency, temporal coherence, and motion accuracy. A key reason for these
shortcomings is the limited representation ability of commonly used control
signals(e.g., landmarks, depth maps, etc.). In addition, the lack of diversity
in identity and pose variations in public datasets further hinders progress in
this area. In this paper, we analyze the shortcomings of current control
signals and introduce a novel control signal representation that is
optimizable, dense, expressive, and 3D consistent. Our method embeds a
learnable neural Gaussian onto a parametric head surface, which greatly
enhances the consistency and expressiveness of diffusion-based head models.
Regarding the dataset, we synthesize a large-scale dataset with multiple poses
and identities. In addition, we use real/synthetic labels to effectively
distinguish real and synthetic data, minimizing the impact of imperfections in
synthetic data on the generated head images. Extensive experiments show that
our model outperforms existing methods in terms of realism, expressiveness, and
3D consistency. Our code, synthetic datasets, and pre-trained models will be
released in our project page: https://ustc3dv.github.io/Learn2Control/","['cs.GR', 'cs.CV']","['Xuan Gao', 'Jingtao Zhou', 'Dongyu Liu', 'Yuqi Zhou', 'Juyong Zhang']",2025-03-20,2025-03-20
2503.15808v1,ChatGPT and U(X): A Rapid Review on Measuring the User Experience,"ChatGPT, powered by a large language model (LLM), has revolutionized everyday
human-computer interaction (HCI) since its 2022 release. While now used by
millions around the world, a coherent pathway for evaluating the user
experience (UX) ChatGPT offers remains missing. In this rapid review (N = 58),
I explored how ChatGPT UX has been approached quantitatively so far. I focused
on the independent variables (IVs) manipulated, the dependent variables (DVs)
measured, and the methods used for measurement. Findings reveal trends, gaps,
and emerging consensus in UX assessments. This work offers a first step towards
synthesizing existing approaches to measuring ChatGPT UX, urgent trajectories
to advance standardization and breadth, and two preliminary frameworks aimed at
guiding future research and tool development. I seek to elevate the field of
ChatGPT UX by empowering researchers and practitioners in optimizing user
interactions with ChatGPT and similar LLM-based systems.","['cs.HC', 'cs.AI', 'cs.CL', 'cs.CY']",['Katie Seaborn'],2025-03-20,2025-03-20
2503.15807v1,Video-VoT-R1: An efficient video inference model integrating image packing and AoE architecture,"In the field of video-language pretraining, existing models face numerous
challenges in terms of inference efficiency and multimodal data processing.
This paper proposes a KunLunBaize-VoT-R1 video inference model based on a
long-sequence image encoder, along with its training and application methods.
By integrating image packing technology, the Autonomy-of-Experts (AoE)
architecture, and combining the video of Thought (VoT), a large language model
(LLM) trained with large-scale reinforcement learning, and multiple training
techniques, the efficiency and accuracy of the model in video inference tasks
are effectively improved. Experiments show that this model performs
outstandingly in multiple tests, providing a new solution for video-language
understanding.",['cs.AI'],"['Cheng Li', 'Jiexiong Liu', 'Yixuan Chen', 'Yanqin Jia']",2025-03-20,2025-03-20
2503.15804v1,Communication Efficient Federated Learning with Linear Convergence on Heterogeneous Data,"By letting local clients perform multiple local updates before communicating
with a parameter server, modern federated learning algorithms such as FedAvg
tackle the communication bottleneck problem in distributed learning and have
found many successful applications. However, this asynchrony between local
updates and communication also leads to a ''client-drift'' problem when the
data is heterogeneous (not independent and identically distributed), resulting
in errors in the final learning result. In this paper, we propose a federated
learning algorithm, which is called FedCET, to ensure accurate convergence even
under heterogeneous distributions of data across clients. Inspired by the
distributed optimization algorithm NIDS, we use learning rates to weight
information received from local clients to eliminate the ''client-drift''. We
prove that under appropriate learning rates, FedCET can ensure linear
convergence to the exact solution. Different from existing algorithms which
have to share both gradients and a drift-correction term to ensure accurate
convergence under heterogeneous data distributions, FedCET only shares one
variable, which significantly reduces communication overhead. Numerical
comparison with existing counterpart algorithms confirms the effectiveness of
FedCET.","['cs.LG', 'math.OC']","['Jie Liu', 'Yongqiang Wang']",2025-03-20,2025-03-20
2503.15801v1,Disentangling Uncertainties by Learning Compressed Data Representation,"We study aleatoric and epistemic uncertainty estimation in a learned
regressive system dynamics model. Disentangling aleatoric uncertainty (the
inherent randomness of the system) from epistemic uncertainty (the lack of
data) is crucial for downstream tasks such as risk-aware control and
reinforcement learning, efficient exploration, and robust policy transfer.
While existing approaches like Gaussian Processes, Bayesian networks, and model
ensembles are widely adopted, they suffer from either high computational
complexity or inaccurate uncertainty estimation. To address these limitations,
we propose the Compressed Data Representation Model (CDRM), a framework that
learns a neural network encoding of the data distribution and enables direct
sampling from the output distribution. Our approach incorporates a novel
inference procedure based on Langevin dynamics sampling, allowing CDRM to
predict arbitrary output distributions rather than being constrained to a
Gaussian prior. Theoretical analysis provides the conditions where CDRM
achieves better memory and computational complexity compared to bin-based
compression methods. Empirical evaluations show that CDRM demonstrates a
superior capability to identify aleatoric and epistemic uncertainties
separately, achieving AUROCs of 0.8876 and 0.9981 on a single test set
containing a mixture of both uncertainties. Qualitative results further show
that CDRM's capability extends to datasets with multimodal output
distributions, a challenging scenario where existing methods consistently fail.
Code and supplementary materials are available at
https://github.com/ryeii/CDRM.",['cs.LG'],"['Zhiyu An', 'Zhibo Hou', 'Wan Du']",2025-03-20,2025-03-20
2503.15800v1,Frequency Enhancement for Image Demosaicking,"Recovering high-frequency textures in image demosaicking remains a
challenging issue. While existing methods introduced elaborate spatial learning
methods, they still exhibit limited performance. To address this issue, a
frequency enhancement approach is proposed. Based on the frequency analysis of
color filter array (CFA)/demosaicked/ground truth images, we propose Dual-path
Frequency Enhancement Network (DFENet), which reconstructs RGB images in a
divide-and-conquer manner through fourier-domain frequency selection. In
DFENet, two frequency selectors are employed, each selecting a set of frequency
components for processing along separate paths. One path focuses on generating
missing information through detail refinement in spatial domain, while the
other aims at suppressing undesirable frequencies with the guidance of CFA
images in frequency domain. Multi-level frequency supervision with a stagewise
training strategy is employed to further improve the reconstruction
performance. With these designs, the proposed DFENet outperforms other
state-of-the-art algorithms on different datasets and demonstrates significant
advantages on hard cases. Moreover, to better assess algorithms' ability to
reconstruct high-frequency textures, a new dataset, LineSet37, is contributed,
which consists of 37 artificially designed and generated images. These images
feature complex line patterns and are prone to severe visual artifacts like
color moir\'e after demosaicking. Experiments on LineSet37 offer a more
targeted evaluation of performance on challenging cases. The code and dataset
are available at https://github.com/VelvetReverie/DFENet-demosaicking.",['cs.CV'],"['Jingyun Liu', 'Daiqin Yang', 'Zhenzhong Chen']",2025-03-20,2025-03-20
2503.15798v1,Mixture of Lookup Experts,"Mixture-of-Experts (MoE) activates only a subset of experts during inference,
allowing the model to maintain low inference FLOPs and latency even as the
parameter count scales up. However, since MoE dynamically selects the experts,
all the experts need to be loaded into VRAM. Their large parameter size still
limits deployment, and offloading, which load experts into VRAM only when
needed, significantly increase inference latency. To address this, we propose
Mixture of Lookup Experts (MoLE), a new MoE architecture that is efficient in
both communication and VRAM usage. In MoLE, the experts are Feed-Forward
Networks (FFNs) during training, taking the output of the embedding layer as
input. Before inference, these experts can be re-parameterized as lookup tables
(LUTs) that retrieves expert outputs based on input ids, and offloaded to
storage devices. Therefore, we do not need to perform expert computations
during inference. Instead, we directly retrieve the expert's computation
results based on input ids and load them into VRAM, and thus the resulting
communication overhead is negligible. Experiments show that, with the same
FLOPs and VRAM usage, MoLE achieves inference speeds comparable to dense models
and significantly faster than MoE with experts offloading, while maintaining
performance on par with MoE.","['cs.LG', 'cs.CL']","['Shibo Jie', 'Yehui Tang', 'Kai Han', 'Yitong Li', 'Duyu Tang', 'Zhi-Hong Deng', 'Yunhe Wang']",2025-03-20,2025-03-20
2503.15796v1,Blend the Separated: Mixture of Synergistic Experts for Data-Scarcity Drug-Target Interaction Prediction,"Drug-target interaction prediction (DTI) is essential in various applications
including drug discovery and clinical application. There are two perspectives
of input data widely used in DTI prediction: Intrinsic data represents how
drugs or targets are constructed, and extrinsic data represents how drugs or
targets are related to other biological entities. However, any of the two
perspectives of input data can be scarce for some drugs or targets, especially
for those unpopular or newly discovered. Furthermore, ground-truth labels for
specific interaction types can also be scarce. Therefore, we propose the first
method to tackle DTI prediction under input data and/or label scarcity. To make
our model functional when only one perspective of input data is available, we
design two separate experts to process intrinsic and extrinsic data
respectively and fuse them adaptively according to different samples.
Furthermore, to make the two perspectives complement each other and remedy
label scarcity, two experts synergize with each other in a mutually supervised
way to exploit the enormous unlabeled data. Extensive experiments on 3
real-world datasets under different extents of input data scarcity and/or label
scarcity demonstrate our model outperforms states of the art significantly and
steadily, with a maximum improvement of 53.53%. We also test our model without
any data scarcity and it still outperforms current methods.","['cs.LG', 'cs.AI']","['Xinlong Zhai', 'Chunchen Wang', 'Ruijia Wang', 'Jiazheng Kang', 'Shujie Li', 'Boyu Chen', 'Tengfei Ma', 'Zikai Zhou', 'Cheng Yang', 'Chuan Shi']",2025-03-20,2025-03-20
2503.15793v1,DNA Bench: When Silence is Smarter -- Benchmarking Over-Reasoning in Reasoning LLMs,"Test-time scaling has significantly improved large language model
performance, enabling deeper reasoning to solve complex problems. However, this
increased reasoning capability also leads to excessive token generation and
unnecessary problem-solving attempts. We introduce Don\'t Answer Bench (DNA
Bench), a new benchmark designed to evaluate LLMs ability to robustly
understand the tricky reasoning triggers and avoiding unnecessary generation.
DNA Bench consists of 150 adversarially designed prompts that are easy for
humans to understand and respond to, but surprisingly not for many of the
recent prominent LLMs. DNA Bench tests models abilities across different
capabilities, such as instruction adherence, hallucination avoidance,
redundancy filtering, and unanswerable question recognition. We evaluate
reasoning LLMs (RLMs), including DeepSeek-R1, OpenAI O3-mini, Claude-3.7-sonnet
and compare them against a powerful non-reasoning model, e.g., GPT-4o. Our
experiments reveal that RLMs generate up to 70x more tokens than necessary,
often failing at tasks that simpler non-reasoning models handle efficiently
with higher accuracy. Our findings underscore the need for more effective
training and inference strategies in RLMs.",['cs.LG'],"['Masoud Hashemi', 'Oluwanifemi Bamgbose', 'Sathwik Tejaswi Madhusudhan', 'Jishnu Sethumadhavan Nair', 'Aman Tiwari', 'Vikas Yadav']",2025-03-20,2025-03-20
2503.15784v1,RL4Med-DDPO: Reinforcement Learning for Controlled Guidance Towards Diverse Medical Image Generation using Vision-Language Foundation Models,"Vision-Language Foundation Models (VLFM) have shown a tremendous increase in
performance in terms of generating high-resolution, photorealistic natural
images. While VLFMs show a rich understanding of semantic content across
modalities, they often struggle with fine-grained alignment tasks that require
precise correspondence between image regions and textual descriptions a
limitation in medical imaging, where accurate localization and detection of
clinical features are essential for diagnosis and analysis. To address this
issue, we propose a multi-stage architecture where a pre-trained VLFM provides
a cursory semantic understanding, while a reinforcement learning (RL) algorithm
refines the alignment through an iterative process that optimizes for
understanding semantic context. The reward signal is designed to align the
semantic information of the text with synthesized images. We demonstrate the
effectiveness of our method on a medical imaging skin dataset where the
generated images exhibit improved generation quality and alignment with prompt
over the fine-tuned Stable Diffusion. We also show that the synthesized samples
could be used to improve disease classifier performance for underrepresented
subgroups through augmentation.",['cs.CV'],"['Parham Saremi', 'Amar Kumar', 'Mohammed Mohammed', 'Zahra TehraniNasab', 'Tal Arbel']",2025-03-20,2025-03-20
2503.15783v1,Grammar and Gameplay-aligned RL for Game Description Generation with LLMs,"Game Description Generation (GDG) is the task of generating a game
description written in a Game Description Language (GDL) from natural language
text. Previous studies have explored generation methods leveraging the
contextual understanding capabilities of Large Language Models (LLMs); however,
accurately reproducing the game features of the game descriptions remains a
challenge. In this paper, we propose reinforcement learning-based fine-tuning
of LLMs for GDG (RLGDG). Our training method simultaneously improves
grammatical correctness and fidelity to game concepts by introducing both
grammar rewards and concept rewards. Furthermore, we adopt a two-stage training
strategy where Reinforcement Learning (RL) is applied following Supervised
Fine-Tuning (SFT). Experimental results demonstrate that our proposed method
significantly outperforms baseline methods using SFT alone.","['cs.CL', 'cs.AI']","['Tsunehiko Tanaka', 'Edgar Simo-Serra']",2025-03-20,2025-03-20
2503.15781v1,UAS Visual Navigation in Large and Unseen Environments via a Meta Agent,"The aim of this work is to develop an approach that enables Unmanned Aerial
System (UAS) to efficiently learn to navigate in large-scale urban environments
and transfer their acquired expertise to novel environments. To achieve this,
we propose a meta-curriculum training scheme. First, meta-training allows the
agent to learn a master policy to generalize across tasks. The resulting model
is then fine-tuned on the downstream tasks. We organize the training curriculum
in a hierarchical manner such that the agent is guided from coarse to fine
towards the target task. In addition, we introduce Incremental Self-Adaptive
Reinforcement learning (ISAR), an algorithm that combines the ideas of
incremental learning and meta-reinforcement learning (MRL). In contrast to
traditional reinforcement learning (RL), which focuses on acquiring a policy
for a specific task, MRL aims to learn a policy with fast transfer ability to
novel tasks. However, the MRL training process is time consuming, whereas our
proposed ISAR algorithm achieves faster convergence than the conventional MRL
algorithm. We evaluate the proposed methodologies in simulated environments and
demonstrate that using this training philosophy in conjunction with the ISAR
algorithm significantly improves the convergence speed for navigation in
large-scale cities and the adaptation proficiency in novel environments.","['cs.RO', 'cs.CV']","['Yuci Han', 'Charles Toth', 'Alper Yilmaz']",2025-03-20,2025-03-20
2503.15779v1,MobiFuse: Learning Universal Human Mobility Patterns through Cross-domain Data Fusion,"Human mobility modeling is critical for urban planning and transportation
management, yet existing datasets often lack the resolution and semantic
richness required for comprehensive analysis. To address this, we proposed a
cross-domain data fusion framework that integrates multi-modal data of distinct
nature and spatio-temporal resolution, including geographical, mobility,
socio-demographic, and traffic information, to construct a privacy-preserving
and semantically enriched human travel trajectory dataset. This framework is
demonstrated through two case studies in Los Angeles (LA) and Egypt, where a
domain adaptation algorithm ensures its transferability across diverse urban
contexts. Quantitative evaluation shows that the generated synthetic dataset
accurately reproduces mobility patterns observed in empirical data. Moreover,
large-scale traffic simulations for LA County based on the generated synthetic
demand align well with observed traffic. On California's I-405 corridor, the
simulation yields a Mean Absolute Percentage Error of 5.85% for traffic volume
and 4.36% for speed compared to Caltrans PeMS observations.","['cs.LG', 'cs.AI']","['Haoxuan Ma', 'Xishun Liao', 'Yifan Liu', 'Qinhua Jiang', 'Chris Stanford', 'Shangqing Cao', 'Jiaqi Ma']",2025-03-20,2025-03-20
2503.15778v1,AutoDrive-QA- Automated Generation of Multiple-Choice Questions for Autonomous Driving Datasets Using Large Vision-Language Models,"In autonomous driving, open-ended question answering often suffers from
unreliable evaluations because freeform responses require either complex
metrics or subjective human judgment. To address this challenge, we introduce
AutoDrive-QA, an automatic pipeline that converts existing driving QA datasets
(including DriveLM, NuScenes-QA, and LingoQA) into a structured multiple-choice
question (MCQ) format. This benchmark systematically assesses perception,
prediction, and planning tasks, providing a standardized and objective
evaluation framework. AutoDrive-QA employs an automated pipeline that leverages
large language models (LLMs) to generate high-quality, contextually relevant
distractors based on domain-specific error patterns commonly found in
autonomous driving scenarios. To evaluate both general capabilities and
generalization performance, we test the benchmark on three public datasets and
conduct zero-shot experiments on an unseen dataset. The zero-shot evaluations
reveal that GPT-4V leads with 69.57% accuracy -- achieving 74.94% in
Perception, 65.33% in Prediction, and 68.45% in Planning -- demonstrating that
while all models excel in Perception, they struggle in Prediction.
Consequently, AutoDrive-QA establishes a rigorous, unbiased standard for
integrating and evaluating different vision-language models across various
autonomous driving datasets, thereby improving generalization in this field. We
release all the codes in the AutoDrive-QA GitHub Repository.","['cs.CV', 'cs.RO']","['Boshra Khalili', 'Andrew W. Smyth']",2025-03-20,2025-03-20
2503.15777v1,Line Space Clustering (LSC): Feature-Based Clustering using K-medians and Dynamic Time Warping for Versatility,"Clustering high-dimensional data is a critical challenge in machine learning
due to the curse of dimensionality and the presence of noise. Traditional
clustering algorithms often fail to capture the intrinsic structures in such
data. This paper explores a combination of clustering methods, which we called
Line Space Clustering (LSC), a representation that transforms data points into
lines in a newly defined feature space, enabling clustering based on the
similarity of feature value patterns, essentially treating features as
sequences. LSC employs a combined distance metric that uses Euclidean and
Dynamic Time Warping (DTW) distances, weighted by a parameter {\alpha},
allowing flexibility in emphasizing shape or magnitude similarities. We delve
deeply into the mechanics of DTW and the Savitzky Golay filter, explaining
their roles in the algorithm. Extensive experiments demonstrate the efficacy of
LSC on synthetic and real-world datasets, showing that randomly experimenting
with time-series optimized methods sometimes might surprisingly work on a
complex dataset, particularly in noisy environments.
  Source code and experiments are available at:
https://github.com/JoanikijChulev/LSC.",['cs.LG'],"['Joanikij Chulev', 'Angela Mladenovska']",2025-03-20,2025-03-20
2503.15772v1,Detecting LLM-Written Peer Reviews,"Editors of academic journals and program chairs of conferences require peer
reviewers to write their own reviews. However, there is growing concern about
the rise of lazy reviewing practices, where reviewers use large language models
(LLMs) to generate reviews instead of writing them independently. Existing
tools for detecting LLM-generated content are not designed to differentiate
between fully LLM-generated reviews and those merely polished by an LLM. In
this work, we employ a straightforward approach to identify LLM-generated
reviews - doing an indirect prompt injection via the paper PDF to ask the LLM
to embed a watermark. Our focus is on presenting watermarking schemes and
statistical tests that maintain a bounded family-wise error rate, when a venue
evaluates multiple reviews, with a higher power as compared to standard methods
like Bonferroni correction. These guarantees hold without relying on any
assumptions about human-written reviews. We also consider various methods for
prompt injection including font embedding and jailbreaking. We evaluate the
effectiveness and various tradeoffs of these methods, including different
reviewer defenses. We find a high success rate in the embedding of our
watermarks in LLM-generated reviews across models. We also find that our
approach is resilient to common reviewer defenses, and that the bounds on error
rates in our statistical tests hold in practice while having the power to flag
LLM-generated reviews, while Bonferroni correction is infeasible.","['cs.DL', 'cs.AI', 'cs.CR']","['Vishisht Rao', 'Aounon Kumar', 'Himabindu Lakkaraju', 'Nihar B. Shah']",2025-03-20,2025-03-20
2503.15770v1,Nano-3D: Metasurface-Based Neural Depth Imaging,"Depth imaging is a foundational building block for broad applications, such
as autonomous driving and virtual/augmented reality. Traditionally, depth
cameras have relied on time-of-flight sensors or multi-lens systems to achieve
physical depth measurements. However, these systems often face a trade-off
between a bulky form factor and imprecise approximations, limiting their
suitability for spatially constrained scenarios. Inspired by the emerging
advancements of nano-optics, we present Nano-3D, a metasurface-based neural
depth imaging solution with an ultra-compact footprint. Nano-3D integrates our
custom-fabricated 700 nm thick TiO2 metasurface with a multi-module deep neural
network to extract precise metric depth information from monocular
metasurface-polarized imagery. We demonstrate the effectiveness of Nano-3D with
both simulated and physical experiments. We hope the exhibited success paves
the way for the community to bridge future graphics systems with emerging
nanomaterial technologies through novel computational approaches.","['physics.optics', 'cs.AR', 'cs.CV']","['Bingxuan Li', 'Jiahao Wu', 'Yuan Xu', 'Yunxiang Zhang', 'Zezheng Zhu', 'Nanfang Yu', 'Qi Sun']",2025-03-20,2025-03-20
2503.15769v1,Prediction of Permissioned Blockchain Performance for Resource Scaling Configurations,"Blockchain is increasingly offered as blockchain-as-a-service (BaaS) by cloud
service providers. However, configuring BaaS appropriately for optimal
performance and reliability resorts to try-and-error. A key challenge is that
BaaS is often perceived as a ``black-box,'' leading to uncertainties in
performance and resource provisioning. Previous studies attempted to address
this challenge; however, the impacts of both vertical and horizontal scaling
remain elusive. To this end, we present machine learning-based models to
predict network reliability and throughput based on scaling configurations. In
our evaluation, the models exhibit prediction errors of ~1.9%, which is highly
accurate and can be applied in the real-world.","['cs.DC', 'cs.LG', 'cs.SY', 'eess.SY']","['Seungwoo Jung', 'Yeonho Yoo', 'Gyeongsik Yang', 'Chuck Yoo']",2025-03-20,2025-03-20
2503.15768v1,Can one size fit all?: Measuring Failure in Multi-Document Summarization Domain Transfer,"Abstractive multi-document summarization (MDS) is the task of automatically
summarizing information in multiple documents, from news articles to
conversations with multiple speakers. The training approaches for current MDS
models can be grouped into four approaches: end-to-end with special
pre-training (""direct""), chunk-then-summarize, extract-then-summarize, and
inference with GPT-style models. In this work, we evaluate MDS models across
training approaches, domains, and dimensions (reference similarity, quality,
and factuality), to analyze how and why models trained on one domain can fail
to summarize documents from another (News, Science, and Conversation) in the
zero-shot domain transfer setting. We define domain-transfer ""failure"" as a
decrease in factuality, higher deviation from the target, and a general
decrease in summary quality. In addition to exploring domain transfer for MDS
models, we examine potential issues with applying popular summarization metrics
out-of-the-box.","['cs.CL', 'cs.AI']","['Alexandra DeLucia', 'Mark Dredze']",2025-03-20,2025-03-20
2503.16558v1,Advancing Problem-Based Learning in Biomedical Engineering in the Era of Generative AI,"Problem-Based Learning (PBL) has significantly impacted biomedical
engineering (BME) education since its introduction in the early 2000s,
effectively enhancing critical thinking and real-world knowledge application
among students. With biomedical engineering rapidly converging with artificial
intelligence (AI), integrating effective AI education into established
curricula has become challenging yet increasingly necessary. Recent
advancements, including AI's recognition by the 2024 Nobel Prize, have
highlighted the importance of training students comprehensively in biomedical
AI. However, effective biomedical AI education faces substantial obstacles,
such as diverse student backgrounds, limited personalized mentoring,
constrained computational resources, and difficulties in safely scaling
hands-on practical experiments due to privacy and ethical concerns associated
with biomedical data. To overcome these issues, we conducted a three-year
(2021-2023) case study implementing an advanced PBL framework tailored
specifically for biomedical AI education, involving 92 undergraduate and 156
graduate students from the joint Biomedical Engineering program of Georgia
Institute of Technology and Emory University. Our approach emphasizes
collaborative, interdisciplinary problem-solving through authentic biomedical
AI challenges. The implementation led to measurable improvements in learning
outcomes, evidenced by high research productivity (16 student-authored
publications), consistently positive peer evaluations, and successful
development of innovative computational methods addressing real biomedical
challenges. Additionally, we examined the role of generative AI both as a
teaching subject and an educational support tool within the PBL framework. Our
study presents a practical and scalable roadmap for biomedical engineering
departments aiming to integrate robust AI education into their curricula.","['cs.CY', 'cs.AI']","['Micky C. Nnamdi', 'J. Ben Tamo', 'Wenqi Shi', 'May D. Wang']",2025-03-20,2025-03-20
2503.15766v1,Accelerating Transient CFD through Machine Learning-Based Flow Initialization,"Transient computational fluid dynamics (CFD) simulations are essential for
many industrial applications, but a significant portion of their computational
cost stems from the time needed to reach statistical steadiness from initial
conditions. We present a novel machine learning-based initialization method
that reduces the cost of this subsequent transient solve substantially,
achieving a 50% reduction in time-to-convergence compared to traditional
uniform and potential flow-based initializations. Through a case study in
automotive aerodynamics using a 16.7M-cell unsteady RANS simulation, we
evaluate three ML-based initialization strategies. Two of these strategies are
recommended for general use: (1) a physics-informed hybrid method combining ML
predictions with potential flow solutions, and (2) a more versatile approach
integrating ML predictions with uniform flow. Both strategies enable CFD
solvers to achieve convergence times comparable to computationally expensive
steady RANS initializations, while requiring only seconds of computation. We
develop a robust statistical convergence metric based on windowed
time-averaging for performance comparison between initialization strategies.
Notably, these improvements are achieved using an ML model trained on a
different dataset of automotive geometries, demonstrating strong generalization
capabilities. The proposed methods integrate seamlessly with existing CFD
workflows without requiring modifications to the underlying flow solver,
providing a practical approach to accelerating industrial CFD simulations
through improved ML-based initialization strategies.","['cs.LG', 'physics.flu-dyn']","['Peter Sharpe', 'Rishikesh Ranade', 'Sanjay Choudhry']",2025-03-20,2025-03-20
2503.15764v1,Towards Agentic AI Networking in 6G: A Generative Foundation Model-as-Agent Approach,"The promising potential of AI and network convergence in improving networking
performance and enabling new service capabilities has recently attracted
significant interest. Existing network AI solutions, while powerful, are mainly
built based on the close-loop and passive learning framework, resulting in
major limitations in autonomous solution finding and dynamic environmental
adaptation. Agentic AI has recently been introduced as a promising solution to
address the above limitations and pave the way for true generally intelligent
and beneficial AI systems. The key idea is to create a networking ecosystem to
support a diverse range of autonomous and embodied AI agents in fulfilling
their goals. In this paper, we focus on the novel challenges and requirements
of agentic AI networking. We propose AgentNet, a novel framework for supporting
interaction, collaborative learning, and knowledge transfer among AI agents. We
introduce a general architectural framework of AgentNet and then propose a
generative foundation model (GFM)-based implementation in which multiple
GFM-as-agents have been created as an interactive knowledge-base to bootstrap
the development of embodied AI agents according to different task requirements
and environmental features. We consider two application scenarios,
digital-twin-based industrial automation and metaverse-based infotainment
system, to describe how to apply AgentNet for supporting efficient task-driven
collaboration and interaction among AI agents.","['cs.NI', 'cs.AI']","['Yong Xiao', 'Guangming Shi', 'Ping Zhang']",2025-03-20,2025-03-20
2503.15763v1,OffsetOPT: Explicit Surface Reconstruction without Normals,"Neural surface reconstruction has been dominated by implicit representations
with marching cubes for explicit surface extraction. However, those methods
typically require high-quality normals for accurate reconstruction. We propose
OffsetOPT, a method that reconstructs explicit surfaces directly from 3D point
clouds and eliminates the need for point normals. The approach comprises two
stages: first, we train a neural network to predict surface triangles based on
local point geometry, given uniformly distributed training point clouds. Next,
we apply the frozen network to reconstruct surfaces from unseen point clouds by
optimizing a per-point offset to maximize the accuracy of triangle predictions.
Compared to state-of-the-art methods, OffsetOPT not only excels at
reconstructing overall surfaces but also significantly preserves sharp surface
features. We demonstrate its accuracy on popular benchmarks, including
small-scale shapes and large-scale open surfaces.",['cs.CV'],['Huan Lei'],2025-03-20,2025-03-20
2503.15762v1,Dialogic Learning in Child-Robot Interaction: A Hybrid Approach to Personalized Educational Content Generation,"Dialogic learning fosters motivation and deeper understanding in education
through purposeful and structured dialogues. Foundational models offer a
transformative potential for child-robot interactions, enabling the design of
personalized, engaging, and scalable interactions. However, their integration
into educational contexts presents challenges in terms of ensuring
age-appropriate and safe content and alignment with pedagogical goals. We
introduce a hybrid approach to designing personalized educational dialogues in
child-robot interactions. By combining rule-based systems with LLMs for
selective offline content generation and human validation, the framework
ensures educational quality and developmental appropriateness. We illustrate
this approach through a project aimed at enhancing reading motivation, in which
a robot facilitated book-related dialogues.",['cs.AI'],"['Elena Malnatsky', 'Shenghui Wang', 'Koen V. Hindriks', 'Mike E. U. Ligthart']",2025-03-20,2025-03-20
2503.15761v1,GraPLUS: Graph-based Placement Using Semantics for Image Composition,"We present GraPLUS (Graph-based Placement Using Semantics), a novel framework
for plausible object placement in images that leverages scene graphs and large
language models. Our approach uniquely combines graph-structured scene
representation with semantic understanding to determine contextually
appropriate object positions. The framework employs GPT-2 to transform
categorical node and edge labels into rich semantic embeddings that capture
both definitional characteristics and typical spatial contexts, enabling
nuanced understanding of object relationships and placement patterns. GraPLUS
achieves placement accuracy of 92.1% and an FID score of 28.83 on the OPA
dataset, outperforming state-of-the-art methods by 8.1% while maintaining
competitive visual quality. In human evaluation studies involving 964 samples
assessed by 19 participants, our method was preferred in 52.1% of cases,
significantly outperforming previous approaches. The framework's key
innovations include: (i) leveraging pre-trained scene graph models that
transfer knowledge from other domains, (ii) edge-aware graph neural networks
that process scene semantics through structured relationships, (iii) a
cross-modal attention mechanism that aligns categorical embeddings with
enhanced scene features, and (iv) a multiobjective training strategy
incorporating semantic consistency constraints.",['cs.CV'],"['Mir Mohammad Khaleghi', 'Mehran Safayani', 'Abdolreza Mirzaei']",2025-03-20,2025-03-20
2503.15758v1,ATTENTION2D: Communication Efficient Distributed Self-Attention Mechanism,"Transformer-based models have emerged as a leading architecture for natural
language processing, natural language generation, and image generation tasks. A
fundamental element of the transformer architecture is self-attention, which
allows the model to capture intricate dependencies within the data. However,
the self-attention mechanism also incurs significant computational and memory
costs, particularly for long sequences.
  In this paper, we introduce ATTENTION2D, a novel approach that exploits
parallelism along two dimensions - query and key/value - of the self-attention
operation. This method enables efficient distribution and parallelization of
computations across multiple devices. Our approach facilitates asymptotically
faster training and inference phases compared to previous methods, without
relying on approximations or incurring additional computational or memory
overheads. Furthermore, unlike existing techniques that struggle to scale with
an increasing number of processing units, our approach effectively scales with
additional processing units.
  Our experimental results confirm the effectiveness of our method in improving
communication efficiency and scalability. Compared to Ring Attention, our
approach demonstrated up to a 5x performance boost on a GPT-3-like model using
64 NVIDIA A100 GPUs across 16 nodes, and up to a 9.4x performance boost on 64
NVIDIA H100 GPUs across 64 nodes.","['cs.LG', 'cs.AI', 'cs.DC']",['Venmugil Elango'],2025-03-20,2025-03-20
2503.15754v1,AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration,"As large language models (LLMs) become increasingly capable, security and
safety evaluation are crucial. While current red teaming approaches have made
strides in assessing LLM vulnerabilities, they often rely heavily on human
input and lack comprehensive coverage of emerging attack vectors. This paper
introduces AutoRedTeamer, a novel framework for fully automated, end-to-end red
teaming against LLMs. AutoRedTeamer combines a multi-agent architecture with a
memory-guided attack selection mechanism to enable continuous discovery and
integration of new attack vectors. The dual-agent framework consists of a red
teaming agent that can operate from high-level risk categories alone to
generate and execute test cases and a strategy proposer agent that autonomously
discovers and implements new attacks by analyzing recent research. This modular
design allows AutoRedTeamer to adapt to emerging threats while maintaining
strong performance on existing attack vectors. We demonstrate AutoRedTeamer's
effectiveness across diverse evaluation settings, achieving 20% higher attack
success rates on HarmBench against Llama-3.1-70B while reducing computational
costs by 46% compared to existing approaches. AutoRedTeamer also matches the
diversity of human-curated benchmarks in generating test cases, providing a
comprehensive, scalable, and continuously evolving framework for evaluating the
security of AI systems.","['cs.CR', 'cs.AI']","['Andy Zhou', 'Kevin Wu', 'Francesco Pinto', 'Zhaorun Chen', 'Yi Zeng', 'Yu Yang', 'Shuang Yang', 'Sanmi Koyejo', 'James Zou', 'Bo Li']",2025-03-20,2025-03-20
2503.15752v1,Using Language Models to Decipher the Motivation Behind Human Behaviors,"AI presents a novel tool for deciphering the motivations behind human
behaviors. We show that by varying prompts to a large language model, we can
elicit a full range of human behaviors in a variety of different scenarios in
terms of classic economic games. Then by analyzing which prompts are needed to
elicit which behaviors, we can infer (decipher) the motivations behind the
human behaviors. We also show how one can analyze the prompts to reveal
relationships between the classic economic games, providing new insight into
what different economic scenarios induce people to think about. We also show
how this deciphering process can be used to understand differences in the
behavioral tendencies of different populations.",['cs.AI'],"['Yutong Xie', 'Qiaozhu Mei', 'Walter Yuan', 'Matthew O. Jackson']",2025-03-20,2025-03-20
2503.15748v1,PARQ: Piecewise-Affine Regularized Quantization,"We develop a principled method for quantization-aware training (QAT) of
large-scale machine learning models. Specifically, we show that convex,
piecewise-affine regularization (PAR) can effectively induce the model
parameters to cluster towards discrete values. We minimize PAR-regularized loss
functions using an aggregate proximal stochastic gradient method (AProx) and
prove that it has last-iterate convergence. Our approach provides an
interpretation of the straight-through estimator (STE), a widely used heuristic
for QAT, as the asymptotic form of PARQ. We conduct experiments to demonstrate
that PARQ obtains competitive performance on convolution- and transformer-based
vision tasks.","['cs.LG', 'math.OC']","['Lisa Jin', 'Jianhao Ma', 'Zechun Liu', 'Andrey Gromov', 'Aaron Defazio', 'Lin Xiao']",2025-03-19,2025-03-19
2503.15742v1,Uncertainty-Aware Diffusion Guided Refinement of 3D Scenes,"Reconstructing 3D scenes from a single image is a fundamentally ill-posed
task due to the severely under-constrained nature of the problem. Consequently,
when the scene is rendered from novel camera views, existing single image to 3D
reconstruction methods render incoherent and blurry views. This problem is
exacerbated when the unseen regions are far away from the input camera. In this
work, we address these inherent limitations in existing single image-to-3D
scene feedforward networks. To alleviate the poor performance due to
insufficient information beyond the input image's view, we leverage a strong
generative prior in the form of a pre-trained latent video diffusion model, for
iterative refinement of a coarse scene represented by optimizable Gaussian
parameters. To ensure that the style and texture of the generated images align
with that of the input image, we incorporate on-the-fly Fourier-style transfer
between the generated images and the input image. Additionally, we design a
semantic uncertainty quantification module that calculates the per-pixel
entropy and yields uncertainty maps used to guide the refinement process from
the most confident pixels while discarding the remaining highly uncertain ones.
We conduct extensive experiments on real-world scene datasets, including
in-domain RealEstate-10K and out-of-domain KITTI-v2, showing that our approach
can provide more realistic and high-fidelity novel view synthesis results
compared to existing state-of-the-art methods.",['cs.CV'],"['Sarosij Bose', 'Arindam Dutta', 'Sayak Nag', 'Junge Zhang', 'Jiachen Li', 'Konstantinos Karydis', 'Amit K. Roy Chowdhury']",2025-03-19,2025-03-19
2503.15739v1,ECLAIR: Enhanced Clarification for Interactive Responses,"We present ECLAIR (Enhanced CLArification for Interactive Responses), a novel
unified and end-to-end framework for interactive disambiguation in enterprise
AI assistants. ECLAIR generates clarification questions for ambiguous user
queries and resolves ambiguity based on the user's response.We introduce a
generalized architecture capable of integrating ambiguity information from
multiple downstream agents, enhancing context-awareness in resolving
ambiguities and allowing enterprise specific definition of agents. We further
define agents within our system that provide domain-specific grounding
information. We conduct experiments comparing ECLAIR to few-shot prompting
techniques and demonstrate ECLAIR's superior performance in clarification
question generation and ambiguity resolution.","['cs.AI', '68T50', 'I.2.7; H.5.2']","['John Murzaku', 'Zifan Liu', 'Md Mehrab Tanjim', 'Vaishnavi Muppala', 'Xiang Chen', 'Yunyao Li']",2025-03-19,2025-03-19
2503.15737v1,KoGNER: A Novel Framework for Knowledge Graph Distillation on Biomedical Named Entity Recognition,"Named Entity Recognition (NER) is a fundamental task in Natural Language
Processing (NLP) that plays a crucial role in information extraction, question
answering, and knowledge-based systems. Traditional deep learning-based NER
models often struggle with domain-specific generalization and suffer from data
sparsity issues. In this work, we introduce Knowledge Graph distilled for Named
Entity Recognition (KoGNER), a novel approach that integrates Knowledge Graph
(KG) distillation into NER models to enhance entity recognition performance.
Our framework leverages structured knowledge representations from KGs to enrich
contextual embeddings, thereby improving entity classification and reducing
ambiguity in entity detection. KoGNER employs a two-step process: (1) Knowledge
Distillation, where external knowledge sources are distilled into a lightweight
representation for seamless integration with NER models, and (2) Entity-Aware
Augmentation, which integrates contextual embeddings that have been enriched
with knowledge graph information directly into GNN, thereby improving the
model's ability to understand and represent entity relationships. Experimental
results on benchmark datasets demonstrate that KoGNER achieves state-of-the-art
performance, outperforming finetuned NER models and LLMs by a significant
margin. These findings suggest that leveraging knowledge graphs as auxiliary
information can significantly improve NER accuracy, making KoGNER a promising
direction for future research in knowledge-aware NLP.",['cs.CL'],"['Heming Zhang', 'Wenyu Li', 'Di Huang', 'Yinjie Tang', 'Yixin Chen', 'Philip Payne', 'Fuhai Li']",2025-03-19,2025-03-19
2503.15731v1,Graph-Weighted Contrastive Learning for Semi-Supervised Hyperspectral Image Classification,"Most existing graph-based semi-supervised hyperspectral image classification
methods rely on superpixel partitioning techniques. However, they suffer from
misclassification of certain pixels due to inaccuracies in superpixel
boundaries, \ie, the initial inaccuracies in superpixel partitioning limit
overall classification performance. In this paper, we propose a novel
graph-weighted contrastive learning approach that avoids the use of superpixel
partitioning and directly employs neural networks to learn hyperspectral image
representation. Furthermore, while many approaches require all graph nodes to
be available during training, our approach supports mini-batch training by
processing only a subset of nodes at a time, reducing computational complexity
and improving generalization to unseen nodes. Experimental results on three
widely-used datasets demonstrate the effectiveness of the proposed approach
compared to baselines relying on superpixel partitioning.",['cs.CV'],"['Yuqing Zhang', 'Qi Han', 'Ligeng Wang', 'Kai Cheng', 'Bo Wang', 'Kun Zhan']",2025-03-19,2025-03-19
2503.15726v1,Reinforcement Learning Environment with LLM-Controlled Adversary in D&D 5th Edition Combat,"The objective of this study is to design and implement a reinforcement
learning (RL) environment using D\&D 5E combat scenarios to challenge smaller
RL agents through interaction with a robust adversarial agent controlled by
advanced Large Language Models (LLMs) like GPT-4o and LLaMA 3 8B. This research
employs Deep Q-Networks (DQN) for the smaller agents, creating a testbed for
strategic AI development that also serves as an educational tool by simulating
dynamic and unpredictable combat scenarios. We successfully integrated
sophisticated language models into the RL framework, enhancing strategic
decision-making processes. Our results indicate that while RL agents generally
outperform LLM-controlled adversaries in standard metrics, the strategic depth
provided by LLMs significantly enhances the overall AI capabilities in this
complex, rule-based setting. The novelty of our approach and its implications
for mastering intricate environments and developing adaptive strategies are
discussed, alongside potential innovations in AI-driven interactive
simulations. This paper aims to demonstrate how integrating LLMs can create
more robust and adaptable AI systems, providing valuable insights for further
research and educational applications.",['cs.AI'],"['Joseph Emmanuel DL Dayo', 'Michel Onasis S. Ogbinar', 'Prospero C. Naval Jr']",2025-03-19,2025-03-19
2503.15724v1,Reward Training Wheels: Adaptive Auxiliary Rewards for Robotics Reinforcement Learning,"Robotics Reinforcement Learning (RL) often relies on carefully engineered
auxiliary rewards to supplement sparse primary learning objectives to
compensate for the lack of large-scale, real-world, trial-and-error data. While
these auxiliary rewards accelerate learning, they require significant
engineering effort, may introduce human biases, and cannot adapt to the robot's
evolving capabilities during training. In this paper, we introduce Reward
Training Wheels (RTW), a teacher-student framework that automates auxiliary
reward adaptation for robotics RL. To be specific, the RTW teacher dynamically
adjusts auxiliary reward weights based on the student's evolving capabilities
to determine which auxiliary reward aspects require more or less emphasis to
improve the primary objective. We demonstrate RTW on two challenging robot
tasks: navigation in highly constrained spaces and off-road vehicle mobility on
vertically challenging terrain. In simulation, RTW outperforms expert-designed
rewards by 2.35% in navigation success rate and improves off-road mobility
performance by 122.62%, while achieving 35% and 3X faster training efficiency,
respectively. Physical robot experiments further validate RTW's effectiveness,
achieving a perfect success rate (5/5 trials vs. 2/5 for expert-designed
rewards) and improving vehicle stability with up to 47.4% reduction in
orientation angles.","['cs.RO', 'cs.AI']","['Linji Wang', 'Tong Xu', 'Yuanjie Lu', 'Xuesu Xiao']",2025-03-19,2025-03-19
2503.15718v1,Am I eligible? Natural Language Inference for Clinical Trial Patient Recruitment: the Patient's Point of View,"Recruiting patients to participate in clinical trials can be challenging and
time-consuming. Usually, participation in a clinical trial is initiated by a
healthcare professional and proposed to the patient. Promoting clinical trials
directly to patients via online recruitment might help to reach them more
efficiently. In this study, we address the case where a patient is initiating
their own recruitment process and wants to determine whether they are eligible
for a given clinical trial, using their own language to describe their medical
profile. To study whether this creates difficulties in the patient trial
matching process, we design a new dataset and task, Natural Language Inference
for Patient Recruitment (NLI4PR), in which patient language profiles must be
matched to clinical trials. We create it by adapting the TREC 2022 Clinical
Trial Track dataset, which provides patients' medical profiles, and rephrasing
them manually using patient language. We also use the associated clinical trial
reports where the patients are either eligible or excluded. We prompt several
open-source Large Language Models on our task and achieve from 56.5 to 71.8 of
F1 score using patient language, against 64.7 to 73.1 for the same task using
medical language. When using patient language, we observe only a small loss in
performance for the best model, suggesting that having the patient as a
starting point could be adopted to help recruit patients for clinical trials.
The corpus and code bases are all freely available on our Github and
HuggingFace repositories.",['cs.CL'],"['Mathilde Aguiar', 'Pierre Zweigenbaum', 'Nona Naderi']",2025-03-19,2025-03-19
2503.15712v1,SPNeRF: Open Vocabulary 3D Neural Scene Segmentation with Superpoints,"Open-vocabulary segmentation, powered by large visual-language models like
CLIP, has expanded 2D segmentation capabilities beyond fixed classes predefined
by the dataset, enabling zero-shot understanding across diverse scenes.
Extending these capabilities to 3D segmentation introduces challenges, as
CLIP's image-based embeddings often lack the geometric detail necessary for 3D
scene segmentation. Recent methods tend to address this by introducing
additional segmentation models or replacing CLIP with variations trained on
segmentation data, which lead to redundancy or loss on CLIP's general language
capabilities. To overcome this limitation, we introduce SPNeRF, a NeRF based
zero-shot 3D segmentation approach that leverages geometric priors. We
integrate geometric primitives derived from the 3D scene into NeRF training to
produce primitive-wise CLIP features, avoiding the ambiguity of point-wise
features. Additionally, we propose a primitive-based merging mechanism enhanced
with affinity scores. Without relying on additional segmentation models, our
method further explores CLIP's capability for 3D segmentation and achieves
notable improvements over original LERF.",['cs.CV'],"['Weiwen Hu', 'Niccolò Parodi', 'Marcus Zepp', 'Ingo Feldmann', 'Oliver Schreer', 'Peter Eisert']",2025-03-19,2025-03-19
2503.15708v1,Sustainable Deep Learning-Based Breast Lesion Segmentation: Impact of Breast Region Segmentation on Performance,"Purpose: Segmentation of the breast lesion in dynamic contrast-enhanced
magnetic resonance imaging (DCE-MRI) is an essential step to accurately
diagnose and plan treatment and monitor progress. This study aims to highlight
the impact of breast region segmentation (BRS) on deep learning-based breast
lesion segmentation (BLS) in breast DCE-MRI.
  Methods Using the Stavanger Dataset containing primarily 59 DCE-MRI scans and
UNet++ as deep learning models, four different process were conducted to
compare effect of BRS on BLS. These four approaches included the whole volume
without BRS and with BRS, BRS with the selected lesion slices and lastly
optimal volume with BRS. Preprocessing methods like augmentation and
oversampling were used to enhance the small dataset, data shape uniformity and
improve model performance. Optimal volume size were investigated by a precise
process to ensure that all lesions existed in slices. To evaluate the model, a
hybrid loss function including dice, focal and cross entropy along with 5-fold
cross validation method were used and lastly a test dataset which was randomly
split used to evaluate the model performance on unseen data for each of four
mentioned approaches.
  Results Results demonstrate that using BRS considerably improved model
performance and validation. Significant improvement in last approach -- optimal
volume with BRS -- compared to the approach without BRS counting around 50
percent demonstrating how effective BRS has been in BLS. Moreover, huge
improvement in energy consumption, decreasing up to 450 percent, introduces a
green solution toward a more environmentally sustainable approach for future
work on large dataset.","['cs.CV', 'physics.med-ph']","['Sam Narimani', 'Solveig Roth Hoff', 'Kathinka Dahli Kurz', 'Kjell-Inge Gjesdal', 'Jurgen Geisler', 'Endre Grovik']",2025-03-19,2025-03-19
2503.15707v1,Safety Aware Task Planning via Large Language Models in Robotics,"The integration of large language models (LLMs) into robotic task planning
has unlocked better reasoning capabilities for complex, long-horizon workflows.
However, ensuring safety in LLM-driven plans remains a critical challenge, as
these models often prioritize task completion over risk mitigation. This paper
introduces SAFER (Safety-Aware Framework for Execution in Robotics), a
multi-LLM framework designed to embed safety awareness into robotic task
planning. SAFER employs a Safety Agent that operates alongside the primary task
planner, providing safety feedback. Additionally, we introduce LLM-as-a-Judge,
a novel metric leveraging LLMs as evaluators to quantify safety violations
within generated task plans. Our framework integrates safety feedback at
multiple stages of execution, enabling real-time risk assessment, proactive
error correction, and transparent safety evaluation. We also integrate a
control framework using Control Barrier Functions (CBFs) to ensure safety
guarantees within SAFER's task planning. We evaluated SAFER against
state-of-the-art LLM planners on complex long-horizon tasks involving
heterogeneous robotic agents, demonstrating its effectiveness in reducing
safety violations while maintaining task efficiency. We also verify the task
planner and safety planner through actual hardware experiments involving
multiple robots and a human.","['cs.RO', 'cs.AI']","['Azal Ahmad Khan', 'Michael Andrev', 'Muhammad Ali Murtaza', 'Sergio Aguilera', 'Rui Zhang', 'Jie Ding', 'Seth Hutchinson', 'Ali Anwar']",2025-03-19,2025-03-19
2503.15706v1,Using machine learning to map simulated noisy and laser-limited multidimensional spectra to molecular electronic couplings,"Two-dimensional electronic spectroscopy (2DES) has enabled significant
discoveries in both biological and synthetic energy-transducing systems.
Although deriving chemical information from 2DES is a complex task, machine
learning (ML) offers exciting opportunities to translate complicated
spectroscopic data into physical insight. Recent studies have found that neural
networks (NNs) can map simulated multidimensional spectra to molecular-scale
properties with high accuracy. However, simulations often do not capture
experimental factors that influence real spectra, including noise and
suboptimal pulse resonance conditions, bringing into question the experimental
utility of NNs trained on simulated data. Here, we show how factors associated
with experimental 2D spectral data influence the ability of NNs to map
simulated 2DES spectra onto underlying intermolecular electronic couplings. By
systematically introducing multisourced noise into a library of 356000
simulated 2D spectra, we show that noise does not hamper NN performance for
spectra exceeding threshold signal-to-noise ratios (SNR) (> 6.6 if background
noise dominates vs. > 2.5 for intensity-dependent noise). In stark contrast to
human-based analyses of 2DES data, we find that the NN accuracy improves
significantly (ca. 84% $\rightarrow$ 96%) when the data are constrained by the
bandwidth and center frequency of the pump pulses. This result is consistent
with the NN learning the optical trends described by Kasha's theory of
molecular excitons. Our findings convey positive prospects for adapting
simulation-trained NNs to extract molecular properties from inherently
imperfect experimental 2DES data. More broadly, we propose that machine-learned
perspectives of nonlinear spectroscopic data may produce unique and, perhaps,
counterintuitive guidelines for experimental design.","['physics.chem-ph', 'cs.LG', 'quant-ph']","['Jonathan D. Schultz', 'Kelsey A. Parker', 'Bashir Sbaiti', 'David N. Beratan']",2025-03-19,2025-03-19
2503.15704v1,Tuning Sequential Monte Carlo Samplers via Greedy Incremental Divergence Minimization,"The performance of sequential Monte Carlo (SMC) samplers heavily depends on
the tuning of the Markov kernels used in the path proposal. For SMC samplers
with unadjusted Markov kernels, standard tuning objectives, such as the
Metropolis-Hastings acceptance rate or the expected-squared jump distance, are
no longer applicable. While stochastic gradient-based end-to-end optimization
has been explored for tuning SMC samplers, they often incur excessive training
costs, even for tuning just the kernel step sizes. In this work, we propose a
general adaptation framework for tuning the Markov kernels in SMC samplers by
minimizing the incremental Kullback-Leibler (KL) divergence between the
proposal and target paths. For step size tuning, we provide a gradient- and
tuning-free algorithm that is generally applicable for kernels such as Langevin
Monte Carlo (LMC). We further demonstrate the utility of our approach by
providing a tailored scheme for tuning \textit{kinetic} LMC used in SMC
samplers. Our implementations are able to obtain a full \textit{schedule} of
tuned parameters at the cost of a few vanilla SMC runs, which is a fraction of
gradient-based approaches.","['stat.ML', 'cs.LG', 'stat.CO']","['Kyurae Kim', 'Zuheng Xu', 'Jacob R. Gardner', 'Trevor Campbell']",2025-03-19,2025-03-19
2503.15703v1,Predicting Multi-Agent Specialization via Task Parallelizability,"Multi-agent systems often rely on specialized agents with distinct roles
rather than general-purpose agents that perform the entire task independently.
However, the conditions that govern the optimal degree of specialization remain
poorly understood. In this work, we propose that specialist teams outperform
generalist ones when environmental constraints limit task parallelizability --
the potential to execute task components concurrently. Drawing inspiration from
distributed systems, we introduce a heuristic to predict the relative
efficiency of generalist versus specialist teams by estimating the speed-up
achieved when two agents perform a task in parallel rather than focus on
complementary subtasks. We validate this heuristic through three multi-agent
reinforcement learning (MARL) experiments in Overcooked-AI, demonstrating that
key factors limiting task parallelizability influence specialization. We also
observe that as the state space expands, agents tend to converge on specialist
strategies, even when generalist ones are theoretically more efficient,
highlighting potential biases in MARL training algorithms. Our findings provide
a principled framework for interpreting specialization given the task and
environment, and introduce a novel benchmark for evaluating whether MARL finds
optimal strategies.","['cs.MA', 'cs.AI']","['Elizabeth Mieczkowski', 'Ruaridh Mon-Williams', 'Neil Bramley', 'Christopher G. Lucas', 'Natalia Velez', 'Thomas L. Griffiths']",2025-03-19,2025-03-19
2503.15699v1,Representational Similarity via Interpretable Visual Concepts,"How do two deep neural networks differ in how they arrive at a decision?
Measuring the similarity of deep networks has been a long-standing open
question. Most existing methods provide a single number to measure the
similarity of two networks at a given layer, but give no insight into what
makes them similar or dissimilar. We introduce an interpretable
representational similarity method (RSVC) to compare two networks. We use RSVC
to discover shared and unique visual concepts between two models. We show that
some aspects of model differences can be attributed to unique concepts
discovered by one model that are not well represented in the other. Finally, we
conduct extensive evaluation across different vision model architectures and
training protocols to demonstrate its effectiveness.","['cs.CV', 'cs.AI', 'q-bio.NC']","['Neehar Kondapaneni', 'Oisin Mac Aodha', 'Pietro Perona']",2025-03-19,2025-03-19
2503.15697v1,Technical Report for the 5th CLVision Challenge at CVPR: Addressing the Class-Incremental with Repetition using Unlabeled Data -- 4th Place Solution,"This paper outlines our approach to the 5th CLVision challenge at CVPR, which
addresses the Class-Incremental with Repetition (CIR) scenario. In contrast to
traditional class incremental learning, this novel setting introduces unique
challenges and research opportunities, particularly through the integration of
unlabeled data into the training process. In the CIR scenario, encountered
classes may reappear in later learning experiences, and each experience may
involve only a subset of the overall class distribution. Additionally, the
unlabeled data provided during training may include instances of unseen
classes, or irrelevant classes which should be ignored. Our approach focuses on
retaining previously learned knowledge by utilizing knowledge distillation and
pseudo-labeling techniques. The key characteristic of our method is the
exploitation of unlabeled data during training, in order to maintain optimal
performance on instances of previously encountered categories and reduce the
detrimental effects of catastrophic forgetting. Our method achieves an average
accuracy of 16.68\% during the pre-selection phase and 21.19% during the final
evaluation phase, outperforming the baseline accuracy of 9.39%. We provide the
implementation code at
https://github.com/panagiotamoraiti/continual-learning-challenge-2024 .",['cs.CV'],"['Panagiota Moraiti', 'Efstathios Karypidis']",2025-03-19,2025-03-19
2503.15696v1,Approximation properties of neural ODEs,"We study the approximation properties of shallow neural networks whose
activation function is defined as the flow of a neural ordinary differential
equation (neural ODE) at the final time of the integration interval. We prove
the universal approximation property (UAP) of such shallow neural networks in
the space of continuous functions. Furthermore, we investigate the
approximation properties of shallow neural networks whose parameters are
required to satisfy some constraints. In particular, we constrain the Lipschitz
constant of the flow of the neural ODE to increase the stability of the shallow
neural network, and we restrict the norm of the weight matrices of the linear
layers to one to make sure that the restricted expansivity of the flow is not
compensated by the increased expansivity of the linear layers. For this
setting, we prove approximation bounds that tell us the accuracy to which we
can approximate a continuous function with a shallow neural network with such
constraints. We prove that the UAP holds if we consider only the constraint on
the Lipschitz constant of the flow or the unit norm constraint on the weight
matrices of the linear layers.","['math.NA', 'cs.LG', 'cs.NA']","['Arturo De Marinis', 'Davide Murari', 'Elena Celledoni', 'Nicola Guglielmi', 'Brynjulf Owren', 'Francesco Tudisco']",2025-03-19,2025-03-19
2503.15693v1,"Good Actions Succeed, Bad Actions Generalize: A Case Study on Why RL Generalizes Better","Supervised learning (SL) and reinforcement learning (RL) are both widely used
to train general-purpose agents for complex tasks, yet their generalization
capabilities and underlying mechanisms are not yet fully understood. In this
paper, we provide a direct comparison between SL and RL in terms of zero-shot
generalization. Using the Habitat visual navigation task as a testbed, we
evaluate Proximal Policy Optimization (PPO) and Behavior Cloning (BC) agents
across two levels of generalization: state-goal pair generalization within seen
environments and generalization to unseen environments. Our experiments show
that PPO consistently outperforms BC across both zero-shot settings and
performance metrics-success rate and SPL. Interestingly, even though additional
optimal training data enables BC to match PPO's zero-shot performance in SPL,
it still falls significantly behind in success rate. We attribute this to a
fundamental difference in how models trained by these algorithms generalize:
BC-trained models generalize by imitating successful trajectories, whereas
TD-based RL-trained models generalize through combinatorial experience
stitching-leveraging fragments of past trajectories (mostly failed ones) to
construct solutions for new tasks. This allows RL to efficiently find solutions
in vast state space and discover novel strategies beyond the scope of human
knowledge. Besides providing empirical evidence and understanding, we also
propose practical guidelines for improving the generalization capabilities of
RL and SL through algorithm design.",['cs.LG'],['Meng Song'],2025-03-19,2025-03-19
2503.15686v1,Multi-focal Conditioned Latent Diffusion for Person Image Synthesis,"The Latent Diffusion Model (LDM) has demonstrated strong capabilities in
high-resolution image generation and has been widely employed for Pose-Guided
Person Image Synthesis (PGPIS), yielding promising results. However, the
compression process of LDM often results in the deterioration of details,
particularly in sensitive areas such as facial features and clothing textures.
In this paper, we propose a Multi-focal Conditioned Latent Diffusion (MCLD)
method to address these limitations by conditioning the model on disentangled,
pose-invariant features from these sensitive regions. Our approach utilizes a
multi-focal condition aggregation module, which effectively integrates facial
identity and texture-specific information, enhancing the model's ability to
produce appearance realistic and identity-consistent images. Our method
demonstrates consistent identity and appearance generation on the DeepFashion
dataset and enables flexible person image editing due to its generation
consistency. The code is available at https://github.com/jqliu09/mcld.",['cs.CV'],"['Jiaqi Liu', 'Jichao Zahng', 'Paolo Rota', 'Nicu Sebe']",2025-03-19,2025-03-19
2503.15685v1,Robotic Paper Wrapping by Learning Force Control,"Robotic packaging using wrapping paper poses significant challenges due to
the material's complex deformation properties. The packaging process itself
involves multiple steps, primarily categorized as folding the paper or creating
creases. Small deviations in the robot's arm trajectory or force vector can
lead to tearing or wrinkling of the paper, exacerbated by the variability in
material properties.
  This study introduces a novel framework that combines imitation learning and
reinforcement learning to enable a robot to perform each step of the packaging
process efficiently. The framework allows the robot to follow approximate
trajectories of the tool-center point (TCP) based on human demonstrations while
optimizing force control parameters to prevent tearing or wrinkling, even with
variable wrapping paper materials.
  The proposed method was validated through ablation studies, which
demonstrated successful task completion with a significant reduction in tear
and wrinkle rates. Furthermore, the force control strategy proved to be
adaptable across different wrapping paper materials and robust against
variations in the size of the target object.","['cs.RO', 'cs.LG']","['Hiroki Hanai', 'Takuya Kiyokawa', 'Weiwei Wan', 'Kensuke Harada']",2025-03-19,2025-03-19
2503.15683v1,The Change You Want To Detect: Semantic Change Detection In Earth Observation With Hybrid Data Generation,"Bi-temporal change detection at scale based on Very High Resolution (VHR)
images is crucial for Earth monitoring. This remains poorly addressed so far:
methods either require large volumes of annotated data (semantic case), or are
limited to restricted datasets (binary set-ups). Most approaches do not exhibit
the versatility required for temporal and spatial adaptation: simplicity in
architecture design and pretraining on realistic and comprehensive datasets.
Synthetic datasets are the key solution but still fail to handle complex and
diverse scenes. In this paper, we present HySCDG a generative pipeline for
creating a large hybrid semantic change detection dataset that contains both
real VHR images and inpainted ones, along with land cover semantic map at both
dates and the change map. Being semantically and spatially guided, HySCDG
generates realistic images, leading to a comprehensive and hybrid
transfer-proof dataset FSC-180k. We evaluate FSC-180k on five change detection
cases (binary and semantic), from zero-shot to mixed and sequential training,
and also under low data regime training. Experiments demonstrate that
pretraining on our hybrid dataset leads to a significant performance boost,
outperforming SyntheWorld, a fully synthetic dataset, in every configuration.
All codes, models, and data are available here:
$\href{https://yb23.github.io/projects/cywd/}{https://yb23.github.io/projects/cywd/}$.",['cs.CV'],"['Benidir Yanis', 'Gonthier Nicolas', 'Mallet Clement']",2025-03-19,2025-03-19
2503.15679v1,Sequential learning based PINNs to overcome temporal domain complexities in unsteady flow past flapping wings,"For a data-driven and physics combined modelling of unsteady flow systems
with moving immersed boundaries, Sundar {\it et al.} introduced an immersed
boundary-aware (IBA) framework, combining Physics-Informed Neural Networks
(PINNs) and the immersed boundary method (IBM). This approach was beneficial
because it avoided case-specific transformations to a body-attached reference
frame. Building on this, we now address the challenges of long time integration
in velocity reconstruction and pressure recovery by extending this IBA
framework with sequential learning strategies. Key difficulties for PINNs in
long time integration include temporal sparsity, long temporal domains and rich
spectral content. To tackle these, a moving boundary-enabled PINN is developed,
proposing two sequential learning strategies: - a time marching with gradual
increase in time domain size, however, this approach struggles with error
accumulation over long time domains; and - a time decomposition which divides
the temporal domain into smaller segments, combined with transfer learning it
effectively reduces error propagation and computational complexity. The key
findings for modelling of incompressible unsteady flows past a flapping airfoil
include: - for quasi-periodic flows, the time decomposition approach with
preferential spatio-temporal sampling improves accuracy and efficiency for
pressure recovery and aerodynamic load reconstruction, and, - for long time
domains, decomposing it into smaller temporal segments and employing multiple
sub-networks, simplifies the problem ensuring stability and reduced network
sizes. This study highlights the limitations of traditional PINNs for long time
integration of flow-structure interaction problems and demonstrates the
benefits of decomposition-based strategies for addressing error accumulation,
computational cost, and complex dynamics.","['physics.flu-dyn', 'cs.LG']","['Rahul Sundar', 'Didier Lucor', 'Sunetra Sarkar']",2025-03-19,2025-03-19
2503.15676v1,High Temporal Consistency through Semantic Similarity Propagation in Semi-Supervised Video Semantic Segmentation for Autonomous Flight,"Semantic segmentation from RGB cameras is essential to the perception of
autonomous flying vehicles. The stability of predictions through the captured
videos is paramount to their reliability and, by extension, to the
trustworthiness of the agents. In this paper, we propose a lightweight video
semantic segmentation approach-suited to onboard real-time inference-achieving
high temporal consistency on aerial data through Semantic Similarity
Propagation across frames. SSP temporally propagates the predictions of an
efficient image segmentation model with global registration alignment to
compensate for camera movements. It combines the current estimation and the
prior prediction with linear interpolation using weights computed from the
features similarities of the two frames. Because data availability is a
challenge in this domain, we propose a consistency-aware Knowledge Distillation
training procedure for sparsely labeled datasets with few annotations. Using a
large image segmentation model as a teacher to train the efficient SSP, we
leverage the strong correlations between labeled and unlabeled frames in the
same training videos to obtain high-quality supervision on all frames. KD-SSP
obtains a significant temporal consistency increase over the base image
segmentation model of 12.5% and 6.7% TC on UAVid and RuralScapes respectively,
with higher accuracy and comparable inference speed. On these aerial datasets,
KD-SSP provides a superior segmentation quality and inference speed trade-off
than other video methods proposed for general applications and shows
considerably higher consistency. The code will be made publicly available upon
acceptance.",['cs.CV'],"['Cédric Vincent', 'Taehyoung Kim', 'Henri Meeß']",2025-03-19,2025-03-19
2503.15672v1,GASP: Unifying Geometric and Semantic Self-Supervised Pre-training for Autonomous Driving,"Self-supervised pre-training based on next-token prediction has enabled large
language models to capture the underlying structure of text, and has led to
unprecedented performance on a large array of tasks when applied at scale.
Similarly, autonomous driving generates vast amounts of spatiotemporal data,
alluding to the possibility of harnessing scale to learn the underlying
geometric and semantic structure of the environment and its evolution over
time. In this direction, we propose a geometric and semantic self-supervised
pre-training method, GASP, that learns a unified representation by predicting,
at any queried future point in spacetime, (1) general occupancy, capturing the
evolving structure of the 3D scene; (2) ego occupancy, modeling the ego vehicle
path through the environment; and (3) distilled high-level features from a
vision foundation model. By modeling geometric and semantic 4D occupancy fields
instead of raw sensor measurements, the model learns a structured,
generalizable representation of the environment and its evolution through time.
We validate GASP on multiple autonomous driving benchmarks, demonstrating
significant improvements in semantic occupancy forecasting, online mapping, and
ego trajectory prediction. Our results demonstrate that continuous 4D geometric
and semantic occupancy prediction provides a scalable and effective
pre-training paradigm for autonomous driving. For code and additional
visualizations, see \href{https://research.zenseact.com/publications/gasp/.","['cs.CV', 'cs.RO']","['William Ljungbergh', 'Adam Lilja', 'Adam Tonderski. Arvid Laveno Ling', 'Carl Lindström', 'Willem Verbeke', 'Junsheng Fu', 'Christoffer Petersson', 'Lars Hammarstrand', 'Michael Felsberg']",2025-03-19,2025-03-19
2503.15671v1,CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-Consistency from a Single Image,"Reconstructing clothed humans from a single image is a fundamental task in
computer vision with wide-ranging applications. Although existing monocular
clothed human reconstruction solutions have shown promising results, they often
rely on the assumption that the human subject is in an occlusion-free
environment. Thus, when encountering in-the-wild occluded images, these
algorithms produce multiview inconsistent and fragmented reconstructions.
Additionally, most algorithms for monocular 3D human reconstruction leverage
geometric priors such as SMPL annotations for training and inference, which are
extremely challenging to acquire in real-world applications. To address these
limitations, we propose CHROME: Clothed Human Reconstruction with
Occlusion-Resilience and Multiview-ConsistEncy from a Single Image, a novel
pipeline designed to reconstruct occlusion-resilient 3D humans with multiview
consistency from a single occluded image, without requiring either ground-truth
geometric prior annotations or 3D supervision. Specifically, CHROME leverages a
multiview diffusion model to first synthesize occlusion-free human images from
the occluded input, compatible with off-the-shelf pose control to explicitly
enforce cross-view consistency during synthesis. A 3D reconstruction model is
then trained to predict a set of 3D Gaussians conditioned on both the occluded
input and synthesized views, aligning cross-view details to produce a cohesive
and accurate 3D representation. CHROME achieves significant improvements in
terms of both novel view synthesis (upto 3 db PSNR) and geometric
reconstruction under challenging conditions.",['cs.CV'],"['Arindam Dutta', 'Meng Zheng', 'Zhongpai Gao', 'Benjamin Planche', 'Anwesha Choudhuri', 'Terrence Chen', 'Amit K. Roy-Chowdhury', 'Ziyan Wu']",2025-03-19,2025-03-19
2503.15668v1,Model Risk Management for Generative AI In Financial Institutions,"The success of OpenAI's ChatGPT in 2023 has spurred financial enterprises
into exploring Generative AI applications to reduce costs or drive revenue
within different lines of businesses in the Financial Industry. While these
applications offer strong potential for efficiencies, they introduce new model
risks, primarily hallucinations and toxicity. As highly regulated entities,
financial enterprises (primarily large US banks) are obligated to enhance their
model risk framework with additional testing and controls to ensure safe
deployment of such applications. This paper outlines the key aspects for model
risk management of generative AI model with a special emphasis on additional
practices required in model validation.","['q-fin.RM', 'cs.LG']","['Anwesha Bhattacharyya', 'Ye Yu', 'Hanyu Yang', 'Rahul Singh', 'Tarun Joshi', 'Jie Chen', 'Kiran Yalavarthy']",2025-03-19,2025-03-19
2503.16557v1,Investigating Cultural Dimensions and Technological Acceptance: The Adoption of Electronic Performance and Tracking Systems in Qatar's Football Sector,"Qatar's football sector has undergone a substantial technological
transformation with the implementation of Electronic Performance and Tracking
Systems (EPTS). This study examines the impact of cultural and technological
factors on EPTS adoption, using Hofstede's Cultural Dimensions Theory and the
Technology Acceptance Model (TAM) as theoretical frameworks. An initial
exploratory study involved ten participants, followed by an expanded dataset
comprising thirty stakeholders, including players, coaches, and staff from
Qatari football organizations. Multiple regression analysis was conducted to
evaluate the relationships between perceived usefulness, perceived ease of use,
power distance, innovation receptiveness, integration complexity, and overall
adoption. The results indicate that perceived usefulness, innovation
receptiveness, and lower power distance significantly drive EPTS adoption,
while ease of use is marginally significant and integration complexity is
non-significant in this sample. These findings provide practical insights for
sports technology stakeholders in Qatar and emphasize the importance of
aligning cultural considerations with technological readiness for successful
EPTS integration.","['cs.CY', 'cs.LG']",['Abdulaziz Al Mannai'],2025-03-19,2025-03-19
2503.15667v1,DiffPortrait360: Consistent Portrait Diffusion for 360 View Synthesis,"Generating high-quality 360-degree views of human heads from single-view
images is essential for enabling accessible immersive telepresence applications
and scalable personalized content creation. While cutting-edge methods for full
head generation are limited to modeling realistic human heads, the latest
diffusion-based approaches for style-omniscient head synthesis can produce only
frontal views and struggle with view consistency, preventing their conversion
into true 3D models for rendering from arbitrary angles. We introduce a novel
approach that generates fully consistent 360-degree head views, accommodating
human, stylized, and anthropomorphic forms, including accessories like glasses
and hats. Our method builds on the DiffPortrait3D framework, incorporating a
custom ControlNet for back-of-head detail generation and a dual appearance
module to ensure global front-back consistency. By training on continuous view
sequences and integrating a back reference image, our approach achieves robust,
locally continuous view synthesis. Our model can be used to produce
high-quality neural radiance fields (NeRFs) for real-time, free-viewpoint
rendering, outperforming state-of-the-art methods in object synthesis and
360-degree head generation for very challenging input portraits.",['cs.CV'],"['Yuming Gu', 'Phong Tran', 'Yujian Zheng', 'Hongyi Xu', 'Heyuan Li', 'Adilbek Karmanov', 'Hao Li']",2025-03-19,2025-03-19
2503.15666v1,"Toward Scalable, Flexible Scene Flow for Point Clouds","Scene flow estimation is the task of describing 3D motion between temporally
successive observations. This thesis aims to build the foundation for building
scene flow estimators with two important properties: they are scalable, i.e.
they improve with access to more data and computation, and they are flexible,
i.e. they work out-of-the-box in a variety of domains and on a variety of
motion patterns without requiring significant hyperparameter tuning.
  In this dissertation we present several concrete contributions towards this.
In Chapter 1 we contextualize scene flow and its prior methods. In Chapter 2 we
present a blueprint to build and scale feedforward scene flow estimators
without requiring expensive human annotations via large scale distillation from
pseudolabels provided by strong unsupervised test-time optimization methods. In
Chapter 3 we introduce a benchmark to better measure estimate quality across
diverse object types, better bringing into focus what we care about and expect
from scene flow estimators, and use this benchmark to host a public challenge
that produced significant progress. In Chapter 4 we present a state-of-the-art
unsupervised scene flow estimator that introduces a new, full sequence problem
formulation and exhibits great promise in adjacent domains like 3D point
tracking. Finally, in Chapter 5 I philosophize about what's next for scene flow
and its potential future broader impacts.",['cs.CV'],['Kyle Vedder'],2025-03-19,2025-03-19
2503.15664v1,Enhancing Pancreatic Cancer Staging with Large Language Models: The Role of Retrieval-Augmented Generation,"Purpose: Retrieval-augmented generation (RAG) is a technology to enhance the
functionality and reliability of large language models (LLMs) by retrieving
relevant information from reliable external knowledge (REK). RAG has gained
interest in radiology, and we previously reported the utility of NotebookLM, an
LLM with RAG (RAG-LLM), for lung cancer staging. However, since the comparator
LLM differed from NotebookLM's internal model, it remained unclear whether its
advantage stemmed from RAG or inherent model differences. To better isolate
RAG's impact and assess its utility across different cancers, we compared
NotebookLM with its internal LLM, Gemini 2.0 Flash, in a pancreatic cancer
staging experiment.
  Materials and Methods: A summary of Japan's pancreatic cancer staging
guidelines was used as REK. We compared three groups - REK+/RAG+ (NotebookLM
with REK), REK+/RAG- (Gemini 2.0 Flash with REK), and REK-/RAG- (Gemini 2.0
Flash without REK) - in staging 100 fictional pancreatic cancer cases based on
CT findings. Staging criteria included TNM classification, local invasion
factors, and resectability classification. In REK+/RAG+, retrieval accuracy was
quantified based on the sufficiency of retrieved REK excerpts.
  Results: REK+/RAG+ achieved a staging accuracy of 70%, outperforming
REK+/RAG- (38%) and REK-/RAG- (35%). For TNM classification, REK+/RAG+ attained
80% accuracy, exceeding REK+/RAG- (55%) and REK-/RAG- (50%). Additionally,
REK+/RAG+ explicitly presented retrieved REK excerpts, achieving a retrieval
accuracy of 92%.
  Conclusion: NotebookLM, a RAG-LLM, outperformed its internal LLM, Gemini 2.0
Flash, in a pancreatic cancer staging experiment, suggesting that RAG may
improve LLM's staging accuracy. Furthermore, its ability to retrieve and
present REK excerpts provides transparency for physicians, highlighting its
applicability for clinical diagnosis and classification.",['cs.CL'],"['Hisashi Johno', 'Yuki Johno', 'Akitomo Amakawa', 'Junichi Sato', 'Ryota Tozuka', 'Atsushi Komaba', 'Hiroaki Watanabe', 'Hiroki Watanabe', 'Chihiro Goto', 'Hiroyuki Morisaka', 'Hiroshi Onishi', 'Kazunori Nakamoto']",2025-03-19,2025-03-19
2503.15661v1,UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction,"Autonomous agents that navigate Graphical User Interfaces (GUIs) to automate
tasks like document editing and file management can greatly enhance computer
workflows. While existing research focuses on online settings, desktop
environments, critical for many professional and everyday tasks, remain
underexplored due to data collection challenges and licensing issues. We
introduce UI-Vision, the first comprehensive, license-permissive benchmark for
offline, fine-grained evaluation of computer use agents in real-world desktop
environments. Unlike online benchmarks, UI-Vision provides: (i) dense,
high-quality annotations of human demonstrations, including bounding boxes, UI
labels, and action trajectories (clicks, drags, and keyboard inputs) across 83
software applications, and (ii) three fine-to-coarse grained tasks-Element
Grounding, Layout Grounding, and Action Prediction-with well-defined metrics to
rigorously evaluate agents' performance in desktop environments. Our evaluation
reveals critical limitations in state-of-the-art models like UI-TARS-72B,
including issues with understanding professional software, spatial reasoning,
and complex actions like drag-and-drop. These findings highlight the challenges
in developing fully autonomous computer use agents. By releasing UI-Vision as
open-source, we aim to advance the development of more capable agents for
real-world desktop tasks.","['cs.CV', 'cs.AI', 'cs.CL']","['Shravan Nayak', 'Xiangru Jian', 'Kevin Qinghong Lin', 'Juan A. Rodriguez', 'Montek Kalsi', 'Rabiul Awal', 'Nicolas Chapados', 'M. Tamer Özsu', 'Aishwarya Agrawal', 'David Vazquez', 'Christopher Pal', 'Perouz Taslakian', 'Spandana Gella', 'Sai Rajeswar']",2025-03-19,2025-03-19
2503.15655v1,R$^2$: A LLM Based Novel-to-Screenplay Generation Framework with Causal Plot Graphs,"Automatically adapting novels into screenplays is important for the TV, film,
or opera industries to promote products with low costs. The strong performances
of large language models (LLMs) in long-text generation call us to propose a
LLM based framework Reader-Rewriter (R$^2$) for this task. However, there are
two fundamental challenges here. First, the LLM hallucinations may cause
inconsistent plot extraction and screenplay generation. Second, the
causality-embedded plot lines should be effectively extracted for coherent
rewriting. Therefore, two corresponding tactics are proposed: 1) A
hallucination-aware refinement method (HAR) to iteratively discover and
eliminate the affections of hallucinations; and 2) a causal plot-graph
construction method (CPC) based on a greedy cycle-breaking algorithm to
efficiently construct plot lines with event causalities. Recruiting those
efficient techniques, R$^2$ utilizes two modules to mimic the human screenplay
rewriting process: The Reader module adopts a sliding window and CPC to build
the causal plot graphs, while the Rewriter module generates first the scene
outlines based on the graphs and then the screenplays. HAR is integrated into
both modules for accurate inferences of LLMs. Experimental results demonstrate
the superiority of R$^2$, which substantially outperforms three existing
approaches (51.3%, 22.6%, and 57.1% absolute increases) in pairwise comparison
at the overall win rate for GPT-4o.",['cs.AI'],"['Zefeng Lin', 'Yi Xiao', 'Zhiqiang Mo', 'Qifan Zhang', 'Jie Wang', 'Jiayang Chen', 'Jiajing Zhang', 'Hui Zhang', 'Zhengyi Liu', 'Xianyong Fang', 'Xiaohua Xu']",2025-03-19,2025-03-19
2503.15653v1,Transport-Related Surface Detection with Machine Learning: Analyzing Temporal Trends in Madrid and Vienna,"This study explores the integration of machine learning into urban aerial
image analysis, with a focus on identifying infrastructure surfaces for cars
and pedestrians and analyzing historical trends. It emphasizes the transition
from convolutional architectures to transformer-based pre-trained models,
underscoring their potential in global geospatial analysis. A workflow is
presented for automatically generating geospatial datasets, enabling the
creation of semantic segmentation datasets from various sources, including
WMS/WMTS links, vectorial cartography, and OpenStreetMap (OSM) overpass-turbo
requests. The developed code allows a fast dataset generation process for
training machine learning models using openly available data without manual
labelling. Using aerial imagery and vectorial data from the respective
geographical offices of Madrid and Vienna, two datasets were generated for car
and pedestrian surface detection. A transformer-based model was trained and
evaluated for each city, demonstrating good accuracy values. The historical
trend analysis involved applying the trained model to earlier images predating
the availability of vectorial data 10 to 20 years, successfully identifying
temporal trends in infrastructure for pedestrians and cars across different
city areas. This technique is applicable for municipal governments to gather
valuable data at a minimal cost.",['cs.CV'],"['Miguel Ureña Pliego', 'Rubén Martínez Marín', 'Nianfang Shi', 'Takeru Shibayama', 'Ulrich Leth', 'Miguel Marchamalo Sacristán']",2025-03-19,2025-03-19
2503.16556v1,Reliable Radiologic Skeletal Muscle Area Assessment -- A Biomarker for Cancer Cachexia Diagnosis,"Cancer cachexia is a common metabolic disorder characterized by severe muscle
atrophy which is associated with poor prognosis and quality of life. Monitoring
skeletal muscle area (SMA) longitudinally through computed tomography (CT)
scans, an imaging modality routinely acquired in cancer care, is an effective
way to identify and track this condition. However, existing tools often lack
full automation and exhibit inconsistent accuracy, limiting their potential for
integration into clinical workflows. To address these challenges, we developed
SMAART-AI (Skeletal Muscle Assessment-Automated and Reliable Tool-based on AI),
an end-to-end automated pipeline powered by deep learning models (nnU-Net 2D)
trained on mid-third lumbar level CT images with 5-fold cross-validation,
ensuring generalizability and robustness. SMAART-AI incorporates an
uncertainty-based mechanism to flag high-error SMA predictions for expert
review, enhancing reliability. We combined the SMA, skeletal muscle index, BMI,
and clinical data to train a multi-layer perceptron (MLP) model designed to
predict cachexia at the time of cancer diagnosis. Tested on the
gastroesophageal cancer dataset, SMAART-AI achieved a Dice score of 97.80% +/-
0.93%, with SMA estimated across all four datasets in this study at a median
absolute error of 2.48% compared to manual annotations with SliceOmatic.
Uncertainty metrics-variance, entropy, and coefficient of variation-strongly
correlated with SMA prediction errors (0.83, 0.76, and 0.73 respectively). The
MLP model predicts cachexia with 79% precision, providing clinicians with a
reliable tool for early diagnosis and intervention. By combining automation,
accuracy, and uncertainty awareness, SMAART-AI bridges the gap between research
and clinical application, offering a transformative approach to managing cancer
cachexia.","['eess.IV', 'cs.AI', 'cs.CE', 'cs.CV']","['Sabeen Ahmed', 'Nathan Parker', 'Margaret Park', 'Daniel Jeong', 'Lauren Peres', 'Evan W. Davis', 'Jennifer B. Permuth', 'Erin Siegel', 'Matthew B. Schabath', 'Yasin Yilmaz', 'Ghulam Rasool']",2025-03-19,2025-03-19
2503.15650v1,Survey on Generalization Theory for Graph Neural Networks,"Message-passing graph neural networks (MPNNs) have emerged as the leading
approach for machine learning on graphs, attracting significant attention in
recent years. While a large set of works explored the expressivity of MPNNs,
i.e., their ability to separate graphs and approximate functions over them,
comparatively less attention has been directed toward investigating their
generalization abilities, i.e., making meaningful predictions beyond the
training data. Here, we systematically review the existing literature on the
generalization abilities of MPNNs. We analyze the strengths and limitations of
various studies in these domains, providing insights into their methodologies
and findings. Furthermore, we identify potential avenues for future research,
aiming to deepen our understanding of the generalization abilities of MPNNs.","['cs.LG', 'cs.AI', 'stat.ML']","['Antonis Vasileiou', 'Stefanie Jegelka', 'Ron Levie', 'Christopher Morris']",2025-03-19,2025-03-19
2503.15648v1,Cancelable Biometric Template Generation Using Random Feature Vector Transformations,"Cancelable biometric schemes are designed to extract an identity-preserving,
non-invertible as well as revocable pseudo-identifier from biometric data.
Recognition systems need to store only this pseudo-identifier, to avoid
tampering and/or stealing of original biometric data during the recognition
process. State-of-the-art cancelable schemes generate pseudo-identifiers by
transforming the original template using either user-specific salting or
many-to-one transformations. In addition to the performance concerns, most of
such schemes are modality-specific and prone to reconstruction attacks as there
are chances for unauthorized access to security-critical transformation keys. A
novel, modality-independent cancelable biometric scheme is proposed to overcome
these limitations. In this scheme, a cancelable template (pseudo identifier) is
generated as a distance vector between multiple random transformations of the
biometric feature vector. These transformations were done by grouping feature
vector components based on a set of user-specific random vectors. The proposed
scheme nullifies the possibility of template reconstruction as the generated
cancelable template contains only the distance values between the different
random transformations of the feature vector and it does not store any details
of the biometric template. The recognition performance of the proposed scheme
is evaluated for face and fingerprint modalities. Equal Error Rate (EER) of 1.5
is obtained for face and 1.7 is obtained for the fingerprint in the worst case.","['cs.CR', 'cs.CV']","['Ragendhu Sp', 'Tony Thomas', 'Sabu Emmanuel']",2025-03-19,2025-03-19
2503.15647v1,Multi-Modal Gesture Recognition from Video and Surgical Tool Pose Information via Motion Invariants,"Recognizing surgical gestures in real-time is a stepping stone towards
automated activity recognition, skill assessment, intra-operative assistance,
and eventually surgical automation. The current robotic surgical systems
provide us with rich multi-modal data such as video and kinematics. While some
recent works in multi-modal neural networks learn the relationships between
vision and kinematics data, current approaches treat kinematics information as
independent signals, with no underlying relation between tool-tip poses.
However, instrument poses are geometrically related, and the underlying
geometry can aid neural networks in learning gesture representation. Therefore,
we propose combining motion invariant measures (curvature and torsion) with
vision and kinematics data using a relational graph network to capture the
underlying relations between different data streams. We show that gesture
recognition improves when combining invariant signals with tool position,
achieving 90.3\% frame-wise accuracy on the JIGSAWS suturing dataset. Our
results show that motion invariant signals coupled with position are better
representations of gesture motion compared to traditional position and
quaternion representations. Our results highlight the need for geometric-aware
modeling of kinematics for gesture recognition.","['cs.CV', 'cs.LG']","['Jumanh Atoum', 'Garrison L. H. Johnston', 'Nabil Simaan', 'Jie Ying Wu']",2025-03-19,2025-03-19
2503.15639v1,A Context-Driven Training-Free Network for Lightweight Scene Text Segmentation and Recognition,"Modern scene text recognition systems often depend on large end-to-end
architectures that require extensive training and are prohibitively expensive
for real-time scenarios. In such cases, the deployment of heavy models becomes
impractical due to constraints on memory, computational resources, and latency.
To address these challenges, we propose a novel, training-free plug-and-play
framework that leverages the strengths of pre-trained text recognizers while
minimizing redundant computations. Our approach uses context-based
understanding and introduces an attention-based segmentation stage, which
refines candidate text regions at the pixel level, improving downstream
recognition. Instead of performing traditional text detection that follows a
block-level comparison between feature map and source image and harnesses
contextual information using pretrained captioners, allowing the framework to
generate word predictions directly from scene context.Candidate texts are
semantically and lexically evaluated to get a final score. Predictions that
meet or exceed a pre-defined confidence threshold bypass the heavier process of
end-to-end text STR profiling, ensuring faster inference and cutting down on
unnecessary computations. Experiments on public benchmarks demonstrate that our
paradigm achieves performance on par with state-of-the-art systems, yet
requires substantially fewer resources.","['cs.CV', 'cs.AI']","['Ritabrata Chakraborty', 'Shivakumara Palaiahnakote', 'Umapada Pal', 'Cheng-Lin Liu']",2025-03-19,2025-03-19
2503.15638v1,Using machine learning to measure evidence of students' sensemaking in physics courses,"In the education system, problem-solving correctness is often inappropriately
conflated with student learning. Advances in both Physics Education Research
(PER) and Machine Learning (ML) provide the initial tools to develop a more
meaningful and efficient measurement scheme for whether physics students are
engaging in sensemaking: a learning process of figuring out the how and why for
a particular phenomena. In this work, we contribute such a measurement scheme,
which quantifies the evidence of students' physical sensemaking given their
written explanations for their solutions to physics problems. We outline how
the proposed human annotation scheme can be automated into a deployable ML
model using language encoders and shared probabilistic classifiers. The
procedure is scalable for a large number of problems and students. We implement
three unique language encoders with logistic regression, and provide a
deployability analysis on 385 real student explanations from the 2023
Introduction to Physics course at Tufts University. Furthermore, we compute
sensemaking scores for all students, and analyze these measurements alongside
their corresponding problem-solving accuracies. We find no linear relationship
between these two variables, supporting the hypothesis that one is not a
reliable proxy for the other. We discuss how sensemaking scores can be used
alongside problem-solving accuracies to provide a more nuanced snapshot of
student performance in physics class.","['physics.ed-ph', 'cs.LG']","['Kaitlin Gili', 'Kyle Heuton', 'Astha Shah', 'Michael C. Hughes']",2025-03-19,2025-03-19
2503.15633v1,Vision-Speech Models: Teaching Speech Models to Converse about Images,"The recent successes of Vision-Language models raise the question of how to
equivalently imbue a pretrained speech model with vision understanding, an
important milestone towards building a multimodal speech model able to freely
converse about images. Building such a conversational Vision-Speech model
brings its unique challenges: (i) paired image-speech datasets are much scarcer
than their image-text counterparts, (ii) ensuring real-time latency at
inference is crucial thus bringing compute and memory constraints, and (iii)
the model should preserve prosodic features (e.g., speaker tone) which cannot
be inferred from text alone. In this work, we introduce MoshiVis, augmenting a
recent dialogue speech LLM, Moshi, with visual inputs through lightweight
adaptation modules. An additional dynamic gating mechanism enables the model to
more easily switch between the visual inputs and unrelated conversation topics.
To reduce training costs, we design a simple one-stage, parameter-efficient
fine-tuning pipeline in which we leverage a mixture of image-text (i.e.,
""speechless"") and image-speech samples. We evaluate the model on downstream
visual understanding tasks with both audio and text prompts, and report
qualitative samples of interactions with MoshiVis. Our inference code will be
made available, as well as the image-speech data used for audio evaluation.",['cs.CV'],"['Amélie Royer', 'Moritz Böhle', 'Gabriel de Marmiesse', 'Laurent Mazaré', 'Neil Zeghidour', 'Alexandre Défossez', 'Patrick Pérez']",2025-03-19,2025-03-19
2503.15629v1,Neural Lyapunov Function Approximation with Self-Supervised Reinforcement Learning,"Control Lyapunov functions are traditionally used to design a controller
which ensures convergence to a desired state, yet deriving these functions for
nonlinear systems remains a complex challenge. This paper presents a novel,
sample-efficient method for neural approximation of nonlinear Lyapunov
functions, leveraging self-supervised Reinforcement Learning (RL) to enhance
training data generation, particularly for inaccurately represented regions of
the state space. The proposed approach employs a data-driven World Model to
train Lyapunov functions from off-policy trajectories. The method is validated
on both standard and goal-conditioned robotic tasks, demonstrating faster
convergence and higher approximation accuracy compared to the state-of-the-art
neural Lyapunov approximation baseline. The code is available at:
https://github.com/CAV-Research-Lab/SACLA.git","['cs.RO', 'cs.AI', 'cs.CG', 'cs.LG']","['Luc McCutcheon', 'Bahman Gharesifard', 'Saber Fallah']",2025-03-19,2025-03-19
2503.15625v1,EarthScape: A Multimodal Dataset for Surficial Geologic Mapping and Earth Surface Analysis,"Surficial geologic mapping is essential for understanding Earth surface
processes, addressing modern challenges such as climate change and national
security, and supporting common applications in engineering and resource
management. However, traditional mapping methods are labor-intensive, limiting
spatial coverage and introducing potential biases. To address these
limitations, we introduce EarthScape, a novel, AI-ready multimodal dataset
specifically designed for surficial geologic mapping and Earth surface
analysis. EarthScape integrates high-resolution aerial RGB and near-infrared
(NIR) imagery, digital elevation models (DEM), multi-scale DEM-derived terrain
features, and hydrologic and infrastructure vector data. The dataset provides
detailed annotations for seven distinct surficial geologic classes encompassing
various geological processes. We present a comprehensive data processing
pipeline using open-sourced raw data and establish baseline benchmarks using
different spatial modalities to demonstrate the utility of EarthScape. As a
living dataset with a vision for expansion, EarthScape bridges the gap between
computer vision and Earth sciences, offering a valuable resource for advancing
research in multimodal learning, geospatial analysis, and geological mapping.
Our code is available at https://github.com/masseygeo/earthscape.",['cs.CV'],"['Matthew Massey', 'Abdullah-Al-Zubaer Imran']",2025-03-19,2025-03-19
2503.15621v1,LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning,"Recent progress in Multimodal Large Language Models (MLLMs) has highlighted
the critical roles of both the visual backbone and the underlying language
model. While prior work has primarily focused on scaling these components to
billions of parameters, the trade-offs between model size, architecture, and
performance remain underexplored. Additionally, inconsistencies in training
data and evaluation protocols have hindered direct comparisons, making it
difficult to derive optimal design choices. In this paper, we introduce
LLaVA-MORE, a new family of MLLMs that integrates recent language models with
diverse visual backbones. To ensure fair comparisons, we employ a unified
training protocol applied consistently across all architectures. Our analysis
systematically explores both small- and medium-scale LLMs -- including Phi-4,
LLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and
instruction following, while examining the relationship between model size and
performance. Beyond evaluating the LLM impact on final results, we conduct a
comprehensive study of various visual encoders, ranging from CLIP-based
architectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional
experiments investigate the effects of increased image resolution and
variations in pre-training datasets. Overall, our results provide insights into
the design of more effective MLLMs, offering a reproducible evaluation
framework that facilitates direct comparisons and can guide future model
development. Our source code and trained models are publicly available at:
https://github.com/aimagelab/LLaVA-MORE.","['cs.CV', 'cs.AI', 'cs.CL', 'cs.MM']","['Federico Cocchi', 'Nicholas Moratelli', 'Davide Caffagni', 'Sara Sarto', 'Lorenzo Baraldi', 'Marcella Cornia', 'Rita Cucchiara']",2025-03-19,2025-03-19
2503.15620v1,Does Context Matter? ContextualJudgeBench for Evaluating LLM-based Judges in Contextual Settings,"The large language model (LLM)-as-judge paradigm has been used to meet the
demand for a cheap, reliable, and fast evaluation of model outputs during AI
system development and post-deployment monitoring. While judge models -- LLMs
finetuned to specialize in assessing and critiquing model outputs -- have been
touted as general purpose evaluators, they are typically evaluated only on
non-contextual scenarios, such as instruction following. The omission of
contextual settings -- those where external information is used as context to
generate an output -- is surprising given the increasing prevalence of
retrieval-augmented generation (RAG) and summarization use cases. Contextual
assessment is uniquely challenging, as evaluation often depends on practitioner
priorities, leading to conditional evaluation criteria (e.g., comparing
responses based on factuality and then considering completeness if they are
equally factual). To address the gap, we propose ContextualJudgeBench, a judge
benchmark with 2,000 challenging response pairs across eight splits inspired by
real-world contextual evaluation scenarios. We build our benchmark with a
multi-pronged data construction pipeline that leverages both existing human
annotations and model-based perturbations. Our comprehensive study across 11
judge models and 9 general purpose models, reveals that the contextual
information and its assessment criteria present a significant challenge to even
state-of-the-art models. For example, OpenAI's o1, the best-performing model,
barely reaches 55% consistent accuracy.","['cs.CL', 'cs.AI', 'cs.LG']","['Austin Xu', 'Srijan Bansal', 'Yifei Ming', 'Semih Yavuz', 'Shafiq Joty']",2025-03-19,2025-03-19
2503.15617v1,CAM-Seg: A Continuous-valued Embedding Approach for Semantic Image Generation,"Traditional transformer-based semantic segmentation relies on quantized
embeddings. However, our analysis reveals that autoencoder accuracy on
segmentation mask using quantized embeddings (e.g. VQ-VAE) is 8% lower than
continuous-valued embeddings (e.g. KL-VAE). Motivated by this, we propose a
continuous-valued embedding framework for semantic segmentation. By
reformulating semantic mask generation as a continuous image-to-embedding
diffusion process, our approach eliminates the need for discrete latent
representations while preserving fine-grained spatial and semantic details. Our
key contribution includes a diffusion-guided autoregressive transformer that
learns a continuous semantic embedding space by modeling long-range
dependencies in image features. Our framework contains a unified architecture
combining a VAE encoder for continuous feature extraction, a diffusion-guided
transformer for conditioned embedding generation, and a VAE decoder for
semantic mask reconstruction. Our setting facilitates zero-shot domain
adaptation capabilities enabled by the continuity of the embedding space.
Experiments across diverse datasets (e.g., Cityscapes and domain-shifted
variants) demonstrate state-of-the-art robustness to distribution shifts,
including adverse weather (e.g., fog, snow) and viewpoint variations. Our model
also exhibits strong noise resilience, achieving robust performance ($\approx$
95% AP compared to baseline) under gaussian noise, moderate motion blur, and
moderate brightness/contrast variations, while experiencing only a moderate
impact ($\approx$ 90% AP compared to baseline) from 50% salt and pepper noise,
saturation and hue shifts. Code available:
https://github.com/mahmed10/CAMSS.git","['cs.CV', 'cs.AI']","['Masud Ahmed', 'Zahid Hasan', 'Syed Arefinul Haque', 'Abu Zaher Md Faridee', 'Sanjay Purushotham', 'Suya You', 'Nirmalya Roy']",2025-03-19,2025-03-19
2503.15615v1,PEnGUiN: Partially Equivariant Graph NeUral Networks for Sample Efficient MARL,"Equivariant Graph Neural Networks (EGNNs) have emerged as a promising
approach in Multi-Agent Reinforcement Learning (MARL), leveraging symmetry
guarantees to greatly improve sample efficiency and generalization. However,
real-world environments often exhibit inherent asymmetries arising from factors
such as external forces, measurement inaccuracies, or intrinsic system biases.
This paper introduces \textit{Partially Equivariant Graph NeUral Networks
(PEnGUiN)}, a novel architecture specifically designed to address these
challenges. We formally identify and categorize various types of partial
equivariance relevant to MARL, including subgroup equivariance, feature-wise
equivariance, regional equivariance, and approximate equivariance. We
theoretically demonstrate that PEnGUiN is capable of learning both fully
equivariant (EGNN) and non-equivariant (GNN) representations within a unified
framework. Through extensive experiments on a range of MARL problems
incorporating various asymmetries, we empirically validate the efficacy of
PEnGUiN. Our results consistently demonstrate that PEnGUiN outperforms both
EGNNs and standard GNNs in asymmetric environments, highlighting their
potential to improve the robustness and applicability of graph-based MARL
algorithms in real-world scenarios.","['cs.LG', 'cs.AI', 'cs.RO']","['Joshua McClellan', 'Greyson Brothers', 'Furong Huang', 'Pratap Tokekar']",2025-03-19,2025-03-19
2503.15485v1,TULIP: Towards Unified Language-Image Pretraining,"Despite the recent success of image-text contrastive models like CLIP and
SigLIP, these models often struggle with vision-centric tasks that demand
high-fidelity image understanding, such as counting, depth estimation, and
fine-grained object recognition. These models, by performing language
alignment, tend to prioritize high-level semantics over visual understanding,
weakening their image understanding. On the other hand, vision-focused models
are great at processing visual information but struggle to understand language,
limiting their flexibility for language-driven tasks. In this work, we
introduce TULIP, an open-source, drop-in replacement for existing CLIP-like
models. Our method leverages generative data augmentation, enhanced image-image
and text-text contrastive learning, and image/text reconstruction
regularization to learn fine-grained visual features while preserving global
semantic alignment. Our approach, scaling to over 1B parameters, outperforms
existing state-of-the-art (SOTA) models across multiple benchmarks,
establishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to
a $2\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot
classification, and improving vision-language models, achieving over $3\times$
higher scores than SigLIP on MMVP. Our code/checkpoints are available at
https://tulip-berkeley.github.io","['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG']","['Zineng Tang', 'Long Lian', 'Seun Eisape', 'XuDong Wang', 'Roei Herzig', 'Adam Yala', 'Alane Suhr', 'Trevor Darrell', 'David M. Chan']",2025-03-19,2025-03-19
2503.15484v1,Value Profiles for Encoding Human Variation,"Modelling human variation in rating tasks is crucial for enabling AI systems
for personalization, pluralistic model alignment, and computational social
science. We propose representing individuals using value profiles -- natural
language descriptions of underlying values compressed from in-context
demonstrations -- along with a steerable decoder model to estimate ratings
conditioned on a value profile or other rater information. To measure the
predictive information in rater representations, we introduce an
information-theoretic methodology. We find that demonstrations contain the most
information, followed by value profiles and then demographics. However, value
profiles offer advantages in terms of scrutability, interpretability, and
steerability due to their compressed natural language format. Value profiles
effectively compress the useful information from demonstrations (>70%
information preservation). Furthermore, clustering value profiles to identify
similarly behaving individuals better explains rater variation than the most
predictive demographic groupings. Going beyond test set performance, we show
that the decoder models interpretably change ratings according to semantic
profile differences, are well-calibrated, and can help explain instance-level
disagreement by simulating an annotator population. These results demonstrate
that value profiles offer novel, predictive ways to describe individual
variation beyond demographics or group information.","['cs.CL', 'cs.AI', 'cs.HC', 'cs.LG']","['Taylor Sorensen', 'Pushkar Mishra', 'Roma Patel', 'Michael Henry Tessler', 'Michiel Bakker', 'Georgina Evans', 'Iason Gabriel', 'Noah Goodman', 'Verena Rieser']",2025-03-19,2025-03-19
2503.15482v1,Natural Quantization of Neural Networks,"We propose a natural quantization of a standard neural network, where the
neurons correspond to qubits and the activation functions are implemented via
quantum gates and measurements. The simplest quantized neural network
corresponds to applying single-qubit rotations, with the rotation angles being
dependent on the weights and measurement outcomes of the previous layer. This
realization has the advantage of being smoothly tunable from the purely
classical limit with no quantum uncertainty (thereby reproducing the classical
neural network exactly) to a quantum case, where superpositions introduce an
intrinsic uncertainty in the network. We benchmark this architecture on a
subset of the standard MNIST dataset and find a regime of ""quantum advantage,""
where the validation error rate in the quantum realization is smaller than that
in the classical model. We also consider another approach where quantumness is
introduced via weak measurements of ancilla qubits entangled with the neuron
qubits. This quantum neural network also allows for smooth tuning of the degree
of quantumness by controlling an entanglement angle, $g$, with $g=\frac\pi 2$
replicating the classical regime. We find that validation error is also
minimized within the quantum regime in this approach. We also observe a quantum
transition, with sharp loss of the quantum network's ability to learn at a
critical point $g_c$. The proposed quantum neural networks are readily
realizable in present-day quantum computers on commercial datasets.","['quant-ph', 'cond-mat.dis-nn', 'cs.LG']","['Richard Barney', 'Djamil Lakhdar-Hamina', 'Victor Galitski']",2025-03-19,2025-03-19
2503.15481v1,Learning to Play Piano in the Real World,"Towards the grand challenge of achieving human-level manipulation in robots,
playing piano is a compelling testbed that requires strategic, precise, and
flowing movements. Over the years, several works demonstrated hand-designed
controllers on real world piano playing, while other works evaluated robot
learning approaches on simulated piano scenarios. In this paper, we develop the
first piano playing robotic system that makes use of learning approaches while
also being deployed on a real world dexterous robot. Specifically, we make use
of Sim2Real to train a policy in simulation using reinforcement learning before
deploying the learned policy on a real world dexterous robot. In our
experiments, we thoroughly evaluate the interplay between domain randomization
and the accuracy of the dynamics model used in simulation. Moreover, we
evaluate the robot's performance across multiple songs with varying complexity
to study the generalization of our learned policy. By providing a
proof-of-concept of learning to play piano in the real world, we want to
encourage the community to adopt piano playing as a compelling benchmark
towards human-level manipulation. We open-source our code and show additional
videos at https://lasr.org/research/learning-to-play-piano .","['cs.RO', 'cs.AI', 'cs.LG']","['Yves-Simon Zeulner', 'Sandeep Selvaraj', 'Roberto Calandra']",2025-03-19,2025-03-19
2503.15478v1,SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks,"Large language model (LLM) agents need to perform multi-turn interactions in
real-world tasks. However, existing multi-turn RL algorithms for optimizing LLM
agents fail to perform effective credit assignment over multiple turns while
leveraging the generalization capabilities of LLMs and it remains unclear how
to develop such algorithms. To study this, we first introduce a new benchmark,
ColBench, where an LLM agent interacts with a human collaborator over multiple
turns to solve realistic tasks in backend programming and frontend design.
Building on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with
Step-WisE Evaluation from Training-time information), that uses a carefully
designed optimization objective to train a critic model with access to
additional training-time information. The critic provides step-level rewards
for improving the policy model. Our experiments demonstrate that SWEET-RL
achieves a 6% absolute improvement in success and win rates on ColBench
compared to other state-of-the-art multi-turn RL algorithms, enabling
Llama-3.1-8B to match or exceed the performance of GPT4-o in realistic
collaborative content creation.",['cs.LG'],"['Yifei Zhou', 'Song Jiang', 'Yuandong Tian', 'Jason Weston', 'Sergey Levine', 'Sainbayar Sukhbaatar', 'Xian Li']",2025-03-19,2025-03-19
2503.15477v1,What Makes a Reward Model a Good Teacher? An Optimization Perspective,"The success of Reinforcement Learning from Human Feedback (RLHF) critically
depends on the quality of the reward model. While this quality is primarily
evaluated through accuracy, it remains unclear whether accuracy fully captures
what makes a reward model an effective teacher. We address this question from
an optimization perspective. First, we prove that regardless of how accurate a
reward model is, if it induces low reward variance, then the RLHF objective
suffers from a flat landscape. Consequently, even a perfectly accurate reward
model can lead to extremely slow optimization, underperforming less accurate
models that induce higher reward variance. We additionally show that a reward
model that works well for one language model can induce low reward variance,
and thus a flat objective landscape, for another. These results establish a
fundamental limitation of evaluating reward models solely based on accuracy or
independently of the language model they guide. Experiments using models of up
to 8B parameters corroborate our theory, demonstrating the interplay between
reward variance, accuracy, and reward maximization rate. Overall, our findings
highlight that beyond accuracy, a reward model needs to induce sufficient
variance for efficient optimization.","['cs.LG', 'cs.AI', 'cs.CL', 'stat.ML']","['Noam Razin', 'Zixuan Wang', 'Hubert Strauss', 'Stanley Wei', 'Jason D. Lee', 'Sanjeev Arora']",2025-03-19,2025-03-19
2503.15475v1,Cube: A Roblox View of 3D Intelligence,"Foundation models trained on vast amounts of data have demonstrated
remarkable reasoning and generation capabilities in the domains of text,
images, audio and video. Our goal at Roblox is to build such a foundation model
for 3D intelligence, a model that can support developers in producing all
aspects of a Roblox experience, from generating 3D objects and scenes to
rigging characters for animation to producing programmatic scripts describing
object behaviors. We discuss three key design requirements for such a 3D
foundation model and then present our first step towards building such a model.
We expect that 3D geometric shapes will be a core data type and describe our
solution for 3D shape tokenizer. We show how our tokenization scheme can be
used in applications for text-to-shape generation, shape-to-text generation and
text-to-scene generation. We demonstrate how these applications can collaborate
with existing large language models (LLMs) to perform scene analysis and
reasoning. We conclude with a discussion outlining our path to building a fully
unified foundation model for 3D intelligence.",['cs.CV'],"['Foundation AI Team', 'Kiran Bhat', 'Nishchaie Khanna', 'Karun Channa', 'Tinghui Zhou', 'Yiheng Zhu', 'Xiaoxia Sun', 'Charles Shang', 'Anirudh Sudarshan', 'Maurice Chu', 'Daiqing Li', 'Kangle Deng', 'Jean-Philippe Fauconnier', 'Tijmen Verhulsdonck', 'Maneesh Agrawala', 'Kayvon Fatahalian', 'Alexander Weiss', 'Christian Reiser', 'Ravi Kiran Chirravuri', 'Ravali Kandur', 'Alejandro Pelaez', 'Akash Garg', 'Michael Palleschi', 'Jessica Wang', 'Skylar Litz', 'Leon Liu', 'Anying Li', 'David Harmon', 'Derek Liu', 'Liangjun Feng', 'Denis Goupil', 'Lukas Kuczynski', 'Jihyun Yoon', 'Naveen Marri', 'Peiye Zhuang', 'Yinan Zhang', 'Brian Yin', 'Haomiao Jiang', 'Marcel van Workum', 'Thomas Lane', 'Bryce Erickson', 'Salil Pathare', 'Kyle Price', 'Anupam Singh', 'David Baszucki']",2025-03-19,2025-03-19
2503.15474v1,Toward task-driven satellite image super-resolution,"Super-resolution is aimed at reconstructing high-resolution images from
low-resolution observations. State-of-the-art approaches underpinned with deep
learning allow for obtaining outstanding results, generating images of high
perceptual quality. However, it often remains unclear whether the reconstructed
details are close to the actual ground-truth information and whether they
constitute a more valuable source for image analysis algorithms. In the
reported work, we address the latter problem, and we present our efforts toward
learning super-resolution algorithms in a task-driven way to make them suitable
for generating high-resolution images that can be exploited for automated image
analysis. In the reported initial research, we propose a methodological
approach for assessing the existing models that perform computer vision tasks
in terms of whether they can be used for evaluating super-resolution
reconstruction algorithms, as well as training them in a task-driven way. We
support our analysis with experimental study and we expect it to establish a
solid foundation for selecting appropriate computer vision tasks that will
advance the capabilities of real-world super-resolution.",['cs.CV'],"['Maciej Ziaja', 'Pawel Kowaleczko', 'Daniel Kostrzewa', 'Nicolas Longépé', 'Michal Kawulok']",2025-03-19,2025-03-19
2503.16554v1,Explainable AI Components for Narrative Map Extraction,"As narrative extraction systems grow in complexity, establishing user trust
through interpretable and explainable outputs becomes increasingly critical.
This paper presents an evaluation of an Explainable Artificial Intelligence
(XAI) system for narrative map extraction that provides meaningful explanations
across multiple levels of abstraction. Our system integrates explanations based
on topical clusters for low-level document relationships, connection
explanations for event relationships, and high-level structure explanations for
overall narrative patterns. In particular, we evaluate the XAI system through a
user study involving 10 participants that examined narratives from the 2021
Cuban protests. The analysis of results demonstrates that participants using
the explanations made the users trust in the system's decisions, with
connection explanations and important event detection proving particularly
effective at building user confidence. Survey responses indicate that the
multi-level explanation approach helped users develop appropriate trust in the
system's narrative extraction capabilities. This work advances the
state-of-the-art in explainable narrative extraction while providing practical
insights for developing reliable narrative extraction systems that support
effective human-AI collaboration.","['cs.CL', 'cs.HC']","['Brian Keith', 'Fausto German', 'Eric Krokos', 'Sarah Joseph', 'Chris North']",2025-03-19,2025-03-19
2503.15586v1,How to Train Your Dragon: Automatic Diffusion-Based Rigging for Characters with Diverse Topologies,"Recent diffusion-based methods have achieved impressive results on animating
images of human subjects. However, most of that success has built on
human-specific body pose representations and extensive training with labeled
real videos. In this work, we extend the ability of such models to animate
images of characters with more diverse skeletal topologies. Given a small
number (3-5) of example frames showing the character in different poses with
corresponding skeletal information, our model quickly infers a rig for that
character that can generate images corresponding to new skeleton poses. We
propose a procedural data generation pipeline that efficiently samples training
data with diverse topologies on the fly. We use it, along with a novel skeleton
representation, to train our model on articulated shapes spanning a large space
of textures and topologies. Then during fine-tuning, our model rapidly adapts
to unseen target characters and generalizes well to rendering new poses, both
for realistic and more stylized cartoon appearances. To better evaluate
performance on this novel and challenging task, we create the first 2D video
dataset that contains both humanoid and non-humanoid subjects with per-frame
keypoint annotations. With extensive experiments, we demonstrate the superior
quality of our results. Project page: https://traindragondiffusion.github.io/","['cs.GR', 'cs.CV']","['Zeqi Gu', 'Difan Liu', 'Timothy Langlois', 'Matthew Fisher', 'Abe Davis']",2025-03-19,2025-03-19
2503.15470v1,EgoDTM: Towards 3D-Aware Egocentric Video-Language Pretraining,"Egocentric video-language pretraining has significantly advanced video
representation learning. Humans perceive and interact with a fully 3D world,
developing spatial awareness that extends beyond text-based understanding.
However, most previous works learn from 1D text or 2D visual cues, such as
bounding boxes, which inherently lack 3D understanding. To bridge this gap, we
introduce EgoDTM, an Egocentric Depth- and Text-aware Model, jointly trained
through large-scale 3D-aware video pretraining and video-text contrastive
learning. EgoDTM incorporates a lightweight 3D-aware decoder to efficiently
learn 3D-awareness from pseudo depth maps generated by depth estimation models.
To further facilitate 3D-aware video pretraining, we enrich the original brief
captions with hand-object visual cues by organically combining several
foundation models. Extensive experiments demonstrate EgoDTM's superior
performance across diverse downstream tasks, highlighting its superior 3D-aware
visual understanding. Our code will be released at
https://github.com/xuboshen/EgoDTM.","['cs.CV', 'cs.AI']","['Boshen Xu', 'Yuting Mei', 'Xinbi Liu', 'Sipeng Zheng', 'Qin Jin']",2025-03-19,2025-03-19
2503.15469v2,Dynamic Bi-Elman Attention Networks (DBEAN): Dual-Directional Context-Aware Representation Learning for Enhanced Text Classification,"Text classification, a fundamental task in natural language processing (NLP),
aims to categorize textual data into predefined labels. Traditional methods
struggled with complex linguistic structures and semantic dependencies. The
advent of deep learning, particularly recurrent neural networks (RNNs) and
Transformer-based models, has significantly advanced the field by enabling
nuanced feature extraction and context-aware predictions. Despite improvements,
existing models exhibit limitations in balancing interpretability,
computational efficiency, and long-range contextual understanding. This paper
proposes the Dynamic Bidirectional Elman with Attention Network (DBEAN), which
integrates bidirectional temporal modelling with self-attention mechanisms.
DBEAN dynamically assigns weights to critical segments of input, improving
contextual representation while maintaining computational efficiency.","['cs.CL', 'cs.AI']","['ZhengLin Lai', 'MengYao Liao', 'Dong Xu']",2025-03-19,2025-03-20
2503.15465v1,FP4DiT: Towards Effective Floating Point Quantization for Diffusion Transformers,"Diffusion Models (DM) have revolutionized the text-to-image visual generation
process. However, the large computational cost and model footprint of DMs
hinders practical deployment, especially on edge devices. Post-training
quantization (PTQ) is a lightweight method to alleviate these burdens without
the need for training or fine-tuning. While recent DM PTQ methods achieve W4A8
on integer-based PTQ, two key limitations remain: First, while most existing DM
PTQ methods evaluate on classical DMs like Stable Diffusion XL, 1.5 or earlier,
which use convolutional U-Nets, newer Diffusion Transformer (DiT) models like
the PixArt series, Hunyuan and others adopt fundamentally different transformer
backbones to achieve superior image synthesis. Second, integer (INT)
quantization is prevailing in DM PTQ but doesn't align well with the network
weight and activation distribution, while Floating-Point Quantization (FPQ) is
still under-investigated, yet it holds the potential to better align the weight
and activation distributions in low-bit settings for DiT. In response, we
introduce FP4DiT, a PTQ method that leverages FPQ to achieve W4A6 quantization.
Specifically, we extend and generalize the Adaptive Rounding PTQ technique to
adequately calibrate weight quantization for FPQ and demonstrate that DiT
activations depend on input patch data, necessitating robust online activation
quantization techniques. Experimental results demonstrate that FP4DiT
outperforms integer-based PTQ at W4A6 and W4A8 precision and generates
convincing visual content on PixArt-$\alpha$, PixArt-$\Sigma$ and Hunyuan in
terms of several T2I metrics such as HPSv2 and CLIP.",['cs.CV'],"['Ruichen Chen', 'Keith G. Mills', 'Di Niu']",2025-03-19,2025-03-19
2503.15463v2,"From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment","Large language models (LLMs) have traditionally been aligned through
one-size-fits-all approaches that assume uniform human preferences,
fundamentally overlooking the diversity in user values and needs. This paper
introduces a comprehensive framework for scalable personalized alignment of
LLMs. We establish a systematic preference space characterizing psychological
and behavioral dimensions, alongside diverse persona representations for robust
preference inference in real-world scenarios. Building upon this foundation, we
introduce \textsc{AlignX}, a large-scale dataset of over 1.3 million
personalized preference examples, and develop two complementary alignment
approaches: \textit{in-context alignment} directly conditioning on persona
representations and \textit{preference-bridged alignment} modeling intermediate
preference distributions. Extensive experiments demonstrate substantial
improvements over existing methods, with an average 17.06\% accuracy gain
across four benchmarks while exhibiting a strong adaptation capability to novel
preferences, robustness to limited user data, and precise preference
controllability. These results validate our framework's effectiveness,
advancing toward truly user-adaptive AI systems.","['cs.CL', 'cs.AI']","['Jia-Nan Li', 'Jian Guan', 'Songhao Wu', 'Wei Wu', 'Rui Yan']",2025-03-19,2025-03-21
2503.15457v1,Di$\mathtt{[M]}$O: Distilling Masked Diffusion Models into One-step Generator,"Masked Diffusion Models (MDMs) have emerged as a powerful generative modeling
technique. Despite their remarkable results, they typically suffer from slow
inference with several steps. In this paper, we propose Di$\mathtt{[M]}$O, a
novel approach that distills masked diffusion models into a one-step generator.
Di$\mathtt{[M]}$O addresses two key challenges: (1) the intractability of using
intermediate-step information for one-step generation, which we solve through
token-level distribution matching that optimizes model output logits by an
'on-policy framework' with the help of an auxiliary model; and (2) the lack of
entropy in the initial distribution, which we address through a token
initialization strategy that injects randomness while maintaining similarity to
teacher training distribution. We show Di$\mathtt{[M]}$O's effectiveness on
both class-conditional and text-conditional image generation, impressively
achieving performance competitive to multi-step teacher outputs while
drastically reducing inference time. To our knowledge, we are the first to
successfully achieve one-step distillation of masked diffusion models and the
first to apply discrete distillation to text-to-image generation, opening new
paths for efficient generative modeling.","['cs.CV', 'cs.AI', 'cs.LG']","['Yuanzhi Zhu', 'Xi Wang', 'Stéphane Lathuilière', 'Vicky Kalogeiton']",2025-03-19,2025-03-19
2503.15456v1,Temporal Encoding Strategies for Energy Time Series Prediction,"In contemporary power systems, energy consumption prediction plays a crucial
role in maintaining grid stability and resource allocation enabling power
companies to minimize energy waste and avoid overloading the grid. While there
are several research works on energy optimization, they often fail to address
the complexities of real-time fluctuations and the cyclic pattern of energy
consumption. This work proposes a novel approach to enhance the accuracy of
predictive models by employing sinusoidal encoding on periodic features of
time-series data. To demonstrate the increase in performance, several
statistical and ensemble machine learning models were trained on an energy
demand dataset, using the proposed sinusoidal encoding. The performance of
these models was then benchmarked against identical models trained on
traditional encoding methods. The results demonstrated a 12.6% improvement of
Root Mean Squared Error (from 0.5497 to 0.4802) and a 7.8% increase in the R^2
score (from 0.7530 to 0.8118), indicating that the proposed encoding better
captures the cyclic nature of temporal patterns than traditional methods. The
proposed methodology significantly improves prediction accuracy while
maintaining computational efficiency, making it suitable for real-time
applications in smart grid systems.",['cs.LG'],"['Aayam Bansal', 'Keertan Balaji', 'Zeus Lalani']",2025-03-19,2025-03-19
2503.15454v1,Evaluating Bias in Retrieval-Augmented Medical Question-Answering Systems,"Medical QA systems powered by Retrieval-Augmented Generation (RAG) models
support clinical decision-making but may introduce biases related to race,
gender, and social determinants of health. We systematically evaluate biases in
RAG-based LLM by examining demographic-sensitive queries and measuring
retrieval discrepancies. Using datasets like MMLU and MedMCQA, we analyze
retrieval overlap and correctness disparities. Our findings reveal substantial
demographic disparities within RAG pipelines, emphasizing the critical need for
retrieval methods that explicitly account for fairness to ensure equitable
clinical decision-making.",['cs.CL'],"['Yuelyu Ji', 'Hang Zhang', 'Yanshan Wang']",2025-03-19,2025-03-19
2503.15451v1,MotionStreamer: Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space,"This paper addresses the challenge of text-conditioned streaming motion
generation, which requires us to predict the next-step human pose based on
variable-length historical motions and incoming texts. Existing methods
struggle to achieve streaming motion generation, e.g., diffusion models are
constrained by pre-defined motion lengths, while GPT-based methods suffer from
delayed response and error accumulation problem due to discretized non-causal
tokenization. To solve these problems, we propose MotionStreamer, a novel
framework that incorporates a continuous causal latent space into a
probabilistic autoregressive model. The continuous latents mitigate information
loss caused by discretization and effectively reduce error accumulation during
long-term autoregressive generation. In addition, by establishing temporal
causal dependencies between current and historical motion latents, our model
fully utilizes the available information to achieve accurate online motion
decoding. Experiments show that our method outperforms existing approaches
while offering more applications, including multi-round generation, long-term
generation, and dynamic motion composition. Project Page:
https://zju3dv.github.io/MotionStreamer/",['cs.CV'],"['Lixing Xiao', 'Shunlin Lu', 'Huaijin Pi', 'Ke Fan', 'Liang Pan', 'Yueer Zhou', 'Ziyong Feng', 'Xiaowei Zhou', 'Sida Peng', 'Jingbo Wang']",2025-03-19,2025-03-19
2503.15450v1,SkyLadder: Better and Faster Pretraining via Context Window Scheduling,"Recent advancements in LLM pretraining have featured ever-expanding context
windows to process longer sequences. However, our pilot study reveals that
models pretrained with shorter context windows consistently outperform their
long-context counterparts under a fixed token budget. This finding motivates us
to explore an optimal context window scheduling strategy to better balance
long-context capability with pretraining efficiency. To this end, we propose
SkyLadder, a simple yet effective approach that implements a short-to-long
context window transition. SkyLadder preserves strong standard benchmark
performance, while matching or exceeding baseline results on long context
tasks. Through extensive experiments, we pre-train 1B-parameter models (up to
32K context) and 3B-parameter models (8K context) on 100B tokens, demonstrating
that SkyLadder yields consistent gains of up to 3.7% on common benchmarks,
while achieving up to 22% faster training speeds compared to baselines. The
code is at https://github.com/sail-sg/SkyLadder.",['cs.CL'],"['Tongyao Zhu', 'Qian Liu', 'Haonan Wang', 'Shiqi Chen', 'Xiangming Gu', 'Tianyu Pang', 'Min-Yen Kan']",2025-03-19,2025-03-19
2503.15448v1,Reducing Communication Overhead in Federated Learning for Network Anomaly Detection with Adaptive Client Selection,"Communication overhead in federated learning (FL) poses a significant
challenge for network anomaly detection systems, where diverse client
configurations and network conditions impact efficiency and detection accuracy.
Existing approaches attempt optimization individually but struggle to balance
reduced overhead with performance. This paper presents an adaptive FL framework
combining batch size optimization, client selection, and asynchronous updates
for efficient anomaly detection. Using UNSW-NB15 for general network traffic
and ROAD for automotive networks, our framework reduces communication overhead
by 97.6% (700.0s to 16.8s) while maintaining comparable accuracy (95.10% vs.
95.12%). The Mann-Whitney U test confirms significant improvements (p < 0.05).
Profiling analysis reveals efficiency gains via reduced GPU operations and
memory transfers, ensuring robust detection across varying client conditions.","['cs.DC', 'cs.LG']","['William Marfo', 'Deepak Tosh', 'Shirley Moore', 'Joshua Suetterlein', 'Joseph Manzano']",2025-03-19,2025-03-19
2503.15441v1,A discontinuity-capturing neural network with categorical embedding and its application to anisotropic elliptic interface problems,"In this paper, we propose a discontinuity-capturing shallow neural network
with categorical embedding to represent piecewise smooth functions. The network
comprises three hidden layers, a discontinuity-capturing layer, a categorical
embedding layer, and a fully-connected layer. Under such a design, we show that
a piecewise smooth function, even with a large number of pieces, can be
approximated by a single neural network with high prediction accuracy. We then
leverage the proposed network model to solve anisotropic elliptic interface
problems. The network is trained by minimizing the mean squared error loss of
the system. Our results show that, despite its simple and shallow structure,
the proposed neural network model exhibits comparable efficiency and accuracy
to traditional grid-based numerical methods.","['math.NA', 'cs.LG', 'cs.NA', '41A46, 65C05, 65N99, 68T01']","['Wei-Fan Hu', 'Te-Sheng Lin', 'Ming-Chih Lai']",2025-03-19,2025-03-19
2503.15438v1,VenusFactory: A Unified Platform for Protein Engineering Data Retrieval and Language Model Fine-Tuning,"Natural language processing (NLP) has significantly influenced scientific
domains beyond human language, including protein engineering, where pre-trained
protein language models (PLMs) have demonstrated remarkable success. However,
interdisciplinary adoption remains limited due to challenges in data
collection, task benchmarking, and application. This work presents
VenusFactory, a versatile engine that integrates biological data retrieval,
standardized task benchmarking, and modular fine-tuning of PLMs. VenusFactory
supports both computer science and biology communities with choices of both a
command-line execution and a Gradio-based no-code interface, integrating $40+$
protein-related datasets and $40+$ popular PLMs. All implementations are
open-sourced on https://github.com/tyang816/VenusFactory.","['cs.CL', 'cs.AI', 'q-bio.QM']","['Yang Tan', 'Chen Liu', 'Jingyuan Gao', 'Banghao Wu', 'Mingchen Li', 'Ruilin Wang', 'Lingrong Zhang', 'Huiqun Yu', 'Guisheng Fan', 'Liang Hong', 'Bingxin Zhou']",2025-03-19,2025-03-19
2503.15436v1,An extensive simulation study evaluating the interaction of resampling techniques across multiple causal discovery contexts,"Despite the accelerating presence of exploratory causal analysis in modern
science and medicine, the available non-experimental methods for validating
causal models are not well characterized. One of the most popular methods is to
evaluate the stability of model features after resampling the data, similar to
resampling methods for estimating confidence intervals in statistics. Many
aspects of this approach have received little to no attention, however, such as
whether the choice of resampling method should depend on the sample size,
algorithms being used, or algorithm tuning parameters. We present theoretical
results proving that certain resampling methods closely emulate the assignment
of specific values to algorithm tuning parameters. We also report the results
of extensive simulation experiments, which verify the theoretical result and
provide substantial data to aid researchers in further characterizing
resampling in the context of causal discovery analysis. Together, the
theoretical work and simulation results provide specific guidance on how
resampling methods and tuning parameters should be selected in practice.","['stat.ME', 'cs.AI']","['Ritwick Banerjee', 'Bryan Andrews', 'Erich Kummerfeld']",2025-03-19,2025-03-19
2503.15435v1,V2X-DG: Domain Generalization for Vehicle-to-Everything Cooperative Perception,"LiDAR-based Vehicle-to-Everything (V2X) cooperative perception has
demonstrated its impact on the safety and effectiveness of autonomous driving.
Since current cooperative perception algorithms are trained and tested on the
same dataset, the generalization ability of cooperative perception systems
remains underexplored. This paper is the first work to study the Domain
Generalization problem of LiDAR-based V2X cooperative perception (V2X-DG) for
3D detection based on four widely-used open source datasets: OPV2V, V2XSet,
V2V4Real and DAIR-V2X. Our research seeks to sustain high performance not only
within the source domain but also across other unseen domains, achieved solely
through training on source domain. To this end, we propose Cooperative Mixup
Augmentation based Generalization (CMAG) to improve the model generalization
capability by simulating the unseen cooperation, which is designed compactly
for the domain gaps in cooperative perception. Furthermore, we propose a
constraint for the regularization of the robust generalized feature
representation learning: Cooperation Feature Consistency (CFC), which aligns
the intermediately fused features of the generalized cooperation by CMAG and
the early fused features of the original cooperation in source domain.
Extensive experiments demonstrate that our approach achieves significant
performance gains when generalizing to other unseen datasets while it also
maintains strong performance on the source dataset.",['cs.CV'],"['Baolu Li', 'Zongzhe Xu', 'Jinlong Li', 'Xinyu Liu', 'Jianwu Fang', 'Xiaopeng Li', 'Hongkai Yu']",2025-03-19,2025-03-19
2503.15432v1,"Accurate, transferable, and verifiable machine-learned interatomic potentials for layered materials","Twisted layered van-der-Waals materials often exhibit unique electronic and
optical properties absent in their non-twisted counterparts. Unfortunately,
predicting such properties is hindered by the difficulty in determining the
atomic structure in materials displaying large moir\'e domains. Here, we
introduce a split machine-learned interatomic potential and dataset curation
approach that separates intralayer and interlayer interactions and
significantly improves model accuracy -- with a tenfold increase in energy and
force prediction accuracy relative to conventional models. We further
demonstrate that traditional MLIP validation metrics -- force and energy errors
-- are inadequate for moir\'e structures and develop a more holistic,
physically-motivated metric based on the distribution of stacking
configurations. This metric effectively compares the entirety of large-scale
moir\'e domains between two structures instead of relying on conventional
measures evaluated on smaller commensurate cells. Finally, we establish that
one-dimensional instead of two-dimensional moir\'e structures can serve as
efficient surrogate systems for validating MLIPs, allowing for a practical
model validation protocol against explicit DFT calculations. Applying our
framework to HfS2/GaS bilayers reveals that accurate structural predictions
directly translate into reliable electronic properties. Our model-agnostic
approach integrates seamlessly with various intralayer and interlayer
interaction models, enabling computationally tractable relaxation of moir\'e
materials, from bilayer to complex multilayers, with rigorously validated
accuracy.","['cond-mat.mtrl-sci', 'cs.LG']","['Johnathan D. Georgaras', 'Akash Ramdas', 'Chung Hsuan Shan', 'Elena Halsted', 'Berwyn', 'Tianshu Li', 'Felipe H. da Jornada']",2025-03-19,2025-03-19
2503.15426v1,Visual Position Prompt for MLLM based Visual Grounding,"Although Multimodal Large Language Models (MLLMs) excel at various
image-related tasks, they encounter challenges in precisely aligning
coordinates with spatial information within images, particularly in
position-aware tasks such as visual grounding. This limitation arises from two
key factors. First, MLLMs lack explicit spatial references, making it difficult
to associate textual descriptions with precise image locations. Second, their
feature extraction processes prioritize global context over fine-grained
spatial details, leading to weak localization capability. To address this
issue, we introduce VPP-LLaVA, an MLLM equipped with Visual Position Prompt
(VPP) to improve its grounding capability. VPP-LLaVA integrates two
complementary mechanisms. The global VPP overlays learnable, axis-like
embeddings onto the input image to provide structured spatial cues. The local
VPP focuses on fine-grained localization by incorporating position-aware
queries, which suggests probable object locations. We also introduce a VPP-SFT
dataset with 0.6M samples, consolidating high-quality visual grounding data
into a compact format for efficient model training. Training on this dataset
with VPP enhances the model's performance, achieving state-of-the-art results
on standard grounding benchmarks despite using fewer training samples compared
to other MLLMs like MiniGPT-v2, which rely on much larger datasets ($\sim$21M
samples). The code and VPP-SFT dataset will be available at
https://github.com/WayneTomas/VPP-LLaVA upon acceptance.","['cs.CV', 'cs.AI']","['Wei Tang', 'Yanpeng Sun', 'Qinying Gu', 'Zechao Li']",2025-03-19,2025-03-19
2503.15421v1,Probing the topology of the space of tokens with structured prompts,"This article presents a general and flexible method for prompting a large
language model (LLM) to reveal its (hidden) token input embedding up to
homeomorphism. Moreover, this article provides strong theoretical justification
-- a mathematical proof for generic LLMs -- for why this method should be
expected to work. With this method in hand, we demonstrate its effectiveness by
recovering the token subspace of Llemma-7B. The results of this paper apply not
only to LLMs but also to general nonlinear autoregressive processes.","['math.DG', 'cs.AI', '53Z50, 58Z05', 'I.2.7']","['Michael Robinson', 'Sourya Dey', 'Taisa Kushner']",2025-03-19,2025-03-19
2503.15420v1,LIFT: Latent Implicit Functions for Task- and Data-Agnostic Encoding,"Implicit Neural Representations (INRs) are proving to be a powerful paradigm
in unifying task modeling across diverse data domains, offering key advantages
such as memory efficiency and resolution independence. Conventional deep
learning models are typically modality-dependent, often requiring custom
architectures and objectives for different types of signals. However, existing
INR frameworks frequently rely on global latent vectors or exhibit
computational inefficiencies that limit their broader applicability. We
introduce LIFT, a novel, high-performance framework that addresses these
challenges by capturing multiscale information through meta-learning. LIFT
leverages multiple parallel localized implicit functions alongside a
hierarchical latent generator to produce unified latent representations that
span local, intermediate, and global features. This architecture facilitates
smooth transitions across local regions, enhancing expressivity while
maintaining inference efficiency. Additionally, we introduce ReLIFT, an
enhanced variant of LIFT that incorporates residual connections and expressive
frequency encodings. With this straightforward approach, ReLIFT effectively
addresses the convergence-capacity gap found in comparable methods, providing
an efficient yet powerful solution to improve capacity and speed up
convergence. Empirical results show that LIFT achieves state-of-the-art (SOTA)
performance in generative modeling and classification tasks, with notable
reductions in computational costs. Moreover, in single-task settings, the
streamlined ReLIFT architecture proves effective in signal representations and
inverse problem tasks.","['cs.LG', 'cs.CV']","['Amirhossein Kazerouni', 'Soroush Mehraban', 'Michael Brudno', 'Babak Taati']",2025-03-19,2025-03-19
2503.15417v1,Temporal Regularization Makes Your Video Generator Stronger,"Temporal quality is a critical aspect of video generation, as it ensures
consistent motion and realistic dynamics across frames. However, achieving high
temporal coherence and diversity remains challenging. In this work, we explore
temporal augmentation in video generation for the first time, and introduce
FluxFlow for initial investigation, a strategy designed to enhance temporal
quality. Operating at the data level, FluxFlow applies controlled temporal
perturbations without requiring architectural modifications. Extensive
experiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow
significantly improves temporal coherence and diversity across various video
generation models, including U-Net, DiT, and AR-based architectures, while
preserving spatial fidelity. These findings highlight the potential of temporal
augmentation as a simple yet effective approach to advancing video generation
quality.","['cs.CV', 'cs.AI']","['Harold Haodong Chen', 'Haojian Huang', 'Xianfeng Wu', 'Yexin Liu', 'Yajing Bai', 'Wen-Jie Shu', 'Harry Yang', 'Ser-Nam Lim']",2025-03-19,2025-03-19
2503.15415v1,Automated Processing of eXplainable Artificial Intelligence Outputs in Deep Learning Models for Fault Diagnostics of Large Infrastructures,"Deep Learning (DL) models processing images to recognize the health state of
large infrastructure components can exhibit biases and rely on non-causal
shortcuts. eXplainable Artificial Intelligence (XAI) can address these issues
but manually analyzing explanations generated by XAI techniques is
time-consuming and prone to errors. This work proposes a novel framework that
combines post-hoc explanations with semi-supervised learning to automatically
identify anomalous explanations that deviate from those of correctly classified
images and may therefore indicate model abnormal behaviors. This significantly
reduces the workload for maintenance decision-makers, who only need to manually
reclassify images flagged as having anomalous explanations. The proposed
framework is applied to drone-collected images of insulator shells for power
grid infrastructure monitoring, considering two different Convolutional Neural
Networks (CNNs), GradCAM explanations and Deep Semi-Supervised Anomaly
Detection. The average classification accuracy on two faulty classes is
improved by 8% and maintenance operators are required to manually reclassify
only 15% of the images. We compare the proposed framework with a
state-of-the-art approach based on the faithfulness metric: the experimental
results obtained demonstrate that the proposed framework consistently achieves
F_1 scores larger than those of the faithfulness-based approach. Additionally,
the proposed framework successfully identifies correct classifications that
result from non-causal shortcuts, such as the presence of ID tags printed on
insulator shells.","['cs.CV', 'cs.AI']","['Giovanni Floreale', 'Piero Baraldi', 'Enrico Zio', 'Olga Fink']",2025-03-19,2025-03-19
2503.15414v1,Federated Continual 3D Segmentation With Single-round Communication,"Federated learning seeks to foster collaboration among distributed clients
while preserving the privacy of their local data. Traditionally, federated
learning methods assume a fixed setting in which client data and learning
objectives remain constant. However, in real-world scenarios, new clients may
join, and existing clients may expand the segmentation label set as task
requirements evolve. In such a dynamic federated analysis setup, the
conventional federated communication strategy of model aggregation per
communication round is suboptimal. As new clients join, this strategy requires
retraining, linearly increasing communication and computation overhead. It also
imposes requirements for synchronized communication, which is difficult to
achieve among distributed clients. In this paper, we propose a federated
continual learning strategy that employs a one-time model aggregation at the
server through multi-model distillation. This approach builds and updates the
global model while eliminating the need for frequent server communication. When
integrating new data streams or onboarding new clients, this approach
efficiently reuses previous client models, avoiding the need to retrain the
global model across the entire federation. By minimizing communication load and
bypassing the need to put unchanged clients online, our approach relaxes
synchronization requirements among clients, providing an efficient and scalable
federated analysis framework suited for real-world applications. Using
multi-class 3D abdominal CT segmentation as an application task, we demonstrate
the effectiveness of the proposed approach.","['eess.IV', 'cs.CV']","['Can Peng', 'Qianhui Men', 'Pramit Saha', 'Qianye Yang', 'Cheng Ouyang', 'J. Alison Noble']",2025-03-19,2025-03-19
2503.15412v1,Learn Your Scales: Towards Scale-Consistent Generative Novel View Synthesis,"Conventional depth-free multi-view datasets are captured using a moving
monocular camera without metric calibration. The scales of camera positions in
this monocular setting are ambiguous. Previous methods have acknowledged scale
ambiguity in multi-view data via various ad-hoc normalization pre-processing
steps, but have not directly analyzed the effect of incorrect scene scales on
their application. In this paper, we seek to understand and address the effect
of scale ambiguity when used to train generative novel view synthesis methods
(GNVS). In GNVS, new views of a scene or object can be minimally synthesized
given a single image and are, thus, unconstrained, necessitating the use of
generative methods. The generative nature of these models captures all aspects
of uncertainty, including any uncertainty of scene scales, which act as
nuisance variables for the task. We study the effect of scene scale ambiguity
in GNVS when sampled from a single image by isolating its effect on the
resulting models and, based on these intuitions, define new metrics that
measure the scale inconsistency of generated views. We then propose a framework
to estimate scene scales jointly with the GNVS model in an end-to-end fashion.
Empirically, we show that our method reduces the scale inconsistency of
generated views without the complexity or downsides of previous scale
normalization methods. Further, we show that removing this ambiguity improves
generated image quality of the resulting GNVS model.",['cs.CV'],"['Fereshteh Forghani', 'Jason J. Yu', 'Tristan Aumentado-Armstrong', 'Konstantinos G. Derpanis', 'Marcus A. Brubaker']",2025-03-19,2025-03-19
2503.15407v1,Exploiting Prior Knowledge in Preferential Learning of Individualized Autonomous Vehicle Driving Styles,"Trajectory planning for automated vehicles commonly employs optimization over
a moving horizon - Model Predictive Control - where the cost function
critically influences the resulting driving style. However, finding a suitable
cost function that results in a driving style preferred by passengers remains
an ongoing challenge. We employ preferential Bayesian optimization to learn the
cost function by iteratively querying a passenger's preference. Due to
increasing dimensionality of the parameter space, preference learning
approaches might struggle to find a suitable optimum with a limited number of
experiments and expose the passenger to discomfort when exploring the parameter
space. We address these challenges by incorporating prior knowledge into the
preferential Bayesian optimization framework. Our method constructs a virtual
decision maker from real-world human driving data to guide parameter sampling.
In a simulation experiment, we achieve faster convergence of the
prior-knowledge-informed learning procedure compared to existing preferential
Bayesian optimization approaches and reduce the number of inadequate driving
styles sampled.","['eess.SY', 'cs.LG', 'cs.SY']","['Lukas Theiner', 'Sebastian Hirt', 'Alexander Steinke', 'Rolf Findeisen']",2025-03-19,2025-03-19
2503.15583v1,Efficient Post-Hoc Uncertainty Calibration via Variance-Based Smoothing,"Since state-of-the-art uncertainty estimation methods are often
computationally demanding, we investigate whether incorporating prior
information can improve uncertainty estimates in conventional deep neural
networks. Our focus is on machine learning tasks where meaningful predictions
can be made from sub-parts of the input. For example, in speaker
classification, the speech waveform can be divided into sequential patches,
each containing information about the same speaker. We observe that the
variance between sub-predictions serves as a reliable proxy for uncertainty in
such settings. Our proposed variance-based scaling framework produces
competitive uncertainty estimates in classification while being less
computationally demanding and allowing for integration as a post-hoc
calibration tool. This approach also leads to a simple extension of deep
ensembles, improving the expressiveness of their predicted distributions.",['cs.LG'],"['Fabian Denoodt', 'José Oramas']",2025-03-19,2025-03-19
2503.15406v1,Visual Persona: Foundation Model for Full-Body Human Customization,"We introduce Visual Persona, a foundation model for text-to-image full-body
human customization that, given a single in-the-wild human image, generates
diverse images of the individual guided by text descriptions. Unlike prior
methods that focus solely on preserving facial identity, our approach captures
detailed full-body appearance, aligning with text descriptions for body
structure and scene variations. Training this model requires large-scale paired
human data, consisting of multiple images per individual with consistent
full-body identities, which is notoriously difficult to obtain. To address
this, we propose a data curation pipeline leveraging vision-language models to
evaluate full-body appearance consistency, resulting in Visual Persona-500K, a
dataset of 580k paired human images across 100k unique identities. For precise
appearance transfer, we introduce a transformer encoder-decoder architecture
adapted to a pre-trained text-to-image diffusion model, which augments the
input image into distinct body regions, encodes these regions as local
appearance features, and projects them into dense identity embeddings
independently to condition the diffusion model for synthesizing customized
images. Visual Persona consistently surpasses existing approaches, generating
high-quality, customized images from in-the-wild inputs. Extensive ablation
studies validate design choices, and we demonstrate the versatility of Visual
Persona across various downstream tasks.",['cs.CV'],"['Jisu Nam', 'Soowon Son', 'Zhan Xu', 'Jing Shi', 'Difan Liu', 'Feng Liu', 'Aashish Misraa', 'Seungryong Kim', 'Yang Zhou']",2025-03-19,2025-03-19
2503.15404v1,Improving Adversarial Transferability on Vision Transformers via Forward Propagation Refinement,"Vision Transformers (ViTs) have been widely applied in various computer
vision and vision-language tasks. To gain insights into their robustness in
practical scenarios, transferable adversarial examples on ViTs have been
extensively studied. A typical approach to improving adversarial
transferability is by refining the surrogate model. However, existing work on
ViTs has restricted their surrogate refinement to backward propagation. In this
work, we instead focus on Forward Propagation Refinement (FPR) and specifically
refine two key modules of ViTs: attention maps and token embeddings. For
attention maps, we propose Attention Map Diversification (AMD), which
diversifies certain attention maps and also implicitly imposes beneficial
gradient vanishing during backward propagation. For token embeddings, we
propose Momentum Token Embedding (MTE), which accumulates historical token
embeddings to stabilize the forward updates in both the Attention and MLP
blocks. We conduct extensive experiments with adversarial examples transferred
from ViTs to various CNNs and ViTs, demonstrating that our FPR outperforms the
current best (backward) surrogate refinement by up to 7.0\% on average. We also
validate its superiority against popular defenses and its compatibility with
other transfer methods. Codes and appendix are available at
https://github.com/RYC-98/FPR.","['cs.CV', 'cs.CR']","['Yuchen Ren', 'Zhengyu Zhao', 'Chenhao Lin', 'Bo Yang', 'Lu Zhou', 'Zhe Liu', 'Chao Shen']",2025-03-19,2025-03-19
2503.15403v1,HQNN-FSP: A Hybrid Classical-Quantum Neural Network for Regression-Based Financial Stock Market Prediction,"Financial time-series forecasting remains a challenging task due to complex
temporal dependencies and market fluctuations. This study explores the
potential of hybrid quantum-classical approaches to assist in financial trend
prediction by leveraging quantum resources for improved feature representation
and learning. A custom Quantum Neural Network (QNN) regressor is introduced,
designed with a novel ansatz tailored for financial applications. Two hybrid
optimization strategies are proposed: (1) a sequential approach where classical
recurrent models (RNN/LSTM) extract temporal dependencies before quantum
processing, and (2) a joint learning framework that optimizes classical and
quantum parameters simultaneously. Systematic evaluation using TimeSeriesSplit,
k-fold cross-validation, and predictive error analysis highlights the ability
of these hybrid models to integrate quantum computing into financial
forecasting workflows. The findings demonstrate how quantum-assisted learning
can contribute to financial modeling, offering insights into the practical role
of quantum resources in time-series analysis.","['q-fin.ST', 'cs.LG', 'quant-ph']","['Prashant Kumar Choudhary', 'Nouhaila Innan', 'Muhammad Shafique', 'Rajeev Singh']",2025-03-19,2025-03-19
2503.15402v1,Towards efficient keyword spotting using spike-based time difference encoders,"Keyword spotting in edge devices is becoming increasingly important as
voice-activated assistants are widely used. However, its deployment is often
limited by the extreme low-power constraints of the target embedded systems.
Here, we explore the Temporal Difference Encoder (TDE) performance in keyword
spotting. This recent neuron model encodes the time difference in instantaneous
frequency and spike count to perform efficient keyword spotting with
neuromorphic processors. We use the TIdigits dataset of spoken digits with a
formant decomposition and rate-based encoding into spikes. We compare three
Spiking Neural Networks (SNNs) architectures to learn and classify
spatio-temporal signals. The proposed SNN architectures are made of three
layers with variation in its hidden layer composed of either (1) feedforward
TDE, (2) feedforward Current-Based Leaky Integrate-and-Fire (CuBa-LIF), or (3)
recurrent CuBa-LIF neurons. We first show that the spike trains of the
frequency-converted spoken digits have a large amount of information in the
temporal domain, reinforcing the importance of better exploiting temporal
encoding for such a task. We then train the three SNNs with the same number of
synaptic weights to quantify and compare their performance based on the
accuracy and synaptic operations. The resulting accuracy of the feedforward TDE
network (89%) is higher than the feedforward CuBa-LIF network (71%) and close
to the recurrent CuBa-LIF network (91%). However, the feedforward TDE-based
network performs 92% fewer synaptic operations than the recurrent CuBa-LIF
network with the same amount of synapses. In addition, the results of the TDE
network are highly interpretable and correlated with the frequency and
timescale features of the spoken keywords in the dataset. Our findings suggest
that the TDE is a promising neuron model for scalable event-driven processing
of spatio-temporal patterns.","['cs.NE', 'cs.AI', 'cs.CV', 'cs.ET']","['Alejandro Pequeño-Zurro', 'Lyes Khacef', 'Stefano Panzeri', 'Elisabetta Chicca']",2025-03-19,2025-03-19
2503.15390v1,FedSCA: Federated Tuning with Similarity-guided Collaborative Aggregation for Heterogeneous Medical Image Segmentation,"Transformer-based foundation models (FMs) have recently demonstrated
remarkable performance in medical image segmentation. However, scaling these
models is challenging due to the limited size of medical image datasets within
isolated hospitals, where data centralization is restricted due to privacy
concerns. These constraints, combined with the data-intensive nature of FMs,
hinder their broader application. Integrating federated learning (FL) with
foundation models (FLFM) fine-tuning offers a potential solution to these
challenges by enabling collaborative model training without data sharing, thus
allowing FMs to take advantage of a diverse pool of sensitive medical image
data across hospitals/clients. However, non-independent and identically
distributed (non-IID) data among clients, paired with computational and
communication constraints in federated environments, presents an additional
challenge that limits further performance improvements and remains inadequately
addressed in existing studies. In this work, we propose a novel FLFM
fine-tuning framework, \underline{\textbf{Fed}}erated tuning with
\underline{\textbf{S}}imilarity-guided \underline{\textbf{C}}ollaborative
\underline{\textbf{A}}ggregation (FedSCA), encompassing all phases of the FL
process. This includes (1) specially designed parameter-efficient fine-tuning
(PEFT) for local client training to enhance computational efficiency; (2)
partial low-level adapter transmission for communication efficiency; and (3)
similarity-guided collaborative aggregation (SGCA) on the server side to
address non-IID issues. Extensive experiments on three FL benchmarks for
medical image segmentation demonstrate the effectiveness of our proposed
FedSCA, establishing new SOTA performance.","['eess.IV', 'cs.CV']","['Yumin Zhang', 'Yan Gao', 'Haoran Duan', 'Hanqing Guo', 'Tejal Shah', 'Rajiv Ranjan', 'Bo Wei']",2025-03-19,2025-03-19
2503.15386v1,CCDP: Composition of Conditional Diffusion Policies with Guided Sampling,"Imitation Learning offers a promising approach to learn directly from data
without requiring explicit models, simulations, or detailed task definitions.
During inference, actions are sampled from the learned distribution and
executed on the robot. However, sampled actions may fail for various reasons,
and simply repeating the sampling step until a successful action is obtained
can be inefficient. In this work, we propose an enhanced sampling strategy that
refines the sampling distribution to avoid previously unsuccessful actions. We
demonstrate that by solely utilizing data from successful demonstrations, our
method can infer recovery actions without the need for additional exploratory
behavior or a high-level controller. Furthermore, we leverage the concept of
diffusion model decomposition to break down the primary problem (which may
require long-horizon history to manage failures) into multiple smaller, more
manageable sub-problems in learning, data collection, and inference, thereby
enabling the system to adapt to variable failure counts. Our approach yields a
low-level controller that dynamically adjusts its sampling space to improve
efficiency when prior samples fall short. We validate our method across several
tasks, including door opening with unknown directions, object manipulation, and
button-searching scenarios, demonstrating that our approach outperforms
traditional baselines.","['cs.RO', 'cs.AI']","['Amirreza Razmjoo', 'Sylvain Calinon', 'Michael Gienger', 'Fan Zhang']",2025-03-19,2025-03-19
2503.15374v1,Real-world validation of a multimodal LLM-powered pipeline for High-Accuracy Clinical Trial Patient Matching leveraging EHR data,"Background: Patient recruitment in clinical trials is hindered by complex
eligibility criteria and labor-intensive chart reviews. Prior research using
text-only models have struggled to address this problem in a reliable and
scalable way due to (1) limited reasoning capabilities, (2) information loss
from converting visual records to text, and (3) lack of a generic EHR
integration to extract patient data.
  Methods: We introduce a broadly applicable, integration-free, LLM-powered
pipeline that automates patient-trial matching using unprocessed documents
extracted from EHRs. Our approach leverages (1) the new reasoning-LLM paradigm,
enabling the assessment of even the most complex criteria, (2) visual
capabilities of latest LLMs to interpret medical records without lossy
image-to-text conversions, and (3) multimodal embeddings for efficient medical
record search. The pipeline was validated on the n2c2 2018 cohort selection
dataset (288 diabetic patients) and a real-world dataset composed of 485
patients from 30 different sites matched against 36 diverse trials.
  Results: On the n2c2 dataset, our method achieved a new state-of-the-art
criterion-level accuracy of 93\%. In real-world trials, the pipeline yielded an
accuracy of 87\%, undermined by the difficulty to replicate human
decision-making when medical records lack sufficient information. Nevertheless,
users were able to review overall eligibility in under 9 minutes per patient on
average, representing an 80\% improvement over traditional manual chart
reviews.
  Conclusion: This pipeline demonstrates robust performance in clinical trial
patient matching without requiring custom integration with site systems or
trial-specific tailoring, thereby enabling scalable deployment across sites
seeking to leverage AI for patient matching.","['cs.CL', 'cs.AI']","['Anatole Callies', 'Quentin Bodinier', 'Philippe Ravaud', 'Kourosh Davarpanah']",2025-03-19,2025-03-19
2503.15371v1,Geometrically-Aware One-Shot Skill Transfer of Category-Level Objects,"Robotic manipulation of unfamiliar objects in new environments is challenging
and requires extensive training or laborious pre-programming. We propose a new
skill transfer framework, which enables a robot to transfer complex object
manipulation skills and constraints from a single human demonstration. Our
approach addresses the challenge of skill acquisition and task execution by
deriving geometric representations from demonstrations focusing on
object-centric interactions. By leveraging the Functional Maps (FM) framework,
we efficiently map interaction functions between objects and their
environments, allowing the robot to replicate task operations across objects of
similar topologies or categories, even when they have significantly different
shapes. Additionally, our method incorporates a Task-Space Imitation Algorithm
(TSIA) which generates smooth, geometrically-aware robot paths to ensure the
transferred skills adhere to the demonstrated task constraints. We validate the
effectiveness and adaptability of our approach through extensive experiments,
demonstrating successful skill transfer and task execution in diverse
real-world environments without requiring additional training.","['cs.RO', 'cs.LG']","['Cristiana de Farias', 'Luis Figueredo', 'Riddhiman Laha', 'Maxime Adjigble', 'Brahim Tamadazte', 'Rustam Stolkin', 'Sami Haddadin', 'Naresh Marturi']",2025-03-19,2025-03-19
2503.15369v1,EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language Models,"While multimodal large language models demonstrate strong performance in
complex reasoning tasks, they pose significant challenges related to model
complexity during deployment, especially for resource-limited devices. In this
paper, we propose an automatic pruning method for large vision-language models
to enhance the efficiency of multimodal reasoning. Conventional methods rely on
the training data of the original model to select the proper pruning ratio for
different network components. However, these methods are impractical for large
vision-language models due to the unaffordable search costs caused by web-scale
training corpus. In contrast, our approach only leverages a small number of
samples to search for the desired pruning policy by maximizing its
generalization ability on unknown training data while maintaining the model
accuracy, which enables the achievement of an optimal trade-off between
accuracy and efficiency for large visual language models. Specifically, we
formulate the generalization gap of the pruning strategy using the structural
risk minimization principle. Based on both task performance and generalization
capability, we iteratively search for the optimal pruning policy within a given
search space and optimize the vision projector to evolve the search space with
higher upper bound of performance. We conduct extensive experiments on the
ScienceQA, Vizwiz, MM-vet, and LLaVA-Bench datasets for the task of visual
question answering. Using only 64 samples for pruning policy search,
EfficientLLaVA achieves an accuracy of 83.05% on ScienceQA, along with a
$\times$ 1.8 speedup compared to the dense LLaVA-v1.5-7B model.",['cs.CV'],"['Yinan Liang', 'Ziwei Wang', 'Xiuwei Xu', 'Jie Zhou', 'Jiwen Lu']",2025-03-19,2025-03-19
2503.15368v1,Online Imitation Learning for Manipulation via Decaying Relative Correction through Teleoperation,"Teleoperated robotic manipulators enable the collection of demonstration
data, which can be used to train control policies through imitation learning.
However, such methods can require significant amounts of training data to
develop robust policies or adapt them to new and unseen tasks. While expert
feedback can significantly enhance policy performance, providing continuous
feedback can be cognitively demanding and time-consuming for experts. To
address this challenge, we propose to use a cable-driven teleoperation system
which can provide spatial corrections with 6 degree of freedom to the
trajectories generated by a policy model. Specifically, we propose a correction
method termed Decaying Relative Correction (DRC) which is based upon the
spatial offset vector provided by the expert and exists temporarily, and which
reduces the intervention steps required by an expert. Our results demonstrate
that DRC reduces the required expert intervention rate by 30\% compared to a
standard absolute corrective method. Furthermore, we show that integrating DRC
within an online imitation learning framework rapidly increases the success
rate of manipulation tasks such as raspberry harvesting and cloth wiping.","['cs.RO', 'cs.LG']","['Cheng Pan', 'Hung Hon Cheng', 'Josie Hughes']",2025-03-19,2025-03-19
2503.15367v1,FedBEns: One-Shot Federated Learning based on Bayesian Ensemble,"One-Shot Federated Learning (FL) is a recent paradigm that enables multiple
clients to cooperatively learn a global model in a single round of
communication with a central server. In this paper, we analyze the One-Shot FL
problem through the lens of Bayesian inference and propose FedBEns, an
algorithm that leverages the inherent multimodality of local loss functions to
find better global models. Our algorithm leverages a mixture of Laplace
approximations for the clients' local posteriors, which the server then
aggregates to infer the global model. We conduct extensive experiments on
various datasets, demonstrating that the proposed method outperforms competing
baselines that typically rely on unimodal approximations of the local losses.",['cs.LG'],"['Jacopo Talpini', 'Marco Savi', 'Giovanni Neglia']",2025-03-19,2025-03-19
2503.15361v1,Boosting HDR Image Reconstruction via Semantic Knowledge Transfer,"Recovering High Dynamic Range (HDR) images from multiple Low Dynamic Range
(LDR) images becomes challenging when the LDR images exhibit noticeable
degradation and missing content. Leveraging scene-specific semantic priors
offers a promising solution for restoring heavily degraded regions. However,
these priors are typically extracted from sRGB Standard Dynamic Range (SDR)
images, the domain/format gap poses a significant challenge when applying it to
HDR imaging. To address this issue, we propose a general framework that
transfers semantic knowledge derived from SDR domain via self-distillation to
boost existing HDR reconstruction. Specifically, the proposed framework first
introduces the Semantic Priors Guided Reconstruction Model (SPGRM), which
leverages SDR image semantic knowledge to address ill-posed problems in the
initial HDR reconstruction results. Subsequently, we leverage a
self-distillation mechanism that constrains the color and content information
with semantic knowledge, aligning the external outputs between the baseline and
SPGRM. Furthermore, to transfer the semantic knowledge of the internal
features, we utilize a semantic knowledge alignment module (SKAM) to fill the
missing semantic contents with the complementary masks. Extensive experiments
demonstrate that our method can significantly improve the HDR imaging quality
of existing methods.",['cs.CV'],"['Qingsen Yan', 'Tao Hu', 'Genggeng Chen', 'Wei Dong', 'Yanning Zhang']",2025-03-19,2025-03-19
2503.15358v1,SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation,"Idiomatic expressions present a unique challenge in NLP, as their meanings
are often not directly inferable from their constituent words. Despite recent
advancements in Large Language Models (LLMs), idiomaticity remains a
significant obstacle to robust semantic representation. We present datasets and
tasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity
Representation), which challenges the community to assess and improve models'
ability to interpret idiomatic expressions in multimodal contexts and in
multiple languages. Participants competed in two subtasks: ranking images based
on their alignment with idiomatic or literal meanings, and predicting the next
image in a sequence. The most effective methods achieved human-level
performance by leveraging pretrained LLMs and vision-language models in
mixture-of-experts settings, with multiple queries used to smooth over the
weaknesses in these models' representations of idiomaticity.","['cs.CL', 'cs.CV', 'I.2.7; I.4.m']","['Thomas Pickard', 'Aline Villavicencio', 'Maggie Mi', 'Wei He', 'Dylan Phelps', 'Carolina Scarton', 'Marco Idiart']",2025-03-19,2025-03-19
2503.15355v1,Robustness of Nonlinear Representation Learning,"We study the problem of unsupervised representation learning in slightly
misspecified settings, and thus formalize the study of robustness of nonlinear
representation learning. We focus on the case where the mixing is close to a
local isometry in a suitable distance and show based on existing rigidity
results that the mixing can be identified up to linear transformations and
small errors. In a second step, we investigate Independent Component Analysis
(ICA) with observations generated according to $x=f(s)=As+h(s)$ where $A$ is an
invertible mixing matrix and $h$ a small perturbation. We show that we can
approximately recover the matrix $A$ and the independent components. Together,
these two results show approximate identifiability of nonlinear ICA with almost
isometric mixing functions. Those results are a step towards identifiability
results for unsupervised representation learning for real-world data that do
not follow restrictive model classes.","['stat.ML', 'cs.LG']","['Simon Buchholz', 'Bernhard Schölkopf']",2025-03-19,2025-03-19
2503.15354v1,Optimizing Decomposition for Optimal Claim Verification,"Current research on the \textit{Decompose-Then-Verify} paradigm for
evaluating the factuality of long-form text typically treats decomposition and
verification in isolation, overlooking their interactions and potential
misalignment. We find that existing decomposition policies, typically
hand-crafted demonstrations, do not align well with downstream verifiers in
terms of atomicity -- a novel metric quantifying information density -- leading
to suboptimal verification results. We formulate finding the optimal
decomposition policy for optimal verification as a bilevel optimization
problem. To approximate a solution for this strongly NP-hard problem, we
propose dynamic decomposition, a reinforcement learning framework that
leverages verifier feedback to learn a policy for dynamically decomposing
claims to verifier-preferred atomicity. Experimental results show that dynamic
decomposition outperforms existing decomposition policies, improving
verification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on
average across varying verifiers, datasets, and atomcities of input claims.","['cs.CL', 'cs.AI']","['Yining Lu', 'Noah Ziems', 'Hy Dang', 'Meng Jiang']",2025-03-19,2025-03-19
2503.15352v1,Leveraging Perfect Multimodal Alignment and Gaussian Assumptions for Cross-modal Transfer,"Multimodal alignment aims to construct a joint latent vector space where two
modalities representing the same concept map to the same vector. We formulate
this as an inverse problem and show that under certain conditions perfect
alignment can be achieved. We then address a specific application of alignment
referred to as cross-modal transfer. Unsupervised cross-modal transfer aims to
leverage a model trained with one modality to perform inference on another
modality, without any labeled fine-tuning on the new modality. Assuming that
semantic classes are represented as a mixture of Gaussians in the latent space,
we show how cross-modal transfer can be performed by projecting the data points
from the representation space onto different subspaces representing each
modality. Our experiments on synthetic multimodal Gaussian data verify the
effectiveness of our perfect alignment and cross-modal transfer method. We hope
these findings inspire further exploration of the applications of perfect
alignment and the use of Gaussian models for cross-modal learning.","['cs.LG', 'cs.AI', 'cs.CV', 'eess.SP']","['Abhi Kamboj', 'Minh N. Do']",2025-03-19,2025-03-19
2503.15351v1,SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling with Large Language Models,"In this paper, we propose Selection and Pooling with Large Language Models
(SPILL), an intuitive and domain-adaptive method for intent clustering without
fine-tuning. Existing embeddings-based clustering methods rely on a few labeled
examples or unsupervised fine-tuning to optimize results for each new dataset,
which makes them less generalizable to multiple datasets. Our goal is to make
these existing embedders more generalizable to new domain datasets without
further fine-tuning. Inspired by our theoretical derivation and simulation
results on the effectiveness of sampling and pooling techniques, we view the
clustering task as a small-scale selection problem. A good solution to this
problem is associated with better clustering performance. Accordingly, we
propose a two-stage approach: First, for each utterance (referred to as the
seed), we derive its embedding using an existing embedder. Then, we apply a
distance metric to select a pool of candidates close to the seed. Because the
embedder is not optimized for new datasets, in the second stage, we use an LLM
to further select utterances from these candidates that share the same intent
as the seed. Finally, we pool these selected candidates with the seed to derive
a refined embedding for the seed. We found that our method generally
outperforms directly using an embedder, and it achieves comparable results to
other state-of-the-art studies, even those that use much larger models and
require fine-tuning, showing its strength and efficiency. Our results indicate
that our method enables existing embedders to be further improved without
additional fine-tuning, making them more adaptable to new domain datasets.
Additionally, viewing the clustering task as a small-scale selection problem
gives the potential of using LLMs to customize clustering tasks according to
the user's goals.",['cs.CL'],"['I-Fan Lin', 'Faegheh Hasibi', 'Suzan Verberne']",2025-03-19,2025-03-19
2503.15342v1,TruthLens:A Training-Free Paradigm for DeepFake Detection,"The proliferation of synthetic images generated by advanced AI models poses
significant challenges in identifying and understanding manipulated visual
content. Current fake image detection methods predominantly rely on binary
classification models that focus on accuracy while often neglecting
interpretability, leaving users without clear insights into why an image is
deemed real or fake. To bridge this gap, we introduce TruthLens, a novel
training-free framework that reimagines deepfake detection as a visual
question-answering (VQA) task. TruthLens utilizes state-of-the-art large
vision-language models (LVLMs) to observe and describe visual artifacts and
combines this with the reasoning capabilities of large language models (LLMs)
like GPT-4 to analyze and aggregate evidence into informed decisions. By
adopting a multimodal approach, TruthLens seamlessly integrates visual and
semantic reasoning to not only classify images as real or fake but also provide
interpretable explanations for its decisions. This transparency enhances trust
and provides valuable insights into the artifacts that signal synthetic
content. Extensive evaluations demonstrate that TruthLens outperforms
conventional methods, achieving high accuracy on challenging datasets while
maintaining a strong emphasis on explainability. By reframing deepfake
detection as a reasoning-driven process, TruthLens establishes a new paradigm
in combating synthetic media, combining cutting-edge performance with
interpretability to address the growing threats of visual disinformation.","['cs.CV', 'cs.AI']","['Ritabrata Chakraborty', 'Rajatsubhra Chakraborty', 'Ali Khaleghi Rahimian', 'Thomas MacDougall']",2025-03-19,2025-03-19
2503.15582v1,Hierarchical clustering with maximum density paths and mixture models,"Hierarchical clustering is an effective and interpretable technique for
analyzing structure in data, offering a nuanced understanding by revealing
insights at multiple scales and resolutions. It is particularly helpful in
settings where the exact number of clusters is unknown, and provides a robust
framework for exploring complex datasets. Additionally, hierarchical clustering
can uncover inner structures within clusters, capturing subtle relationships
and nested patterns that may be obscured by traditional flat clustering
methods. However, existing hierarchical clustering methods struggle with
high-dimensional data, especially when there are no clear density gaps between
modes. Our method addresses this limitation by leveraging a two-stage approach,
first employing a Gaussian or Student's t mixture model to overcluster the
data, and then hierarchically merging clusters based on the induced density
landscape. This approach yields state-of-the-art clustering performance while
also providing a meaningful hierarchy, making it a valuable tool for
exploratory data analysis. Code is available at
https://github.com/ecker-lab/tneb clustering.","['stat.ML', 'cs.LG']","['Martin Ritzert', 'Polina Turishcheva', 'Laura Hansel', 'Paul Wollenhaupt', 'Marissa Weis', 'Alexander Ecker']",2025-03-19,2025-03-19
2503.15338v1,Solla: Towards a Speech-Oriented LLM That Hears Acoustic Context,"Large Language Models (LLMs) have recently shown remarkable ability to
process not only text but also multimodal inputs such as speech and audio.
However, most existing models primarily focus on analyzing input signals using
text instructions, overlooking scenarios in which speech instructions and audio
are mixed and serve as inputs to the model. To address these challenges, we
introduce Solla, a novel framework designed to understand speech-based
questions and hear the acoustic context concurrently. Solla incorporates an
audio tagging module to effectively identify and represent audio events, as
well as an ASR-assisted prediction method to improve comprehension of spoken
content. To rigorously evaluate Solla and other publicly available models, we
propose a new benchmark dataset called SA-Eval, which includes three tasks:
audio event classification, audio captioning, and audio question answering.
SA-Eval has diverse speech instruction with various speaking styles,
encompassing two difficulty levels, easy and hard, to capture the range of
real-world acoustic conditions. Experimental results show that Solla performs
on par with or outperforms baseline models on both the easy and hard test sets,
underscoring its effectiveness in jointly understanding speech and audio.","['eess.AS', 'cs.CL', 'cs.SD']","['Junyi Ao', 'Dekun Chen', 'Xiaohai Tian', 'Wenjie Feng', 'Jun Zhang', 'Lu Lu', 'Yuxuan Wang', 'Haizhou Li', 'Zhizheng Wu']",2025-03-19,2025-03-19
2503.15337v1,Recover and Match: Open-Vocabulary Multi-Label Recognition through Knowledge-Constrained Optimal Transport,"Identifying multiple novel classes in an image, known as open-vocabulary
multi-label recognition, is a challenging task in computer vision. Recent
studies explore the transfer of powerful vision-language models such as CLIP.
However, these approaches face two critical challenges: (1) The local semantics
of CLIP are disrupted due to its global pre-training objectives, resulting in
unreliable regional predictions. (2) The matching property between image
regions and candidate labels has been neglected, relying instead on naive
feature aggregation such as average pooling, which leads to spurious
predictions from irrelevant regions. In this paper, we present RAM (Recover And
Match), a novel framework that effectively addresses the above issues. To
tackle the first problem, we propose Ladder Local Adapter (LLA) to enforce
refocusing on local regions, recovering local semantics in a memory-friendly
way. For the second issue, we propose Knowledge-Constrained Optimal Transport
(KCOT) to suppress meaningless matching to non-GT labels by formulating the
task as an optimal transport problem. As a result, RAM achieves
state-of-the-art performance on various datasets from three distinct domains,
and shows great potential to boost the existing methods. Code:
https://github.com/EricTan7/RAM.",['cs.CV'],"['Hao Tan', 'Zichang Tan', 'Jun Li', 'Ajian Liu', 'Jun Wan', 'Zhen Lei']",2025-03-19,2025-03-19
2503.16171v1,Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation,"Modern text-to-image generative models can inadvertently reproduce
copyrighted content memorized in their training data, raising serious concerns
about potential copyright infringement. We introduce Guardians of Generation, a
model agnostic inference time framework for dynamic copyright shielding in AI
image generation. Our approach requires no retraining or modification of the
generative model weights, instead integrating seamlessly with existing
diffusion pipelines. It augments the generation process with an adaptive
guidance mechanism comprising three components: a detection module, a prompt
rewriting module, and a guidance adjustment module. The detection module
monitors user prompts and intermediate generation steps to identify features
indicative of copyrighted content before they manifest in the final output. If
such content is detected, the prompt rewriting mechanism dynamically transforms
the user's prompt by sanitizing or replacing references that could trigger
copyrighted material while preserving the prompt's intended semantics. The
adaptive guidance module adaptively steers the diffusion process away from
flagged content by modulating the model's sampling trajectory. Together, these
components form a robust shield that enables a tunable balance between
preserving creative fidelity and ensuring copyright compliance. We validate our
method on a variety of generative models such as Stable Diffusion, SDXL, and
Flux, demonstrating substantial reductions in copyrighted content generation
with negligible impact on output fidelity or alignment with user intent. This
work provides a practical, plug-and-play safeguard for generative image models,
enabling more responsible deployment under real-world copyright constraints.
Source code is available at: https://respailab.github.io/gog",['cs.CV'],"['Soham Roy', 'Abhishek Mishra', 'Shirish Karande', 'Murari Mandal']",2025-03-19,2025-03-19
2503.15321v1,Euclid Quick Data Release (Q1). Active galactic nuclei identification using diffusion-based inpainting of Euclid VIS images,"Light emission from galaxies exhibit diverse brightness profiles, influenced
by factors such as galaxy type, structural features and interactions with other
galaxies. Elliptical galaxies feature more uniform light distributions, while
spiral and irregular galaxies have complex, varied light profiles due to their
structural heterogeneity and star-forming activity. In addition, galaxies with
an active galactic nucleus (AGN) feature intense, concentrated emission from
gas accretion around supermassive black holes, superimposed on regular galactic
light, while quasi-stellar objects (QSO) are the extreme case of the AGN
emission dominating the galaxy. The challenge of identifying AGN and QSO has
been discussed many times in the literature, often requiring multi-wavelength
observations. This paper introduces a novel approach to identify AGN and QSO
from a single image. Diffusion models have been recently developed in the
machine-learning literature to generate realistic-looking images of everyday
objects. Utilising the spatial resolving power of the Euclid VIS images, we
created a diffusion model trained on one million sources, without using any
source pre-selection or labels. The model learns to reconstruct light
distributions of normal galaxies, since the population is dominated by them. We
condition the prediction of the central light distribution by masking the
central few pixels of each source and reconstruct the light according to the
diffusion model. We further use this prediction to identify sources that
deviate from this profile by examining the reconstruction error of the few
central pixels regenerated in each source's core. Our approach, solely using
VIS imaging, features high completeness compared to traditional methods of AGN
and QSO selection, including optical, near-infrared, mid-infrared, and X-rays.
[abridged]","['astro-ph.GA', 'cs.CV']","['Euclid Collaboration', 'G. Stevens', 'S. Fotopoulou', 'M. N. Bremer', 'T. Matamoro Zatarain', 'K. Jahnke', 'B. Margalef-Bentabol', 'M. Huertas-Company', 'M. J. Smith', 'M. Walmsley', 'M. Salvato', 'M. Mezcua', 'A. Paulino-Afonso', 'M. Siudek', 'M. Talia', 'F. Ricci', 'W. Roster', 'N. Aghanim', 'B. Altieri', 'S. Andreon', 'H. Aussel', 'C. Baccigalupi', 'M. Baldi', 'S. Bardelli', 'P. Battaglia', 'A. Biviano', 'A. Bonchi', 'E. Branchini', 'M. Brescia', 'J. Brinchmann', 'S. Camera', 'G. Cañas-Herrera', 'V. Capobianco', 'C. Carbone', 'J. Carretero', 'M. Castellano', 'G. Castignani', 'S. Cavuoti', 'K. C. Chambers', 'A. Cimatti', 'C. Colodro-Conde', 'G. Congedo', 'C. J. Conselice', 'L. Conversi', 'Y. Copin', 'A. Costille', 'F. Courbin', 'H. M. Courtois', 'M. Cropper', 'A. Da Silva', 'H. Degaudenzi', 'G. De Lucia', 'C. Dolding', 'H. Dole', 'M. Douspis', 'F. Dubath', 'X. Dupac', 'S. Dusini', 'S. Escoffier', 'M. Farina', 'S. Ferriol', 'K. George', 'C. Giocoli', 'B. R. Granett', 'A. Grazian', 'F. Grupp', 'S. V. H. Haugan', 'I. M. Hook', 'F. Hormuth', 'A. Hornstrup', 'P. Hudelot', 'M. Jhabvala', 'E. Keihänen', 'S. Kermiche', 'A. Kiessling', 'M. Kilbinger', 'B. Kubik', 'M. Kümmel', 'H. Kurki-Suonio', ""Q. Le Boulc'h"", 'A. M. C. Le Brun', 'D. Le Mignant', 'P. B. Lilje', 'V. Lindholm', 'I. Lloro', 'G. Mainetti', 'D. Maino', 'E. Maiorano', 'O. Marggraf', 'M. Martinelli', 'N. Martinet', 'F. Marulli', 'R. Massey', 'S. Maurogordato', 'H. J. McCracken', 'E. Medinaceli', 'S. Mei', 'M. Melchior', 'M. Meneghetti', 'E. Merlin', 'G. Meylan', 'A. Mora', 'M. Moresco', 'L. Moscardini', 'R. Nakajima', 'C. Neissner', 'S. -M. Niemi', 'C. Padilla', 'S. Paltani', 'F. Pasian', 'K. Pedersen', 'W. J. Percival', 'V. Pettorino', 'G. Polenta', 'M. Poncet', 'L. A. Popa', 'L. Pozzetti', 'F. Raison', 'R. Rebolo', 'A. Renzi', 'J. Rhodes', 'G. Riccio', 'E. Romelli', 'M. Roncarelli', 'R. Saglia', 'A. G. Sánchez', 'D. Sapone', 'J. A. Schewtschenko', 'M. Schirmer', 'P. Schneider', 'T. Schrabback', 'A. Secroun', 'S. Serrano', 'P. Simon', 'C. Sirignano', 'G. Sirri', 'J. Skottfelt', 'L. Stanco', 'J. Steinwagner', 'P. Tallada-Crespí', 'A. N. Taylor', 'I. Tereno', 'S. Toft', 'R. Toledo-Moreo', 'F. Torradeflot', 'I. Tutusaus', 'L. Valenziano', 'J. Valiviita', 'T. Vassallo', 'G. Verdoes Kleijn', 'A. Veropalumbo', 'Y. Wang', 'J. Weller', 'A. Zacchei', 'G. Zamorani', 'F. M. Zerbi', 'I. A. Zinchenko', 'E. Zucca', 'V. Allevato', 'M. Ballardini', 'M. Bolzonella', 'E. Bozzo', 'C. Burigana', 'R. Cabanac', 'A. Cappi', 'J. A. Escartin Vigo', 'L. Gabarra', 'W. G. Hartley', 'J. Martín-Fleitas', 'S. Matthew', 'R. B. Metcalf', 'A. Pezzotta', 'M. Pöntinen', 'I. Risso', 'V. Scottez', 'M. Sereno', 'M. Tenti', 'M. Wiesmann', 'Y. Akrami', 'S. Alvi', 'I. T. Andika', 'S. Anselmi', 'M. Archidiacono', 'F. Atrio-Barandela', 'D. Bertacca', 'M. Bethermin', 'L. Bisigello', 'A. Blanchard', 'L. Blot', 'S. Borgani', 'M. L. Brown', 'S. Bruton', 'A. Calabro', 'F. Caro', 'T. Castro', 'F. Cogato', 'S. Davini', 'G. Desprez', 'A. Díaz-Sánchez', 'J. J. Diaz', 'S. Di Domizio', 'J. M. Diego', 'P. -A. Duc', 'A. Enia', 'Y. Fang', 'A. G. Ferrari', 'A. Finoguenov', 'A. Fontana', 'A. Franco', 'J. García-Bellido', 'T. Gasparetto', 'V. Gautard', 'E. Gaztanaga', 'F. Giacomini', 'F. Gianotti', 'M. Guidi', 'C. M. Gutierrez', 'A. Hall', 'S. Hemmati', 'H. Hildebrandt', 'J. Hjorth', 'J. J. E. Kajava', 'Y. Kang', 'V. Kansal', 'D. Karagiannis', 'C. C. Kirkpatrick', 'S. Kruk', 'L. Legrand', 'M. Lembo', 'F. Lepori', 'G. Leroy', 'J. Lesgourgues', 'L. Leuzzi', 'T. I. Liaudat', 'J. Macias-Perez', 'M. Magliocchetti', 'F. Mannucci', 'R. Maoli', 'C. J. A. P. Martins', 'L. Maurin', 'M. Miluzio', 'P. Monaco', 'G. Morgante', 'K. Naidoo', 'A. Navarro-Alsina', 'F. Passalacqua', 'K. Paterson', 'L. Patrizii', 'A. Pisani', 'D. Potter', 'S. Quai', 'M. Radovich', 'P. -F. Rocci', 'G. Rodighiero', 'S. Sacquegna', 'M. Sahlén', 'D. B. Sanders', 'E. Sarpa', 'A. Schneider', 'M. Schultheis', 'D. Sciotti', 'E. Sellentin', 'F. Shankar', 'L. C. Smith', 'K. Tanidis', 'G. Testera', 'R. Teyssier', 'S. Tosi', 'A. Troja', 'M. Tucci', 'C. Valieri', 'D. Vergani', 'G. Verza', 'N. A. Walton']",2025-03-19,2025-03-19
2503.15300v2,SUM Parts: Benchmarking Part-Level Semantic Segmentation of Urban Meshes,"Semantic segmentation in urban scene analysis has mainly focused on images or
point clouds, while textured meshes - offering richer spatial representation -
remain underexplored. This paper introduces SUM Parts, the first large-scale
dataset for urban textured meshes with part-level semantic labels, covering
about 2.5 km2 with 21 classes. The dataset was created using our own annotation
tool, which supports both face- and texture-based annotations with efficient
interactive selection. We also provide a comprehensive evaluation of 3D
semantic segmentation and interactive annotation methods on this dataset. Our
project page is available at https://tudelft3d.github.io/SUMParts/.",['cs.CV'],"['Weixiao Gao', 'Liangliang Nan', 'Hugo Ledoux']",2025-03-19,2025-03-21
2503.15299v1,Inside-Out: Hidden Factual Knowledge in LLMs,"This work presents a framework for assessing whether large language models
(LLMs) encode more factual knowledge in their parameters than what they express
in their outputs. While a few studies hint at this possibility, none has
clearly defined or demonstrated this phenomenon. We first propose a formal
definition of knowledge, quantifying it for a given question as the fraction of
correct-incorrect answer pairs where the correct one is ranked higher. This
gives rise to external and internal knowledge, depending on the information
used to score individual answer candidates: either the model's observable
token-level probabilities or its intermediate computations. Hidden knowledge
arises when internal knowledge exceeds external knowledge. We then present a
case study, applying this framework to three popular open-weights LLMs in a
closed-book QA setup. Our results indicate that: (1) LLMs consistently encode
more factual knowledge internally than what they express externally, with an
average gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a
model can internally know an answer perfectly, yet fail to generate it even
once, despite large-scale repeated sampling of 1,000 answers. This reveals
fundamental limitations in the generation capabilities of LLMs, which (3) puts
a practical constraint on scaling test-time compute via repeated answer
sampling in closed-book QA: significant performance improvements remain
inaccessible because some answers are practically never sampled, yet if they
were, we would be guaranteed to rank them first.",['cs.CL'],"['Zorik Gekhman', 'Eyal Ben David', 'Hadas Orgad', 'Eran Ofek', 'Yonatan Belinkov', 'Idan Szpector', 'Jonathan Herzig', 'Roi Reichart']",2025-03-19,2025-03-19
2503.15295v1,DCA: Dividing and Conquering Amnesia in Incremental Object Detection,"Incremental object detection (IOD) aims to cultivate an object detector that
can continuously localize and recognize novel classes while preserving its
performance on previous classes. Existing methods achieve certain success by
improving knowledge distillation and exemplar replay for transformer-based
detection frameworks, but the intrinsic forgetting mechanisms remain
underexplored. In this paper, we dive into the cause of forgetting and discover
forgetting imbalance between localization and recognition in transformer-based
IOD, which means that localization is less-forgetting and can generalize to
future classes, whereas catastrophic forgetting occurs primarily on
recognition. Based on these insights, we propose a Divide-and-Conquer Amnesia
(DCA) strategy, which redesigns the transformer-based IOD into a
localization-then-recognition process. DCA can well maintain and transfer the
localization ability, leaving decoupled fragile recognition to be specially
conquered. To reduce feature drift in recognition, we leverage semantic
knowledge encoded in pre-trained language models to anchor class
representations within a unified feature space across incremental tasks. This
involves designing a duplex classifier fusion and embedding class semantic
features into the recognition decoding process in the form of queries.
Extensive experiments validate that our approach achieves state-of-the-art
performance, especially for long-term incremental scenarios. For example, under
the four-step setting on MS-COCO, our DCA strategy significantly improves the
final AP by 6.9%.",['cs.CV'],"['Aoting Zhang', 'Dongbao Yang', 'Chang Liu', 'Xiaopeng Hong', 'Miao Shang', 'Yu Zhou']",2025-03-19,2025-03-19
2503.15294v1,Borsuk-Ulam and Replicable Learning of Large-Margin Halfspaces,"Recent advances in learning theory have established that, for total concepts,
list replicability, global stability, differentially private (DP) learnability,
and shared-randomness replicability coincide precisely with the finiteness of
the Littlestone dimension. Does the same hold for partial concept classes?
  We answer this question by studying the large-margin half-spaces class, which
has bounded Littlestone dimension and is purely DP-learnable and
shared-randomness replicable even in high dimensions.
  We prove that the list replicability number of $\gamma$-margin half-spaces
satisfies \[ \frac{d}{2} + 1 \le \mathrm{LR}(H_{\gamma}^d) \le d, \] which
increases with the dimension $d$. This reveals a surprising separation for
partial concepts: list replicability and global stability do not follow from
bounded Littlestone dimension, DP-learnability, or shared-randomness
replicability.
  By applying our main theorem, we also answer the following open problems.
  - We prove that any disambiguation of an infinite-dimensional large-margin
half-space to a total concept class has unbounded Littlestone dimension,
answering an open question of Alon et al. (FOCS '21). - We prove that the
maximum list-replicability number of any *finite* set of points and homogeneous
half-spaces in $d$-dimensional Euclidean space is $d$, resolving a problem of
Chase et al. (FOCS '23). - We prove that any disambiguation of the Gap Hamming
Distance problem in the large gap regime has unbounded public-coin randomized
communication complexity. This answers an open problem of Fang et al. (STOC
'25).
  We prove the lower bound via a topological argument involving the local
Borsuk-Ulam theorem of Chase et al. (STOC '24). For the upper bound, we design
a learning rule that relies on certain triangulations of the cross-polytope and
recent results on the generalization properties of SVM.",['cs.LG'],"['Ari Blondal', 'Hamed Hatami', 'Pooya Hatami', 'Chavdar Lalov', 'Sivan Tretiak']",2025-03-19,2025-03-19
2503.15293v1,Test-Time Backdoor Detection for Object Detection Models,"Object detection models are vulnerable to backdoor attacks, where attackers
poison a small subset of training samples by embedding a predefined trigger to
manipulate prediction. Detecting poisoned samples (i.e., those containing
triggers) at test time can prevent backdoor activation. However, unlike image
classification tasks, the unique characteristics of object detection --
particularly its output of numerous objects -- pose fresh challenges for
backdoor detection. The complex attack effects (e.g., ""ghost"" object emergence
or ""vanishing"" object) further render current defenses fundamentally
inadequate. To this end, we design TRAnsformation Consistency Evaluation
(TRACE), a brand-new method for detecting poisoned samples at test time in
object detection. Our journey begins with two intriguing observations: (1)
poisoned samples exhibit significantly more consistent detection results than
clean ones across varied backgrounds. (2) clean samples show higher detection
consistency when introduced to different focal information. Based on these
phenomena, TRACE applies foreground and background transformations to each test
sample, then assesses transformation consistency by calculating the variance in
objects confidences. TRACE achieves black-box, universal backdoor detection,
with extensive experiments showing a 30% improvement in AUROC over
state-of-the-art defenses and resistance to adaptive attacks.",['cs.CV'],"['Hangtao Zhang', 'Yichen Wang', 'Shihui Yan', 'Chenyu Zhu', 'Ziqi Zhou', 'Linshan Hou', 'Shengshan Hu', 'Minghui Li', 'Yanjun Zhang', 'Leo Yu Zhang']",2025-03-19,2025-03-19
2503.15289v1,TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence Tracing and Relationship Classification,"LLMs have achieved remarkable fluency and coherence in text generation, yet
their widespread adoption has raised concerns about content reliability and
accountability. In high-stakes domains such as healthcare, law, and news, it is
crucial to understand where and how the content is created. To address this, we
introduce the Text pROVEnance (TROVE) challenge, designed to trace each
sentence of a target text back to specific source sentences within potentially
lengthy or multi-document inputs. Beyond identifying sources, TROVE annotates
the fine-grained relationships (quotation, compression, inference, and others),
providing a deep understanding of how each target sentence is formed. To
benchmark TROVE, we construct our dataset by leveraging three public datasets
covering 11 diverse scenarios (e.g., QA and summarization) in English and
Chinese, spanning source texts of varying lengths (0-5k, 5-10k, 10k+),
emphasizing the multi-document and long-document settings essential for
provenance. To ensure high-quality data, we employ a three-stage annotation
process: sentence retrieval, GPT provenance, and human provenance. We evaluate
11 LLMs under direct prompting and retrieval-augmented paradigms, revealing
that retrieval is essential for robust performance, larger models perform
better in complex relationship classification, and closed-source models often
lead, yet open-source models show significant promise, particularly with
retrieval augmentation.",['cs.CL'],"['Junnan Zhu', 'Min Xiao', 'Yining Wang', 'Feifei Zhai', 'Yu Zhou', 'Chengqing Zong']",2025-03-19,2025-03-19
2503.15288v1,Beacon2Science: Enhancing STEREO/HI beacon data1 with machine learning for efficient CME tracking,"Observing and forecasting coronal mass ejections (CME) in real-time is
crucial due to the strong geomagnetic storms they can generate that can have a
potentially damaging effect, for example, on satellites and electrical devices.
With its near-real-time availability, STEREO/HI beacon data is the perfect
candidate for early forecasting of CMEs. However, previous work concluded that
CME arrival prediction based on beacon data could not achieve the same accuracy
as with high-resolution science data due to data gaps and lower quality. We
present our novel pipeline entitled ''Beacon2Science'', bridging the gap
between beacon and science data to improve CME tracking. Through this pipeline,
we first enhance the quality (signal-to-noise ratio and spatial resolution) of
beacon data. We then increase the time resolution of enhanced beacon images
through learned interpolation to match science data's 40-minute resolution. We
maximize information coherence between consecutive frames with adapted model
architecture and loss functions through the different steps. The improved
beacon images are comparable to science data, showing better CME visibility
than the original beacon data. Furthermore, we compare CMEs tracked in beacon,
enhanced beacon, and science images. The tracks extracted from enhanced beacon
data are closer to those from science images, with a mean average error of
$\sim 0.5 ^\circ$ of elongation compared to $1^\circ$ with original beacon
data. The work presented in this paper paves the way for its application to
forthcoming missions such as Vigil and PUNCH.","['physics.space-ph', 'cs.CV', 'cs.LG']","['Justin Le Louëdec', 'Maike Bauer', 'Tanja Amerstorfer', 'Jackie A. Davies']",2025-03-19,2025-03-19
2503.16553v1,A Foundational individual Mobility Prediction Model based on Open-Source Large Language Models,"Large Language Models (LLMs) are widely applied to domain-specific tasks due
to their massive general knowledge and remarkable inference capacities. Current
studies on LLMs have shown immense potential in applying LLMs to model
individual mobility prediction problems. However, most LLM-based mobility
prediction models only train on specific datasets or use single well-designed
prompts, leading to difficulty in adapting to different cities and users with
diverse contexts. To fill these gaps, this paper proposes a unified fine-tuning
framework to train a foundational open source LLM-based mobility prediction
model. We conducted extensive experiments on six real-world mobility datasets
to validate the proposed model. The results showed that the proposed model
achieved the best performance in prediction accuracy and transferability over
state-of-the-art models based on deep learning and LLMs.",['cs.CL'],"['Zhenlin Qin', 'Leizhen Wang', 'Francisco Camara Pereira', 'Zhenlinag Ma']",2025-03-19,2025-03-19
2503.15285v1,PAPI-Reg: Patch-to-Pixel Solution for Efficient Cross-Modal Registration between LiDAR Point Cloud and Camera Image,"The primary requirement for cross-modal data fusion is the precise alignment
of data from different sensors. However, the calibration between LiDAR point
clouds and camera images is typically time-consuming and needs external
calibration board or specific environmental features. Cross-modal registration
effectively solves this problem by aligning the data directly without requiring
external calibration. However, due to the domain gap between the point cloud
and the image, existing methods rarely achieve satisfactory registration
accuracy while maintaining real-time performance. To address this issue, we
propose a framework that projects point clouds into several 2D representations
for matching with camera images, which not only leverages the geometric
characteristic of LiDAR point clouds more effectively but also bridge the
domain gap between the point cloud and image. Moreover, to tackle the
challenges of cross modal differences and the limited overlap between LiDAR
point clouds and images in the image matching task, we introduce a multi-scale
feature extraction network to effectively extract features from both camera
images and the projection maps of LiDAR point cloud. Additionally, we propose a
patch-to-pixel matching network to provide more effective supervision and
achieve higher accuracy. We validate the performance of our model through
experiments on the KITTI and nuScenes datasets. Our network achieves real-time
performance and extremely high registration accuracy. On the KITTI dataset, our
model achieves a registration accuracy rate of over 99\%.",['cs.CV'],"['Yuanchao Yue', 'Zhengxin Li', 'Wei Zhang', 'Hui Yuan']",2025-03-19,2025-03-19
2503.15284v1,EdgeRegNet: Edge Feature-based Multimodal Registration Network between Images and LiDAR Point Clouds,"Cross-modal data registration has long been a critical task in computer
vision, with extensive applications in autonomous driving and robotics.
Accurate and robust registration methods are essential for aligning data from
different modalities, forming the foundation for multimodal sensor data fusion
and enhancing perception systems' accuracy and reliability. The registration
task between 2D images captured by cameras and 3D point clouds captured by
Light Detection and Ranging (LiDAR) sensors is usually treated as a visual pose
estimation problem. High-dimensional feature similarities from different
modalities are leveraged to identify pixel-point correspondences, followed by
pose estimation techniques using least squares methods. However, existing
approaches often resort to downsampling the original point cloud and image data
due to computational constraints, inevitably leading to a loss in precision.
Additionally, high-dimensional features extracted using different feature
extractors from various modalities require specific techniques to mitigate
cross-modal differences for effective matching. To address these challenges, we
propose a method that uses edge information from the original point clouds and
images for cross-modal registration. We retain crucial information from the
original data by extracting edge points and pixels, enhancing registration
accuracy while maintaining computational efficiency. The use of edge points and
edge pixels allows us to introduce an attention-based feature exchange block to
eliminate cross-modal disparities. Furthermore, we incorporate an optimal
matching layer to improve correspondence identification. We validate the
accuracy of our method on the KITTI and nuScenes datasets, demonstrating its
state-of-the-art performance.",['cs.CV'],"['Yuanchao Yue', 'Hui Yuan', 'Qinglong Miao', 'Xiaolong Mao', 'Raouf Hamzaoui', 'Peter Eisert']",2025-03-19,2025-03-19
2503.15283v1,TF-TI2I: Training-Free Text-and-Image-to-Image Generation via Multi-Modal Implicit-Context Learning in Text-to-Image Models,"Text-and-Image-To-Image (TI2I), an extension of Text-To-Image (T2I),
integrates image inputs with textual instructions to enhance image generation.
Existing methods often partially utilize image inputs, focusing on specific
elements like objects or styles, or they experience a decline in generation
quality with complex, multi-image instructions. To overcome these challenges,
we introduce Training-Free Text-and-Image-to-Image (TF-TI2I), which adapts
cutting-edge T2I models such as SD3 without the need for additional training.
Our method capitalizes on the MM-DiT architecture, in which we point out that
textual tokens can implicitly learn visual information from vision tokens. We
enhance this interaction by extracting a condensed visual representation from
reference images, facilitating selective information sharing through Reference
Contextual Masking -- this technique confines the usage of contextual tokens to
instruction-relevant visual information. Additionally, our Winner-Takes-All
module mitigates distribution shifts by prioritizing the most pertinent
references for each vision token. Addressing the gap in TI2I evaluation, we
also introduce the FG-TI2I Bench, a comprehensive benchmark tailored for TI2I
and compatible with existing T2I methods. Our approach shows robust performance
across various benchmarks, confirming its effectiveness in handling complex
image-generation tasks.",['cs.CV'],"['Teng-Fang Hsiao', 'Bo-Kai Ruan', 'Yi-Lun Wu', 'Tzu-Ling Lin', 'Hong-Han Shuai']",2025-03-19,2025-03-19
2503.15581v1,Performance-bounded Online Ensemble Learning Method Based on Multi-armed bandits and Its Applications in Real-time Safety Assessment,"Ensemble learning plays a crucial role in practical applications of online
learning due to its enhanced classification performance and adaptable
adjustment mechanisms. However, most weight allocation strategies in ensemble
learning are heuristic, making it challenging to theoretically guarantee that
the ensemble classifier outperforms its base classifiers. To address this
issue, a performance-bounded online ensemble learning method based on
multi-armed bandits, named PB-OEL, is proposed in this paper. Specifically,
multi-armed bandit with expert advice is incorporated into online ensemble
learning, aiming to update the weights of base classifiers and make
predictions. A theoretical framework is established to bound the performance of
the ensemble classifier relative to base classifiers. By setting expert advice
of bandits, the bound exceeds the performance of any base classifier when the
length of data stream is sufficiently large. Additionally, performance bounds
for scenarios with limited annotations are also derived. Numerous experiments
on benchmark datasets and a dataset of real-time safety assessment tasks are
conducted. The experimental results validate the theoretical bound to a certain
extent and demonstrate that the proposed method outperforms existing
state-of-the-art methods.","['cs.LG', 'cs.SY', 'eess.SY']","['Songqiao Hu', 'Zeyi Liu', 'Xiao He']",2025-03-19,2025-03-19
2503.15275v1,Challenges and Trends in Egocentric Vision: A Survey,"With the rapid development of artificial intelligence technologies and
wearable devices, egocentric vision understanding has emerged as a new and
challenging research direction, gradually attracting widespread attention from
both academia and industry. Egocentric vision captures visual and multimodal
data through cameras or sensors worn on the human body, offering a unique
perspective that simulates human visual experiences. This paper provides a
comprehensive survey of the research on egocentric vision understanding,
systematically analyzing the components of egocentric scenes and categorizing
the tasks into four main areas: subject understanding, object understanding,
environment understanding, and hybrid understanding. We explore in detail the
sub-tasks within each category. We also summarize the main challenges and
trends currently existing in the field. Furthermore, this paper presents an
overview of high-quality egocentric vision datasets, offering valuable
resources for future research. By summarizing the latest advancements, we
anticipate the broad applications of egocentric vision technologies in fields
such as augmented reality, virtual reality, and embodied intelligence, and
propose future research directions based on the latest developments in the
field.","['cs.CV', 'cs.AI']","['Xiang Li', 'Heqian Qiu', 'Lanxiao Wang', 'Hanwen Zhang', 'Chenghao Qi', 'Linfeng Han', 'Huiyu Xiong', 'Hongliang Li']",2025-03-19,2025-03-19
2503.15580v1,How Well Can AI Build SD Models?,"Introduction: As system dynamics (SD) embraces automation, AI offers
efficiency but risks bias from missing data and flawed models. Models that omit
multiple perspectives and data threaten model quality, whether created by
humans or with the assistance of AI. To reduce uncertainty about how well AI
can build SD models, we introduce two metrics for evaluation of AI-generated
causal maps: technical correctness (causal translation) and adherence to
instructions (conformance).
  Approach: We developed an open source project called sd-ai to provide a basis
for collaboration in the SD community, aiming to fully harness the potential of
AI based tools like ChatGPT for dynamic modeling. Additionally, we created an
evaluation theory along with a comprehensive suite of tests designed to
evaluate any such tools developed within the sd-ai ecosystem.
  Results: We tested 11 different LLMs on their ability to do causal
translation as well as conform to user instruction. gpt-4.5-preview was the top
performer, scoring 92.9% overall, excelling in both tasks. o1 scored 100% in
causal translation. gpt-4o identified all causal links but struggled with
positive polarity in decreasing terms. While gpt-4.5-preview and o1 are most
accurate, gpt-4o is the cheapest.
  Discussion: Causal translation and conformance tests applied to the sd-ai
engine reveal significant variations across lLLMs, underscoring the need for
continued evaluation to ensure responsible development of AI tools for dynamic
modeling. To address this, an open collaboration among tool developers,
modelers, and stakeholders is launched to standardize measures for evaluating
the capacity of AI tools to improve the modeling process.",['cs.AI'],"['William Schoenberg', 'Davidson Girard', 'Saras Chung', ""Ellen O'Neill"", 'Janet Velasquez', 'Sara Metcalf']",2025-03-19,2025-03-19
2503.15272v1,MAMM-Refine: A Recipe for Improving Faithfulness in Generation with Multi-Agent Collaboration,"Multi-agent collaboration among models has shown promise in reasoning tasks
but is underexplored in long-form generation tasks like summarization and
question-answering. We extend multi-agent multi-model reasoning to generation,
specifically to improving faithfulness through refinement, i.e., revising
model-generated outputs to remove factual inconsistencies. We investigate how
iterative collaboration among multiple instances and types of large language
models (LLMs) enhances subtasks in the refinement process, such as error
detection, critiquing unfaithful sentences, and making corrections based on
critiques. We design intrinsic evaluations for each subtask, with our findings
indicating that both multi-agent (multiple instances) and multi-model (diverse
LLM types) approaches benefit error detection and critiquing. Additionally,
reframing critiquing and refinement as reranking rather than generation tasks
improves multi-agent performance. We consolidate these insights into a final
""recipe"" called Multi-Agent Multi-Model Refinement (MAMM-Refine), where
multi-agent and multi-model collaboration significantly boosts performance on
three summarization datasets as well as on long-form question answering,
demonstrating the effectiveness and generalizability of our recipe.","['cs.CL', 'cs.AI']","['David Wan', 'Justin Chih-Yao Chen', 'Elias Stengel-Eskin', 'Mohit Bansal']",2025-03-19,2025-03-19
2503.15268v1,"Do Chains-of-Thoughts of Large Language Models Suffer from Hallucinations, Cognitive Biases, or Phobias in Bayesian Reasoning?","Learning to reason and carefully explain arguments is central to students'
cognitive, mathematical, and computational thinking development. This is
particularly challenging in problems under uncertainty and in Bayesian
reasoning. With the new generation of large language models (LLMs) capable of
reasoning using Chain-of-Thought (CoT), there is an excellent opportunity to
learn with them as they explain their reasoning through a dialogue with their
artificial internal voice. It is an engaging and excellent opportunity to learn
Bayesian reasoning. Furthermore, given that different LLMs sometimes arrive at
opposite solutions, CoT generates opportunities for deep learning by detailed
comparisons of reasonings. However, unlike humans, we found that they do not
autonomously explain using ecologically valid strategies like natural
frequencies, whole objects, and embodied heuristics. This is unfortunate, as
these strategies help humans avoid critical mistakes and have proven
pedagogical value in Bayesian reasoning. In order to overcome these biases and
aid understanding and learning, we included prompts that induce LLMs to use
these strategies. We found that LLMs with CoT incorporate them but not
consistently. They show persistent biases towards symbolic reasoning and
avoidance or phobia of ecologically valid strategies.","['cs.AI', 'I.2.0']",['Roberto Araya'],2025-03-19,2025-03-19
2503.15267v1,Learning to quantify graph nodes,"Network Quantification is the problem of estimating the class proportions in
unlabeled subsets of graph nodes. When prior probability shift is at play, this
task cannot be effectively addressed by first classifying the nodes and then
counting the class predictions. In addition, unlike non-relational
quantification on i.i.d. datapoints, Network Quantification demands enhanced
flexibility to capture a broad range of connectivity patterns, resilience to
the challenge of heterophily, and efficiency to scale to larger networks. To
meet these stringent requirements we introduce XNQ, a novel method that
synergizes the flexibility and efficiency of the unsupervised node embeddings
computed by randomized recursive Graph Neural Networks, with an
Expectation-Maximization algorithm that provides a robust quantification-aware
adjustment to the output probabilities of a calibrated node classifier. We
validate the design choices underpinning our method through comprehensive
ablation experiments. In an extensive evaluation, we find that our approach
consistently and significantly improves on the best Network Quantification
methods to date, thereby setting the new state of the art for this challenging
task. Simultaneously, it provides a training speed-up of up to 10x-100x over
other graph learning based methods.",['cs.LG'],"['Alessio Micheli', 'Alejandro Moreo', 'Marco Podda', 'Fabrizio Sebastiani', 'William Simoni', 'Domenico Tortorella']",2025-03-19,2025-03-19
2503.15265v1,DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement Learning,"Triangle meshes play a crucial role in 3D applications for efficient
manipulation and rendering. While auto-regressive methods generate structured
meshes by predicting discrete vertex tokens, they are often constrained by
limited face counts and mesh incompleteness. To address these challenges, we
propose DeepMesh, a framework that optimizes mesh generation through two key
innovations: (1) an efficient pre-training strategy incorporating a novel
tokenization algorithm, along with improvements in data curation and
processing, and (2) the introduction of Reinforcement Learning (RL) into 3D
mesh generation to achieve human preference alignment via Direct Preference
Optimization (DPO). We design a scoring standard that combines human evaluation
with 3D metrics to collect preference pairs for DPO, ensuring both visual
appeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh
generates meshes with intricate details and precise topology, outperforming
state-of-the-art methods in both precision and quality. Project page:
https://zhaorw02.github.io/DeepMesh/",['cs.CV'],"['Ruowen Zhao', 'Junliang Ye', 'Zhengyi Wang', 'Guangce Liu', 'Yiwen Chen', 'Yikai Wang', 'Jun Zhu']",2025-03-19,2025-03-19
2503.15264v1,LEGION: Learning to Ground and Explain for Synthetic Image Detection,"The rapid advancements in generative technology have emerged as a
double-edged sword. While offering powerful tools that enhance convenience,
they also pose significant social concerns. As defenders, current synthetic
image detection methods often lack artifact-level textual interpretability and
are overly focused on image manipulation detection, and current datasets
usually suffer from outdated generators and a lack of fine-grained annotations.
In this paper, we introduce SynthScars, a high-quality and diverse dataset
consisting of 12,236 fully synthetic images with human-expert annotations. It
features 4 distinct image content types, 3 categories of artifacts, and
fine-grained annotations covering pixel-level segmentation, detailed textual
explanations, and artifact category labels. Furthermore, we propose LEGION
(LEarning to Ground and explain for Synthetic Image detectiON), a multimodal
large language model (MLLM)-based image forgery analysis framework that
integrates artifact detection, segmentation, and explanation. Building upon
this capability, we further explore LEGION as a controller, integrating it into
image refinement pipelines to guide the generation of higher-quality and more
realistic images. Extensive experiments show that LEGION outperforms existing
methods across multiple benchmarks, particularly surpassing the second-best
traditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score.
Moreover, the refined images generated under its guidance exhibit stronger
alignment with human preferences. The code, model, and dataset will be
released.",['cs.CV'],"['Hengrui Kang', 'Siwei Wen', 'Zichen Wen', 'Junyan Ye', 'Weijia Li', 'Peilin Feng', 'Baichuan Zhou', 'Bin Wang', 'Dahua Lin', 'Linfeng Zhang', 'Conghui He']",2025-03-19,2025-03-19
2503.15260v1,DEPT: Deep Extreme Point Tracing for Ultrasound Image Segmentation,"Automatic medical image segmentation plays a crucial role in computer aided
diagnosis. However, fully supervised learning approaches often require
extensive and labor-intensive annotation efforts. To address this challenge,
weakly supervised learning methods, particularly those using extreme points as
supervisory signals, have the potential to offer an effective solution. In this
paper, we introduce Deep Extreme Point Tracing (DEPT) integrated with
Feature-Guided Extreme Point Masking (FGEPM) algorithm for ultrasound image
segmentation. Notably, our method generates pseudo labels by identifying the
lowest-cost path that connects all extreme points on the feature map-based cost
matrix. Additionally, an iterative training strategy is proposed to refine
pseudo labels progressively, enabling continuous network improvement.
Experimental results on two public datasets demonstrate the effectiveness of
our proposed method. The performance of our method approaches that of the fully
supervised method and outperforms several existing weakly supervised methods.",['cs.CV'],"['Lei Shi', 'Xi Fang', 'Naiyu Wang', 'Junxing Zhang']",2025-03-19,2025-03-19
2503.15259v1,Fast MLE and MAPE-Based Device Activity Detection for Grant-Free Access via PSCA and PSCA-Net,"Fast and accurate device activity detection is the critical challenge in
grant-free access for supporting massive machine-type communications (mMTC) and
ultra-reliable low-latency communications (URLLC) in 5G and beyond. The
state-of-the-art methods have unsatisfactory error rates or computation times.
To address these outstanding issues, we propose new maximum likelihood
estimation (MLE) and maximum a posterior estimation (MAPE) based device
activity detection methods for known and unknown pathloss that achieve superior
error rate and computation time tradeoffs using optimization and deep learning
techniques. Specifically, we investigate four non-convex optimization problems
for MLE and MAPE in the two pathloss cases, with one MAPE problem being
formulated for the first time. For each non-convex problem, we develop an
innovative parallel iterative algorithm using the parallel successive convex
approximation (PSCA) method. Each PSCA-based algorithm allows parallel
computations, uses up to the objective function's second-order information,
converges to the problem's stationary points, and has a low per-iteration
computational complexity compared to the state-of-the-art algorithms. Then, for
each PSCA-based iterative algorithm, we present a deep unrolling neural network
implementation, called PSCA-Net, to further reduce the computation time. Each
PSCA-Net elegantly marries the underlying PSCA-based algorithm's parallel
computation mechanism with the parallelizable neural network architecture and
effectively optimizes its step sizes based on vast data samples to speed up the
convergence. Numerical results demonstrate that the proposed methods can
significantly reduce the error rate and computation time compared to the
state-of-the-art methods, revealing their significant values for grant-free
access.","['math.OC', 'cs.LG']","['Bowen Tan', 'Ying Cui']",2025-03-19,2025-03-19
2503.15250v1,ImputeGAP: A Comprehensive Library for Time Series Imputation,"With the prevalence of sensor failures, imputation--the process of estimating
missing values--has emerged as the cornerstone of time series data preparation.
While numerous imputation algorithms have been developed to address these data
gaps, existing libraries provide limited support. Furthermore, they often lack
the ability to simulate realistic patterns of time series missing data and fail
to account for the impact of imputation on subsequent downstream analysis.
  This paper introduces ImputeGAP, a comprehensive library for time series
imputation that supports a diverse range of imputation methods and modular
missing data simulation catering to datasets with varying characteristics. The
library includes extensive customization options, such as automated
hyperparameter tuning, benchmarking, explainability, downstream evaluation, and
compatibility with popular time series frameworks.","['cs.LG', 'cs.DB']","['Quentin Nater', 'Mourad Khayati', 'Jacques Pasquier']",2025-03-19,2025-03-19
2503.15248v1,Automated Non-Functional Requirements Generation in Software Engineering with Large Language Models: A Comparative Study,"Neglecting non-functional requirements (NFRs) early in software development
can lead to critical challenges. Despite their importance, NFRs are often
overlooked or difficult to identify, impacting software quality. To support
requirements engineers in eliciting NFRs, we developed a framework that
leverages Large Language Models (LLMs) to derive quality-driven NFRs from
functional requirements (FRs). Using a custom prompting technique within a
Deno-based pipeline, the system identifies relevant quality attributes for each
functional requirement and generates corresponding NFRs, aiding systematic
integration. A crucial aspect is evaluating the quality and suitability of
these generated requirements. Can LLMs produce high-quality NFR suggestions?
Using 34 functional requirements - selected as a representative subset of 3,964
FRs-the LLMs inferred applicable attributes based on the ISO/IEC 25010:2023
standard, generating 1,593 NFRs. A horizontal evaluation covered three
dimensions: NFR validity, applicability of quality attributes, and
classification precision. Ten industry software quality evaluators, averaging
13 years of experience, assessed a subset for relevance and quality. The
evaluation showed strong alignment between LLM-generated NFRs and expert
assessments, with median validity and applicability scores of 5.0 (means: 4.63
and 4.59, respectively) on a 1-5 scale. In the classification task, 80.4% of
LLM-assigned attributes matched expert choices, with 8.3% near misses and 11.3%
mismatches. A comparative analysis of eight LLMs highlighted variations in
performance, with gemini-1.5-pro exhibiting the highest attribute accuracy,
while llama-3.3-70B achieved higher validity and applicability scores. These
findings provide insights into the feasibility of using LLMs for automated NFR
generation and lay the foundation for further exploration of AI-assisted
requirements engineering.","['cs.SE', 'cs.AI']","['Jomar Thomas Almonte', 'Santhosh Anitha Boominathan', 'Nathalia Nascimento']",2025-03-19,2025-03-19
2503.15242v2,BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space Complexity?,"We introduce BigO(Bench), a novel coding benchmark designed to evaluate the
capabilities of generative language models in understanding and generating code
with specified time and space complexities. This benchmark addresses the gap in
current evaluations that often overlook the ability of models to comprehend and
produce code constrained by computational complexity. BigO(Bench) includes
tooling to infer the algorithmic complexity of any Python function from
profiling measurements, including human- or LLM-generated solutions.
BigO(Bench) also includes of set of 3,105 coding problems and 1,190,250
solutions from Code Contests annotated with inferred (synthetic) time and space
complexity labels from the complexity framework, as well as corresponding
runtime and memory footprint values for a large set of input sizes. We present
results from evaluating multiple state-of-the-art language models on this
benchmark, highlighting their strengths and weaknesses in handling complexity
requirements. In particular, token-space reasoning models are unrivaled in code
generation but not in complexity understanding, hinting that they may not
generalize well to tasks for which no reward was given at training time.","['cs.CL', 'cs.AI', 'cs.CC']","['Pierre Chambon', 'Baptiste Roziere', 'Benoit Sagot', 'Gabriel Synnaeve']",2025-03-19,2025-03-20
2503.15234v1,CoE: Chain-of-Explanation via Automatic Visual Concept Circuit Description and Polysemanticity Quantification,"Explainability is a critical factor influencing the wide deployment of deep
vision models (DVMs). Concept-based post-hoc explanation methods can provide
both global and local insights into model decisions. However, current methods
in this field face challenges in that they are inflexible to automatically
construct accurate and sufficient linguistic explanations for global concepts
and local circuits. Particularly, the intrinsic polysemanticity in semantic
Visual Concepts (VCs) impedes the interpretability of concepts and DVMs, which
is underestimated severely. In this paper, we propose a Chain-of-Explanation
(CoE) approach to address these issues. Specifically, CoE automates the
decoding and description of VCs to construct global concept explanation
datasets. Further, to alleviate the effect of polysemanticity on model
explainability, we design a concept polysemanticity disentanglement and
filtering mechanism to distinguish the most contextually relevant concept
atoms. Besides, a Concept Polysemanticity Entropy (CPE), as a measure of model
interpretability, is formulated to quantify the degree of concept uncertainty.
The modeling of deterministic concepts is upgraded to uncertain concept atom
distributions. Finally, CoE automatically enables linguistic local explanations
of the decision-making process of DVMs by tracing the concept circuit. GPT-4o
and human-based experiments demonstrate the effectiveness of CPE and the
superiority of CoE, achieving an average absolute improvement of 36% in terms
of explainability scores.","['cs.CV', 'cs.AI']","['Wenlong Yu', 'Qilong Wang', 'Chuang Liu', 'Dong Li', 'Qinghua Hu']",2025-03-19,2025-03-19
2503.15235v1,Exploring Large Language Models for Word Games:Who is the Spy?,"Word games hold significant research value for natural language processing
(NLP), game theory, and related fields due to their rule-based and situational
nature. This study explores how large language models (LLMs) can be effectively
involved in word games and proposes a training-free framework. ""Shei Shi Wo Di""
or ""Who is the Spy"" in English, is a classic word game. Using this game as an
example, we introduce a Chain-of-Thought (CoT)-based scheduling framework to
enable LLMs to achieve excellent performance in tasks such as inferring role
words and disguising their identities. We evaluate the framework's performance
based on game success rates and the accuracy of the LLM agents' analytical
results. Experimental results affirm the framework's effectiveness,
demonstrating notable improvements in LLM performance across multiple datasets.
This work highlights the potential of LLMs in mastering situational reasoning
and social interactions within structured game environments. Our code is
publicly available at https://github.com/ct-wei/Who-is-The-Spy.","['cs.CL', 'cs.AI']","['Chentian Wei', 'Jiewei Chen', 'Jinzhu Xu']",2025-03-19,2025-03-19
2503.15225v1,A Personalized Data-Driven Generative Model of Human Motion,"The deployment of autonomous virtual avatars (in extended reality) and robots
in human group activities - such as rehabilitation therapy, sports, and
manufacturing - is expected to increase as these technologies become more
pervasive. Designing cognitive architectures and control strategies to drive
these agents requires realistic models of human motion. However, existing
models only provide simplified descriptions of human motor behavior. In this
work, we propose a fully data-driven approach, based on Long Short-Term Memory
neural networks, to generate original motion that captures the unique
characteristics of specific individuals. We validate the architecture using
real data of scalar oscillatory motion. Extensive analyses show that our model
effectively replicates the velocity distribution and amplitude envelopes of the
individual it was trained on, remaining different from other individuals, and
outperforming state-of-the-art models in terms of similarity to human data.","['cs.GR', 'cs.AI', 'cs.LG', 'cs.SY', 'eess.SY']","['Angelo Di Porzio', 'Marco Coraggio']",2025-03-19,2025-03-19
2503.15222v1,"Model Hubs and Beyond: Analyzing Model Popularity, Performance, and Documentation","With the massive surge in ML models on platforms like Hugging Face, users
often lose track and struggle to choose the best model for their downstream
tasks, frequently relying on model popularity indicated by download counts,
likes, or recency. We investigate whether this popularity aligns with actual
model performance and how the comprehensiveness of model documentation
correlates with both popularity and performance. In our study, we evaluated a
comprehensive set of 500 Sentiment Analysis models on Hugging Face. This
evaluation involved massive annotation efforts, with human annotators
completing nearly 80,000 annotations, alongside extensive model training and
evaluation. Our findings reveal that model popularity does not necessarily
correlate with performance. Additionally, we identify critical inconsistencies
in model card reporting: approximately 80\% of the models analyzed lack
detailed information about the model, training, and evaluation processes.
Furthermore, about 88\% of model authors overstate their models' performance in
the model cards. Based on our findings, we provide a checklist of guidelines
for users to choose good models for downstream tasks.",['cs.CL'],"['Pritam Kadasi', 'Sriman Reddy', 'Srivathsa Vamsi Chaturvedula', 'Rudranshu Sen', 'Agnish Saha', 'Soumavo Sikdar', 'Sayani Sarkar', 'Suhani Mittal', 'Rohit Jindal', 'Mayank Singh']",2025-03-19,2025-03-19
2503.15221v1,A Foundation Model for Patient Behavior Monitoring and Suicide Detection,"Foundation models (FMs) have achieved remarkable success across various
domains, yet their adoption in healthcare remains limited. While significant
advances have been made in medical imaging, genetic biomarkers, and time series
from electronic health records, the potential of FMs for patient behavior
monitoring through wearable devices remains underexplored. These datasets are
inherently heterogeneous, multisource, and often exhibit high rates of missing
data, posing unique challenges. This paper introduces a novel FM based on a
modified vector quantized variational autoencoder (VQ-VAE), specifically
designed to process real-world data from wearable devices. We demonstrate that
our pretrained FM, trained on a broad cohort of psychiatric patients, performs
downstream tasks via its latent representation without fine-tuning on a
held-out cohort of suicidal patients. To illustrate this, we develop a
probabilistic change-point detection algorithm for suicide detection and
demonstrate the FM's effectiveness in predicting emotional states. Our results
show that the discrete latent structure of the VQ-VAE outperforms a
state-of-the-art Informer architecture in unsupervised suicide detection, while
matching its performance in supervised emotion prediction when the latent
dimensionality is increased, though at the cost of reduced unsupervised
accuracy. This trade-off highlights the need for future FMs to integrate hybrid
discrete-continuous structures for balanced performance across tasks.",['cs.LG'],"['Rodrigo Oliver', 'Josué Pérez-Sabater', 'Leire Paz-Arbaizar', 'Alejandro Lancho', 'Antonio Artés', 'Pablo M. Olmos']",2025-03-19,2025-03-19
2503.15220v2,Entity-aware Cross-lingual Claim Detection for Automated Fact-checking,"Identifying claims requiring verification is a critical task in automated
fact-checking, especially given the proliferation of misinformation on social
media platforms. Despite significant progress in the task, there remain open
challenges such as dealing with multilingual and multimodal data prevalent in
online discourse. Addressing the multilingual challenge, recent efforts have
focused on fine-tuning pre-trained multilingual language models. While these
models can handle multiple languages, their ability to effectively transfer
cross-lingual knowledge for detecting claims spreading on social media remains
under-explored. In this paper, we introduce EX-Claim, an entity-aware
cross-lingual claim detection model that generalizes well to handle claims
written in any language. The model leverages entity information derived from
named entity recognition and entity linking techniques to improve the
language-level performance of both seen and unseen languages during training.
Extensive experiments conducted on three datasets from different social media
platforms demonstrate that our proposed model significantly outperforms the
baselines, across 27 languages, and achieves the highest rate of knowledge
transfer, even with limited training data.",['cs.CL'],"['Rrubaa Panchendrarajan', 'Arkaitz Zubiaga']",2025-03-19,2025-03-20
2503.15211v1,GO-N3RDet: Geometry Optimized NeRF-enhanced 3D Object Detector,"We propose GO-N3RDet, a scene-geometry optimized multi-view 3D object
detector enhanced by neural radiance fields. The key to accurate 3D object
detection is in effective voxel representation. However, due to occlusion and
lack of 3D information, constructing 3D features from multi-view 2D images is
challenging. Addressing that, we introduce a unique 3D positional information
embedded voxel optimization mechanism to fuse multi-view features. To
prioritize neural field reconstruction in object regions, we also devise a
double importance sampling scheme for the NeRF branch of our detector. We
additionally propose an opacity optimization module for precise voxel opacity
prediction by enforcing multi-view consistency constraints. Moreover, to
further improve voxel density consistency across multiple perspectives, we
incorporate ray distance as a weighting factor to minimize cumulative ray
errors. Our unique modules synergetically form an end-to-end neural model that
establishes new state-of-the-art in NeRF-based multi-view 3D detection,
verified with extensive experiments on ScanNet and ARKITScenes. Code will be
available at https://github.com/ZechuanLi/GO-N3RDet.",['cs.CV'],"['Zechuan Li', 'Hongshan Yu', 'Yihao Ding', 'Jinhao Qiao', 'Basim Azam', 'Naveed Akhtar']",2025-03-19,2025-03-19
2503.16550v1,Unified Enhancement of the Generalization and Robustness of Language Models via Bi-Stage Optimization,"Neural network language models (LMs) are confronted with significant
challenges in generalization and robustness. Currently, many studies focus on
improving either generalization or robustness in isolation, without methods
addressing both aspects simultaneously, which presents a significant challenge
in developing LMs that are both robust and generalized. In this paper, we
propose a bi-stage optimization framework to uniformly enhance both the
generalization and robustness of LMs, termed UEGR. Specifically, during the
forward propagation stage, we enrich the output probability distributions of
adversarial samples by adaptive dropout to generate diverse sub models, and
incorporate JS divergence and adversarial losses of these output distributions
to reinforce output stability. During backward propagation stage, we compute
parameter saliency scores and selectively update only the most critical
parameters to minimize unnecessary deviations and consolidate the model's
resilience. Theoretical analysis shows that our framework includes gradient
regularization to limit the model's sensitivity to input perturbations and
selective parameter updates to flatten the loss landscape, thus improving both
generalization and robustness. The experimental results show that our method
significantly improves the generalization and robustness of LMs compared to
other existing methods across 13 publicly available language datasets,
achieving state-of-the-art (SOTA) performance.",['cs.CL'],"['Yudao Sun', 'Juan Yin', 'Juan Zhao', 'Fan Zhang', 'Yongheng Liu', 'Hongji Chen']",2025-03-19,2025-03-19
2503.15210v1,Online federated learning framework for classification,"In this paper, we develop a novel online federated learning framework for
classification, designed to handle streaming data from multiple clients while
ensuring data privacy and computational efficiency. Our method leverages the
generalized distance-weighted discriminant technique, making it robust to both
homogeneous and heterogeneous data distributions across clients. In particular,
we develop a new optimization algorithm based on the Majorization-Minimization
principle, integrated with a renewable estimation procedure, enabling efficient
model updates without full retraining. We provide a theoretical guarantee for
the convergence of our estimator, proving its consistency and asymptotic
normality under standard regularity conditions. In addition, we establish that
our method achieves Bayesian risk consistency, ensuring its reliability for
classification tasks in federated environments. We further incorporate
differential privacy mechanisms to enhance data security, protecting client
information while maintaining model performance. Extensive numerical
experiments on both simulated and real-world datasets demonstrate that our
approach delivers high classification accuracy, significant computational
efficiency gains, and substantial savings in data storage requirements compared
to existing methods.","['stat.ML', 'cs.LG']","['Wenxing Guo', 'Jinhan Xie', 'Jianya Lu', 'Bei jiang', 'Hongsheng Dai', 'Linglong Kong']",2025-03-19,2025-03-19
2503.15209v1,Kolmogorov-Arnold Network for Transistor Compact Modeling,"Neural network (NN)-based transistor compact modeling has recently emerged as
a transformative solution for accelerating device modeling and SPICE circuit
simulations. However, conventional NN architectures, despite their widespread
adoption in state-of-the-art methods, primarily function as black-box problem
solvers. This lack of interpretability significantly limits their capacity to
extract and convey meaningful insights into learned data patterns, posing a
major barrier to their broader adoption in critical modeling tasks. This work
introduces, for the first time, Kolmogorov-Arnold network (KAN) for the
transistor - a groundbreaking NN architecture that seamlessly integrates
interpretability with high precision in physics-based function modeling. We
systematically evaluate the performance of KAN and Fourier KAN for FinFET
compact modeling, benchmarking them against the golden industry-standard
compact model and the widely used MLP architecture. Our results reveal that KAN
and FKAN consistently achieve superior prediction accuracy for critical figures
of merit, including gate current, drain charge, and source charge. Furthermore,
we demonstrate and improve the unique ability of KAN to derive symbolic
formulas from learned data patterns - a capability that not only enhances
interpretability but also facilitates in-depth transistor analysis and
optimization. This work highlights the transformative potential of KAN in
bridging the gap between interpretability and precision in NN-driven transistor
compact modeling. By providing a robust and transparent approach to transistor
modeling, KAN represents a pivotal advancement for the semiconductor industry
as it navigates the challenges of advanced technology scaling.",['cs.LG'],"['Rodion Novkin', 'Hussam Amrouch']",2025-03-19,2025-03-19
2503.15208v1,DiST-4D: Disentangled Spatiotemporal Diffusion with Metric Depth for 4D Driving Scene Generation,"Current generative models struggle to synthesize dynamic 4D driving scenes
that simultaneously support temporal extrapolation and spatial novel view
synthesis (NVS) without per-scene optimization. A key challenge lies in finding
an efficient and generalizable geometric representation that seamlessly
connects temporal and spatial synthesis. To address this, we propose DiST-4D,
the first disentangled spatiotemporal diffusion framework for 4D driving scene
generation, which leverages metric depth as the core geometric representation.
DiST-4D decomposes the problem into two diffusion processes: DiST-T, which
predicts future metric depth and multi-view RGB sequences directly from past
observations, and DiST-S, which enables spatial NVS by training only on
existing viewpoints while enforcing cycle consistency. This cycle consistency
mechanism introduces a forward-backward rendering constraint, reducing the
generalization gap between observed and unseen viewpoints. Metric depth is
essential for both accurate reliable forecasting and accurate spatial NVS, as
it provides a view-consistent geometric representation that generalizes well to
unseen perspectives. Experiments demonstrate that DiST-4D achieves
state-of-the-art performance in both temporal prediction and NVS tasks, while
also delivering competitive performance in planning-related evaluations.",['cs.CV'],"['Jiazhe Guo', 'Yikang Ding', 'Xiwu Chen', 'Shuo Chen', 'Bohan Li', 'Yingshuang Zou', 'Xiaoyang Lyu', 'Feiyang Tan', 'Xiaojuan Qi', 'Zhiheng Li', 'Hao Zhao']",2025-03-19,2025-03-19
2503.15204v1,When Pigs Get Sick: Multi-Agent AI for Swine Disease Detection,"Swine disease surveillance is critical to the sustainability of global
agriculture, yet its effectiveness is frequently undermined by limited
veterinary resources, delayed identification of cases, and variability in
diagnostic accuracy. To overcome these barriers, we introduce a novel
AI-powered, multi-agent diagnostic system that leverages Retrieval-Augmented
Generation (RAG) to deliver timely, evidence-based disease detection and
clinical guidance. By automatically classifying user inputs into either
Knowledge Retrieval Queries or Symptom-Based Diagnostic Queries, the system
ensures targeted information retrieval and facilitates precise diagnostic
reasoning. An adaptive questioning protocol systematically collects relevant
clinical signs, while a confidence-weighted decision fusion mechanism
integrates multiple diagnostic hypotheses to generate robust disease
predictions and treatment recommendations. Comprehensive evaluations
encompassing query classification, disease diagnosis, and knowledge retrieval
demonstrate that the system achieves high accuracy, rapid response times, and
consistent reliability. By providing a scalable, AI-driven diagnostic
framework, this approach enhances veterinary decision-making, advances
sustainable livestock management practices, and contributes substantively to
the realization of global food security.","['cs.HC', 'cs.AI', 'cs.CL', 'cs.IR', 'cs.MA']","['Tittaya Mairittha', 'Tanakon Sawanglok', 'Panuwit Raden', 'Sorrawit Treesuk']",2025-03-19,2025-03-19
2503.15202v2,"A Unified Framework for Real-Time Failure Handling in Robotics Using Vision-Language Models, Reactive Planner and Behavior Trees","Robotic systems often face execution failures due to unexpected obstacles,
sensor errors, or environmental changes. Traditional failure recovery methods
rely on predefined strategies or human intervention, making them less
adaptable. This paper presents a unified failure recovery framework that
combines Vision-Language Models (VLMs), a reactive planner, and Behavior Trees
(BTs) to enable real-time failure handling. Our approach includes pre-execution
verification, which checks for potential failures before execution, and
reactive failure handling, which detects and corrects failures during execution
by verifying existing BT conditions, adding missing preconditions and, when
necessary, generating new skills. The framework uses a scene graph for
structured environmental perception and an execution history for continuous
monitoring, enabling context-aware and adaptive failure handling. We evaluate
our framework through real-world experiments with an ABB YuMi robot on tasks
like peg insertion, object sorting, and drawer placement, as well as in
AI2-THOR simulator. Compared to using pre-execution and reactive methods
separately, our approach achieves higher task success rates and greater
adaptability. Ablation studies highlight the importance of VLM-based reasoning,
structured scene representation, and execution history tracking for effective
failure recovery in robotics.","['cs.RO', 'cs.AI']","['Faseeh Ahmad', 'Hashim Ismail', 'Jonathan Styrud', 'Maj Stenmark', 'Volker Krueger']",2025-03-19,2025-03-21
2503.15579v1,Understanding the Generalization of In-Context Learning in Transformers: An Empirical Study,"Large language models (LLMs) like GPT-4 and LLaMA-3 utilize the powerful
in-context learning (ICL) capability of Transformer architecture to learn on
the fly from limited examples. While ICL underpins many LLM applications, its
full potential remains hindered by a limited understanding of its
generalization boundaries and vulnerabilities. We present a systematic
investigation of transformers' generalization capability with ICL relative to
training data coverage by defining a task-centric framework along three
dimensions: inter-problem, intra-problem, and intra-task generalization.
Through extensive simulation and real-world experiments, encompassing tasks
such as function fitting, API calling, and translation, we find that
transformers lack inter-problem generalization with ICL, but excel in
intra-task and intra-problem generalization. When the training data includes a
greater variety of mixed tasks, it significantly enhances the generalization
ability of ICL on unseen tasks and even on known simple tasks. This guides us
in designing training data to maximize the diversity of tasks covered and to
combine different tasks whenever possible, rather than solely focusing on the
target task for testing.",['cs.LG'],"['Xingxuan Zhang', 'Haoran Wang', 'Jiansheng Li', 'Yuan Xue', 'Shikai Guan', 'Renzhe Xu', 'Hao Zou', 'Han Yu', 'Peng Cui']",2025-03-19,2025-03-19
2503.15200v1,Partially Observable Reinforcement Learning with Memory Traces,"Partially observable environments present a considerable computational
challenge in reinforcement learning due to the need to consider long histories.
Learning with a finite window of observations quickly becomes intractable as
the window length grows. In this work, we introduce memory traces. Inspired by
eligibility traces, these are compact representations of the history of
observations in the form of exponential moving averages. We prove sample
complexity bounds for the problem of offline on-policy evaluation that quantify
the value errors achieved with memory traces for the class of Lipschitz
continuous value estimates. We establish a close connection to the window
approach, and demonstrate that, in certain environments, learning with memory
traces is significantly more sample efficient. Finally, we underline the
effectiveness of memory traces empirically in online reinforcement learning
experiments for both value prediction and control.",['cs.LG'],"['Onno Eberhard', 'Michael Muehlebach', 'Claire Vernade']",2025-03-19,2025-03-19
2503.15197v1,Detect-and-Guide: Self-regulation of Diffusion Models for Safe Text-to-Image Generation via Guideline Token Optimization,"Text-to-image diffusion models have achieved state-of-the-art results in
synthesis tasks; however, there is a growing concern about their potential
misuse in creating harmful content. To mitigate these risks, post-hoc model
intervention techniques, such as concept unlearning and safety guidance, have
been developed. However, fine-tuning model weights or adapting the hidden
states of the diffusion model operates in an uninterpretable way, making it
unclear which part of the intermediate variables is responsible for unsafe
generation. These interventions severely affect the sampling trajectory when
erasing harmful concepts from complex, multi-concept prompts, thus hindering
their practical use in real-world settings. In this work, we propose the safe
generation framework Detect-and-Guide (DAG), leveraging the internal knowledge
of diffusion models to perform self-diagnosis and fine-grained self-regulation
during the sampling process. DAG first detects harmful concepts from noisy
latents using refined cross-attention maps of optimized tokens, then applies
safety guidance with adaptive strength and editing regions to negate unsafe
generation. The optimization only requires a small annotated dataset and can
provide precise detection maps with generalizability and concept specificity.
Moreover, DAG does not require fine-tuning of diffusion models, and therefore
introduces no loss to their generation diversity. Experiments on erasing sexual
content show that DAG achieves state-of-the-art safe generation performance,
balancing harmfulness mitigation and text-following performance on
multi-concept real-world prompts.",['cs.CV'],"['Feifei Li', 'Mi Zhang', 'Yiming Sun', 'Min Yang']",2025-03-19,2025-03-19
2503.15195v2,Benchmarking Large Language Models for Handwritten Text Recognition,"Traditional machine learning models for Handwritten Text Recognition (HTR)
rely on supervised training, requiring extensive manual annotations, and often
produce errors due to the separation between layout and text processing. In
contrast, Multimodal Large Language Models (MLLMs) offer a general approach to
recognizing diverse handwriting styles without the need for model-specific
training. The study benchmarks various proprietary and open-source LLMs against
Transkribus models, evaluating their performance on both modern and historical
datasets written in English, French, German, and Italian. In addition, emphasis
is placed on testing the models' ability to autonomously correct previously
generated outputs. Findings indicate that proprietary models, especially Claude
3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs
achieve excellent results in recognizing modern handwriting and exhibit a
preference for the English language due to their pre-training dataset
composition. Comparisons with Transkribus show no consistent advantage for
either approach. Moreover, LLMs demonstrate limited ability to autonomously
correct errors in zero-shot transcriptions.",['cs.CV'],"['Giorgia Crosilla', 'Lukas Klic', 'Giovanni Colavizza']",2025-03-19,2025-03-20
2503.15578v1,Sparseformer: a Transferable Transformer with Multi-granularity Token Sparsification for Medical Time Series Classification,"Medical time series (MedTS) classification is crucial for improved diagnosis
in healthcare, and yet it is challenging due to the varying granularity of
patterns, intricate inter-channel correlation, information redundancy, and
label scarcity. While existing transformer-based models have shown promise in
time series analysis, they mainly focus on forecasting and fail to fully
exploit the distinctive characteristics of MedTS data. In this paper, we
introduce Sparseformer, a transformer specifically designed for MedTS
classification. We propose a sparse token-based dual-attention mechanism that
enables global modeling and token compression, allowing dynamic focus on the
most informative tokens while distilling redundant features. This mechanism is
then applied to the multi-granularity, cross-channel encoding of medical
signals, capturing intra- and inter-granularity correlations and inter-channel
connections. The sparsification design allows our model to handle heterogeneous
inputs of varying lengths and channels directly. Further, we introduce an
adaptive label encoder to address label space misalignment across datasets,
equipping our model with cross-dataset transferability to alleviate the medical
label scarcity issue. Our model outperforms 12 baselines across seven medical
datasets under supervised learning. In the few-shot learning experiments, our
model also achieves superior average results. In addition, the in-domain and
cross-domain experiments among three diagnostic scenarios demonstrate our
model's zero-shot learning capability. Collectively, these findings underscore
the robustness and transferability of our model in various medical
applications.",['cs.LG'],"['Jiexia Ye', 'Weiqi Zhang', 'Ziyue Li', 'Jia Li', 'Fugee Tsung']",2025-03-19,2025-03-19
2503.15190v1,Learning Topology Actions for Power Grid Control: A Graph-Based Soft-Label Imitation Learning Approach,"The rising proportion of renewable energy in the electricity mix introduces
significant operational challenges for power grid operators. Effective power
grid management demands adaptive decision-making strategies capable of handling
dynamic conditions. With the increase in complexity, more and more Deep
Learning (DL) approaches have been proposed to find suitable grid topologies
for congestion management. In this work, we contribute to this research by
introducing a novel Imitation Learning (IL) approach that leverages soft labels
derived from simulated topological action outcomes, thereby capturing multiple
viable actions per state. Unlike traditional IL methods that rely on hard
labels to enforce a single optimal action, our method constructs soft labels
over actions, by leveraging effective actions that prove suitable in resolving
grid congestion. To further enhance decision-making, we integrate Graph Neural
Networks (GNNs) to encode the structural properties of power grids, ensuring
that the topology-aware representations contribute to better agent performance.
Our approach significantly outperforms state-of-the-art baselines, all of which
use only topological actions, as well as feedforward and GNN-based
architectures with hard labels. Most notably, it achieves a 17% better
performance compared to the greedy expert agent from which the imitation
targets were derived.",['cs.LG'],"['Mohamed Hassouna', 'Clara Holzhüter', 'Malte Lehna', 'Matthijs de Jong', 'Jan Viebahn', 'Bernhard Sick', 'Christoph Scholz']",2025-03-19,2025-03-19
2503.15576v1,A Bird Song Detector for improving bird identification through Deep Learning: a case study from Doñana,"Passive Acoustic Monitoring with automatic recorders is essential for
ecosystem conservation but generates vast unsupervised audio data, posing
challenges for extracting meaningful information. Deep Learning techniques
offer a promising solution. BirdNET, a widely used model for bird
identification, has shown success in many study systems but is limited in some
regions due to biases in its training data. A key challenge in bird species
detection is that many recordings either lack target species or contain
overlapping vocalizations. To overcome these problems, we developed a
multi-stage pipeline for automatic bird vocalization identification in Do\~nana
National Park (SW Spain), a region facing significant conservation threats. Our
approach included a Bird Song Detector to isolate vocalizations and custom
classifiers trained with BirdNET embeddings. We manually annotated 461 minutes
of audio from three habitats across nine locations, yielding 3,749 annotations
for 34 classes. Spectrograms facilitated the use of image processing
techniques. Applying the Bird Song Detector before classification improved
species identification, as all classification models performed better when
analyzing only the segments where birds were detected. Specifically, the
combination of the Bird Song Detector and fine-tuned BirdNET compared to the
baseline without the Bird Song Detector. Our approach demonstrated the
effectiveness of integrating a Bird Song Detector with fine-tuned
classification models for bird identification at local soundscapes. These
findings highlight the need to adapt general-purpose tools for specific
ecological challenges, as demonstrated in Do\~nana. Automatically detecting
bird species serves for tracking the health status of this threatened
ecosystem, given the sensitivity of birds to environmental changes, and helps
in the design of conservation measures for reducing biodiversity loss","['cs.SD', 'cs.AI', 'cs.CV', 'cs.LG', 'cs.NE', 'I.5.4; I.2.6; I.4.8']","['Alba Márquez-Rodríguez', 'Miguel Ángel Mohedano-Munoz', 'Manuel J. Marín-Jiménez', 'Eduardo Santamaría-García', 'Giulia Bastianelli', 'Pedro Jordano', 'Irene Mendoza']",2025-03-19,2025-03-19
2503.15185v1,3D Occupancy Prediction with Low-Resolution Queries via Prototype-aware View Transformation,"The resolution of voxel queries significantly influences the quality of view
transformation in camera-based 3D occupancy prediction. However, computational
constraints and the practical necessity for real-time deployment require
smaller query resolutions, which inevitably leads to an information loss.
Therefore, it is essential to encode and preserve rich visual details within
limited query sizes while ensuring a comprehensive representation of 3D
occupancy. To this end, we introduce ProtoOcc, a novel occupancy network that
leverages prototypes of clustered image segments in view transformation to
enhance low-resolution context. In particular, the mapping of 2D prototypes
onto 3D voxel queries encodes high-level visual geometries and complements the
loss of spatial information from reduced query resolutions. Additionally, we
design a multi-perspective decoding strategy to efficiently disentangle the
densely compressed visual cues into a high-dimensional 3D occupancy scene.
Experimental results on both Occ3D and SemanticKITTI benchmarks demonstrate the
effectiveness of the proposed method, showing clear improvements over the
baselines. More importantly, ProtoOcc achieves competitive performance against
the baselines even with 75\% reduced voxel resolution.","['cs.CV', 'cs.AI']","['Gyeongrok Oh', 'Sungjune Kim', 'Heeju Ko', 'Hyung-gun Chi', 'Jinkyu Kim', 'Dongwook Lee', 'Daehyun Ji', 'Sungjoon Choi', 'Sujin Jang', 'Sangpil Kim']",2025-03-19,2025-03-19
2503.15182v1,Foundation models may exhibit staged progression in novel CBRN threat disclosure,"The extent to which foundation models can disclose novel chemical,
biological, radiation, and nuclear (CBRN) threats to expert users is unclear
due to a lack of test cases. I leveraged the unique opportunity presented by an
upcoming publication describing a novel catastrophic biothreat - ""Technical
Report on Mirror Bacteria: Feasibility and Risks"" - to conduct a small
controlled study before it became public. Graduate-trained biologists tasked
with predicting the consequences of releasing mirror E. coli showed no
significant differences in rubric-graded accuracy using Claude Sonnet 3.5 new
(n=10) or web search only (n=2); both groups scored comparably to a web
baseline (28 and 43 versus 36). However, Sonnet reasoned correctly when
prompted by a report author, but a smaller model, Haiku 3.5, failed even with
author guidance (80 versus 5). These results suggest distinct stages of model
capability: Haiku is unable to reason about mirror life even with threat-aware
expert guidance (Stage 1), while Sonnet correctly reasons only with
threat-aware prompting (Stage 2). Continued advances may allow future models to
disclose novel CBRN threats to naive experts (Stage 3) or unskilled users
(Stage 4). While mirror life represents only one case study, monitoring new
models' ability to reason about privately known threats may allow protective
measures to be implemented before widespread disclosure.","['cs.CY', 'cs.AI', 'q-bio.OT']",['Kevin M Esvelt'],2025-03-19,2025-03-19
2503.15177v1,Food Delivery Time Prediction in Indian Cities Using Machine Learning Models,"Accurate prediction of food delivery times significantly impacts customer
satisfaction, operational efficiency, and profitability in food delivery
services. However, existing studies primarily utilize static historical data
and often overlook dynamic, real-time contextual factors crucial for precise
prediction, particularly in densely populated Indian cities. This research
addresses these gaps by integrating real-time contextual variables such as
traffic density, weather conditions, local events, and geospatial data
(restaurant and delivery location coordinates) into predictive models. We
systematically compare various machine learning algorithms, including Linear
Regression, Decision Trees, Bagging, Random Forest, XGBoost, and LightGBM, on a
comprehensive food delivery dataset specific to Indian urban contexts. Rigorous
data preprocessing and feature selection significantly enhanced model
performance. Experimental results demonstrate that the LightGBM model achieves
superior predictive accuracy, with an R2 score of 0.76 and Mean Squared Error
(MSE) of 20.59, outperforming traditional baseline approaches. Our study thus
provides actionable insights for improving logistics strategies in complex
urban environments. The complete methodology and code are publicly available
for reproducibility and further research.",['cs.LG'],"['Ananya Garg', 'Mohmmad Ayaan', 'Swara Parekh', 'Vikranth Udandarao']",2025-03-19,2025-03-19
2503.15176v1,A Review on Large Language Models for Visual Analytics,"This paper provides a comprehensive review of the integration of Large
Language Models (LLMs) with visual analytics, addressing their foundational
concepts, capabilities, and wide-ranging applications. It begins by outlining
the theoretical underpinnings of visual analytics and the transformative
potential of LLMs, specifically focusing on their roles in natural language
understanding, natural language generation, dialogue systems, and text-to-media
transformations. The review further investigates how the synergy between LLMs
and visual analytics enhances data interpretation, visualization techniques,
and interactive exploration capabilities. Key tools and platforms including
LIDA, Chat2VIS, Julius AI, and Zoho Analytics, along with specialized
multimodal models such as ChartLlama and CharXIV, are critically evaluated. The
paper discusses their functionalities, strengths, and limitations in supporting
data exploration, visualization enhancement, automated reporting, and insight
extraction. The taxonomy of LLM tasks, ranging from natural language
understanding (NLU), natural language generation (NLG), to dialogue systems and
text-to-media transformations, is systematically explored. This review provides
a SWOT analysis of integrating Large Language Models (LLMs) with visual
analytics, highlighting strengths like accessibility and flexibility,
weaknesses such as computational demands and biases, opportunities in
multimodal integration and user collaboration, and threats including privacy
concerns and skill degradation. It emphasizes addressing ethical considerations
and methodological improvements for effective integration.","['cs.HC', 'cs.CL', 'cs.CV']","['Navya Sonal Agarwal', 'Sanjay Kumar Sonbhadra']",2025-03-19,2025-03-19
2503.15172v1,Multi-Agent Actor-Critic with Harmonic Annealing Pruning for Dynamic Spectrum Access Systems,"Multi-Agent Deep Reinforcement Learning (MADRL) has emerged as a powerful
tool for optimizing decentralized decision-making systems in complex settings,
such as Dynamic Spectrum Access (DSA). However, deploying deep learning models
on resource-constrained edge devices remains challenging due to their high
computational cost. To address this challenge, in this paper, we present a
novel sparse recurrent MARL framework integrating gradual neural network
pruning into the independent actor global critic paradigm. Additionally, we
introduce a harmonic annealing sparsity scheduler, which achieves comparable,
and in certain cases superior, performance to standard linear and polynomial
pruning schedulers at large sparsities. Our experimental investigation
demonstrates that the proposed DSA framework can discover superior policies,
under diverse training conditions, outperforming conventional DSA, MADRL
baselines, and state-of-the-art pruning techniques.","['cs.LG', 'cs.AI', 'cs.NI']","['George Stamatelis', 'Angelos-Nikolaos Kanatas', 'George C. Alexandropoulos']",2025-03-19,2025-03-19
2503.15169v1,Comparing Llama3 and DeepSeekR1 on Biomedical Text Classification Tasks,"This study compares the performance of two open-source large language models
(LLMs)-Llama3-70B and DeepSeekR1-distill-Llama3-70B-on six biomedical text
classification tasks. Four tasks involve data from social media, while two
tasks focus on clinical notes from electronic health records, and all
experiments were performed in zero-shot settings. Performance metrics,
including precision, recall, and F1 scores, were measured for each task, along
with their 95% confidence intervals. Results demonstrated that
DeepSeekR1-distill-Llama3-70B generally performs better in terms of precision
on most tasks, with mixed results on recall. While the zero-shot LLMs
demonstrated high F1 scores for some tasks, they grossly underperformed on
others, for data from both sources. The findings suggest that model selection
should be guided by the specific requirements of the health-related text
classification tasks, particularly when considering the precision-recall
trade-offs, and that, in the presence of annotated data, supervised
classification approaches may be more reliable than zero-shot LLMs.","['cs.CL', 'cs.AI']","['Yuting Guo', 'Abeed Sarker']",2025-03-19,2025-03-19
2503.15168v1,"World Models in Artificial Intelligence: Sensing, Learning, and Reasoning Like a Child","World Models help Artificial Intelligence (AI) predict outcomes, reason about
its environment, and guide decision-making. While widely used in reinforcement
learning, they lack the structured, adaptive representations that even young
children intuitively develop. Advancing beyond pattern recognition requires
dynamic, interpretable frameworks inspired by Piaget's cognitive development
theory. We highlight six key research areas -- physics-informed learning,
neurosymbolic learning, continual learning, causal inference, human-in-the-loop
AI, and responsible AI -- as essential for enabling true reasoning in AI. By
integrating statistical learning with advances in these areas, AI can evolve
from pattern recognition to genuine understanding, adaptation and reasoning
capabilities.","['cs.AI', 'cs.CV', 'cs.ET', 'cs.LG', '68T05']","['Javier Del Ser', 'Jesus L. Lobo', 'Heimo Müller', 'Andreas Holzinger']",2025-03-19,2025-03-19
2503.15167v1,Volumetric Reconstruction From Partial Views for Task-Oriented Grasping,"Object affordance and volumetric information are essential in devising
effective grasping strategies under task-specific constraints. This paper
presents an approach for inferring suitable grasping strategies from limited
partial views of an object. To achieve this, a recurrent generative adversarial
network (R-GAN) was proposed by incorporating a recurrent generator with long
short-term memory (LSTM) units for it to process a variable number of depth
scans. To determine object affordances, the AffordPose knowledge dataset is
utilized as prior knowledge. Affordance retrieving is defined by the volume
similarity measured via Chamfer Distance and action similarities. A Proximal
Policy Optimization (PPO) reinforcement learning model is further implemented
to refine the retrieved grasp strategies for task-oriented grasping. The
retrieved grasp strategies were evaluated on a dual-arm mobile manipulation
robot with an overall grasping accuracy of 89% for four tasks: lift, handle
grasp, wrap grasp, and press.","['cs.RO', 'cs.AI']","['Fujian Yan', 'Hui Li', 'Hongsheng He']",2025-03-19,2025-03-19
2503.15166v1,Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive Learning: Adapting Alignment Calibration to MERU,"Machine unlearning methods have become increasingly important for selective
concept removal in large pre-trained models. While recent work has explored
unlearning in Euclidean contrastive vision-language models, the effectiveness
of concept removal in hyperbolic spaces remains unexplored. This paper
investigates machine unlearning in hyperbolic contrastive learning by adapting
Alignment Calibration to MERU, a model that embeds images and text in
hyperbolic space to better capture semantic hierarchies. Through systematic
experiments and ablation studies, we demonstrate that hyperbolic geometry
offers distinct advantages for concept removal, achieving near perfect
forgetting with reasonable performance on retained concepts, particularly when
scaling to multiple concept removal. Our approach introduces
hyperbolic-specific components including entailment calibration and norm
regularization that leverage the unique properties of hyperbolic space.
Comparative analysis with Euclidean models reveals fundamental differences in
unlearning dynamics, with hyperbolic unlearning reorganizing the semantic
hierarchy while Euclidean approaches merely disconnect cross-modal
associations. These findings not only advance machine unlearning techniques but
also provide insights into the geometric properties that influence concept
representation and removal in multimodal models. Source code available at
https://github.com/alex-pv01/HAC","['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG', 'cs.MM']","['Àlex Pujol Vidal', 'Sergio Escalera', 'Kamal Nasrollahi', 'Thomas B. Moeslund']",2025-03-19,2025-03-19
2503.15163v1,Global Group Fairness in Federated Learning via Function Tracking,"We investigate group fairness regularizers in federated learning, aiming to
train a globally fair model in a distributed setting. Ensuring global fairness
in distributed training presents unique challenges, as fairness regularizers
typically involve probability metrics between distributions across all clients
and are not naturally separable by client. To address this, we introduce a
function-tracking scheme for the global fairness regularizer based on a Maximum
Mean Discrepancy (MMD), which incurs a small communication overhead. This
scheme seamlessly integrates into most federated learning algorithms while
preserving rigorous convergence guarantees, as demonstrated in the context of
FedAvg. Additionally, when enforcing differential privacy, the kernel-based MMD
regularization enables straightforward analysis through a change of kernel,
leveraging an intuitive interpretation of kernel convolution. Numerical
experiments confirm our theoretical insights.","['cs.LG', 'math.OC', 'stat.ME']","['Yves Rychener', 'Daniel Kuhn', 'Yifan Hu']",2025-03-19,2025-03-19
2503.15161v1,UltraFlwr -- An Efficient Federated Medical and Surgical Object Detection Framework,"Object detection shows promise for medical and surgical applications such as
cell counting and tool tracking. However, its faces multiple real-world edge
deployment challenges including limited high-quality annotated data, data
sharing restrictions, and computational constraints. In this work, we introduce
UltraFlwr, a framework for federated medical and surgical object detection. By
leveraging Federated Learning (FL), UltraFlwr enables decentralized model
training across multiple sites without sharing raw data. To further enhance
UltraFlwr's efficiency, we propose YOLO-PA, a set of novel Partial Aggregation
(PA) strategies specifically designed for YOLO models in FL. YOLO-PA
significantly reduces communication overhead by up to 83% per round while
maintaining performance comparable to Full Aggregation (FA) strategies. Our
extensive experiments on BCCD and m2cai16-tool-locations datasets demonstrate
that YOLO-PA not only provides better client models compared to client-wise
centralized training and FA strategies, but also facilitates efficient training
and deployment across resource-constrained edge devices. Further, we also
establish one of the first benchmarks in federated medical and surgical object
detection. This paper advances the feasibility of training and deploying
detection models on the edge, making federated object detection more practical
for time-critical and resource-constrained medical and surgical applications.
UltraFlwr is publicly available at https://github.com/KCL-BMEIS/UltraFlwr.",['cs.CV'],"['Yang Li', 'Soumya Snigdha Kundu', 'Maxence Boels', 'Toktam Mahmoodi', 'Sebastien Ourselin', 'Tom Vercauteren', 'Prokar Dasgupta', 'Jonathan Shapey', 'Alejandro Granados']",2025-03-19,2025-03-19
2503.15156v1,ARC: Anchored Representation Clouds for High-Resolution INR Classification,"Implicit neural representations (INRs) encode signals in neural network
weights as a memory-efficient representation, decoupling sampling resolution
from the associated resource costs. Current INR image classification methods
are demonstrated on low-resolution data and are sensitive to image-space
transformations. We attribute these issues to the global, fully-connected MLP
neural network architecture encoding of current INRs, which lack mechanisms for
local representation: MLPs are sensitive to absolute image location and
struggle with high-frequency details. We propose ARC: Anchored Representation
Clouds, a novel INR architecture that explicitly anchors latent vectors locally
in image-space. By introducing spatial structure to the latent vectors, ARC
captures local image data which in our testing leads to state-of-the-art
implicit image classification of both low- and high-resolution images and
increased robustness against image-space translation. Code can be found at
https://github.com/JLuij/anchored_representation_clouds.",['cs.CV'],"['Joost Luijmes', 'Alexander Gielisse', 'Roman Knyazhitskiy', 'Jan van Gemert']",2025-03-19,2025-03-19
2503.15150v1,Preference Construction: A Bayesian Interactive Preference Elicitation Framework Based on Monte Carlo Tree Search,"We present a novel preference learning framework to capture participant
preferences efficiently within limited interaction rounds. It involves three
main contributions. First, we develop a variational Bayesian approach to infer
the participant's preference model by estimating posterior distributions and
managing uncertainty from limited information. Second, we propose an adaptive
questioning policy that maximizes cumulative uncertainty reduction, formulating
questioning as a finite Markov decision process and using Monte Carlo Tree
Search to prioritize promising question trajectories. By considering long-term
effects and leveraging the efficiency of the Bayesian approach, the policy
avoids shortsightedness. Third, we apply the framework to Multiple Criteria
Decision Aiding, with pairwise comparison as the preference information and an
additive value function as the preference model. We integrate the
reparameterization trick to address high-variance issues, enhancing robustness
and efficiency. Computational studies on real-world and synthetic datasets
demonstrate the framework's practical usability, outperforming baselines in
capturing preferences and achieving superior uncertainty reduction within
limited interactions.",['cs.LG'],"['Yan Wang', 'Jiapeng Liu', 'Milosz Kadziński', 'Xiuwu Liao']",2025-03-19,2025-03-19
2503.15149v1,Machine learning surrogate models of many-body dispersion interactions in polymer melts,"Accurate prediction of many-body dispersion (MBD) interactions is essential
for understanding the van der Waals forces that govern the behavior of many
complex molecular systems. However, the high computational cost of MBD
calculations limits their direct application in large-scale simulations. In
this work, we introduce a machine learning surrogate model specifically
designed to predict MBD forces in polymer melts, a system that demands accurate
MBD description and offers structural advantages for machine learning
approaches. Our model is based on a trimmed SchNet architecture that
selectively retains the most relevant atomic connections and incorporates
trainable radial basis functions for geometric encoding. We validate our
surrogate model on datasets from polyethylene, polypropylene, and polyvinyl
chloride melts, demonstrating high predictive accuracy and robust
generalization across diverse polymer systems. In addition, the model captures
key physical features, such as the characteristic decay behavior of MBD
interactions, providing valuable insights for optimizing cutoff strategies.
Characterized by high computational efficiency, our surrogate model enables
practical incorporation of MBD effects into large-scale molecular simulations.","['cs.LG', 'physics.comp-ph']","['Zhaoxiang Shen', 'Raúl I. Sosa', 'Jakub Lengiewicz', 'Alexandre Tkatchenko', 'Stéphane P. A. Bordas']",2025-03-19,2025-03-19
2503.15144v1,PointSFDA: Source-free Domain Adaptation for Point Cloud Completion,"Conventional methods for point cloud completion, typically trained on
synthetic datasets, face significant challenges when applied to
out-of-distribution real-world scans. In this paper, we propose an effective
yet simple source-free domain adaptation framework for point cloud completion,
termed \textbf{PointSFDA}. Unlike unsupervised domain adaptation that reduces
the domain gap by directly leveraging labeled source data, PointSFDA uses only
a pretrained source model and unlabeled target data for adaptation, avoiding
the need for inaccessible source data in practical scenarios. Being the first
source-free domain adaptation architecture for point cloud completion, our
method offers two core contributions. First, we introduce a coarse-to-fine
distillation solution to explicitly transfer the global geometry knowledge
learned from the source dataset. Second, as noise may be introduced due to
domain gaps, we propose a self-supervised partial-mask consistency training
strategy to learn local geometry information in the target domain. Extensive
experiments have validated that our method significantly improves the
performance of state-of-the-art networks in cross-domain shape completion. Our
code is available at
\emph{\textcolor{magenta}{https://github.com/Starak-x/PointSFDA}}.",['cs.CV'],"['Xing He', 'Zhe Zhu', 'Liangliang Nan', 'Honghua Chen', 'Jing Qin', 'Mingqiang Wei']",2025-03-19,2025-03-19
2503.15141v1,Object-Centric Pretraining via Target Encoder Bootstrapping,"Object-centric representation learning has recently been successfully applied
to real-world datasets. This success can be attributed to pretrained
non-object-centric foundation models, whose features serve as reconstruction
targets for slot attention. However, targets must remain frozen throughout the
training, which sets an upper bound on the performance object-centric models
can attain. Attempts to update the target encoder by bootstrapping result in
large performance drops, which can be attributed to its lack of object-centric
inductive biases, causing the object-centric model's encoder to drift away from
representations useful as reconstruction targets. To address these limitations,
we propose Object-CEntric Pretraining by Target Encoder BOotstrapping, a
self-distillation setup for training object-centric models from scratch, on
real-world data, for the first time ever. In OCEBO, the target encoder is
updated as an exponential moving average of the object-centric model, thus
explicitly being enriched with object-centric inductive biases introduced by
slot attention while removing the upper bound on performance present in other
models. We mitigate the slot collapse caused by random initialization of the
target encoder by introducing a novel cross-view patch filtering approach that
limits the supervision to sufficiently informative patches. When pretrained on
241k images from COCO, OCEBO achieves unsupervised object discovery performance
comparable to that of object-centric models with frozen non-object-centric
target encoders pretrained on hundreds of millions of images. The code and
pretrained models are publicly available at https://github.com/djukicn/ocebo.",['cs.CV'],"['Nikola Đukić', 'Tim Lebailly', 'Tinne Tuytelaars']",2025-03-19,2025-03-19
2503.15138v2,VideoGen-of-Thought: Step-by-step generating multi-shot video with minimal manual intervention,"Current video generation models excel at short clips but fail to produce
cohesive multi-shot narratives due to disjointed visual dynamics and fractured
storylines. Existing solutions either rely on extensive manual
scripting/editing or prioritize single-shot fidelity over cross-scene
continuity, limiting their practicality for movie-like content. We introduce
VideoGen-of-Thought (VGoT), a step-by-step framework that automates multi-shot
video synthesis from a single sentence by systematically addressing three core
challenges: (1) Narrative Fragmentation: Existing methods lack structured
storytelling. We propose dynamic storyline modeling, which first converts the
user prompt into concise shot descriptions, then elaborates them into detailed,
cinematic specifications across five domains (character dynamics, background
continuity, relationship evolution, camera movements, HDR lighting), ensuring
logical narrative progression with self-validation. (2) Visual Inconsistency:
Existing approaches struggle with maintaining visual consistency across shots.
Our identity-aware cross-shot propagation generates identity-preserving
portrait (IPP) tokens that maintain character fidelity while allowing trait
variations (expressions, aging) dictated by the storyline. (3) Transition
Artifacts: Abrupt shot changes disrupt immersion. Our adjacent latent
transition mechanisms implement boundary-aware reset strategies that process
adjacent shots' features at transition points, enabling seamless visual flow
while preserving narrative continuity. VGoT generates multi-shot videos that
outperform state-of-the-art baselines by 20.4% in within-shot face consistency
and 17.4% in style consistency, while achieving over 100% better cross-shot
consistency and 10x fewer manual adjustments than alternatives.",['cs.CV'],"['Mingzhe Zheng', 'Yongqi Xu', 'Haojian Huang', 'Xuran Ma', 'Yexin Liu', 'Wenjie Shu', 'Yatian Pang', 'Feilong Tang', 'Qifeng Chen', 'Harry Yang', 'Ser-Nam Lim']",2025-03-19,2025-03-20
2503.15133v1,EmoGRACE: Aspect-based emotion analysis for social media data,"While sentiment analysis has advanced from sentence to aspect-level, i.e.,
the identification of concrete terms related to a sentiment, the equivalent
field of Aspect-based Emotion Analysis (ABEA) is faced with dataset bottlenecks
and the increased complexity of emotion classes in contrast to binary
sentiments. This paper addresses these gaps, by generating a first ABEA
training dataset, consisting of 2,621 English Tweets, and fine-tuning a
BERT-based model for the ABEA sub-tasks of Aspect Term Extraction (ATE) and
Aspect Emotion Classification (AEC).
  The dataset annotation process was based on the hierarchical emotion theory
by Shaver et al. [1] and made use of group annotation and majority voting
strategies to facilitate label consistency. The resulting dataset contained
aspect-level emotion labels for Anger, Sadness, Happiness, Fear, and a None
class. Using the new ABEA training dataset, the state-of-the-art ABSA model
GRACE by Luo et al. [2] was fine-tuned for ABEA. The results reflected a
performance plateau at an F1-score of 70.1% for ATE and 46.9% for joint ATE and
AEC extraction. The limiting factors for model performance were broadly
identified as the small training dataset size coupled with the increased task
complexity, causing model overfitting and limited abilities to generalize well
on new data.",['cs.CL'],"['Christina Zorenböhmer', 'Sebastian Schmidt', 'Bernd Resch']",2025-03-19,2025-03-19
2503.16549v1,MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical Problems,"Despite impressive performance across diverse tasks, Multimodal Large
Language Models (MLLMs) have yet to fully demonstrate their potential in visual
mathematical problem-solving, particularly in accurately perceiving and
interpreting diagrams. Inspired by typical processes of humans, we hypothesize
that the perception capabilities to extract meaningful information from
diagrams is crucial, as it directly impacts subsequent inference processes. To
validate this hypothesis, we developed FlowVerse, a comprehensive benchmark
that categorizes all information used during problem-solving into four
components, which are then combined into six problem versions for evaluation.
Our preliminary results on FlowVerse reveal that existing MLLMs exhibit
substantial limitations when extracting essential information and reasoned
property from diagrams and performing complex reasoning based on these visual
inputs. In response, we introduce MathFlow, a modular problem-solving pipeline
that decouples perception and inference into distinct stages, thereby
optimizing each independently. Given the perceptual limitations observed in
current MLLMs, we trained MathFlow-P-7B as a dedicated perception model.
Experimental results indicate that MathFlow-P-7B yields substantial performance
gains when integrated with various closed-source and open-source inference
models. This demonstrates the effectiveness of the MathFlow pipeline and its
compatibility to diverse inference frameworks. The FlowVerse benchmark and code
are available at https://github.com/MathFlow-zju/MathFlow.",['cs.CV'],"['Felix Chen', 'Hangjie Yuan', 'Yunqiu Xu', 'Tao Feng', 'Jun Cen', 'Pengwei Liu', 'Zeying Huang', 'Yi Yang']",2025-03-19,2025-03-19
2503.15130v1,A Foundational Theory for Decentralized Sensory Learning,"In both neuroscience and artificial intelligence, popular functional
frameworks and neural network formulations operate by making use of extrinsic
error measurements and global learning algorithms. Through a set of conjectures
based on evolutionary insights on the origin of cellular adaptive mechanisms,
we reinterpret the core meaning of sensory signals to allow the brain to be
interpreted as a negative feedback control system, and show how this could lead
to local learning algorithms without the need for global error correction
metrics. Thereby, a sufficiently good minima in sensory activity can be the
complete reward signal of the network, as well as being both necessary and
sufficient for biological learning to arise. We show that this method of
learning was likely already present in the earliest unicellular life forms on
earth. We show evidence that the same principle holds and scales to
multicellular organisms where it in addition can lead to division of labour
between cells. Available evidence shows that the evolution of the nervous
system likely was an adaptation to more effectively communicate intercellular
signals to support such division of labour. We therefore propose that the same
learning principle that evolved already in the earliest unicellular life forms,
i.e. negative feedback control of externally and internally generated sensor
signals, has simply been scaled up to become a fundament of the learning we see
in biological brains today. We illustrate diverse biological settings, from the
earliest unicellular organisms to humans, where this operational principle
appears to be a plausible interpretation of the meaning of sensor signals in
biology, and how this relates to current neuroscientific theories and findings.","['q-bio.NC', 'cs.AI']","['Linus Mårtensson', 'Jonas M. D. Enander', 'Udaya B. Rongala', 'Henrik Jörntell']",2025-03-19,2025-03-19
2503.15129v1,Aligning Crowd-sourced Human Feedback for Reinforcement Learning on Code Generation by Large Language Models,"This paper studies how AI-assisted programming and large language models
(LLM) improve software developers' ability via AI tools (LLM agents) like
Github Copilot and Amazon CodeWhisperer, while integrating human feedback to
enhance reinforcement learning (RLHF) with crowd-sourced computation to enhance
text-to-code generation. Additionally, we demonstrate that our Bayesian
optimization framework supports AI alignment in code generation by distributing
the feedback collection burden, highlighting the value of collecting human
feedback of good quality. Our empirical evaluations demonstrate the efficacy of
this approach, showcasing how LLM agents can be effectively trained for
improved text-to-code generation. Our Bayesian optimization framework can be
designed for general domain-specific languages, promoting the alignment of
large language model capabilities with human feedback in AI-assisted
programming for code generation.",['cs.AI'],"['Man Fai Wong', 'Chee Wei Tan']",2025-03-19,2025-03-19
2503.15128v1,Increasing the Robustness of the Fine-tuned Multilingual Machine-Generated Text Detectors,"Since the proliferation of LLMs, there have been concerns about their misuse
for harmful content creation and spreading. Recent studies justify such fears,
providing evidence of LLM vulnerabilities and high potential of their misuse.
Humans are no longer able to distinguish between high-quality machine-generated
and authentic human-written texts. Therefore, it is crucial to develop
automated means to accurately detect machine-generated content. It would enable
to identify such content in online information space, thus providing an
additional information about its credibility. This work addresses the problem
by proposing a robust fine-tuning process of LLMs for the detection task,
making the detectors more robust against obfuscation and more generalizable to
out-of-distribution data.","['cs.CL', 'cs.AI']","['Dominik Macko', 'Robert Moro', 'Ivan Srba']",2025-03-19,2025-03-19
2503.15126v1,Text-Derived Relational Graph-Enhanced Network for Skeleton-Based Action Segmentation,"Skeleton-based Temporal Action Segmentation (STAS) aims to segment and
recognize various actions from long, untrimmed sequences of human skeletal
movements. Current STAS methods typically employ spatio-temporal modeling to
establish dependencies among joints as well as frames, and utilize one-hot
encoding with cross-entropy loss for frame-wise classification supervision.
However, these methods overlook the intrinsic correlations among joints and
actions within skeletal features, leading to a limited understanding of human
movements. To address this, we propose a Text-Derived Relational Graph-Enhanced
Network (TRG-Net) that leverages prior graphs generated by Large Language
Models (LLM) to enhance both modeling and supervision. For modeling, the
Dynamic Spatio-Temporal Fusion Modeling (DSFM) method incorporates Text-Derived
Joint Graphs (TJG) with channel- and frame-level dynamic adaptation to
effectively model spatial relations, while integrating spatio-temporal core
features during temporal modeling. For supervision, the Absolute-Relative
Inter-Class Supervision (ARIS) method employs contrastive learning between
action features and text embeddings to regularize the absolute class
distributions, and utilizes Text-Derived Action Graphs (TAG) to capture the
relative inter-class relationships among action features. Additionally, we
propose a Spatial-Aware Enhancement Processing (SAEP) method, which
incorporates random joint occlusion and axial rotation to enhance spatial
generalization. Performance evaluations on four public datasets demonstrate
that TRG-Net achieves state-of-the-art results.","['cs.CV', 'cs.AI']","['Haoyu Ji', 'Bowen Chen', 'Weihong Ren', 'Wenze Huang', 'Zhihao Yang', 'Zhiyong Wang', 'Honghai Liu']",2025-03-19,2025-03-19
2503.15574v1,Machine Learning Techniques for Multifactor Analysis of National Carbon Dioxide Emissions,"This paper presents a comprehensive study leveraging Support Vector Machine
(SVM) regression and Principal Component Regression (PCR) to analyze carbon
dioxide emissions in a global dataset of 62 countries and their dependence on
idiosyncratic, country-specific parameters. The objective is to understand the
factors contributing to carbon dioxide emissions and identify the most
predictive elements. The analysis provides country-specific emission estimates,
highlighting diverse national trajectories and pinpointing areas for targeted
interventions in climate change mitigation, sustainable development, and the
growing carbon credit markets and green finance sector. The study aims to
support policymaking with accurate representations of carbon dioxide emissions,
offering nuanced information for formulating effective strategies to address
climate change while informing initiatives related to carbon trading and
environmentally sustainable investments.",['cs.LG'],"['Wenjia Xie', 'Jinhui Li', 'Kai Zong', 'Luis Seco']",2025-03-19,2025-03-19
2503.15573v1,Neuronal Activation States as Sample Embeddings for Data Selection in Task-Specific Instruction Tuning,"Task-specific instruction tuning enhances the performance of large language
models (LLMs) on specialized tasks, yet efficiently selecting relevant data for
this purpose remains a challenge. Inspired by neural coactivation in the human
brain, we propose a novel data selection method called NAS, which leverages
neuronal activation states as embeddings for samples in the feature space.
Extensive experiments show that NAS outperforms classical data selection
methods in terms of both effectiveness and robustness across different models,
datasets, and selection ratios.",['cs.LG'],"['Da Ma', 'Gonghu Shang', 'Zhi Chen', 'Libo Qin', 'Yijie Luo', 'Lei Pan', 'Shuai Fan', 'Lu Chen', 'Kai Yu']",2025-03-19,2025-03-19
2503.15124v1,Evaluating ASR Confidence Scores for Automated Error Detection in User-Assisted Correction Interfaces,"Despite advances in Automatic Speech Recognition (ASR), transcription errors
persist and require manual correction. Confidence scores, which indicate the
certainty of ASR results, could assist users in identifying and correcting
errors. This study evaluates the reliability of confidence scores for error
detection through a comprehensive analysis of end-to-end ASR models and a user
study with 36 participants. The results show that while confidence scores
correlate with transcription accuracy, their error detection performance is
limited. Classifiers frequently miss errors or generate many false positives,
undermining their practical utility. Confidence-based error detection neither
improved correction efficiency nor was perceived as helpful by participants.
These findings highlight the limitations of confidence scores and the need for
more sophisticated approaches to improve user interaction and explainability of
ASR results.","['cs.HC', 'cs.CL', 'cs.SD', 'eess.AS', 'I.2.7']","['Korbinian Kuhn', 'Verena Kersken', 'Gottfried Zimmermann']",2025-03-19,2025-03-19
2503.15117v1,Exploring Model Editing for LLM-based Aspect-Based Sentiment Classification,"Model editing aims at selectively updating a small subset of a neural model's
parameters with an interpretable strategy to achieve desired modifications. It
can significantly reduce computational costs to adapt to large language models
(LLMs). Given its ability to precisely target critical components within LLMs,
model editing shows great potential for efficient fine-tuning applications. In
this work, we investigate model editing to serve an efficient method for
adapting LLMs to solve aspect-based sentiment classification. Through causal
interventions, we trace and determine which neuron hidden states are essential
for the prediction of the model. By performing interventions and restorations
on each component of an LLM, we identify the importance of these components for
aspect-based sentiment classification. Our findings reveal that a distinct set
of mid-layer representations is essential for detecting the sentiment polarity
of given aspect words. Leveraging these insights, we develop a model editing
approach that focuses exclusively on these critical parts of the LLM, leading
to a more efficient method for adapting LLMs. Our in-domain and out-of-domain
experiments demonstrate that this approach achieves competitive results
compared to the currently strongest methods with significantly fewer trainable
parameters, highlighting a more efficient and interpretable fine-tuning
strategy.",['cs.CL'],"['Shichen Li', 'Zhongqing Wang', 'Zheyu Zhao', 'Yue Zhang', 'Peifeng Li']",2025-03-19,2025-03-19
2503.15114v1,DeCaFlow: A Deconfounding Causal Generative Model,"Causal generative models (CGMs) have recently emerged as capable approaches
to simulate the causal mechanisms generating our observations, enabling causal
inference. Unfortunately, existing approaches either are overly restrictive,
assuming the absence of hidden confounders, or lack generality, being tailored
to a particular query and graph. In this work, we introduce DeCaFlow, a CGM
that accounts for hidden confounders in a single amortized training process
using only observational data and the causal graph. Importantly, DeCaFlow can
provably identify all causal queries with a valid adjustment set or
sufficiently informative proxy variables. Remarkably, for the first time to our
knowledge, we show that a confounded counterfactual query is identifiable, and
thus solvable by DeCaFlow, as long as its interventional counterpart is as
well. Our empirical results on diverse settings (including the Ecoli70 dataset,
with 3 independent hidden confounders, tens of observed variables and hundreds
of causal queries) show that DeCaFlow outperforms existing approaches, while
demonstrating its out-of-the-box flexibility.",['cs.LG'],"['Alejandro Almodóvar', 'Adrián Javaloy', 'Juan Parras', 'Santiago Zazo', 'Isabel Valera']",2025-03-19,2025-03-19
2503.15113v1,Reasoning Effort and Problem Complexity: A Scaling Analysis in LLMs,"Large Language Models (LLMs) have demonstrated remarkable text generation
capabilities, and recent advances in training paradigms have led to
breakthroughs in their reasoning performance. In this work, we investigate how
the reasoning effort of such models scales with problem complexity. We use the
infinitely scalable Tents puzzle, which has a known linear-time solution, to
analyze this scaling behavior. Our results show that reasoning effort scales
with problem size, but only up to a critical problem complexity. Beyond this
threshold, the reasoning effort does not continue to increase, and may even
decrease. This observation highlights a critical limitation in the logical
coherence of current LLMs as problem complexity increases, and underscores the
need for strategies to improve reasoning scalability. Furthermore, our results
reveal significant performance differences between current state-of-the-art
reasoning models when faced with increasingly complex logical puzzles.",['cs.AI'],"['Benjamin Estermann', 'Roger Wattenhofer']",2025-03-19,2025-03-19
2503.15111v1,FedLWS: Federated Learning with Adaptive Layer-wise Weight Shrinking,"In Federated Learning (FL), weighted aggregation of local models is conducted
to generate a new global model, and the aggregation weights are typically
normalized to 1. A recent study identifies the global weight shrinking effect
in FL, indicating an enhancement in the global model's generalization when the
sum of weights (i.e., the shrinking factor) is smaller than 1, where how to
learn the shrinking factor becomes crucial. However, principled approaches to
this solution have not been carefully studied from the adequate consideration
of privacy concerns and layer-wise distinctions. To this end, we propose a
novel model aggregation strategy, Federated Learning with Adaptive Layer-wise
Weight Shrinking (FedLWS), which adaptively designs the shrinking factor in a
layer-wise manner and avoids optimizing the shrinking factors on a proxy
dataset. We initially explored the factors affecting the shrinking factor
during the training process. Then we calculate the layer-wise shrinking factors
by considering the distinctions among each layer of the global model. FedLWS
can be easily incorporated with various existing methods due to its
flexibility. Extensive experiments under diverse scenarios demonstrate the
superiority of our method over several state-of-the-art approaches, providing a
promising tool for enhancing the global model in FL.",['cs.LG'],"['Changlong Shi', 'Jinmeng Li', 'He Zhao', 'Dan dan Guo', 'Yi Chang']",2025-03-19,2025-03-19
2503.15110v2,GIVEPose: Gradual Intra-class Variation Elimination for RGB-based Category-Level Object Pose Estimation,"Recent advances in RGBD-based category-level object pose estimation have been
limited by their reliance on precise depth information, restricting their
broader applicability. In response, RGB-based methods have been developed.
Among these methods, geometry-guided pose regression that originated from
instance-level tasks has demonstrated strong performance. However, we argue
that the NOCS map is an inadequate intermediate representation for
geometry-guided pose regression method, as its many-to-one correspondence with
category-level pose introduces redundant instance-specific information,
resulting in suboptimal results. This paper identifies the intra-class
variation problem inherent in pose regression based solely on the NOCS map and
proposes the Intra-class Variation-Free Consensus (IVFC) map, a novel
coordinate representation generated from the category-level consensus model. By
leveraging the complementary strengths of the NOCS map and the IVFC map, we
introduce GIVEPose, a framework that implements Gradual Intra-class Variation
Elimination for category-level object pose estimation. Extensive evaluations on
both synthetic and real-world datasets demonstrate that GIVEPose significantly
outperforms existing state-of-the-art RGB-based approaches, achieving
substantial improvements in category-level object pose estimation. Our code is
available at https://github.com/ziqin-h/GIVEPose.",['cs.CV'],"['Zinqin Huang', 'Gu Wang', 'Chenyangguang Zhang', 'Ruida Zhang', 'Xiu Li', 'Xiangyang Ji']",2025-03-19,2025-03-20
2503.15108v1,VIPER: Visual Perception and Explainable Reasoning for Sequential Decision-Making,"While Large Language Models (LLMs) excel at reasoning on text and
Vision-Language Models (VLMs) are highly effective for visual perception,
applying those models for visual instruction-based planning remains a widely
open problem. In this paper, we introduce VIPER, a novel framework for
multimodal instruction-based planning that integrates VLM-based perception with
LLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM
generates textual descriptions of image observations, which are then processed
by an LLM policy to predict actions based on the task goal. We fine-tune the
reasoning module using behavioral cloning and reinforcement learning, improving
our agent's decision-making capabilities. Experiments on the ALFWorld benchmark
show that VIPER significantly outperforms state-of-the-art visual
instruction-based planners while narrowing the gap with purely text-based
oracles. By leveraging text as an intermediate representation, VIPER also
enhances explainability, paving the way for a fine-grained analysis of
perception and reasoning components.","['cs.LG', 'cs.AI', 'cs.RO']","['Mohamed Salim Aissi', 'Clemence Grislain', 'Mohamed Chetouani', 'Olivier Sigaud', 'Laure Soulier', 'Nicolas Thome']",2025-03-19,2025-03-19
2503.15107v1,Interpretability of Graph Neural Networks to Assert Effects of Global Change Drivers on Ecological Networks,"Pollinators play a crucial role for plant reproduction, either in natural
ecosystem or in human-modified landscape. Global change drivers,including
climate change or land use modifications, can alter the plant-pollinator
interactions. To assert the potential influence of global change drivers on
pollination, large-scale interactions, climate and land use data are required.
While recent machine learning methods, such as graph neural networks (GNNs),
allow the analysis of such datasets, interpreting their results can be
challenging. We explore existing methods for interpreting GNNs in order to
highlight the effects of various environmental covariates on pollination
network connectivity. A large simulation study is performed to confirm whether
these methods can detect the interactive effect between a covariate and a genus
of plant on connectivity, and whether the application of debiasing techniques
influences the estimation of these effects. An application on the Spipoll
dataset, with and without accounting for sampling effects, highlights the
potential impact of land use on network connectivity and shows that accounting
for sampling effects partially alters the estimation of these effects.","['stat.ML', 'cs.LG']","['Emre Anakok', 'Pierre Barbillon', 'Colin Fontaine', 'Elisa Thebault']",2025-03-19,2025-03-19
2503.15106v2,Distilling 3D distinctive local descriptors for 6D pose estimation,"Three-dimensional local descriptors are crucial for encoding geometric
surface properties, making them essential for various point cloud understanding
tasks. Among these descriptors, GeDi has demonstrated strong zero-shot 6D pose
estimation capabilities but remains computationally impractical for real-world
applications due to its expensive inference process. Can we retain GeDi's
effectiveness while significantly improving its efficiency? In this paper, we
explore this question by introducing a knowledge distillation framework that
trains an efficient student model to regress local descriptors from a GeDi
teacher. Our key contributions include: an efficient large-scale training
procedure that ensures robustness to occlusions and partial observations while
operating under compute and storage constraints, and a novel loss formulation
that handles weak supervision from non-distinctive teacher descriptors. We
validate our approach on five BOP Benchmark datasets and demonstrate a
significant reduction in inference time while maintaining competitive
performance with existing methods, bringing zero-shot 6D pose estimation closer
to real-time feasibility. Project Website: https://tev-fbk.github.io/dGeDi/",['cs.CV'],"['Amir Hamza', 'Andrea Caraffa', 'Davide Boscaini', 'Fabio Poiesi']",2025-03-19,2025-03-20
2503.15105v1,"Control, Optimal Transport and Neural Differential Equations in Supervised Learning","From the perspective of control theory, neural differential equations (neural
ODEs) have become an important tool for supervised learning. In the fundamental
work of Ruiz-Balet and Zuazua (SIAM REVIEW 2023), the authors pose an open
problem regarding the connection between control theory, optimal transport
theory, and neural differential equations. More precisely, they inquire how one
can quantify the closeness of the optimal flows in neural transport equations
to the true dynamic optimal transport. In this work, we propose a construction
of neural differential equations that converge to the true dynamic optimal
transport in the limit, providing a significant step in solving the formerly
mentioned open problem.","['math.NA', 'cs.LG', 'cs.NA', 'math.OC']","['Minh-Nhat Phung', 'Minh-Binh Tran']",2025-03-19,2025-03-19
2503.15571v1,LLM-Aided Customizable Profiling of Code Data Based On Programming Language Concepts,"Data profiling is critical in machine learning for generating descriptive
statistics, supporting both deeper understanding and downstream tasks like data
valuation and curation. This work addresses profiling specifically in the
context of code datasets for Large Language Models (code-LLMs), where data
quality directly influences tasks such as code generation and summarization.
Characterizing code datasets in terms of programming language concepts enables
better insights and targeted data curation. Our proposed methodology decomposes
code data profiling into two phases: (1) an offline phase where LLMs are
leveraged to derive and learn rules for extracting syntactic and semantic
concepts across various programming languages, including previously unseen or
low-resource languages, and (2) an online deterministic phase applying these
derived rules for efficient real-time analysis. This hybrid approach is
customizable, extensible to new syntactic and semantic constructs, and scalable
to multiple languages. Experimentally, our LLM-aided method achieves a mean
accuracy of 90.33% for syntactic extraction rules and semantic classification
accuracies averaging 80% and 77% across languages and semantic concepts,
respectively.","['cs.SE', 'cs.ET', 'cs.IR', 'cs.LG', 'cs.PL']","['Pankaj Thorat', 'Adnan Qidwai', 'Adrija Dhar', 'Aishwariya Chakraborty', 'Anand Eswaran', 'Hima Patel', 'Praveen Jayachandran']",2025-03-19,2025-03-19
2503.15096v1,When the Future Becomes the Past: Taming Temporal Correspondence for Self-supervised Video Representation Learning,"The past decade has witnessed notable achievements in self-supervised
learning for video tasks. Recent efforts typically adopt the Masked Video
Modeling (MVM) paradigm, leading to significant progress on multiple video
tasks. However, two critical challenges remain: 1) Without human annotations,
the random temporal sampling introduces uncertainty, increasing the difficulty
of model training. 2) Previous MVM methods primarily recover the masked patches
in the pixel space, leading to insufficient information compression for
downstream tasks. To address these challenges jointly, we propose a
self-supervised framework that leverages Temporal Correspondence for video
Representation learning (T-CoRe). For challenge 1), we propose a sandwich
sampling strategy that selects two auxiliary frames to reduce reconstruction
uncertainty in a two-side-squeezing manner. Addressing challenge 2), we
introduce an auxiliary branch into a self-distillation architecture to restore
representations in the latent space, generating high-level semantic
representations enriched with temporal information. Experiments of T-CoRe
consistently present superior performance across several downstream tasks,
demonstrating its effectiveness for video representation learning. The code is
available at https://github.com/yafeng19/T-CORE.",['cs.CV'],"['Yang Liu', 'Qianqian Xu', 'Peisong Wen', 'Siran Dai', 'Qingming Huang']",2025-03-19,2025-03-19
2503.15095v1,Diffusion-Based Forecasting for Uncertainty-Aware Model Predictive Control,"We propose Diffusion-Informed Model Predictive Control (D-I MPC), a generic
framework for uncertainty-aware prediction and decision-making in partially
observable stochastic systems by integrating diffusion-based time series
forecasting models in Model Predictive Control algorithms. In our approach, a
diffusion-based time series forecasting model is used to probabilistically
estimate the evolution of the system's stochastic components. These forecasts
are then incorporated into MPC algorithms to estimate future trajectories and
optimize action selection under the uncertainty of the future. We evaluate the
framework on the task of energy arbitrage, where a Battery Energy Storage
System participates in the day-ahead electricity market of the New York state.
Experimental results indicate that our model-based approach with a
diffusion-based forecaster significantly outperforms both implementations with
classical forecasting methods and model-free reinforcement learning baselines.","['cs.LG', 'cs.AI', 'cs.SY', 'eess.SY', 'I.2.6; I.5.1']","['Stelios Zarifis', 'Ioannis Kordonis', 'Petros Maragos']",2025-03-19,2025-03-19
2503.15092v1,Towards Understanding the Safety Boundaries of DeepSeek Models: Evaluation and Findings,"This study presents the first comprehensive safety evaluation of the DeepSeek
models, focusing on evaluating the safety risks associated with their generated
content. Our evaluation encompasses DeepSeek's latest generation of large
language models, multimodal large language models, and text-to-image models,
systematically examining their performance regarding unsafe content generation.
Notably, we developed a bilingual (Chinese-English) safety evaluation dataset
tailored to Chinese sociocultural contexts, enabling a more thorough evaluation
of the safety capabilities of Chinese-developed models. Experimental results
indicate that despite their strong general capabilities, DeepSeek models
exhibit significant safety vulnerabilities across multiple risk dimensions,
including algorithmic discrimination and sexual content. These findings provide
crucial insights for understanding and improving the safety of large foundation
models. Our code is available at
https://github.com/NY1024/DeepSeek-Safety-Eval.","['cs.CR', 'cs.AI', 'cs.CL']","['Zonghao Ying', 'Guangyi Zheng', 'Yongxin Huang', 'Deyue Zhang', 'Wenxin Zhang', 'Quanchen Zou', 'Aishan Liu', 'Xianglong Liu', 'Dacheng Tao']",2025-03-19,2025-03-19
2503.15091v1,Intelligent Spatial Perception by Building Hierarchical 3D Scene Graphs for Indoor Scenarios with the Help of LLMs,"This paper addresses the high demand in advanced intelligent robot navigation
for a more holistic understanding of spatial environments, by introducing a
novel system that harnesses the capabilities of Large Language Models (LLMs) to
construct hierarchical 3D Scene Graphs (3DSGs) for indoor scenarios. The
proposed framework constructs 3DSGs consisting of a fundamental layer with rich
metric-semantic information, an object layer featuring precise point-cloud
representation of object nodes as well as visual descriptors, and higher layers
of room, floor, and building nodes. Thanks to the innovative application of
LLMs, not only object nodes but also nodes of higher layers, e.g., room nodes,
are annotated in an intelligent and accurate manner. A polling mechanism for
room classification using LLMs is proposed to enhance the accuracy and
reliability of the room node annotation. Thorough numerical experiments
demonstrate the system's ability to integrate semantic descriptions with
geometric data, creating an accurate and comprehensive representation of the
environment instrumental for context-aware navigation and task planning.","['cs.RO', 'cs.CV']","['Yao Cheng', 'Zhe Han', 'Fengyang Jiang', 'Huaizhen Wang', 'Fengyu Zhou', 'Qingshan Yin', 'Lei Wei']",2025-03-19,2025-03-19
2503.15089v1,Continual Contrastive Learning on Tabular Data with Out of Distribution,"Out-of-distribution (OOD) prediction remains a significant challenge in
machine learning, particularly for tabular data where traditional methods often
fail to generalize beyond their training distribution. This paper introduces
Tabular Continual Contrastive Learning (TCCL), a novel framework designed to
address OOD challenges in tabular data processing. TCCL integrates contrastive
learning principles with continual learning mechanisms, featuring a
three-component architecture: an Encoder for data transformation, a Decoder for
representation learning, and a Learner Head. We evaluate TCCL against 14
baseline models, including state-of-the-art deep learning approaches and
gradient-boosted decision trees (GBDT), across eight diverse tabular datasets.
Our experimental results demonstrate that TCCL consistently outperforms
existing methods in both classification and regression tasks on OOD data, with
particular strength in handling distribution shifts. These findings suggest
that TCCL represents a significant advancement in handling OOD scenarios for
tabular data.",['cs.LG'],"['Achmad Ginanjar', 'Xue Li', 'Priyanka Singh', 'Wen Hua']",2025-03-19,2025-03-19
2503.15087v1,An Investigation of Beam Density on LiDAR Object Detection Performance,"Accurate 3D object detection is a critical component of autonomous driving,
enabling vehicles to perceive their surroundings with precision and make
informed decisions. LiDAR sensors, widely used for their ability to provide
detailed 3D measurements, are key to achieving this capability. However,
variations between training and inference data can cause significant
performance drops when object detection models are employed in different sensor
settings. One critical factor is beam density, as inference on sparse,
cost-effective LiDAR sensors is often preferred in real-world applications.
Despite previous work addressing the beam-density-induced domain gap,
substantial knowledge gaps remain, particularly concerning dense 128-beam
sensors in cross-domain scenarios. To gain better understanding of the impact
of beam density on domain gaps, we conduct a comprehensive investigation that
includes an evaluation of different object detection architectures. Our
architecture evaluation reveals that combining voxel- and point-based
approaches yields superior cross-domain performance by leveraging the strengths
of both representations. Building on these findings, we analyze
beam-density-induced domain gaps and argue that these domain gaps must be
evaluated in conjunction with other domain shifts. Contrary to conventional
beliefs, our experiments reveal that detectors benefit from training on denser
data and exhibit robustness to beam density variations during inference.",['cs.CV'],"['Christoph Griesbacher', 'Christian Fruhwirth-Reisinger']",2025-03-19,2025-03-19
2503.15082v1,StyleLoco: Generative Adversarial Distillation for Natural Humanoid Robot Locomotion,"Humanoid robots are anticipated to acquire a wide range of locomotion
capabilities while ensuring natural movement across varying speeds and
terrains. Existing methods encounter a fundamental dilemma in learning humanoid
locomotion: reinforcement learning with handcrafted rewards can achieve agile
locomotion but produces unnatural gaits, while Generative Adversarial Imitation
Learning (GAIL) with motion capture data yields natural movements but suffers
from unstable training processes and restricted agility. Integrating these
approaches proves challenging due to the inherent heterogeneity between expert
policies and human motion datasets. To address this, we introduce StyleLoco, a
novel two-stage framework that bridges this gap through a Generative
Adversarial Distillation (GAD) process. Our framework begins by training a
teacher policy using reinforcement learning to achieve agile and dynamic
locomotion. It then employs a multi-discriminator architecture, where distinct
discriminators concurrently extract skills from both the teacher policy and
motion capture data. This approach effectively combines the agility of
reinforcement learning with the natural fluidity of human-like movements while
mitigating the instability issues commonly associated with adversarial
training. Through extensive simulation and real-world experiments, we
demonstrate that StyleLoco enables humanoid robots to perform diverse
locomotion tasks with the precision of expertly trained policies and the
natural aesthetics of human motion, successfully transferring styles across
different movement types while maintaining stable locomotion across a broad
spectrum of command inputs.","['cs.RO', 'cs.AI']","['Le Ma', 'Ziyu Meng', 'Tengyu Liu', 'Yuhan Li', 'Ran Song', 'Wei Zhang', 'Siyuan Huang']",2025-03-19,2025-03-19
2503.15070v1,MultiBARF: Integrating Imagery of Different Wavelength Regions by Using Neural Radiance Fields,"Optical sensor applications have become popular through digital
transformation. Linking observed data to real-world locations and combining
different image sensors is essential to make the applications practical and
efficient. However, data preparation to try different sensor combinations
requires high sensing and image processing expertise. To make data preparation
easier for users unfamiliar with sensing and image processing, we have
developed MultiBARF. This method replaces the co-registration and geometric
calibration by synthesizing pairs of two different sensor images and depth
images at assigned viewpoints. Our method extends Bundle Adjusting Neural
Radiance Fields(BARF), a deep neural network-based novel view synthesis method,
for the two imagers. Through experiments on visible light and thermographic
images, we demonstrate that our method superimposes two color channels of those
sensor images on NeRF.",['cs.CV'],"['Kana Kurata', 'Hitoshi Niigaki', 'Xiaojun Wu', 'Ryuichi Tanida']",2025-03-19,2025-03-19
2503.15060v2,Conjuring Positive Pairs for Efficient Unification of Representation Learning and Image Synthesis,"While representation learning and generative modeling seek to understand
visual data, unifying both domains remains unexplored. Recent Unified
Self-Supervised Learning (SSL) methods have started to bridge the gap between
both paradigms. However, they rely solely on semantic token reconstruction,
which requires an external tokenizer during training -- introducing a
significant overhead. In this work, we introduce Sorcen, a novel unified SSL
framework, incorporating a synergic Contrastive-Reconstruction objective. Our
Contrastive objective, ""Echo Contrast"", leverages the generative capabilities
of Sorcen, eliminating the need for additional image crops or augmentations
during training. Sorcen ""generates"" an echo sample in the semantic token space,
forming the contrastive positive pair. Sorcen operates exclusively on
precomputed tokens, eliminating the need for an online token transformation
during training, thereby significantly reducing computational overhead.
Extensive experiments on ImageNet-1k demonstrate that Sorcen outperforms the
previous Unified SSL SoTA by 0.4%, 1.48 FID, 1.76%, and 1.53% on linear
probing, unconditional image generation, few-shot learning, and transfer
learning, respectively, while being 60.8% more efficient. Additionally, Sorcen
surpasses previous single-crop MIM SoTA in linear probing and achieves SoTA
performance in unconditional image generation, highlighting significant
improvements and breakthroughs in Unified SSL models.","['cs.CV', 'cs.AI', 'I.5.4; I.5.1; I.2.10']","['Imanol G. Estepa', 'Jesús M. Rodríguez-de-Vera', 'Ignacio Sarasúa', 'Bhalaji Nagarajan', 'Petia Radeva']",2025-03-19,2025-03-20
2503.15058v1,Texture-Aware StarGAN for CT data harmonisation,"Computed Tomography (CT) plays a pivotal role in medical diagnosis; however,
variability across reconstruction kernels hinders data-driven approaches, such
as deep learning models, from achieving reliable and generalized performance.
To this end, CT data harmonization has emerged as a promising solution to
minimize such non-biological variances by standardizing data across different
sources or conditions. In this context, Generative Adversarial Networks (GANs)
have proved to be a powerful framework for harmonization, framing it as a
style-transfer problem. However, GAN-based approaches still face limitations in
capturing complex relationships within the images, which are essential for
effective harmonization. In this work, we propose a novel texture-aware StarGAN
for CT data harmonization, enabling one-to-many translations across different
reconstruction kernels. Although the StarGAN model has been successfully
applied in other domains, its potential for CT data harmonization remains
unexplored. Furthermore, our approach introduces a multi-scale texture loss
function that embeds texture information across different spatial and angular
scales into the harmonization process, effectively addressing kernel-induced
texture variations. We conducted extensive experimentation on a publicly
available dataset, utilizing a total of 48667 chest CT slices from 197 patients
distributed over three different reconstruction kernels, demonstrating the
superiority of our method over the baseline StarGAN.","['eess.IV', 'cs.AI', 'cs.CV']","['Francesco Di Feola', 'Ludovica Pompilio', 'Cecilia Assolito', 'Valerio Guarrasi', 'Paolo Soda']",2025-03-19,2025-03-19
2503.15057v1,"A Data-driven Investigation of Euphemistic Language: Comparing the usage of ""slave"" and ""servant"" in 19th century US newspapers","This study investigates the usage of ""slave"" and ""servant"" in the 19th
century US newspapers using computational methods. While both terms were used
to refer to enslaved African Americans, they were used in distinct ways. In the
Chronicling America corpus, we included possible OCR errors by using FastText
embedding and excluded text reprints to consider text reprint culture in the
19th century. Word2vec embedding was used to find semantically close words to
""slave"" and ""servant"" and log-odds ratio was calculated to identify
over-represented discourse words in the Southern and Northern newspapers. We
found that ""slave"" is associated with socio-economic, legal, and administrative
words, however, ""servant"" is linked to religious words in the Northern
newspapers while Southern newspapers associated ""servant"" with domestic and
familial words. We further found that slave discourse words in Southern
newspapers are more prevalent in Northern newspapers while servant discourse
words from each side are prevalent in their own region. This study contributes
to the understanding of how newspapers created different discourses around
enslaved African Americans in the 19th century US.",['cs.CL'],"['Jaihyun Park', 'Ryan Cordell']",2025-03-19,2025-03-19
2503.15056v1,Single-Step Bidirectional Unpaired Image Translation Using Implicit Bridge Consistency Distillation,"Unpaired image-to-image translation has seen significant progress since the
introduction of CycleGAN. However, methods based on diffusion models or
Schr\""odinger bridges have yet to be widely adopted in real-world applications
due to their iterative sampling nature. To address this challenge, we propose a
novel framework, Implicit Bridge Consistency Distillation (IBCD), which enables
single-step bidirectional unpaired translation without using adversarial loss.
IBCD extends consistency distillation by using a diffusion implicit bridge
model that connects PF-ODE trajectories between distributions. Additionally, we
introduce two key improvements: 1) distribution matching for consistency
distillation and 2) adaptive weighting method based on distillation difficulty.
Experimental results demonstrate that IBCD achieves state-of-the-art
performance on benchmark datasets in a single generation step. Project page
available at https://hyn2028.github.io/project_page/IBCD/index.html",['cs.CV'],"['Suhyeon Lee', 'Kwanyoung Kim', 'Jong Chul Ye']",2025-03-19,2025-03-19
2503.15055v1,ELTEX: A Framework for Domain-Driven Synthetic Data Generation,"We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework
for generating high-quality synthetic training data in specialized domains.
While Large Language Models (LLMs) have shown impressive general capabilities,
their performance in specialized domains like cybersecurity remains limited by
the scarcity of domain-specific training data. ELTEX addresses this challenge
by systematically integrating explicit domain indicator extraction with dynamic
prompting to preserve critical domain knowledge throughout the generation
process. We demonstrate ELTEX's effectiveness in the context of
blockchain-related cyberattack detection, where we fine-tune Gemma-2B using
various combinations of real and ELTEX-generated data. Our results show that
the ELTEX-enhanced model achieves performance competitive with GPT-4 across
both standard classification metrics and uncertainty calibration, while
requiring significantly fewer computational resources. We release a curated
synthetic dataset of social media texts for cyberattack detection in
blockchain. Our work demonstrates that domain-driven synthetic data generation
can effectively bridge the performance gap between resource-efficient models
and larger architectures in specialized domains.",['cs.CL'],"['Arina Razmyslovich', 'Kseniia Murasheva', 'Sofia Sedlova', 'Julien Capitaine', 'Eugene Dmitriev']",2025-03-19,2025-03-19
2503.15049v1,HAD-Gen: Human-like and Diverse Driving Behavior Modeling for Controllable Scenario Generation,"Simulation-based testing has emerged as an essential tool for verifying and
validating autonomous vehicles (AVs). However, contemporary methodologies, such
as deterministic and imitation learning-based driver models, struggle to
capture the variability of human-like driving behavior. Given these challenges,
we propose HAD-Gen, a general framework for realistic traffic scenario
generation that simulates diverse human-like driving behaviors. The framework
first clusters the vehicle trajectory data into different driving styles
according to safety features. It then employs maximum entropy inverse
reinforcement learning on each of the clusters to learn the reward function
corresponding to each driving style. Using these reward functions, the method
integrates offline reinforcement learning pre-training and multi-agent
reinforcement learning algorithms to obtain general and robust driving
policies. Multi-perspective simulation results show that our proposed scenario
generation framework can simulate diverse, human-like driving behaviors with
strong generalization capability. The proposed framework achieves a 90.96%
goal-reaching rate, an off-road rate of 2.08%, and a collision rate of 6.91% in
the generalization test, outperforming prior approaches by over 20% in
goal-reaching performance. The source code is released at
https://github.com/RoboSafe-Lab/Sim4AD.","['cs.RO', 'cs.AI', 'cs.MA']","['Cheng Wang', 'Lingxin Kong', 'Massimiliano Tamborski', 'Stefano V. Albrecht']",2025-03-19,2025-03-19
2503.15044v1,SPADE: Systematic Prompt Framework for Automated Dialogue Expansion in Machine-Generated Text Detection,"The increasing capability of large language models (LLMs) to generate
synthetic content has heightened concerns about their misuse, driving the
development of Machine-Generated Text (MGT) detection models. However, these
detectors face significant challenges due to the lack of systematically
generated, high-quality datasets for training. To address this issue, we
propose five novel data augmentation frameworks for synthetic user dialogue
generation through a structured prompting approach, reducing the costs
associated with traditional data collection methods. Our proposed method yields
14 new dialogue datasets, which we benchmark against seven MGT detection
models. The results demonstrate improved generalization performance when
utilizing a mixed dataset produced by our proposed augmentation framework.
Furthermore, considering that real-world agents lack knowledge of future
opponent utterances, we simulate online dialogue detection and examine the
relationship between chat history length and detection accuracy. We also
benchmark online detection performance with limited chat history on our
frameworks. Our open-source datasets can be downloaded from
https://github.com/AngieYYF/SPADE-customer-service-dialogue.",['cs.CL'],"['Haoyi Li', 'Angela Yifei Yuan', 'Soyeon Caren Han', 'Christopher Leckie']",2025-03-19,2025-03-19
2503.15569v1,RAG-based User Profiling for Precision Planning in Mixed-precision Over-the-Air Federated Learning,"Mixed-precision computing, a widely applied technique in AI, offers a larger
trade-off space between accuracy and efficiency. The recent purposed
Mixed-Precision Over-the-Air Federated Learning (MP-OTA-FL) enables clients to
operate at appropriate precision levels based on their heterogeneous hardware,
taking advantages of the larger trade-off space while covering the quantization
overheads in the mixed-precision modulation scheme for the OTA aggregation
process. A key to further exploring the potential of the MP-OTA-FL framework is
the optimization of client precision levels. The choice of precision level
hinges on multifaceted factors including hardware capability, potential client
contribution, and user satisfaction, among which factors can be difficult to
define or quantify.
  In this paper, we propose a RAG-based User Profiling for precision planning
framework that integrates retrieval-augmented LLMs and dynamic client profiling
to optimize satisfaction and contributions. This includes a hybrid interface
for gathering device/user insights and an RAG database storing historical
quantization decisions with feedback. Experiments show that our method boosts
satisfaction, energy savings, and global model accuracy in MP-OTA-FL systems.","['cs.LG', 'cs.HC']","['Jinsheng Yuan', 'Yun Tang', 'Weisi Guo']",2025-03-19,2025-03-19
2503.15036v1,Multivariate Gaussian Topic Modelling: A novel approach to discover topics with greater semantic coherence,"An important aspect of text mining involves information retrieval in form of
discovery of semantic themes (topics) from documents using topic modelling.
While generative topic models like Latent Dirichlet Allocation (LDA) elegantly
model topics as probability distributions and are useful in identifying latent
topics from large document corpora with minimal supervision, they suffer from
difficulty in topic interpretability and reduced performance in shorter texts.
Here we propose a novel Multivariate Gaussian Topic modelling (MGD) approach.
In this approach topics are presented as Multivariate Gaussian Distributions
and documents as Gaussian Mixture Models. Using EM algorithm, the various
constituent Multivariate Gaussian Distributions and their corresponding
parameters are identified. Analysis of the parameters helps identify the
keywords having the highest variance and mean contributions to the topic, and
from these key-words topic annotations are carried out. This approach is first
applied on a synthetic dataset to demonstrate the interpretability benefits
vis-\`a-vis LDA. A real-world application of this topic model is demonstrated
in analysis of risks and hazards at a petrochemical plant by applying the model
on safety incident reports to identify the major latent hazards plaguing the
plant. This model achieves a higher mean topic coherence of 0.436 vis-\`a-vis
0.294 for LDA.",['cs.LG'],"['Satyajeet Sahoo', 'Jhareswar Maiti', 'Virendra Kumar Tewari']",2025-03-19,2025-03-19
2503.15035v1,GraspCorrect: Robotic Grasp Correction via Vision-Language Model-Guided Feedback,"Despite significant advancements in robotic manipulation, achieving
consistent and stable grasping remains a fundamental challenge, often limiting
the successful execution of complex tasks. Our analysis reveals that even
state-of-the-art policy models frequently exhibit unstable grasping behaviors,
leading to failure cases that create bottlenecks in real-world robotic
applications. To address these challenges, we introduce GraspCorrect, a
plug-and-play module designed to enhance grasp performance through
vision-language model-guided feedback. GraspCorrect employs an iterative visual
question-answering framework with two key components: grasp-guided prompting,
which incorporates task-specific constraints, and object-aware sampling, which
ensures the selection of physically feasible grasp candidates. By iteratively
generating intermediate visual goals and translating them into joint-level
actions, GraspCorrect significantly improves grasp stability and consistently
enhances task success rates across existing policy models in the RLBench and
CALVIN datasets.","['cs.AI', 'cs.RO']","['Sungjae Lee', 'Yeonjoo Hong', 'Kwang In Kim']",2025-03-19,2025-03-19
2503.15029v1,DRoPE: Directional Rotary Position Embedding for Efficient Agent Interaction Modeling,"Accurate and efficient modeling of agent interactions is essential for
trajectory generation, the core of autonomous driving systems. Existing
methods, scene-centric, agent-centric, and query-centric frameworks, each
present distinct advantages and drawbacks, creating an impossible triangle
among accuracy, computational time, and memory efficiency. To break this
limitation, we propose Directional Rotary Position Embedding (DRoPE), a novel
adaptation of Rotary Position Embedding (RoPE), originally developed in natural
language processing. Unlike traditional relative position embedding (RPE),
which introduces significant space complexity, RoPE efficiently encodes
relative positions without explicitly increasing complexity but faces inherent
limitations in handling angular information due to periodicity. DRoPE overcomes
this limitation by introducing a uniform identity scalar into RoPE's 2D rotary
transformation, aligning rotation angles with realistic agent headings to
naturally encode relative angular information. We theoretically analyze DRoPE's
correctness and efficiency, demonstrating its capability to simultaneously
optimize trajectory generation accuracy, time complexity, and space complexity.
Empirical evaluations compared with various state-of-the-art trajectory
generation models, confirm DRoPE's good performance and significantly reduced
space complexity, indicating both theoretical soundness and practical
effectiveness. The video documentation is available at
https://drope-traj.github.io/.","['cs.RO', 'cs.CV']","['Jianbo Zhao', 'Taiyu Ban', 'Zhihao Liu', 'Hangning Zhou', 'Xiyang Wang', 'Qibin Zhou', 'Hailong Qin', 'Mu Yang', 'Lei Liu', 'Bin Li']",2025-03-19,2025-03-19
2503.15024v1,Forensics-Bench: A Comprehensive Forgery Detection Benchmark Suite for Large Vision Language Models,"Recently, the rapid development of AIGC has significantly boosted the
diversities of fake media spread in the Internet, posing unprecedented threats
to social security, politics, law, and etc. To detect the ever-increasingly
diverse malicious fake media in the new era of AIGC, recent studies have
proposed to exploit Large Vision Language Models (LVLMs) to design robust
forgery detectors due to their impressive performance on a wide range of
multimodal tasks. However, it still lacks a comprehensive benchmark designed to
comprehensively assess LVLMs' discerning capabilities on forgery media. To fill
this gap, we present Forensics-Bench, a new forgery detection evaluation
benchmark suite to assess LVLMs across massive forgery detection tasks,
requiring comprehensive recognition, location and reasoning capabilities on
diverse forgeries. Forensics-Bench comprises 63,292 meticulously curated
multi-choice visual questions, covering 112 unique forgery detection types from
5 perspectives: forgery semantics, forgery modalities, forgery tasks, forgery
types and forgery models. We conduct thorough evaluations on 22 open-sourced
LVLMs and 3 proprietary models GPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet,
highlighting the significant challenges of comprehensive forgery detection
posed by Forensics-Bench. We anticipate that Forensics-Bench will motivate the
community to advance the frontier of LVLMs, striving for all-around forgery
detectors in the era of AIGC. The deliverables will be updated at
https://Forensics-Bench.github.io/.",['cs.CV'],"['Jin Wang', 'Chenghui Lv', 'Xian Li', 'Shichao Dong', 'Huadong Li', 'kelu Yao', 'Chao Li', 'Wenqi Shao', 'Ping Luo']",2025-03-19,2025-03-19
2503.15023v1,Bridging the Gap: Fusing CNNs and Transformers to Decode the Elegance of Handwritten Arabic Script,"Handwritten Arabic script recognition is a challenging task due to the
script's dynamic letter forms and contextual variations. This paper proposes a
hybrid approach combining convolutional neural networks (CNNs) and
Transformer-based architectures to address these complexities. We evaluated
custom and fine-tuned models, including EfficientNet-B7 and Vision Transformer
(ViT-B16), and introduced an ensemble model that leverages confidence-based
fusion to integrate their strengths. Our ensemble achieves remarkable
performance on the IFN/ENIT dataset, with 96.38% accuracy for letter
classification and 97.22% for positional classification. The results highlight
the complementary nature of CNNs and Transformers, demonstrating their combined
potential for robust Arabic handwriting recognition. This work advances OCR
systems, offering a scalable solution for real-world applications.",['cs.CV'],"['Chaouki Boufenar', 'Mehdi Ayoub Rabiai', 'Boualem Nadjib Zahaf', 'Khelil Rafik Ouaras']",2025-03-19,2025-03-19
2503.15022v1,xMOD: Cross-Modal Distillation for 2D/3D Multi-Object Discovery from 2D motion,"Object discovery, which refers to the task of localizing objects without
human annotations, has gained significant attention in 2D image analysis.
However, despite this growing interest, it remains under-explored in 3D data,
where approaches rely exclusively on 3D motion, despite its several challenges.
In this paper, we present a novel framework that leverages advances in 2D
object discovery which are based on 2D motion to exploit the advantages of such
motion cues being more flexible and generalizable and to bridge the gap between
2D and 3D modalities. Our primary contributions are twofold: (i) we introduce
DIOD-3D, the first baseline for multi-object discovery in 3D data using 2D
motion, incorporating scene completion as an auxiliary task to enable dense
object localization from sparse input data; (ii) we develop xMOD, a cross-modal
training framework that integrates 2D and 3D data while always using 2D motion
cues. xMOD employs a teacher-student training paradigm across the two
modalities to mitigate confirmation bias by leveraging the domain gap. During
inference, the model supports both RGB-only and point cloud-only inputs.
Additionally, we propose a late-fusion technique tailored to our pipeline that
further enhances performance when both modalities are available at inference.
We evaluate our approach extensively on synthetic (TRIP-PD) and challenging
real-world datasets (KITTI and Waymo). Notably, our approach yields a
substantial performance improvement compared with the 2D object discovery
state-of-the-art on all datasets with gains ranging from +8.7 to +15.1 in F1@50
score. The code is available at https://github.com/CEA-LIST/xMOD",['cs.CV'],"['Saad Lahlali', 'Sandra Kara', 'Hejer Ammar', 'Florian Chabot', 'Nicolas Granger', 'Hervé Le Borgne', 'Quoc-Cuong Pham']",2025-03-19,2025-03-19
2503.15568v1,Mixed precision accumulation for neural network inference guided by componentwise forward error analysis,"This work proposes a mathematically founded mixed precision accumulation
strategy for the inference of neural networks. Our strategy is based on a new
componentwise forward error analysis that explains the propagation of errors in
the forward pass of neural networks. Specifically, our analysis shows that the
error in each component of the output of a layer is proportional to the
condition number of the inner product between the weights and the input,
multiplied by the condition number of the activation function. These condition
numbers can vary widely from one component to the other, thus creating a
significant opportunity to introduce mixed precision: each component should be
accumulated in a precision inversely proportional to the product of these
condition numbers. We propose a practical algorithm that exploits this
observation: it first computes all components in low precision, uses this
output to estimate the condition numbers, and recomputes in higher precision
only the components associated with large condition numbers. We test our
algorithm on various networks and datasets and confirm experimentally that it
can significantly improve the cost--accuracy tradeoff compared with uniform
precision accumulation baselines.","['cs.LG', 'cs.NA', 'math.NA']","['El-Mehdi El Arar', 'Silviu-Ioan Filip', 'Theo Mary', 'Elisa Riccietti']",2025-03-19,2025-03-19
2503.15019v1,Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene,"The latest emerged 4D Panoptic Scene Graph (4D-PSG) provides an advanced-ever
representation for comprehensively modeling the dynamic 4D visual real world.
Unfortunately, current pioneering 4D-PSG research can primarily suffer from
data scarcity issues severely, as well as the resulting out-of-vocabulary
problems; also, the pipeline nature of the benchmark generation method can lead
to suboptimal performance. To address these challenges, this paper investigates
a novel framework for 4D-PSG generation that leverages rich 2D visual scene
annotations to enhance 4D scene learning. First, we introduce a 4D Large
Language Model (4D-LLM) integrated with a 3D mask decoder for end-to-end
generation of 4D-PSG. A chained SG inference mechanism is further designed to
exploit LLMs' open-vocabulary capabilities to infer accurate and comprehensive
object and relation labels iteratively. Most importantly, we propose a 2D-to-4D
visual scene transfer learning framework, where a spatial-temporal scene
transcending strategy effectively transfers dimension-invariant features from
abundant 2D SG annotations to 4D scenes, effectively compensating for data
scarcity in 4D-PSG. Extensive experiments on the benchmark data demonstrate
that we strikingly outperform baseline models by a large margin, highlighting
the effectiveness of our method.",['cs.CV'],"['Shengqiong Wu', 'Hao Fei', 'Jingkang Yang', 'Xiangtai Li', 'Juncheng Li', 'Hanwang Zhang', 'Tat-seng Chua']",2025-03-19,2025-03-19
2503.15017v1,Exploiting Diffusion Prior for Real-World Image Dehazing with Unpaired Training,"Unpaired training has been verified as one of the most effective paradigms
for real scene dehazing by learning from unpaired real-world hazy and clear
images. Although numerous studies have been proposed, current methods
demonstrate limited generalization for various real scenes due to limited
feature representation and insufficient use of real-world prior. Inspired by
the strong generative capabilities of diffusion models in producing both hazy
and clear images, we exploit diffusion prior for real-world image dehazing, and
propose an unpaired framework named Diff-Dehazer. Specifically, we leverage
diffusion prior as bijective mapping learners within the CycleGAN, a classic
unpaired learning framework. Considering that physical priors contain pivotal
statistics information of real-world data, we further excavate real-world
knowledge by integrating physical priors into our framework. Furthermore, we
introduce a new perspective for adequately leveraging the representation
ability of diffusion models by removing degradation in image and text
modalities, so as to improve the dehazing effect. Extensive experiments on
multiple real-world datasets demonstrate the superior performance of our
method. Our code https://github.com/ywxjm/Diff-Dehazer.",['cs.CV'],"['Yunwei Lan', 'Zhigao Cui', 'Chang Liu', 'Jialun Peng', 'Nian Wang', 'Xin Luo', 'Dong Liu']",2025-03-19,2025-03-19
2503.15016v1,Manifold Learning for Hyperspectral Images,"Traditional feature extraction and projection techniques, such as Principal
Component Analysis, struggle to adequately represent X-Ray Transmission (XRT)
Multi-Energy (ME) images, limiting the performance of neural networks in
decision-making processes. To address this issue, we propose a method that
approximates the dataset topology by constructing adjacency graphs using the
Uniform Manifold Approximation and Projection. This approach captures nonlinear
correlations within the data, significantly improving the performance of
machine learning algorithms, particularly in processing Hyperspectral Images
(HSI) from X-ray transmission spectroscopy. This technique not only preserves
the global structure of the data but also enhances feature separability,
leading to more accurate and robust classification results.","['cs.CV', 'cs.LG']","['Fethi Harkat', 'Tiphaine Deuberet', 'Guillaume Gey', 'Valérie Perrier', 'Kévin Polisano']",2025-03-19,2025-03-19
2503.15013v1,Ambient Noise Full Waveform Inversion with Neural Operators,"Numerical simulations of seismic wave propagation are crucial for
investigating velocity structures and improving seismic hazard assessment.
However, standard methods such as finite difference or finite element are
computationally expensive. Recent studies have shown that a new class of
machine learning models, called neural operators, can solve the elastodynamic
wave equation orders of magnitude faster than conventional methods. Full
waveform inversion is a prime beneficiary of the accelerated simulations.
Neural operators, as end-to-end differentiable operators, combined with
automatic differentiation, provide an alternative approach to the adjoint-state
method. Since neural operators do not involve the Born approximation, when used
for full waveform inversion they have the potential to include additional
phases and alleviate cycle-skipping problems present in traditional
adjoint-state formulations. In this study, we demonstrate the application of
neural operators for full waveform inversion on a real seismic dataset, which
consists of several nodal transects collected across the San Gabriel, Chino,
and San Bernardino basins in the Los Angeles metropolitan area.","['physics.geo-ph', 'cs.LG']","['Caifeng Zou', 'Zachary E. Ross', 'Robert W. Clayton', 'Fan-Chi Lin', 'Kamyar Azizzadenesheli']",2025-03-19,2025-03-19
2503.15008v1,A Novel Channel Boosted Residual CNN-Transformer with Regional-Boundary Learning for Breast Cancer Detection,"Recent advancements in detecting tumors using deep learning on breast
ultrasound images (BUSI) have demonstrated significant success. Deep CNNs and
vision-transformers (ViTs) have demonstrated individually promising initial
performance. However, challenges related to model complexity and contrast,
texture, and tumor morphology variations introduce uncertainties that hinder
the effectiveness of current methods. This study introduces a novel hybrid
framework, CB-Res-RBCMT, combining customized residual CNNs and new ViT
components for detailed BUSI cancer analysis. The proposed RBCMT uses stem
convolution blocks with CNN Meet Transformer (CMT) blocks, followed by new
Regional and boundary (RB) feature extraction operations for capturing contrast
and morphological variations. Moreover, the CMT block incorporates global
contextual interactions through multi-head attention, enhancing computational
efficiency with a lightweight design. Additionally, the customized inverse
residual and stem CNNs within the CMT effectively extract local texture
information and handle vanishing gradients. Finally, the new channel-boosted
(CB) strategy enriches the feature diversity of the limited dataset by
combining the original RBCMT channels with transfer learning-based residual
CNN-generated maps. These diverse channels are processed through a spatial
attention block for optimal pixel selection, reducing redundancy and improving
the discrimination of minor contrast and texture variations. The proposed
CB-Res-RBCMT achieves an F1-score of 95.57%, accuracy of 95.63%, sensitivity of
96.42%, and precision of 94.79% on the standard harmonized stringent BUSI
dataset, outperforming existing ViT and CNN methods. These results demonstrate
the versatility of our integrated CNN-Transformer framework in capturing
diverse features and delivering superior performance in BUSI cancer diagnosis.","['eess.IV', 'cs.AI', 'cs.CV', 'cs.LG']","['Aamir Mehmood', 'Yue Hu', 'Saddam Hussain Khan']",2025-03-19,2025-03-19
2503.15567v1,Towards Unified Latent Space for 3D Molecular Latent Diffusion Modeling,"3D molecule generation is crucial for drug discovery and material science,
requiring models to process complex multi-modalities, including atom types,
chemical bonds, and 3D coordinates. A key challenge is integrating these
modalities of different shapes while maintaining SE(3) equivariance for 3D
coordinates. To achieve this, existing approaches typically maintain separate
latent spaces for invariant and equivariant modalities, reducing efficiency in
both training and sampling. In this work, we propose \textbf{U}nified
Variational \textbf{A}uto-\textbf{E}ncoder for \textbf{3D} Molecular Latent
Diffusion Modeling (\textbf{UAE-3D}), a multi-modal VAE that compresses 3D
molecules into latent sequences from a unified latent space, while maintaining
near-zero reconstruction error. This unified latent space eliminates the
complexities of handling multi-modality and equivariance when performing latent
diffusion modeling. We demonstrate this by employing the Diffusion
Transformer--a general-purpose diffusion model without any molecular inductive
bias--for latent generation. Extensive experiments on GEOM-Drugs and QM9
datasets demonstrate that our method significantly establishes new benchmarks
in both \textit{de novo} and conditional 3D molecule generation, achieving
leading efficiency and quality.",['cs.LG'],"['Yanchen Luo', 'Zhiyuan Liu', 'Yi Zhao', 'Sihang Li', 'Kenji Kawaguchi', 'Tat-Seng Chua', 'Xiang Wang']",2025-03-19,2025-03-19
2503.15005v1,Universal Scene Graph Generation,"Scene graph (SG) representations can neatly and efficiently describe scene
semantics, which has driven sustained intensive research in SG generation. In
the real world, multiple modalities often coexist, with different types, such
as images, text, video, and 3D data, expressing distinct characteristics.
Unfortunately, current SG research is largely confined to single-modality scene
modeling, preventing the full utilization of the complementary strengths of
different modality SG representations in depicting holistic scene semantics. To
this end, we introduce Universal SG (USG), a novel representation capable of
fully characterizing comprehensive semantic scenes from any given combination
of modality inputs, encompassing modality-invariant and modality-specific
scenes. Further, we tailor a niche-targeting USG parser, USG-Par, which
effectively addresses two key bottlenecks of cross-modal object alignment and
out-of-domain challenges. We design the USG-Par with modular architecture for
end-to-end USG generation, in which we devise an object associator to relieve
the modality gap for cross-modal object alignment. Further, we propose a
text-centric scene contrasting learning mechanism to mitigate domain imbalances
by aligning multimodal objects and relations with textual SGs. Through
extensive experiments, we demonstrate that USG offers a stronger capability for
expressing scene semantics than standalone SGs, and also that our USG-Par
achieves higher efficacy and performance.",['cs.CV'],"['Shengqiong Wu', 'Hao Fei', 'Tat-Seng Chua']",2025-03-19,2025-03-19
2503.15004v1,Semantic Segmentation of Transparent and Opaque Drinking Glasses with the Help of Zero-shot Learning,"Segmenting transparent structures in images is challenging since they are
difficult to distinguish from the background. Common examples are drinking
glasses, which are a ubiquitous part of our lives and appear in many different
shapes and sizes. In this work we propose TransCaGNet, a modified version of
the zero-shot model CaGNet. We exchange the segmentation backbone with the
architecture of Trans4Trans to be capable of segmenting transparent objects.
Since some glasses are rarely captured, we use zeroshot learning to be able to
create semantic segmentations of glass categories not given during training. We
propose a novel synthetic dataset covering a diverse set of different
environmental conditions. Additionally we capture a real-world evaluation
dataset since most applications take place in the real world. Comparing our
model with Zeg-Clip we are able to show that TransCaGNet produces better mean
IoU and accuracy values while ZegClip outperforms it mostly for unseen classes.
To improve the segmentation results, we combine the semantic segmentation of
the models with the segmentation results of SAM 2. Our evaluation emphasizes
that distinguishing between different classes is challenging for the models due
to similarity, points of view, or coverings. Taking this behavior into account,
we assign glasses multiple possible categories. The modification leads to an
improvement up to 13.68% for the mean IoU and up to 17.88% for the mean
accuracy values on the synthetic dataset. Using our difficult synthetic dataset
for training, the models produce even better results on the real-world dataset.
The mean IoU is improved up to 5.55% and the mean accuracy up to 5.72% on the
real-world dataset.",['cs.CV'],"['Annalena Blänsdorf', 'Tristan Wirth', 'Arne Rak', 'Thomas Pöllabauer', 'Volker Knauthe', 'Arjan Kuijper']",2025-03-19,2025-03-19
2503.15003v1,LLM Alignment for the Arabs: A Homogenous Culture or Diverse Ones?,"Large language models (LLMs) have the potential of being useful tools that
can automate tasks and assist humans. However, these models are more fluent in
English and more aligned with Western cultures, norms, and values.
Arabic-specific LLMs are being developed to better capture the nuances of the
Arabic language, as well as the views of the Arabs. Yet, Arabs are sometimes
assumed to share the same culture. In this position paper, I discuss the
limitations of this assumption and provide preliminary thoughts for how to
build systems that can better represent the cultural diversity within the Arab
world. The invalidity of the cultural homogeneity assumption might seem
obvious, yet, it is widely adopted in developing multilingual and
Arabic-specific LLMs. I hope that this paper will encourage the NLP community
to be considerate of the cultural diversity within various communities speaking
the same language.",['cs.CL'],['Amr Keleg'],2025-03-19,2025-03-19
2503.15002v1,Scalable Trajectory-User Linking with Dual-Stream Representation Networks,"Trajectory-user linking (TUL) aims to match anonymous trajectories to the
most likely users who generated them, offering benefits for a wide range of
real-world spatio-temporal applications. However, existing TUL methods are
limited by high model complexity and poor learning of the effective
representations of trajectories, rendering them ineffective in handling
large-scale user trajectory data. In this work, we propose a novel
$\underline{Scal}$abl$\underline{e}$ Trajectory-User Linking with dual-stream
representation networks for large-scale $\underline{TUL}$ problem, named
ScaleTUL. Specifically, ScaleTUL generates two views using temporal and spatial
augmentations to exploit supervised contrastive learning framework to
effectively capture the irregularities of trajectories. In each view, a
dual-stream trajectory encoder, consisting of a long-term encoder and a
short-term encoder, is designed to learn unified trajectory representations
that fuse different temporal-spatial dependencies. Then, a TUL layer is used to
associate the trajectories with the corresponding users in the representation
space using a two-stage training model. Experimental results on check-in
mobility datasets from three real-world cities and the nationwide U.S.
demonstrate the superiority of ScaleTUL over state-of-the-art baselines for
large-scale TUL tasks.",['cs.LG'],"['Hao Zhang', 'Wei Chen', 'Xingyu Zhao', 'Jianpeng Qi', 'Guiyuan Jiang', 'Yanwei Yu']",2025-03-19,2025-03-19
2503.15001v1,Low-Complexity Patch-based No-Reference Point Cloud Quality Metric exploiting Weighted Structure and Texture Features,"During the compression, transmission, and rendering of point clouds, various
artifacts are introduced, affecting the quality perceived by the end user.
However, evaluating the impact of these distortions on the overall quality is a
challenging task. This study introduces PST-PCQA, a no-reference point cloud
quality metric based on a low-complexity, learning-based framework. It
evaluates point cloud quality by analyzing individual patches, integrating
local and global features to predict the Mean Opinion Score. In summary, the
process involves extracting features from patches, combining them, and using
correlation weights to predict the overall quality. This approach allows us to
assess point cloud quality without relying on a reference point cloud, making
it particularly useful in scenarios where reference data is unavailable.
Experimental tests on three state-of-the-art datasets show good prediction
capabilities of PST-PCQA, through the analysis of different feature pooling
strategies and its ability to generalize across different datasets. The
ablation study confirms the benefits of evaluating quality on a patch-by-patch
basis. Additionally, PST-PCQA's light-weight structure, with a small number of
parameters to learn, makes it well-suited for real-time applications and
devices with limited computational capacity. For reproducibility purposes, we
made code, model, and pretrained weights available at
https://github.com/michaelneri/PST-PCQA.","['cs.CV', 'cs.MM', 'eess.IV']","['Michael Neri', 'Federica Battisti']",2025-03-19,2025-03-19
2503.14998v1,TGV: Tabular Data-Guided Learning of Visual Cardiac Representations,"Contrastive learning methods in computer vision typically rely on different
views of the same image to form pairs. However, in medical imaging, we often
seek to compare entire patients with different phenotypes rather than just
multiple augmentations of one scan. We propose harnessing clinically relevant
tabular data to identify distinct patient phenotypes and form more meaningful
pairs in a contrastive learning framework. Our method uses tabular attributes
to guide the training of visual representations, without requiring a joint
embedding space. We demonstrate its strength using short-axis cardiac MR images
and clinical attributes from the UK Biobank, where tabular data helps to more
effectively distinguish between patient subgroups. Evaluation on downstream
tasks, including fine-tuning and zero-shot prediction of cardiovascular artery
diseases and cardiac phenotypes, shows that incorporating tabular data yields
stronger visual representations than conventional methods that rely solely on
image augmentations or combined image-tabular embeddings. Furthermore, we
demonstrate that image encoders trained with tabular guidance are capable of
embedding demographic information in their representations, allowing them to
use insights from tabular data for unimodal predictions, making them
well-suited to real-world medical settings where extensive clinical annotations
may not be routinely available at inference time. The code will be available on
GitHub.",['cs.CV'],"['Marta Hasny', 'Maxime Di Folco', 'Keno Bressem', 'Julia Schnabel']",2025-03-19,2025-03-19
2503.16547v1,Empowering Medical Multi-Agents with Clinical Consultation Flow for Dynamic Diagnosis,"Traditional AI-based healthcare systems often rely on single-modal data,
limiting diagnostic accuracy due to incomplete information. However, recent
advancements in foundation models show promising potential for enhancing
diagnosis combining multi-modal information. While these models excel in static
tasks, they struggle with dynamic diagnosis, failing to manage multi-turn
interactions and often making premature diagnostic decisions due to
insufficient persistence in information collection.To address this, we propose
a multi-agent framework inspired by consultation flow and reinforcement
learning (RL) to simulate the entire consultation process, integrating multiple
clinical information for effective diagnosis. Our approach incorporates a
hierarchical action set, structured from clinic consultation flow and medical
textbook, to effectively guide the decision-making process. This strategy
improves agent interactions, enabling them to adapt and optimize actions based
on the dynamic state. We evaluated our framework on a public dynamic diagnosis
benchmark. The proposed framework evidentially improves the baseline methods
and achieves state-of-the-art performance compared to existing foundation
model-based methods.","['cs.AI', 'cs.MA']","['Sihan Wang', 'Suiyang Jiang', 'Yibo Gao', 'Boming Wang', 'Shangqi Gao', 'Xiahai Zhuang']",2025-03-19,2025-03-19
2503.14996v1,"Right Answer, Wrong Score: Uncovering the Inconsistencies of LLM Evaluation in Multiple-Choice Question Answering","One of the most widely used tasks to evaluate Large Language Models (LLMs) is
Multiple-Choice Question Answering (MCQA). While open-ended question answering
tasks are more challenging to evaluate, MCQA tasks are, in principle, easier to
assess, as the model's answer is thought to be simple to extract and is
directly compared to a set of predefined choices. However, recent studies have
started to question the reliability of MCQA evaluation, showing that multiple
factors can significantly impact the reported performance of LLMs, especially
when the model generates free-form text before selecting one of the answer
choices. In this work, we shed light on the inconsistencies of MCQA evaluation
strategies, which can lead to inaccurate and misleading model comparisons. We
systematically analyze whether existing answer extraction methods are aligned
with human judgment, and how they are influenced by answer constraints in the
prompt across different domains. Our experiments demonstrate that traditional
evaluation strategies often underestimate LLM capabilities, while LLM-based
answer extractors are prone to systematic errors. Moreover, we reveal a
fundamental trade-off between including format constraints in the prompt to
simplify answer extraction and allowing models to generate free-form text to
improve reasoning. Our findings call for standardized evaluation methodologies
and highlight the need for more reliable and consistent MCQA evaluation
practices.",['cs.CL'],"['Francesco Maria Molfese', 'Luca Moroni', 'Luca Gioffrè', 'Alessandro Scirè', 'Simone Conia', 'Roberto Navigli']",2025-03-19,2025-03-19
2503.16546v1,"A Comprehensive Survey on Architectural Advances in Deep CNNs: Challenges, Applications, and Emerging Research Directions","Deep Convolutional Neural Networks (CNNs) have significantly advanced deep
learning, driving breakthroughs in computer vision, natural language
processing, medical diagnosis, object detection, and speech recognition.
Architectural innovations including 1D, 2D, and 3D convolutional models,
dilated and grouped convolutions, depthwise separable convolutions, and
attention mechanisms address domain-specific challenges and enhance feature
representation and computational efficiency. Structural refinements such as
spatial-channel exploitation, multi-path design, and feature-map enhancement
contribute to robust hierarchical feature extraction and improved
generalization, particularly through transfer learning. Efficient preprocessing
strategies, including Fourier transforms, structured transforms, low-precision
computation, and weight compression, optimize inference speed and facilitate
deployment in resource-constrained environments. This survey presents a unified
taxonomy that classifies CNN architectures based on spatial exploitation,
multi-path structures, depth, width, dimensionality expansion, channel
boosting, and attention mechanisms. It systematically reviews CNN applications
in face recognition, pose estimation, action recognition, text classification,
statistical language modeling, disease diagnosis, radiological analysis,
cryptocurrency sentiment prediction, 1D data processing, video analysis, and
speech recognition. In addition to consolidating architectural advancements,
the review highlights emerging learning paradigms such as few-shot, zero-shot,
weakly supervised, federated learning frameworks and future research directions
include hybrid CNN-transformer models, vision-language integration, generative
learning, etc. This review provides a comprehensive perspective on CNN's
evolution from 2015 to 2025, outlining key innovations, challenges, and
opportunities.","['cs.CV', 'cs.AI', 'cs.LG', 'eess.IV']","['Saddam Hussain Khan', 'Rashid Iqbal']",2025-03-19,2025-03-19
2503.14991v1,Inspecting the Representation Manifold of Differentially-Private Text,"Differential Privacy (DP) for text has recently taken the form of text
paraphrasing using language models and temperature sampling to better balance
privacy and utility. However, the geometric distortion of DP regarding the
structure and complexity in the representation space remains unexplored. By
estimating the intrinsic dimension of paraphrased text across varying privacy
budgets, we find that word-level methods severely raise the representation
manifold, while sentence-level methods produce paraphrases whose manifolds are
topologically more consistent with human-written paraphrases. Among
sentence-level methods, masked paraphrasing, compared to causal paraphrasing,
demonstrates superior preservation of structural complexity, suggesting that
autoregressive generation propagates distortions from unnatural word choices
that cascade and inflate the representation space.",['cs.CL'],['Stefan Arnold'],2025-03-19,2025-03-19
2503.14990v1,Disentangling Modes and Interference in the Spectrogram of Multicomponent Signals,"In this paper, we investigate how the spectrogram of multicomponent signals
can be decomposed into a mode part and an interference part. We explore two
approaches: (i) a variational method inspired by texture-geometry decomposition
in image processing, and (ii) a supervised learning approach using a U-Net
architecture, trained on a dataset encompassing diverse interference patterns
and noise conditions. Once the interference component is identified, we explain
how it enables us to define a criterion to locally adapt the window length used
in the definition of the spectrogram, for the sake of improving ridge detection
in the presence of close modes. Numerical experiments illustrate the advantages
and limitations of both approaches for spectrogram decomposition, highlighting
their potential for enhancing time-frequency analysis in the presence of strong
interference.","['cs.CV', 'eess.SP']","['Kévin Polisano', 'Sylvain Meignen', 'Nils Laurent', 'Hubert Leterme']",2025-03-19,2025-03-19
2503.14985v1,"ML-Triton, A Multi-Level Compilation and Language Extension to Triton GPU Programming","In the era of LLMs, dense operations such as GEMM and MHA are critical
components. These operations are well-suited for parallel execution using a
tilebased approach. While traditional GPU programming often relies on low level
interfaces like CUDA or SYCL, Triton has emerged as a DSL that offers a more
user-friendly and portable alternative by programming at a higher level. The
current Triton starts at the workgroup (aka threadblock) level, and directly
lowers to per-thread level. And then attempt to coalesce and amend through a
series of passes, promoting information from low-level representation. We
believe this is pre-mature lowering based on the below observations. 1. GPU has
a hierarchical structure both physically and logically. Modern GPUs often
feature SIMD units capable of directly operating on tiles on a warp or
warpgroup basis, such as blocked load and blocked MMA. 2. Multi-level gradual
lowering can make compiler decoupled and clean by separating considerations
inter and intra a logical layer. 3. Kernel developers often need fine control
to get good performance on the latest hardware. FlashAttention2 advocates
explicit data partition between warps to make a performance boost. In this
context, we propose ML-Triton which features multi-level compilation flow and
programming interface. Our approach begins at the workgroup level and
progressively lowers to the warp and intrinsic level, implementing a multilevel
lowering align with the hierarchical nature of GPU. Additionally, we extend
triton language to support user-set compiler hint and warp level programming,
enabling researchers to get good out-of-the box performance without awaiting
compiler updates. Experimental results demonstrate that our approach achieves
performance above 95% of expert-written kernels on Intel GPU, as measured by
the geometric mean.",['cs.CL'],"['Dewei Wang', 'Wei Zhu', 'Liyang Ling', 'Ettore Tiotto', 'Quintin Wang', 'Whitney Tsang', 'Julian Opperman', 'Jacky Deng']",2025-03-19,2025-03-19
2503.16545v1,EmpathyAgent: Can Embodied Agents Conduct Empathetic Actions?,"Empathy is fundamental to human interactions, yet it remains unclear whether
embodied agents can provide human-like empathetic support. Existing works have
studied agents' tasks solving and social interactions abilities, but whether
agents can understand empathetic needs and conduct empathetic behaviors remains
overlooked. To address this, we introduce EmpathyAgent, the first benchmark to
evaluate and enhance agents' empathetic actions across diverse scenarios.
EmpathyAgent contains 10,000 multimodal samples with corresponding empathetic
task plans and three different challenges. To systematically evaluate the
agents' empathetic actions, we propose an empathy-specific evaluation suite
that evaluates the agents' empathy process. We benchmark current models and
found that exhibiting empathetic actions remains a significant challenge.
Meanwhile, we train Llama3-8B using EmpathyAgent and find it can potentially
enhance empathetic behavior. By establishing a standard benchmark for
evaluating empathetic actions, we hope to advance research in empathetic
embodied agents. Our code and data are publicly available at
https://github.com/xinyan-cxy/EmpathyAgent.","['cs.CY', 'cs.CL']","['Xinyan Chen', 'Jiaxin Ge', 'Hongming Dai', 'Qiang Zhou', 'Qiuxuan Feng', 'Jingtong Hu', 'Yizhou Wang', 'Jiaming Liu', 'Shanghang Zhang']",2025-03-19,2025-03-19
2503.14983v1,Semi-KAN: KAN Provides an Effective Representation for Semi-Supervised Learning in Medical Image Segmentation,"Deep learning-based medical image segmentation has shown remarkable success;
however, it typically requires extensive pixel-level annotations, which are
both expensive and time-intensive. Semi-supervised medical image segmentation
(SSMIS) offers a viable alternative, driven by advancements in CNNs and ViTs.
However, these networks often rely on single fixed activation functions and
linear modeling patterns, limiting their ability to effectively learn robust
representations. Given the limited availability of labeled date, achieving
robust representation learning becomes crucial. Inspired by Kolmogorov-Arnold
Networks (KANs), we propose Semi-KAN, which leverages the untapped potential of
KANs to enhance backbone architectures for representation learning in SSMIS.
Our findings indicate that: (1) compared to networks with fixed activation
functions, KANs exhibit superior representation learning capabilities with
fewer parameters, and (2) KANs excel in high-semantic feature spaces. Building
on these insights, we integrate KANs into tokenized intermediate
representations, applying them selectively at the encoder's bottleneck and the
decoder's top layers within a U-Net pipeline to extract high-level semantic
features. Although learnable activation functions improve feature expansion,
they introduce significant computational overhead with only marginal
performance gains. To mitigate this, we reduce the feature dimensions and
employ horizontal scaling to capture multiple pattern representations.
Furthermore, we design a multi-branch U-Net architecture with uncertainty
estimation to effectively learn diverse pattern representations. Extensive
experiments on four public datasets demonstrate that Semi-KAN surpasses
baseline networks, utilizing fewer KAN layers and lower computational cost,
thereby underscoring the potential of KANs as a promising approach for SSMIS.",['cs.CV'],"['Zanting Ye', 'Xiaolong Niu', 'Xuanbin Wu', 'Wenxiang Yi', 'Yuan Chang', 'Lijun Lu']",2025-03-19,2025-03-19
2503.14980v1,Embedding spatial context in urban traffic forecasting with contrastive pre-training,"Urban traffic forecasting is a commonly encountered problem, with
wide-ranging applications in fields such as urban planning, civil engineering
and transport. In this paper, we study the enhancement of traffic forecasting
with pre-training, focusing on spatio-temporal graph methods. While various
machine learning methods to solve traffic forecasting problems have been
explored and extensively studied, there is a gap of a more contextual approach:
studying how relevant non-traffic data can improve prediction performance on
traffic forecasting problems. We call this data spatial context. We introduce a
novel method of combining road and traffic information through the notion of a
traffic quotient graph, a quotient graph formed from road geometry and traffic
sensors. We also define a way to encode this relationship in the form of a
geometric encoder, pre-trained using contrastive learning methods and enhanced
with OpenStreetMap data. We introduce and discuss ways to integrate this
geometric encoder with existing graph neural network (GNN)-based traffic
forecasting models, using a contrastive pre-training paradigm. We demonstrate
the potential for this hybrid model to improve generalisation and performance
with zero additional traffic data. Code for this paper is available at
https://github.com/mattchrlw/forecasting-on-new-roads.",['cs.LG'],"['Matthew Low', 'Arian Prabowo', 'Hao Xue', 'Flora Salim']",2025-03-19,2025-03-19
2503.14979v1,One-Shot Medical Video Object Segmentation via Temporal Contrastive Memory Networks,"Video object segmentation is crucial for the efficient analysis of complex
medical video data, yet it faces significant challenges in data availability
and annotation. We introduce the task of one-shot medical video object
segmentation, which requires separating foreground and background pixels
throughout a video given only the mask annotation of the first frame. To
address this problem, we propose a temporal contrastive memory network
comprising image and mask encoders to learn feature representations, a temporal
contrastive memory bank that aligns embeddings from adjacent frames while
pushing apart distant ones to explicitly model inter-frame relationships and
stores these features, and a decoder that fuses encoded image features and
memory readouts for segmentation. We also collect a diverse, multi-source
medical video dataset spanning various modalities and anatomies to benchmark
this task. Extensive experiments demonstrate state-of-the-art performance in
segmenting both seen and unseen structures from a single exemplar, showing
ability to generalize from scarce labels. This highlights the potential to
alleviate annotation burdens for medical video analysis. Code is available at
https://github.com/MedAITech/TCMN.",['cs.CV'],"['Yaxiong Chen', 'Junjian Hu', 'Chunlei Li', 'Zixuan Zheng', 'Jingliang Hu', 'Yilei Shi', 'Shengwu Xiong', 'Xiao Xiang Zhu', 'Lichao Mou']",2025-03-19,2025-03-19
2503.14976v2,Application of linear regression method to the deep reinforcement learning in continuous action cases,"The linear regression (LR) method offers the advantage that optimal
parameters can be calculated relatively easily, although its representation
capability is limited than that of the deep learning technique. To improve deep
reinforcement learning, the Least Squares Deep Q Network (LS-DQN) method was
proposed by Levine et al., which combines Deep Q Network (DQN) with LR method.
However, the LS-DQN method assumes that the actions are discrete. In this
study, we propose the Double Least Squares Deep Deterministic Policy Gradient
(DLS-DDPG) method to address this limitation. This method combines the LR
method with the Deep Deterministic Policy Gradient (DDPG) technique, one of the
representative deep reinforcement learning algorithms for continuous action
cases. Numerical experiments conducted in MuJoCo environments showed that the
LR update improved performance at least in some tasks, although there are
difficulties such as the inability to make the regularization terms small.","['cs.LG', 'cs.AI']",['Hisato Komatsu'],2025-03-19,2025-03-21
2503.14975v1,Taming Flow Matching with Unbalanced Optimal Transport into Fast Pansharpening,"Pansharpening, a pivotal task in remote sensing for fusing high-resolution
panchromatic and multispectral imagery, has garnered significant research
interest. Recent advancements employing diffusion models based on stochastic
differential equations (SDEs) have demonstrated state-of-the-art performance.
However, the inherent multi-step sampling process of SDEs imposes substantial
computational overhead, hindering practical deployment. While existing methods
adopt efficient samplers, knowledge distillation, or retraining to reduce
sampling steps (e.g., from 1,000 to fewer steps), such approaches often
compromise fusion quality. In this work, we propose the Optimal Transport Flow
Matching (OTFM) framework, which integrates the dual formulation of unbalanced
optimal transport (UOT) to achieve one-step, high-quality pansharpening. Unlike
conventional OT formulations that enforce rigid distribution alignment, UOT
relaxes marginal constraints to enhance modeling flexibility, accommodating the
intrinsic spectral and spatial disparities in remote sensing data. Furthermore,
we incorporate task-specific regularization into the UOT objective, enhancing
the robustness of the flow model. The OTFM framework enables simulation-free
training and single-step inference while maintaining strict adherence to
pansharpening constraints. Experimental evaluations across multiple datasets
demonstrate that OTFM matches or exceeds the performance of previous
regression-based models and leading diffusion-based methods while only needing
one sampling step. Codes are available at https://github.com/294coder/PAN-OTFM.",['cs.CV'],"['Zihan Cao', 'Yu Zhong', 'Liang-Jian Deng']",2025-03-19,2025-03-19
2503.14974v1,Language-based Image Colorization: A Benchmark and Beyond,"Image colorization aims to bring colors back to grayscale images. Automatic
image colorization methods, which requires no additional guidance, struggle to
generate high-quality images due to color ambiguity, and provides limited user
controllability. Thanks to the emergency of cross-modality datasets and models,
language-based colorization methods are proposed to fully utilize the
efficiency and flexibly of text descriptions to guide colorization. In view of
the lack of a comprehensive review of language-based colorization literature,
we conduct a thorough analysis and benchmarking. We first briefly summarize
existing automatic colorization methods. Then, we focus on language-based
methods and point out their core challenge on cross-modal alignment. We further
divide these methods into two categories: one attempts to train a
cross-modality network from scratch, while the other utilizes the pre-trained
cross-modality model to establish the textual-visual correspondence. Based on
the analyzed limitations of existing language-based methods, we propose a
simple yet effective method based on distilled diffusion model. Extensive
experiments demonstrate that our simple baseline can produces better results
than previous complex methods with 14 times speed up. To the best of our
knowledge, this is the first comprehensive review and benchmark on
language-based image colorization field, providing meaningful insights for the
community. The code is available at https://github.com/lyf1212/Color-Turbo.",['cs.CV'],"['Yifan Li', 'Shuai Yang', 'Jiaying Liu']",2025-03-19,2025-03-19
2503.14973v1,Behaviour Discovery and Attribution for Explainable Reinforcement Learning,"Explaining the decisions made by reinforcement learning (RL) agents is
critical for building trust and ensuring reliability in real-world
applications. Traditional approaches to explainability often rely on saliency
analysis, which can be limited in providing actionable insights. Recently,
there has been growing interest in attributing RL decisions to specific
trajectories within a dataset. However, these methods often generalize
explanations to long trajectories, potentially involving multiple distinct
behaviors. Often, providing multiple more fine grained explanations would
improve clarity. In this work, we propose a framework for behavior discovery
and action attribution to behaviors in offline RL trajectories. Our method
identifies meaningful behavioral segments, enabling more precise and granular
explanations associated with high level agent behaviors. This approach is
adaptable across diverse environments with minimal modifications, offering a
scalable and versatile solution for behavior discovery and attribution for
explainable RL.",['cs.AI'],"['Rishav Rishav', 'Somjit Nath', 'Vincent Michalski', 'Samira Ebrahimi Kahou']",2025-03-19,2025-03-19
2503.14966v1,Ultrasound Image-to-Video Synthesis via Latent Dynamic Diffusion Models,"Ultrasound video classification enables automated diagnosis and has emerged
as an important research area. However, publicly available ultrasound video
datasets remain scarce, hindering progress in developing effective video
classification models. We propose addressing this shortage by synthesizing
plausible ultrasound videos from readily available, abundant ultrasound images.
To this end, we introduce a latent dynamic diffusion model (LDDM) to
efficiently translate static images to dynamic sequences with realistic video
characteristics. We demonstrate strong quantitative results and visually
appealing synthesized videos on the BUSV benchmark. Notably, training video
classification models on combinations of real and LDDM-synthesized videos
substantially improves performance over using real data alone, indicating our
method successfully emulates dynamics critical for discrimination. Our
image-to-video approach provides an effective data augmentation solution to
advance ultrasound video analysis. Code is available at
https://github.com/MedAITech/U_I2V.","['cs.CV', 'eess.IV']","['Tingxiu Chen', 'Yilei Shi', 'Zixuan Zheng', 'Bingcong Yan', 'Jingliang Hu', 'Xiao Xiang Zhu', 'Lichao Mou']",2025-03-19,2025-03-19
2503.14963v1,Continual Multimodal Contrastive Learning,"Multimodal contrastive learning (MCL) advances in aligning different
modalities and generating multimodal representations in a joint space. By
leveraging contrastive learning across diverse modalities, large-scale
multimodal data enhances representational quality. However, a critical yet
often overlooked challenge remains: multimodal data is rarely collected in a
single process, and training from scratch is computationally expensive.
Instead, emergent multimodal data can be used to optimize existing models
gradually, \textit{i.e.}, models are trained on a sequence of modality pair
data. We define this problem as Continual Multimodal Contrastive Learning
(CMCL), an underexplored yet crucial research direction at the intersection of
multimodal and continual learning. In this paper, we formulate CMCL through two
specialized principles of stability and plasticity. We theoretically derive a
novel optimization-based method, which projects updated gradients from dual
sides onto subspaces where any gradient is prevented from interfering with the
previously learned knowledge. Two upper bounds provide theoretical insights on
both stability and plasticity in our solution. Beyond our theoretical
contributions, we conduct experiments on multiple datasets by comparing our
method against advanced continual learning baselines. The empirical results
further support our claims and demonstrate the efficacy of our method. The code
will be publicly available.",['cs.LG'],"['Xiaohao Liu', 'Xiaobo Xia', 'See-Kiong Ng', 'Tat-Seng Chua']",2025-03-19,2025-03-19
2503.14960v1,Body-Hand Modality Expertized Networks with Cross-attention for Fine-grained Skeleton Action Recognition,"Skeleton-based Human Action Recognition (HAR) is a vital technology in
robotics and human-robot interaction. However, most existing methods
concentrate primarily on full-body movements and often overlook subtle hand
motions that are critical for distinguishing fine-grained actions. Recent work
leverages a unified graph representation that combines body, hand, and foot
keypoints to capture detailed body dynamics. Yet, these models often blur fine
hand details due to the disparity between body and hand action characteristics
and the loss of subtle features during the spatial-pooling. In this paper, we
propose BHaRNet (Body-Hand action Recognition Network), a novel framework that
augments a typical body-expert model with a hand-expert model. Our model
jointly trains both streams with an ensemble loss that fosters cooperative
specialization, functioning in a manner reminiscent of a Mixture-of-Experts
(MoE). Moreover, cross-attention is employed via an expertized branch method
and a pooling-attention module to enable feature-level interactions and
selectively fuse complementary information. Inspired by MMNet, we also
demonstrate the applicability of our approach to multi-modal tasks by
leveraging RGB information, where body features guide RGB learning to capture
richer contextual cues. Experiments on large-scale benchmarks (NTU RGB+D 60,
NTU RGB+D 120, PKU-MMD, and Northwestern-UCLA) demonstrate that BHaRNet
achieves SOTA accuracies -- improving from 86.4\% to 93.0\% in hand-intensive
actions -- while maintaining fewer GFLOPs and parameters than the relevant
unified methods.",['cs.CV'],"['Seungyeon Cho', 'Tae-Kyun Kim']",2025-03-19,2025-03-19
2503.14958v1,Reducing Annotation Burden: Exploiting Image Knowledge for Few-Shot Medical Video Object Segmentation via Spatiotemporal Consistency Relearning,"Few-shot video object segmentation aims to reduce annotation costs; however,
existing methods still require abundant dense frame annotations for training,
which are scarce in the medical domain. We investigate an extremely low-data
regime that utilizes annotations from only a few video frames and leverages
existing labeled images to minimize costly video annotations. Specifically, we
propose a two-phase framework. First, we learn a few-shot segmentation model
using labeled images. Subsequently, to improve performance without full
supervision, we introduce a spatiotemporal consistency relearning approach on
medical videos that enforces consistency between consecutive frames.
Constraints are also enforced between the image model and relearning model at
both feature and prediction levels. Experiments demonstrate the superiority of
our approach over state-of-the-art few-shot segmentation methods. Our model
bridges the gap between abundant annotated medical images and scarce, sparsely
labeled medical videos to achieve strong video segmentation performance in this
low data regime. Code is available at https://github.com/MedAITech/RAB.",['cs.CV'],"['Zixuan Zheng', 'Yilei Shi', 'Chunlei Li', 'Jingliang Hu', 'Xiao Xiang Zhu', 'Lichao Mou']",2025-03-19,2025-03-19
2503.14957v1,Neuro Symbolic Knowledge Reasoning for Procedural Video Question Answering,"This paper introduces a new video question-answering (VQA) dataset that
challenges models to leverage procedural knowledge for complex reasoning. It
requires recognizing visual entities, generating hypotheses, and performing
contextual, causal, and counterfactual reasoning. To address this, we propose
neuro symbolic reasoning module that integrates neural networks and LLM-driven
constrained reasoning over variables for interpretable answer generation.
Results show that combining LLMs with structured knowledge reasoning with logic
enhances procedural reasoning on the STAR benchmark and our dataset. Code and
dataset at https://github.com/LUNAProject22/KML soon.",['cs.CV'],"['Thanh-Son Nguyen', 'Hong Yang', 'Tzeh Yuan Neoh', 'Hao Zhang', 'Ee Yeo Keat', 'Basura Fernando']",2025-03-19,2025-03-19
2503.14955v1,Depth-Aware Range Image-Based Model for Point Cloud Segmentation,"Point cloud segmentation (PCS) aims to separate points into different and
meaningful groups. The task plays an important role in robotics because PCS
enables robots to understand their physical environments directly. To process
sparse and large-scale outdoor point clouds in real time, range image-based
models are commonly adopted. However, in a range image, the lack of explicit
depth information inevitably causes some separate objects in 3D space to touch
each other, bringing difficulty for the range image-based models in correctly
segmenting the objects. Moreover, previous PCS models are usually derived from
the existing color image-based models and unable to make full use of the
implicit but ordered depth information inherent in the range image, thereby
achieving inferior performance. In this paper, we propose Depth-Aware Module
(DAM) and Fast FMVNet V3. DAM perceives the ordered depth information in the
range image by explicitly modelling the interdependence among channels. Fast
FMVNet V3 incorporates DAM by integrating it into the last block in each
architecture stage. Extensive experiments conducted on SemanticKITTI, nuScenes,
and SemanticPOSS demonstrate that DAM brings a significant improvement for Fast
FMVNet V3 with negligible computational cost.",['cs.CV'],"['Bike Chen', 'Antti Tikanmäki', 'Juha Röning']",2025-03-19,2025-03-19
2503.14953v1,Aligning Information Capacity Between Vision and Language via Dense-to-Sparse Feature Distillation for Image-Text Matching,"Enabling Visual Semantic Models to effectively handle multi-view description
matching has been a longstanding challenge. Existing methods typically learn a
set of embeddings to find the optimal match for each view's text and compute
similarity. However, the visual and text embeddings learned through these
approaches have limited information capacity and are prone to interference from
locally similar negative samples. To address this issue, we argue that the
information capacity of embeddings is crucial and propose Dense-to-Sparse
Feature Distilled Visual Semantic Embedding (D2S-VSE), which enhances the
information capacity of sparse text by leveraging dense text distillation.
Specifically, D2S-VSE is a two-stage framework. In the pre-training stage, we
align images with dense text to enhance the information capacity of visual
semantic embeddings. In the fine-tuning stage, we optimize two tasks
simultaneously, distilling dense text embeddings to sparse text embeddings
while aligning images and sparse texts, enhancing the information capacity of
sparse text embeddings. Our proposed D2S-VSE model is extensively evaluated on
the large-scale MS-COCO and Flickr30K datasets, demonstrating its superiority
over recent state-of-the-art methods.",['cs.CV'],"['Yang Liu', 'Wentao Feng', 'Zhuoyao Liu', 'Shudong Huang', 'Jiancheng Lv']",2025-03-19,2025-03-19
2503.14950v1,USAM-Net: A U-Net-based Network for Improved Stereo Correspondence and Scene Depth Estimation using Features from a Pre-trained Image Segmentation network,"The increasing demand for high-accuracy depth estimation in autonomous
driving and augmented reality applications necessitates advanced neural
architectures capable of effectively leveraging multiple data modalities. In
this context, we introduce the Unified Segmentation Attention Mechanism Network
(USAM-Net), a novel convolutional neural network that integrates stereo image
inputs with semantic segmentation maps and attention to enhance depth
estimation performance. USAM-Net employs a dual-pathway architecture, which
combines a pre-trained segmentation model (SAM) and a depth estimation model.
The segmentation pathway preprocesses the stereo images to generate semantic
masks, which are then concatenated with the stereo images as inputs to the
depth estimation pathway. This integration allows the model to focus on
important features such as object boundaries and surface textures which are
crucial for accurate depth perception. Empirical evaluation on the
DrivingStereo dataset demonstrates that USAM-Net achieves superior performance
metrics, including a Global Difference (GD) of 3.61\% and an End-Point Error
(EPE) of 0.88, outperforming traditional models such as CFNet, SegStereo, and
iResNet. These results underscore the effectiveness of integrating segmentation
information into stereo depth estimation tasks, highlighting the potential of
USAM-Net in applications demanding high-precision depth data.","['cs.CV', 'cs.AI']","['Joseph Emmanuel DL Dayo', 'Prospero C. Naval Jr']",2025-03-19,2025-03-19
2503.14948v1,ChatStitch: Visualizing Through Structures via Surround-View Unsupervised Deep Image Stitching with Collaborative LLM-Agents,"Collaborative perception has garnered significant attention for its ability
to enhance the perception capabilities of individual vehicles through the
exchange of information with surrounding vehicle-agents. However, existing
collaborative perception systems are limited by inefficiencies in user
interaction and the challenge of multi-camera photorealistic visualization. To
address these challenges, this paper introduces ChatStitch, the first
collaborative perception system capable of unveiling obscured blind spot
information through natural language commands integrated with external digital
assets. To adeptly handle complex or abstract commands, ChatStitch employs a
multi-agent collaborative framework based on Large Language Models. For
achieving the most intuitive perception for humans, ChatStitch proposes
SV-UDIS, the first surround-view unsupervised deep image stitching method under
the non-global-overlapping condition. We conducted extensive experiments on the
UDIS-D, MCOV-SLAM open datasets, and our real-world dataset. Specifically, our
SV-UDIS method achieves state-of-the-art performance on the UDIS-D dataset for
3, 4, and 5 image stitching tasks, with PSNR improvements of 9%, 17%, and 21%,
and SSIM improvements of 8%, 18%, and 26%, respectively.","['cs.CV', 'cs.HC']","['Hao Liang', 'Zhipeng Dong', 'Yi Yang', 'Mengyin Fu']",2025-03-19,2025-03-19
2503.14945v1,Generating Multimodal Driving Scenes via Next-Scene Prediction,"Generative models in Autonomous Driving (AD) enable diverse scene creation,
yet existing methods fall short by only capturing a limited range of
modalities, restricting the capability of generating controllable scenes for
comprehensive evaluation of AD systems. In this paper, we introduce a
multimodal generation framework that incorporates four major data modalities,
including a novel addition of map modality. With tokenized modalities, our
scene sequence generation framework autoregressively predicts each scene while
managing computational demands through a two-stage approach. The Temporal
AutoRegressive (TAR) component captures inter-frame dynamics for each modality
while the Ordered AutoRegressive (OAR) component aligns modalities within each
scene by sequentially predicting tokens in a fixed order. To maintain coherence
between map and ego-action modalities, we introduce the Action-aware Map
Alignment (AMA) module, which applies a transformation based on the ego-action
to maintain coherence between these modalities. Our framework effectively
generates complex, realistic driving scenes over extended sequences, ensuring
multimodal consistency and offering fine-grained control over scene elements.",['cs.CV'],"['Yanhao Wu', 'Haoyang Zhang', 'Tianwei Lin', 'Lichao Huang', 'Shujie Luo', 'Rui Wu', 'Congpei Qiu', 'Wei Ke', 'Tong Zhang']",2025-03-19,2025-03-19
2503.14944v1,MMAIF: Multi-task and Multi-degradation All-in-One for Image Fusion with Language Guidance,"Image fusion, a fundamental low-level vision task, aims to integrate multiple
image sequences into a single output while preserving as much information as
possible from the input. However, existing methods face several significant
limitations: 1) requiring task- or dataset-specific models; 2) neglecting
real-world image degradations (\textit{e.g.}, noise), which causes failure when
processing degraded inputs; 3) operating in pixel space, where attention
mechanisms are computationally expensive; and 4) lacking user interaction
capabilities. To address these challenges, we propose a unified framework for
multi-task, multi-degradation, and language-guided image fusion. Our framework
includes two key components: 1) a practical degradation pipeline that simulates
real-world image degradations and generates interactive prompts to guide the
model; 2) an all-in-one Diffusion Transformer (DiT) operating in latent space,
which fuses a clean image conditioned on both the degraded inputs and the
generated prompts. Furthermore, we introduce principled modifications to the
original DiT architecture to better suit the fusion task. Based on this
framework, we develop two versions of the model: Regression-based and Flow
Matching-based variants. Extensive qualitative and quantitative experiments
demonstrate that our approach effectively addresses the aforementioned
limitations and outperforms previous restoration+fusion and all-in-one
pipelines. Codes are available at https://github.com/294coder/MMAIF.",['cs.CV'],"['Zihan Cao', 'Yu Zhong', 'Ziqi Wang', 'Liang-Jian Deng']",2025-03-19,2025-03-19
2503.14943v1,3D Engine-ready Photorealistic Avatars via Dynamic Textures,"As the digital and physical worlds become more intertwined, there has been a
lot of interest in digital avatars that closely resemble their real-world
counterparts. Current digitization methods used in 3D production pipelines
require costly capture setups, making them impractical for mass usage among
common consumers. Recent academic literature has found success in
reconstructing humans from limited data using implicit representations (e.g.,
voxels used in NeRFs), which are able to produce impressive videos. However,
these methods are incompatible with traditional rendering pipelines, making it
difficult to use them in applications such as games. In this work, we propose
an end-to-end pipeline that builds explicitly-represented photorealistic 3D
avatars using standard 3D assets. Our key idea is the use of
dynamically-generated textures to enhance the realism and visually mask
deficiencies in the underlying mesh geometry. This allows for seamless
integration with current graphics pipelines while achieving comparable visual
quality to state-of-the-art 3D avatar generation methods.",['cs.CV'],"['Yifan Wang', 'Ivan Molodetskikh', 'Ondrej Texler', 'Dimitar Dinev']",2025-03-19,2025-03-19
2503.14941v1,UPME: An Unsupervised Peer Review Framework for Multimodal Large Language Model Evaluation,"Multimodal Large Language Models (MLLMs) have emerged to tackle the
challenges of Visual Question Answering (VQA), sparking a new research focus on
conducting objective evaluations of these models. Existing evaluation methods
face limitations due to the significant human workload required to design Q&A
pairs for visual images, which inherently restricts the scale and scope of
evaluations. Although automated MLLM-as-judge approaches attempt to reduce the
human workload through automatic evaluations, they often introduce biases. To
address these problems, we propose an Unsupervised Peer review MLLM Evaluation
framework. It utilizes only image data, allowing models to automatically
generate questions and conduct peer review assessments of answers from other
models, effectively alleviating the reliance on human workload. Additionally,
we introduce the vision-language scoring system to mitigate the bias issues,
which focuses on three aspects: (i) response correctness; (ii) visual
understanding and reasoning; and (iii) image-text correlation. Experimental
results demonstrate that UPME achieves a Pearson correlation of 0.944 with
human evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset,
indicating that our framework closely aligns with human-designed benchmarks and
inherent human preferences.",['cs.CV'],"['Qihui Zhang', 'Munan Ning', 'Zheyuan Liu', 'Yanbo Wang', 'Jiayi Ye', 'Yue Huang', 'Shuo Yang', 'Xiao Chen', 'Yibing Song', 'Li Yuan']",2025-03-19,2025-03-19
2503.14939v1,VisNumBench: Evaluating Number Sense of Multimodal Large Language Models,"Can Multimodal Large Language Models (MLLMs) develop an intuitive number
sense similar to humans? Targeting this problem, we introduce Visual Number
Benchmark (VisNumBench) to evaluate the number sense abilities of MLLMs across
a wide range of visual numerical tasks. VisNumBench consists of about 1,900
multiple-choice question-answer pairs derived from both synthetic and
real-world visual data, covering seven visual numerical attributes and four
types of visual numerical estimation tasks. Our experiments on VisNumBench led
to the following key findings: (i) The 17 MLLMs we tested, including
open-source models such as Qwen2.5-VL and InternVL2.5, as well as proprietary
models like GPT-4o and Gemini 2.0 Flash, perform significantly below human
levels in number sense-related tasks. (ii) Multimodal mathematical models and
multimodal chain-of-thought (CoT) models did not exhibit significant
improvements in number sense abilities. (iii) Stronger MLLMs with larger
parameter sizes and broader general abilities demonstrate modest gains in
number sense abilities. We believe VisNumBench will serve as a valuable
resource for the research community, encouraging further advancements in
enhancing MLLMs' number sense abilities. All benchmark resources, including
code and datasets, will be publicly available at
https://wwwtttjjj.github.io/VisNumBench/.",['cs.CV'],"['Tengjin Weng', 'Jingyi Wang', 'Wenhao Jiang', 'Zhong Ming']",2025-03-19,2025-03-19
2503.14938v1,Optimal Transport Adapter Tuning for Bridging Modality Gaps in Few-Shot Remote Sensing Scene Classification,"Few-Shot Remote Sensing Scene Classification (FS-RSSC) presents the challenge
of classifying remote sensing images with limited labeled samples. Existing
methods typically emphasize single-modal feature learning, neglecting the
potential benefits of optimizing multi-modal representations. To address this
limitation, we propose a novel Optimal Transport Adapter Tuning (OTAT)
framework aimed at constructing an ideal Platonic representational space
through optimal transport (OT) theory. This framework seeks to harmonize rich
visual information with less dense textual cues, enabling effective cross-modal
information transfer and complementarity. Central to this approach is the
Optimal Transport Adapter (OTA), which employs a cross-modal attention
mechanism to enrich textual representations and facilitate subsequent better
information interaction. By transforming the network optimization into an OT
optimization problem, OTA establishes efficient pathways for balanced
information exchange between modalities. Moreover, we introduce a sample-level
Entropy-Aware Weighted (EAW) loss, which combines difficulty-weighted
similarity scores with entropy-based regularization. This loss function
provides finer control over the OT optimization process, enhancing its
solvability and stability. Our framework offers a scalable and efficient
solution for advancing multimodal learning in remote sensing applications.
Extensive experiments on benchmark datasets demonstrate that OTAT achieves
state-of-the-art performance in FS-RSSC, significantly improving the model
performance and generalization.",['cs.CV'],"['Zhong Ji', 'Ci Liu', 'Jingren Liu', 'Chen Tang', 'Yanwei Pang', 'Xuelong Li']",2025-03-19,2025-03-19
2503.14937v1,Proceedings of the 3rd Italian Conference on Big Data and Data Science (ITADATA2024),"Proceedings of the 3rd Italian Conference on Big Data and Data Science
(ITADATA2024), held in Pisa, Italy, September 17-19, 2024.
  The Italian Conference on Big Data and Data Science (ITADATA2024) is the
annual event supported by the CINI Big Data National Laboratory and ISTI CNR
that aims to put together Italian researchers and professionals from academia,
industry, government, and public administration working in the field of big
data and data science, as well as related fields (e.g., security and privacy,
HPC, Cloud).
  ITADATA2024 covered research on all theoretical and practical aspects of Big
Data and data science including data governance, data processing, data
analysis, data reporting, data protection, as well as experimental studies and
lessons learned. In particular, ITADATA2024 focused on
  - Data spaces
  - Data processing life cycle
  - Machine learning and Large Language Models
  - Applications of big data and data science in healthcare, finance, industry
5.0, and beyond
  - Data science for social network analysis","['cs.DB', 'cs.LG']","['Nicola Bena', 'Claudia Diamantini', 'Michela Natilli', 'Luigi Romano', 'Giovanni Stilo', 'Valentina Pansanella', 'Claudio A. Ardagna', 'Anna Monreale', 'Roberto Trasarti']",2025-03-19,2025-03-19
2503.14936v1,Enhancing Code LLM Training with Programmer Attention,"Human attention provides valuable yet underexploited signals for code LLM
training, offering a perspective beyond purely machine-driven attention.
Despite the complexity and cost of collecting eye-tracking data, there has also
been limited progress in systematically using these signals for code LLM
training. To address both issues, we propose a cohesive pipeline spanning
augmentation and reward-based fine-tuning. Specifically, we introduce (1) an
eye-tracking path augmentation method to expand programmer attention datasets,
(2) a pattern abstraction step that refines raw fixations into learnable
attention motifs, and (3) a reward-guided strategy for integrating these
insights directly into a CodeT5 supervised fine-tuning process. Our experiments
yield +7.16 in CodeBLEU on the CodeXGlue benchmark for code summarization,
underscoring how uniting human and machine attention can boost code
intelligence. We hope this work encourages broader exploration of human-centric
methods in next-generation AI4SE.","['cs.SE', 'cs.HC', 'cs.LG']","['Yifan Zhang', 'Chen Huang', 'Zachary Karas', 'Dung Thuy Nguyen', 'Kevin Leach', 'Yu Huang']",2025-03-19,2025-03-19
2503.14935v1,FAVOR-Bench: A Comprehensive Benchmark for Fine-Grained Video Motion Understanding,"Multimodal Large Language Models (MLLMs) have shown remarkable capabilities
in video content understanding but still struggle with fine-grained motion
comprehension. To comprehensively assess the motion understanding ability of
existing MLLMs, we introduce FAVOR-Bench, comprising 1,776 videos with
structured manual annotations of various motions. Our benchmark includes both
close-ended and open-ended tasks. For close-ended evaluation, we carefully
design 8,184 multiple-choice question-answer pairs spanning six distinct
sub-tasks. For open-ended evaluation, we develop both a novel cost-efficient
LLM-free and a GPT-assisted caption assessment method, where the former can
enhance benchmarking interpretability and reproducibility. Comprehensive
experiments with 21 state-of-the-art MLLMs reveal significant limitations in
their ability to comprehend and describe detailed temporal dynamics in video
motions. To alleviate this limitation, we further build FAVOR-Train, a dataset
consisting of 17,152 videos with fine-grained motion annotations. The results
of finetuning Qwen2.5-VL on FAVOR-Train yield consistent improvements on
motion-related tasks of TVBench, MotionBench and our FAVOR-Bench. Comprehensive
assessment results demonstrate that the proposed FAVOR-Bench and FAVOR-Train
provide valuable tools to the community for developing more powerful video
understanding models. Project page:
\href{https://favor-bench.github.io/}{https://favor-bench.github.io/}.","['cs.CV', 'cs.AI']","['Chongjun Tu', 'Lin Zhang', 'Pengtao Chen', 'Peng Ye', 'Xianfang Zeng', 'Wei Cheng', 'Gang Yu', 'Tao Chen']",2025-03-19,2025-03-19
2503.14933v1,A Language Vision Model Approach for Automated Tumor Contouring in Radiation Oncology,"Background: Lung cancer ranks as the leading cause of cancer-related
mortality worldwide. The complexity of tumor delineation, crucial for radiation
therapy, requires expertise often unavailable in resource-limited settings.
Artificial Intelligence(AI), particularly with advancements in deep learning
(DL) and natural language processing (NLP), offers potential solutions yet is
challenged by high false positive rates. Purpose: The Oncology Contouring
Copilot (OCC) system is developed to leverage oncologist expertise for precise
tumor contouring using textual descriptions, aiming to increase the efficiency
of oncological workflows by combining the strengths of AI with human oversight.
Methods: Our OCC system initially identifies nodule candidates from CT scans.
Employing Language Vision Models (LVMs) like GPT-4V, OCC then effectively
reduces false positives with clinical descriptive texts, merging textual and
visual data to automate tumor delineation, designed to elevate the quality of
oncology care by incorporating knowledge from experienced domain experts.
Results: Deployments of the OCC system resulted in a significant reduction in
the false discovery rate by 35.0%, a 72.4% decrease in false positives per
scan, and an F1-score of 0.652 across our dataset for unbiased evaluation.
Conclusions: OCC represents a significant advance in oncology care,
particularly through the use of the latest LVMs to improve contouring results
by (1) streamlining oncology treatment workflows by optimizing tumor
delineation, reducing manual processes; (2) offering a scalable and intuitive
framework to reduce false positives in radiotherapy planning using LVMs; (3)
introducing novel medical language vision prompt techniques to minimize LVMs
hallucinations with ablation study, and (4) conducting a comparative analysis
of LVMs, highlighting their potential in addressing medical language vision
challenges.","['eess.IV', 'cs.CV', 'physics.med-ph']","['Yi Luo', 'Hamed Hooshangnejad', 'Xue Feng', 'Gaofeng Huang', 'Xiaojian Chen', 'Rui Zhang', 'Quan Chen', 'Wil Ngwa', 'Kai Ding']",2025-03-19,2025-03-19
2503.14932v1,Prada: Black-Box LLM Adaptation with Private Data on Resource-Constrained Devices,"In recent years, Large Language Models (LLMs) have demonstrated remarkable
abilities in various natural language processing tasks. However, adapting these
models to specialized domains using private datasets stored on
resource-constrained edge devices, such as smartphones and personal computers,
remains challenging due to significant privacy concerns and limited
computational resources. Existing model adaptation methods either compromise
data privacy by requiring data transmission or jeopardize model privacy by
exposing proprietary LLM parameters. To address these challenges, we propose
Prada, a novel privacy-preserving and efficient black-box LLM adaptation system
using private on-device datasets. Prada employs a lightweight proxy model
fine-tuned with Low-Rank Adaptation (LoRA) locally on user devices. During
inference, Prada leverages the logits offset, i.e., difference in outputs
between the base and adapted proxy models, to iteratively refine outputs from a
remote black-box LLM. This offset-based adaptation approach preserves both data
privacy and model privacy, as there is no need to share sensitive data or
proprietary model parameters. Furthermore, we incorporate speculative decoding
to further speed up the inference process of Prada, making the system
practically deployable on bandwidth-constrained edge devices, enabling a more
practical deployment of Prada. Extensive experiments on various downstream
tasks demonstrate that Prada achieves performance comparable to centralized
fine-tuning methods while significantly reducing computational overhead by up
to 60% and communication costs by up to 80%.","['cs.CR', 'cs.DC', 'cs.LG']","['Ziyao Wang', 'Yexiao He', 'Zheyu Shen', 'Yu Li', 'Guoheng Sun', 'Myungjin Lee', 'Ang Li']",2025-03-19,2025-03-19
2503.15566v1,Enforcing Consistency and Fairness in Multi-level Hierarchical Classification with a Mask-based Output Layer,"Traditional Multi-level Hierarchical Classification (MLHC) classifiers often
rely on backbone models with $n$ independent output layers. This structure
tends to overlook the hierarchical relationships between classes, leading to
inconsistent predictions that violate the underlying taxonomy. Additionally,
once a backbone architecture for an MLHC classifier is selected, adapting the
model to accommodate new tasks can be challenging. For example, incorporating
fairness to protect sensitive attributes within a hierarchical classifier
necessitates complex adjustments to maintain the class hierarchy while
enforcing fairness constraints. In this paper, we extend this concept to
hierarchical classification by introducing a fair, model-agnostic layer
designed to enforce taxonomy and optimize specific objectives, including
consistency, fairness, and exact match. Our evaluations demonstrate that the
proposed layer not only improves the fairness of predictions but also enforces
the taxonomy, resulting in consistent predictions and superior performance.
Compared to Large Language Models (LLMs) employing in-processing de-biasing
techniques and models without any bias correction, our approach achieves better
outcomes in both fairness and accuracy, making it particularly valuable in
sectors like e-commerce, healthcare, and education, where predictive
reliability is crucial.",['cs.LG'],"['Shijing Chen', 'Shoaib Jameel', 'Mohamed Reda Bouadjenek', 'Feilong Tang', 'Usman Naseem', 'Basem Suleiman', 'Hakim Hacid', 'Flora D. Salim', 'Imran Razzak']",2025-03-19,2025-03-19
2503.14929v1,ACE: A Cardinality Estimator for Set-Valued Queries,"Cardinality estimation is a fundamental functionality in database systems.
Most existing cardinality estimators focus on handling predicates over numeric
or categorical data. They have largely omitted an important data type,
set-valued data, which frequently occur in contemporary applications such as
information retrieval and recommender systems. The few existing estimators for
such data either favor high-frequency elements or rely on a partial
independence assumption, which limits their practical applicability. We propose
ACE, an Attention-based Cardinality Estimator for estimating the cardinality of
queries over set-valued data. We first design a distillation-based data encoder
to condense the dataset into a compact matrix. We then design an
attention-based query analyzer to capture correlations among query elements. To
handle variable-sized queries, a pooling module is introduced, followed by a
regression model (MLP) to generate final cardinality estimates. We evaluate ACE
on three datasets with varying query element distributions, demonstrating that
ACE outperforms the state-of-the-art competitors in terms of both accuracy and
efficiency.","['cs.DB', 'cs.LG']","['Yufan Sheng', 'Xin Cao', 'Kaiqi Zhao', 'Yixiang Fang', 'Jianzhong Qi', 'Wenjie Zhang', 'Christian S. Jensen']",2025-03-19,2025-03-19
2503.14928v1,Shushing! Let's Imagine an Authentic Speech from the Silent Video,"Vision-guided speech generation aims to produce authentic speech from facial
appearance or lip motions without relying on auditory signals, offering
significant potential for applications such as dubbing in filmmaking and
assisting individuals with aphonia. Despite recent progress, existing methods
struggle to achieve unified cross-modal alignment across semantics, timbre, and
emotional prosody from visual cues, prompting us to propose Consistent
Video-to-Speech (CV2S) as an extended task to enhance cross-modal consistency.
To tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal
diffusion framework that generates faithful speech using only visual input,
operating within a discrete space. Specifically, we propose a discrete lip
aligner that predicts discrete speech tokens from lip videos to capture
semantic information, while an error detector identifies misaligned tokens,
which are subsequently refined through masked language modeling with BERT. To
further enhance the expressiveness of the generated speech, we develop a style
diffusion transformer equipped with a face-style adapter that adaptively
customizes identity and prosody dynamics across both the channel and temporal
dimensions while ensuring synchronization with lip-aware semantic features.
Extensive experiments demonstrate that ImaginTalk can generate high-fidelity
speech with more accurate semantic details and greater expressiveness in timbre
and emotion compared to state-of-the-art baselines. Demos are shown at our
project page: https://imagintalk.github.io.","['cs.CV', 'cs.AI', 'cs.SD', 'eess.AS']","['Jiaxin Ye', 'Hongming Shan']",2025-03-19,2025-03-19
2503.14927v1,Semi-Gradient SARSA Routing with Theoretical Guarantee on Traffic Stability and Weight Convergence,"We consider the traffic control problem of dynamic routing over parallel
servers, which arises in a variety of engineering systems such as
transportation and data transmission. We propose a semi-gradient, on-policy
algorithm that learns an approximate optimal routing policy. The algorithm uses
generic basis functions with flexible weights to approximate the value function
across the unbounded state space. Consequently, the training process lacks
Lipschitz continuity of the gradient, boundedness of the temporal-difference
error, and a prior guarantee on ergodicity, which are the standard
prerequisites in existing literature on reinforcement learning theory. To
address this, we combine a Lyapunov approach and an ordinary differential
equation-based method to jointly characterize the behavior of traffic state and
approximation weights. Our theoretical analysis proves that the training scheme
guarantees traffic state stability and ensures almost surely convergence of the
weights to the approximate optimum. We also demonstrate via simulations that
our algorithm attains significantly faster convergence than neural
network-based methods with an insignificant approximation error.","['cs.LG', 'cs.SY', 'eess.SY', 'math.DS']","['Yidan Wu', 'Yu Yu', 'Jianan Zhang', 'Li Jin']",2025-03-19,2025-03-19
2503.14926v1,Covering Cracks in Content Moderation: Delexicalized Distant Supervision for Illicit Drug Jargon Detection,"In light of rising drug-related concerns and the increasing role of social
media, sales and discussions of illicit drugs have become commonplace online.
Social media platforms hosting user-generated content must therefore perform
content moderation, which is a difficult task due to the vast amount of jargon
used in drug discussions. Previous works on drug jargon detection were limited
to extracting a list of terms, but these approaches have fundamental problems
in practical application. First, they are trivially evaded using word
substitutions. Second, they cannot distinguish whether euphemistic terms such
as ""pot"" or ""crack"" are being used as drugs or in their benign meanings. We
argue that drug content moderation should be done using contexts rather than
relying on a banlist. However, manually annotated datasets for training such a
task are not only expensive but also prone to becoming obsolete. We present
JEDIS, a framework for detecting illicit drug jargon terms by analyzing their
contexts. JEDIS utilizes a novel approach that combines distant supervision and
delexicalization, which allows JEDIS to be trained without human-labeled data
while being robust to new terms and euphemisms. Experiments on two manually
annotated datasets show JEDIS significantly outperforms state-of-the-art
word-based baselines in terms of F1-score and detection coverage in drug jargon
detection. We also conduct qualitative analysis that demonstrates JEDIS is
robust against pitfalls faced by existing approaches.",['cs.CL'],"['Minkyoo Song', 'Eugene Jang', 'Jaehan Kim', 'Seungwon Shin']",2025-03-19,2025-03-19
2503.14925v1,pFedFair: Towards Optimal Group Fairness-Accuracy Trade-off in Heterogeneous Federated Learning,"Federated learning (FL) algorithms commonly aim to maximize clients' accuracy
by training a model on their collective data. However, in several FL
applications, the model's decisions should meet a group fairness constraint to
be independent of sensitive attributes such as gender or race. While such group
fairness constraints can be incorporated into the objective function of the FL
optimization problem, in this work, we show that such an approach would lead to
suboptimal classification accuracy in an FL setting with heterogeneous client
distributions. To achieve an optimal accuracy-group fairness trade-off, we
propose the Personalized Federated Learning for Client-Level Group Fairness
(pFedFair) framework, where clients locally impose their fairness constraints
over the distributed training process. Leveraging the image embedding models,
we extend the application of pFedFair to computer vision settings, where we
numerically show that pFedFair achieves an optimal group fairness-accuracy
trade-off in heterogeneous FL settings. We present the results of several
numerical experiments on benchmark and synthetic datasets, which highlight the
suboptimality of non-personalized FL algorithms and the improvements made by
the pFedFair method.",['cs.LG'],"['Haoyu Lei', 'Shizhan Gong', 'Qi Dou', 'Farzan Farnia']",2025-03-19,2025-03-19
2503.16544v1,Causal Discovery and Counterfactual Reasoning to Optimize Persuasive Dialogue Policies,"Tailoring persuasive conversations to users leads to more effective
persuasion. However, existing dialogue systems often struggle to adapt to
dynamically evolving user states. This paper presents a novel method that
leverages causal discovery and counterfactual reasoning for optimizing system
persuasion capability and outcomes. We employ the Greedy Relaxation of the
Sparsest Permutation (GRaSP) algorithm to identify causal relationships between
user and system utterance strategies, treating user strategies as states and
system strategies as actions. GRaSP identifies user strategies as causal
factors influencing system responses, which inform Bidirectional Conditional
Generative Adversarial Networks (BiCoGAN) in generating counterfactual
utterances for the system. Subsequently, we use the Dueling Double Deep
Q-Network (D3QN) model to utilize counterfactual data to determine the best
policy for selecting system utterances. Our experiments with the
PersuasionForGood dataset show measurable improvements in persuasion outcomes
using our approach over baseline methods. The observed increase in cumulative
rewards and Q-values highlights the effectiveness of causal discovery in
enhancing counterfactual reasoning and optimizing reinforcement learning
policies for online dialogue systems.","['cs.CL', 'cs.AI', 'cs.HC']","['Donghuo Zeng', 'Roberto Legaspi', 'Yuewen Sun', 'Xinshuai Dong', 'Kazushi Ikeda', 'Peter Spirtes', 'Kun Zhang']",2025-03-19,2025-03-19
2503.14922v1,A Semantic and Clean-label Backdoor Attack against Graph Convolutional Networks,"Graph Convolutional Networks (GCNs) have shown excellent performance in
graph-structured tasks such as node classification and graph classification.
However, recent research has shown that GCNs are vulnerable to a new type of
threat called the backdoor attack, where the adversary can inject a hidden
backdoor into the GCNs so that the backdoored model performs well on benign
samples, whereas its prediction will be maliciously changed to the
attacker-specified target label if the hidden backdoor is activated by the
attacker-defined trigger. Clean-label backdoor attack and semantic backdoor
attack are two new backdoor attacks to Deep Neural Networks (DNNs), they are
more imperceptible and have posed new and serious threats. The semantic and
clean-label backdoor attack is not fully explored in GCNs. In this paper, we
propose a semantic and clean-label backdoor attack against GCNs under the
context of graph classification to reveal the existence of this security
vulnerability in GCNs. Specifically, SCLBA conducts an importance analysis on
graph samples to select one type of node as semantic trigger, which is then
inserted into the graph samples to create poisoning samples without changing
the labels of the poisoning samples to the attacker-specified target label. We
evaluate SCLBA on multiple datasets and the results show that SCLBA can achieve
attack success rates close to 99% with poisoning rates of less than 3%, and
with almost no impact on the performance of model on benign samples.","['cs.LG', 'cs.AI', 'cs.CR']","['Jiazhu Dai', 'Haoyu Sun']",2025-03-19,2025-03-19
2503.14919v1,GenM$^3$: Generative Pretrained Multi-path Motion Model for Text Conditional Human Motion Generation,"Scaling up motion datasets is crucial to enhance motion generation
capabilities. However, training on large-scale multi-source datasets introduces
data heterogeneity challenges due to variations in motion content. To address
this, we propose Generative Pretrained Multi-path Motion Model (GenM$^3$), a
comprehensive framework designed to learn unified motion representations.
GenM$^3$ comprises two components: 1) a Multi-Expert VQ-VAE (MEVQ-VAE) that
adapts to different dataset distributions to learn a unified discrete motion
representation, and 2) a Multi-path Motion Transformer (MMT) that improves
intra-modal representations by using separate modality-specific pathways, each
with densely activated experts to accommodate variations within that modality,
and improves inter-modal alignment by the text-motion shared pathway. To enable
large-scale training, we integrate and unify 11 high-quality motion datasets
(approximately 220 hours of motion data) and augment it with textual
annotations (nearly 10,000 motion sequences labeled by a large language model
and 300+ by human experts). After training on our integrated dataset, GenM$^3$
achieves a state-of-the-art FID of 0.035 on the HumanML3D benchmark, surpassing
state-of-the-art methods by a large margin. It also demonstrates strong
zero-shot generalization on IDEA400 dataset, highlighting its effectiveness and
adaptability across diverse motion scenarios.",['cs.CV'],"['Junyu Shi', 'Lijiang Liu', 'Yong Sun', 'Zhiyuan Zhang', 'Jinni Zhou', 'Qiang Nie']",2025-03-19,2025-03-19
2503.14917v1,MASS: Mathematical Data Selection via Skill Graphs for Pretraining Large Language Models,"High-quality data plays a critical role in the pretraining and fine-tuning of
large language models (LLMs), even determining their performance ceiling to
some degree. Consequently, numerous data selection methods have been proposed
to identify subsets of data that can effectively and efficiently enhance model
performance. However, most of these methods focus on general data selection and
tend to overlook the specific nuances of domain-related data. In this paper, we
introduce MASS, a \textbf{MA}thematical data \textbf{S}election framework using
the \textbf{S}kill graph for pretraining LLMs in the mathematical reasoning
domain. By taking into account the unique characteristics of mathematics and
reasoning, we construct a skill graph that captures the mathematical skills and
their interrelations from a reference dataset. This skill graph guides us in
assigning quality scores to the target dataset, enabling us to select the
top-ranked subset which is further used to pretrain LLMs. Experimental results
demonstrate the efficiency and effectiveness of MASS across different model
sizes (1B and 7B) and pretraining datasets (web data and synthetic data).
Specifically, in terms of efficiency, models trained on subsets selected by
MASS can achieve similar performance to models trained on the original
datasets, with a significant reduction in the number of trained tokens -
ranging from 50\% to 70\% fewer tokens. In terms of effectiveness, when trained
on the same amount of tokens, models trained on the data selected by MASS
outperform those trained on the original datasets by 3.3\% to 5.9\%. These
results underscore the potential of MASS to improve both the efficiency and
effectiveness of pretraining LLMs.","['cs.CL', 'cs.AI']","['Jiazheng Li', 'Lu Yu', 'Qing Cui', 'Zhiqiang Zhang', 'Jun Zhou', 'Yanfang Ye', 'Chuxu Zhang']",2025-03-19,2025-03-19
2503.14912v1,Deep Polycuboid Fitting for Compact 3D Representation of Indoor Scenes,"This paper presents a novel framework for compactly representing a 3D indoor
scene using a set of polycuboids through a deep learning-based fitting method.
Indoor scenes mainly consist of man-made objects, such as furniture, which
often exhibit rectilinear geometry. This property allows indoor scenes to be
represented using combinations of polycuboids, providing a compact
representation that benefits downstream applications like furniture
rearrangement. Our framework takes a noisy point cloud as input and first
detects six types of cuboid faces using a transformer network. Then, a graph
neural network is used to validate the spatial relationships of the detected
faces to form potential polycuboids. Finally, each polycuboid instance is
reconstructed by forming a set of boxes based on the aggregated face labels. To
train our networks, we introduce a synthetic dataset encompassing a diverse
range of cuboid and polycuboid shapes that reflect the characteristics of
indoor scenes. Our framework generalizes well to real-world indoor scene
datasets, including Replica, ScanNet, and scenes captured with an iPhone. The
versatility of our method is demonstrated through practical applications, such
as virtual room tours and scene editing.","['cs.CV', 'I.4.8; I.3.5']","['Gahye Lee', 'Hyejeong Yoon', 'Jungeon Kim', 'Seungyong Lee']",2025-03-19,2025-03-19
2503.14911v1,Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology,"The emergence of vision-language models has transformed medical AI, enabling
unprecedented advances in diagnostic capability and clinical applications.
However, progress in dermatology has lagged behind other medical domains due to
the lack of standard image-text pairs. Existing dermatological datasets are
limited in both scale and depth, offering only single-label annotations across
a narrow range of diseases instead of rich textual descriptions, and lacking
the crucial clinical context needed for real-world applications. To address
these limitations, we present Derm1M, the first large-scale vision-language
dataset for dermatology, comprising 1,029,761 image-text pairs. Built from
diverse educational resources and structured around a standard ontology
collaboratively developed by experts, Derm1M provides comprehensive coverage
for over 390 skin conditions across four hierarchical levels and 130 clinical
concepts with rich contextual information such as medical history, symptoms,
and skin tone. To demonstrate Derm1M potential in advancing both AI research
and clinical application, we pretrained a series of CLIP-like models,
collectively called DermLIP, on this dataset. The DermLIP family significantly
outperforms state-of-the-art foundation models on eight diverse datasets across
multiple tasks, including zero-shot skin disease classification, clinical and
artifacts concept identification, few-shot/full-shot learning, and cross-modal
retrieval. Our dataset and code will be public.",['cs.CV'],"['Siyuan Yan', 'Ming Hu', 'Yiwen Jiang', 'Xieji Li', 'Hao Fei', 'Philipp Tschandl', 'Harald Kittler', 'Zongyuan Ge']",2025-03-19,2025-03-19
2503.14910v1,Robust Distribution Alignment for Industrial Anomaly Detection under Distribution Shift,"Anomaly detection plays a crucial role in quality control for industrial
applications. However, ensuring robustness under unseen domain shifts such as
lighting variations or sensor drift remains a significant challenge. Existing
methods attempt to address domain shifts by training generalizable models but
often rely on prior knowledge of target distributions and can hardly generalise
to backbones designed for other data modalities. To overcome these limitations,
we build upon memory-bank-based anomaly detection methods, optimizing a robust
Sinkhorn distance on limited target training data to enhance generalization to
unseen target domains. We evaluate the effectiveness on both 2D and 3D anomaly
detection benchmarks with simulated distribution shifts. Our proposed method
demonstrates superior results compared with state-of-the-art anomaly detection
and domain adaptation methods.",['cs.CV'],"['Jingyi Liao', 'Xun Xu', 'Yongyi Su', 'Rong-Cheng Tu', 'Yifan Liu', 'Dacheng Tao', 'Xulei Yang']",2025-03-19,2025-03-19
2503.14908v1,POSTA: A Go-to Framework for Customized Artistic Poster Generation,"Poster design is a critical medium for visual communication. Prior work has
explored automatic poster design using deep learning techniques, but these
approaches lack text accuracy, user customization, and aesthetic appeal,
limiting their applicability in artistic domains such as movies and
exhibitions, where both clear content delivery and visual impact are essential.
To address these limitations, we present POSTA: a modular framework powered by
diffusion models and multimodal large language models (MLLMs) for customized
artistic poster generation. The framework consists of three modules. Background
Diffusion creates a themed background based on user input. Design MLLM then
generates layout and typography elements that align with and complement the
background style. Finally, to enhance the poster's aesthetic appeal, ArtText
Diffusion applies additional stylization to key text elements. The final result
is a visually cohesive and appealing poster, with a fully modular process that
allows for complete customization. To train our models, we develop the
PosterArt dataset, comprising high-quality artistic posters annotated with
layout, typography, and pixel-level stylized text segmentation. Our
comprehensive experimental analysis demonstrates POSTA's exceptional
controllability and design diversity, outperforming existing models in both
text accuracy and aesthetic quality.","['cs.GR', 'cs.AI', 'cs.CV']","['Haoyu Chen', 'Xiaojie Xu', 'Wenbo Li', 'Jingjing Ren', 'Tian Ye', 'Songhua Liu', 'Ying-Cong Chen', 'Lei Zhu', 'Xinchao Wang']",2025-03-19,2025-03-19
2503.14906v1,FetalFlex: Anatomy-Guided Diffusion Model for Flexible Control on Fetal Ultrasound Image Synthesis,"Fetal ultrasound (US) examinations require the acquisition of multiple
planes, each providing unique diagnostic information to evaluate fetal
development and screening for congenital anomalies. However, obtaining a
comprehensive, multi-plane annotated fetal US dataset remains challenging,
particularly for rare or complex anomalies owing to their low incidence and
numerous subtypes. This poses difficulties in training novice radiologists and
developing robust AI models, especially for detecting abnormal fetuses. In this
study, we introduce a Flexible Fetal US image generation framework (FetalFlex)
to address these challenges, which leverages anatomical structures and
multimodal information to enable controllable synthesis of fetal US images
across diverse planes. Specifically, FetalFlex incorporates a pre-alignment
module to enhance controllability and introduces a repaint strategy to ensure
consistent texture and appearance. Moreover, a two-stage adaptive sampling
strategy is developed to progressively refine image quality from coarse to fine
levels. We believe that FetalFlex is the first method capable of generating
both in-distribution normal and out-of-distribution abnormal fetal US images,
without requiring any abnormal data. Experiments on multi-center datasets
demonstrate that FetalFlex achieved state-of-the-art performance across
multiple image quality metrics. A reader study further confirms the close
alignment of the generated results with expert visual assessments. Furthermore,
synthetic images by FetalFlex significantly improve the performance of six
typical deep models in downstream classification and anomaly detection tasks.
Lastly, FetalFlex's anatomy-level controllable generation offers a unique
advantage for anomaly simulation and creating paired or counterfactual data at
the pixel level. The demo is available at:
https://dyf1023.github.io/FetalFlex/.","['eess.IV', 'cs.CV']","['Yaofei Duan', 'Tao Tan', 'Zhiyuan Zhu', 'Yuhao Huang', 'Yuanji Zhang', 'Rui Gao', 'Patrick Cheong-Iao Pang', 'Xinru Gao', 'Guowei Tao', 'Xiang Cong', 'Zhou Li', 'Lianying Liang', 'Guangzhi He', 'Linliang Yin', 'Xuedong Deng', 'Xin Yang', 'Dong Ni']",2025-03-19,2025-03-19
2503.14905v1,Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection with Artifact Explanation,"With the rapid advancement of Artificial Intelligence Generated Content
(AIGC) technologies, synthetic images have become increasingly prevalent in
everyday life, posing new challenges for authenticity assessment and detection.
Despite the effectiveness of existing methods in evaluating image authenticity
and locating forgeries, these approaches often lack human interpretability and
do not fully address the growing complexity of synthetic data. To tackle these
challenges, we introduce FakeVLM, a specialized large multimodal model designed
for both general synthetic image and DeepFake detection tasks. FakeVLM not only
excels in distinguishing real from fake images but also provides clear, natural
language explanations for image artifacts, enhancing interpretability.
Additionally, we present FakeClue, a comprehensive dataset containing over
100,000 images across seven categories, annotated with fine-grained artifact
clues in natural language. FakeVLM demonstrates performance comparable to
expert models while eliminating the need for additional classifiers, making it
a robust solution for synthetic data detection. Extensive evaluations across
multiple datasets confirm the superiority of FakeVLM in both authenticity
classification and artifact explanation tasks, setting a new benchmark for
synthetic image detection. The dataset and code will be released in:
https://github.com/opendatalab/FakeVLM.",['cs.CV'],"['Siwei Wen', 'Junyan Ye', 'Peilin Feng', 'Hengrui Kang', 'Zichen Wen', 'Yize Chen', 'Jiang Wu', 'Wenjun Wu', 'Conghui He', 'Weijia Li']",2025-03-19,2025-03-19
2503.14900v1,Deep Contrastive Unlearning for Language Models,"The past a few years have witnessed the great success of large language
models, demonstrating powerful capabilities in comprehending textual data and
generating human-like languages. Large language models achieve success by being
trained on vast amounts of textual data, including online sources with
copyrighted content and user-generated knowledge. However, this comes at a
cost: the potential risk of exposing users' privacy and violating copyright
protections. Thus, to safeguard individuals' ""right to be forgotten"", there has
been increasing interests in machine unlearning -- the process of removing
information carried by particular training samples from a model while not
deteriorating its predictive quality. This is a challenging task due to the
black-box nature of language models. Most existing studies focus on mitigating
the impact of those forgot samples upon a model's outputs, and do not
explicitly consider the geometric distributions of samples in the latent space
of a model. To address this issue, we propose a machine unlearning framework,
named Deep Contrastive Unlearning for fine-Tuning (DeepCUT) language models.
Our proposed model achieves machine unlearning by directly optimizing the
latent space of a model. Comprehensive experiments on real-world datasets
demonstrate the effectiveness and efficiency of DeepCUT with consistent and
significant improvement over baseline methods.","['cs.CL', 'cs.AI']","['Estrid He', 'Tabinda Sarwar', 'Ibrahim Khalil', 'Xun Yi', 'Ke Wang']",2025-03-19,2025-03-19
2503.14897v2,When Domain Generalization meets Generalized Category Discovery: An Adaptive Task-Arithmetic Driven Approach,"Generalized Class Discovery (GCD) clusters base and novel classes in a target
domain using supervision from a source domain with only base classes. Current
methods often falter with distribution shifts and typically require access to
target data during training, which can sometimes be impractical. To address
this issue, we introduce the novel paradigm of Domain Generalization in GCD
(DG-GCD), where only source data is available for training, while the target
domain, with a distinct data distribution, remains unseen until inference. To
this end, our solution, DG2CD-Net, aims to construct a domain-independent,
discriminative embedding space for GCD. The core innovation is an episodic
training strategy that enhances cross-domain generalization by adapting a base
model on tasks derived from source and synthetic domains generated by a
foundation model. Each episode focuses on a cross-domain GCD task, diversifying
task setups over episodes and combining open-set domain adaptation with a novel
margin loss and representation learning for optimizing the feature space
progressively. To capture the effects of fine-tuning on the base model, we
extend task arithmetic by adaptively weighting the local task vectors
concerning the fine-tuned models based on their GCD performance on a validation
distribution. This episodic update mechanism boosts the adaptability of the
base model to unseen targets. Experiments across three datasets confirm that
DG2CD-Net outperforms existing GCD methods customized for DG-GCD.",['cs.CV'],"['Vaibhav Rathore', 'Shubhranil B', 'Saikat Dutta', 'Sarthak Mehrotra', 'Zsolt Kira', 'Biplab Banerjee']",2025-03-19,2025-03-21
2503.14895v1,Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations,"Recently, multimodal large language models (MLLMs) have demonstrated
remarkable performance in visual-language tasks. However, the authenticity of
the responses generated by MLLMs is often compromised by object hallucinations.
We identify that a key cause of these hallucinations is the model's
over-susceptibility to specific image frequency features in detecting objects.
In this paper, we introduce Multi-Frequency Perturbations (MFP), a simple,
cost-effective, and pluggable method that leverages both low-frequency and
high-frequency features of images to perturb visual feature representations and
explicitly suppress redundant frequency-domain features during inference,
thereby mitigating hallucinations. Experimental results demonstrate that our
method significantly mitigates object hallucinations across various model
architectures. Furthermore, as a training-time method, MFP can be combined with
inference-time methods to achieve state-of-the-art performance on the CHAIR
benchmark.","['cs.CV', 'cs.AI', 'cs.CL']","['Shuo Li', 'Jiajun Sun', 'Guodong Zheng', 'Xiaoran Fan', 'Yujiong Shen', 'Yi Lu', 'Zhiheng Xi', 'Yuming Yang', 'Wenming Tan', 'Tao Ji', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang']",2025-03-19,2025-03-19
2503.14892v1,Degradation Alchemy: Self-Supervised Unknown-to-Known Transformation for Blind Hyperspectral Image Fusion,"Hyperspectral image (HSI) fusion is an efficient technique that combines
low-resolution HSI (LR-HSI) and high-resolution multispectral images (HR-MSI)
to generate high-resolution HSI (HR-HSI). Existing supervised learning methods
(SLMs) can yield promising results when test data degradation matches the
training ones, but they face challenges in generalizing to unknown
degradations. To unleash the potential and generalization ability of SLMs, we
propose a novel self-supervised unknown-to-known degradation transformation
framework (U2K) for blind HSI fusion, which adaptively transforms unknown
degradation into the same type of degradation as those handled by pre-trained
SLMs. Specifically, the proposed U2K framework consists of: (1) spatial and
spectral Degradation Wrapping (DW) modules that map HR-HSI to unknown degraded
HR-MSI and LR-HSI, and (2) Degradation Transformation (DT) modules that convert
these wrapped data into predefined degradation patterns. The transformed HR-MSI
and LR-HSI pairs are then processed by a pre-trained network to reconstruct the
target HR-HSI. We train the U2K framework in a self-supervised manner using
consistency loss and greedy alternating optimization, significantly improving
the flexibility of blind HSI fusion. Extensive experiments confirm the
effectiveness of our proposed U2K framework in boosting the adaptability of
five existing SLMs under various degradation settings and surpassing
state-of-the-art blind methods.","['eess.IV', 'cs.CV']","['He Huang', 'Yong Chen', 'Yujun Guo', 'Wei He']",2025-03-19,2025-03-19
2503.14891v1,MetaLadder: Ascending Mathematical Solution Quality via Analogical-Problem Reasoning Transfer,"Large Language Models (LLMs) have demonstrated promising capabilities in
solving mathematical reasoning tasks, leveraging Chain-of-Thought (CoT) data as
a vital component in guiding answer generation. Current paradigms typically
generate CoT and answers directly for a given problem, diverging from human
problem-solving strategies to some extent. Humans often solve problems by
recalling analogous cases and leveraging their solutions to reason about the
current task. Inspired by this cognitive process, we propose
\textbf{MetaLadder}, a novel framework that explicitly prompts LLMs to recall
and reflect on meta-problems, those structurally or semantically analogous
problems, alongside their CoT solutions before addressing the target problem.
Additionally, we introduce a problem-restating mechanism to enhance the model's
comprehension of the target problem by regenerating the original question,
which further improves reasoning accuracy. Therefore, the model can achieve
reasoning transfer from analogical problems, mimicking human-like ""learning
from examples"" and generalization abilities. Extensive experiments on
mathematical benchmarks demonstrate that our MetaLadder significantly boosts
LLMs' problem-solving accuracy, largely outperforming standard CoT-based
methods (\textbf{10.3\%} accuracy gain) and other methods. Our code and data
has been released at https://github.com/LHL3341/MetaLadder.","['cs.CL', 'cs.AI']","['Honglin Lin', 'Zhuoshi Pan', 'Yu Li', 'Qizhi Pei', 'Xin Gao', 'Mengzhang Cai', 'Conghui He', 'Lijun Wu']",2025-03-19,2025-03-19
2503.14887v1,Pseudo-Relevance Feedback Can Improve Zero-Shot LLM-Based Dense Retrieval,"Pseudo-relevance feedback (PRF) refines queries by leveraging initially
retrieved documents to improve retrieval effectiveness. In this paper, we
investigate how large language models (LLMs) can facilitate PRF for zero-shot
LLM-based dense retrieval, extending the recently proposed PromptReps method.
Specifically, our approach uses LLMs to extract salient passage features-such
as keywords and summaries-from top-ranked documents, which are then integrated
into PromptReps to produce enhanced query representations. Experiments on
passage retrieval benchmarks demonstrate that incorporating PRF significantly
boosts retrieval performance. Notably, smaller rankers with PRF can match the
effectiveness of larger rankers without PRF, highlighting PRF's potential to
improve LLM-driven search while maintaining an efficient balance between
effectiveness and resource usage.","['cs.IR', 'cs.LG']","['Hang Li', 'Xiao Wang', 'Bevan Koopman', 'Guido Zuccon']",2025-03-19,2025-03-19
2503.14883v1,Envisioning an AI-Enhanced Mental Health Ecosystem,"The rapid advancement of Large Language Models (LLMs), reasoning models, and
agentic AI approaches coincides with a growing global mental health crisis,
where increasing demand has not translated into adequate access to professional
support, particularly for underserved populations. This presents a unique
opportunity for AI to complement human-led interventions, offering scalable and
context-aware support while preserving human connection in this sensitive
domain. We explore various AI applications in peer support, self-help
interventions, proactive monitoring, and data-driven insights, using a
human-centred approach that ensures AI supports rather than replaces human
interaction. However, AI deployment in mental health fields presents challenges
such as ethical concerns, transparency, privacy risks, and risks of
over-reliance. We propose a hybrid ecosystem where where AI assists but does
not replace human providers, emphasising responsible deployment and evaluation.
We also present some of our early work and findings in several of these AI
applications. Finally, we outline future research directions for refining
AI-enhanced interventions while adhering to ethical and culturally sensitive
guidelines.","['cs.HC', 'cs.AI', 'H.5.0']","['Kellie Yu Hui Sim', 'Kenny Tsu Wei Choo']",2025-03-19,2025-03-19
2503.14881v1,Exploring the Limits of KV Cache Compression in Visual Autoregressive Transformers,"A fundamental challenge in Visual Autoregressive models is the substantial
memory overhead required during inference to store previously generated
representations. Despite various attempts to mitigate this issue through
compression techniques, prior works have not explicitly formalized the problem
of KV-cache compression in this context. In this work, we take the first step
in formally defining the KV-cache compression problem for Visual Autoregressive
transformers. We then establish a fundamental negative result, proving that any
mechanism for sequential visual token generation under attention-based
architectures must use at least $\Omega(n^2 d)$ memory, when $d = \Omega(\log
n)$, where $n$ is the number of tokens generated and $d$ is the embedding
dimensionality. This result demonstrates that achieving truly sub-quadratic
memory usage is impossible without additional structural constraints. Our proof
is constructed via a reduction from a computational lower bound problem,
leveraging randomized embedding techniques inspired by dimensionality reduction
principles. Finally, we discuss how sparsity priors on visual representations
can influence memory efficiency, presenting both impossibility results and
potential directions for mitigating memory overhead.","['cs.LG', 'cs.AI', 'cs.CV']","['Bo Chen', 'Xiaoyu Li', 'Yekun Ke', 'Yingyu Liang', 'Zhenmei Shi', 'Zhao Song']",2025-03-19,2025-03-19
2503.14880v1,DPFlow: Adaptive Optical Flow Estimation with a Dual-Pyramid Framework,"Optical flow estimation is essential for video processing tasks, such as
restoration and action recognition. The quality of videos is constantly
increasing, with current standards reaching 8K resolution. However, optical
flow methods are usually designed for low resolution and do not generalize to
large inputs due to their rigid architectures. They adopt downscaling or input
tiling to reduce the input size, causing a loss of details and global
information. There is also a lack of optical flow benchmarks to judge the
actual performance of existing methods on high-resolution samples. Previous
works only conducted qualitative high-resolution evaluations on hand-picked
samples. This paper fills this gap in optical flow estimation in two ways. We
propose DPFlow, an adaptive optical flow architecture capable of generalizing
up to 8K resolution inputs while trained with only low-resolution samples. We
also introduce Kubric-NK, a new benchmark for evaluating optical flow methods
with input resolutions ranging from 1K to 8K. Our high-resolution evaluation
pushes the boundaries of existing methods and reveals new insights about their
generalization capabilities. Extensive experimental results show that DPFlow
achieves state-of-the-art results on the MPI-Sintel, KITTI 2015, Spring, and
other high-resolution benchmarks.",['cs.CV'],"['Henrique Morimitsu', 'Xiaobin Zhu', 'Roberto M. Cesar Jr.', 'Xiangyang Ji', 'Xu-Cheng Yin']",2025-03-19,2025-03-19
2503.15564v1,GReaTER: Generate Realistic Tabular data after data Enhancement and Reduction,"Tabular data synthesis involves not only multi-table synthesis but also
generating multi-modal data (e.g., strings and categories), which enables
diverse knowledge synthesis. However, separating numerical and categorical data
has limited the effectiveness of tabular data generation. The GReaT (Generate
Realistic Tabular Data) framework uses Large Language Models (LLMs) to encode
entire rows, eliminating the need to partition data types. Despite this, the
framework's performance is constrained by two issues: (1) tabular data entries
lack sufficient semantic meaning, limiting LLM's ability to leverage
pre-trained knowledge for in-context learning, and (2) complex multi-table
datasets struggle to establish effective relationships for collaboration. To
address these, we propose GReaTER (Generate Realistic Tabular Data after data
Enhancement and Reduction), which includes: (1) a data semantic enhancement
system that improves LLM's understanding of tabular data through mapping,
enabling better in-context learning, and (2) a cross-table connecting method to
establish efficient relationships across complex tables. Experimental results
show that GReaTER outperforms the GReaT framework.",['cs.LG'],"['Tung Sum Thomas Kwok', 'Chi-Hua Wang', 'Guang Cheng']",2025-03-19,2025-03-19
2503.14873v1,Robust Support Vector Machines for Imbalanced and Noisy Data via Benders Decomposition,"This study introduces a novel formulation to enhance Support Vector Machines
(SVMs) in handling class imbalance and noise. Unlike the conventional Soft
Margin SVM, which penalizes the magnitude of constraint violations, the
proposed model quantifies the number of violations and aims to minimize their
frequency. To achieve this, a binary variable is incorporated into the
objective function of the primal SVM formulation, replacing the traditional
slack variable. Furthermore, each misclassified sample is assigned a priority
and an associated constraint. The resulting formulation is a mixed-integer
programming model, efficiently solved using Benders decomposition. The proposed
model's performance was benchmarked against existing models, including Soft
Margin SVM, weighted SVM, and NuSVC. Two primary hypotheses were examined: 1)
The proposed model improves the F1-score for the minority class in imbalanced
classification tasks. 2) The proposed model enhances classification accuracy in
noisy datasets. These hypotheses were evaluated using a Wilcoxon test across
multiple publicly available datasets from the OpenML repository. The results
supported both hypotheses (\( p < 0.05 \)). In addition, the proposed model
exhibited several interesting properties, such as improved robustness to noise,
a decision boundary shift favoring the minority class, a reduced number of
support vectors, and decreased prediction time. The open-source Python
implementation of the proposed SVM model is available.",['cs.LG'],"['Seyed Mojtaba Mohasel', 'Hamidreza Koosha']",2025-03-19,2025-03-19
2503.14869v1,Evaluating Time Series Models with Knowledge Discovery,"Time series data is one of the most ubiquitous data modalities existing in a
diverse critical domains such as healthcare, seismology, manufacturing and
energy. Recent years, there are increasing interest of the data mining
community to develop time series deep learning models to pursue better
performance. The models performance often evaluate by certain evaluation
metrics such as RMSE, Accuracy, and F1-score. Yet time series data are often
hard to interpret and are collected with unknown environmental factors, sensor
configuration, latent physic mechanisms, and non-stationary evolving behavior.
As a result, a model that is better on standard metric-based evaluation may not
always perform better in real-world tasks. In this blue sky paper, we aim to
explore the challenge that exists in the metric-based evaluation framework for
time series data mining and propose a potential blue-sky idea -- developing a
knowledge-discovery-based evaluation framework, which aims to effectively
utilize domain-expertise knowledge to evaluate a model. We demonstrate that an
evidence-seeking explanation can potentially have stronger persuasive power
than metric-based evaluation and obtain better generalization ability for time
series data mining tasks.",['cs.LG'],['Li Zhang'],2025-03-19,2025-03-19
2503.14868v1,Efficient Personalization of Quantized Diffusion Model without Backpropagation,"Diffusion models have shown remarkable performance in image synthesis, but
they demand extensive computational and memory resources for training,
fine-tuning and inference. Although advanced quantization techniques have
successfully minimized memory usage for inference, training and fine-tuning
these quantized models still require large memory possibly due to
dequantization for accurate computation of gradients and/or backpropagation for
gradient-based algorithms. However, memory-efficient fine-tuning is
particularly desirable for applications such as personalization that often must
be run on edge devices like mobile phones with private data. In this work, we
address this challenge by quantizing a diffusion model with personalization via
Textual Inversion and by leveraging a zeroth-order optimization on
personalization tokens without dequantization so that it does not require
gradient and activation storage for backpropagation that consumes considerable
memory. Since a gradient estimation using zeroth-order optimization is quite
noisy for a single or a few images in personalization, we propose to denoise
the estimated gradient by projecting it onto a subspace that is constructed
with the past history of the tokens, dubbed Subspace Gradient. In addition, we
investigated the influence of text embedding in image generation, leading to
our proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for
sampling with effective diffusion timesteps. Our method achieves comparable
performance to prior methods in image and text alignment scores for
personalizing Stable Diffusion with only forward passes while reducing training
memory demand up to $8.2\times$.","['cs.CV', 'cs.AI']","['Hoigi Seo', 'Wongi Jeong', 'Kyungryeol Lee', 'Se Young Chun']",2025-03-19,2025-03-19
2503.14867v1,DVHGNN: Multi-Scale Dilated Vision HGNN for Efficient Vision Recognition,"Recently, Vision Graph Neural Network (ViG) has gained considerable attention
in computer vision. Despite its groundbreaking innovation, Vision Graph Neural
Network encounters key issues including the quadratic computational complexity
caused by its K-Nearest Neighbor (KNN) graph construction and the limitation of
pairwise relations of normal graphs. To address the aforementioned challenges,
we propose a novel vision architecture, termed Dilated Vision HyperGraph Neural
Network (DVHGNN), which is designed to leverage multi-scale hypergraph to
efficiently capture high-order correlations among objects. Specifically, the
proposed method tailors Clustering and Dilated HyperGraph Construction (DHGC)
to adaptively capture multi-scale dependencies among the data samples.
Furthermore, a dynamic hypergraph convolution mechanism is proposed to
facilitate adaptive feature exchange and fusion at the hypergraph level.
Extensive qualitative and quantitative evaluations of the benchmark image
datasets demonstrate that the proposed DVHGNN significantly outperforms the
state-of-the-art vision backbones. For instance, our DVHGNN-S achieves an
impressive top-1 accuracy of 83.1% on ImageNet-1K, surpassing ViG-S by +1.0%
and ViHGNN-S by +0.6%.",['cs.CV'],"['Caoshuo Li', 'Tanzhe Li', 'Xiaobin Hu', 'Donghao Luo', 'Taisong Jin']",2025-03-19,2025-03-19
2503.14863v1,Temporal-Consistent Video Restoration with Pre-trained Diffusion Models,"Video restoration (VR) aims to recover high-quality videos from degraded
ones. Although recent zero-shot VR methods using pre-trained diffusion models
(DMs) show good promise, they suffer from approximation errors during reverse
diffusion and insufficient temporal consistency. Moreover, dealing with 3D
video data, VR is inherently computationally intensive. In this paper, we
advocate viewing the reverse process in DMs as a function and present a novel
Maximum a Posterior (MAP) framework that directly parameterizes video frames in
the seed space of DMs, eliminating approximation errors. We also introduce
strategies to promote bilevel temporal consistency: semantic consistency by
leveraging clustering structures in the seed space, and pixel-level consistency
by progressive warping with optical flow refinements. Extensive experiments on
multiple virtual reality tasks demonstrate superior visual quality and temporal
consistency achieved by our method compared to the state-of-the-art.",['cs.CV'],"['Hengkang Wang', 'Yang Liu', 'Huidong Liu', 'Chien-Chih Wang', 'Yanhui Guo', 'Hongdong Li', 'Bryan Wang', 'Ju Sun']",2025-03-19,2025-03-19
2503.14862v2,"Fine-Grained Open-Vocabulary Object Detection with Fined-Grained Prompts: Task, Dataset and Benchmark","Open-vocabulary detectors are proposed to locate and recognize objects in
novel classes. However, variations in vision-aware language vocabulary data
used for open-vocabulary learning can lead to unfair and unreliable
evaluations. Recent evaluation methods have attempted to address this issue by
incorporating object properties or adding locations and characteristics to the
captions. Nevertheless, since these properties and locations depend on the
specific details of the images instead of classes, detectors can not make
accurate predictions without precise descriptions provided through human
annotation. This paper introduces 3F-OVD, a novel task that extends supervised
fine-grained object detection to the open-vocabulary setting. Our task is
intuitive and challenging, requiring a deep understanding of Fine-grained
captions and careful attention to Fine-grained details in images in order to
accurately detect Fine-grained objects. Additionally, due to the scarcity of
qualified fine-grained object detection datasets, we have created a new
dataset, NEU-171K, tailored for both supervised and open-vocabulary settings.
We benchmark state-of-the-art object detectors on our dataset for both
settings. Furthermore, we propose a simple yet effective post-processing
technique.","['cs.CV', 'I.2.0']","['Ying Liu', 'Yijing Hua', 'Haojiang Chai', 'Yanbo Wang', 'TengQi Ye']",2025-03-19,2025-03-20
2503.14860v1,Global Renewables Watch: A Temporal Dataset of Solar and Wind Energy Derived from Satellite Imagery,"We present a comprehensive global temporal dataset of commercial solar
photovoltaic (PV) farms and onshore wind turbines, derived from high-resolution
satellite imagery analyzed quarterly from the fourth quarter of 2017 to the
second quarter of 2024. We create this dataset by training deep learning-based
segmentation models to identify these renewable energy installations from
satellite imagery, then deploy them on over 13 trillion pixels covering the
world. For each detected feature, we estimate the construction date and the
preceding land use type. This dataset offers crucial insights into progress
toward sustainable development goals and serves as a valuable resource for
policymakers, researchers, and stakeholders aiming to assess and promote
effective strategies for renewable energy deployment. Our final spatial dataset
includes 375,197 individual wind turbines and 86,410 solar PV installations. We
aggregate our predictions to the country level -- estimating total power
capacity based on construction date, solar PV area, and number of windmills --
and find an $r^2$ value of $0.96$ and $0.93$ for solar PV and onshore wind
respectively compared to IRENA's most recent 2023 country-level capacity
estimates.","['cs.LG', 'cs.CV']","['Caleb Robinson', 'Anthony Ortiz', 'Allen Kim', 'Rahul Dodhia', 'Andrew Zolli', 'Shivaprakash K Nagaraju', 'James Oakleaf', 'Joe Kiesecker', 'Juan M. Lavista Ferres']",2025-03-19,2025-03-19
2503.14858v1,1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities,"Scaling up self-supervised learning has driven breakthroughs in language and
vision, yet comparable progress has remained elusive in reinforcement learning
(RL). In this paper, we study building blocks for self-supervised RL that
unlock substantial improvements in scalability, with network depth serving as a
critical factor. Whereas most RL papers in recent years have relied on shallow
architectures (around 2 - 5 layers), we demonstrate that increasing the depth
up to 1024 layers can significantly boost performance. Our experiments are
conducted in an unsupervised goal-conditioned setting, where no demonstrations
or rewards are provided, so an agent must explore (from scratch) and learn how
to maximize the likelihood of reaching commanded goals. Evaluated on simulated
locomotion and manipulation tasks, our approach increases performance by
$2\times$ - $50\times$. Increasing the model depth not only increases success
rates but also qualitatively changes the behaviors learned.","['cs.LG', 'cs.AI']","['Kevin Wang', 'Ishaan Javali', 'Michał Bortkiewicz', 'Tomasz Trzciński', 'Benjamin Eysenbach']",2025-03-19,2025-03-19
2503.16543v1,Comprehensive Review of Reinforcement Learning for Medical Ultrasound Imaging,"Medical Ultrasound (US) imaging has seen increasing demands over the past
years, becoming one of the most preferred imaging modalities in clinical
practice due to its affordability, portability, and real-time capabilities.
However, it faces several challenges that limit its applicability, such as
operator dependency, variability in interpretation, and limited resolution,
which are amplified by the low availability of trained experts. This calls for
the need of autonomous systems that are capable of reducing the dependency on
humans for increased efficiency and throughput. Reinforcement Learning (RL)
comes as a rapidly advancing field under Artificial Intelligence (AI) that
allows the development of autonomous and intelligent agents that are capable of
executing complex tasks through rewarded interactions with their environments.
Existing surveys on advancements in the US scanning domain predominantly focus
on partially autonomous solutions leveraging AI for scanning guidance, organ
identification, plane recognition, and diagnosis. However, none of these
surveys explore the intersection between the stages of the US process and the
recent advancements in RL solutions. To bridge this gap, this review proposes a
comprehensive taxonomy that integrates the stages of the US process with the RL
development pipeline. This taxonomy not only highlights recent RL advancements
in the US domain but also identifies unresolved challenges crucial for
achieving fully autonomous US systems. This work aims to offer a thorough
review of current research efforts, highlighting the potential of RL in
building autonomous US solutions while identifying limitations and
opportunities for further advancements in this field.","['eess.IV', 'cs.CV']","['Hanae Elmekki', 'Saidul Islam', 'Ahmed Alagha', 'Hani Sami', 'Amanda Spilkin', 'Ehsan Zakeri', 'Antonela Mariel Zanuttini', 'Jamal Bentahar', 'Lyes Kadem', 'Wen-Fang Xie', 'Philippe Pibarot', 'Rabeb Mizouni', 'Hadi Otrok', 'Shakti Singh', 'Azzam Mourad']",2025-03-19,2025-03-19
2503.14853v1,Unlocking the Capabilities of Vision-Language Models for Generalizable and Explainable Deepfake Detection,"Current vision-language models (VLMs) have demonstrated remarkable
capabilities in understanding multimodal data, but their potential remains
underexplored for deepfake detection due to the misaligned of their knowledge
and forensics patterns. To this end, we present a novel paradigm that unlocks
VLMs' potential capabilities through three components: (1) A knowledge-guided
forgery adaptation module that aligns VLM's semantic space with forensic
features through contrastive learning with external manipulation knowledge; (2)
A multi-modal prompt tuning framework that jointly optimizes visual-textual
embeddings for both localization and explainability; (3) An iterative
refinement strategy enabling multi-turn dialog for evidence-based reasoning.
Our framework includes a VLM-based Knowledge-guided Forgery Detector (KFD), a
VLM image encoder, and a Large Language Model (LLM). The VLM image encoder
extracts visual prompt embeddings from images, while the LLM receives visual
and question prompt embeddings for inference. The KFD is used to calculate
correlations between image features and pristine/deepfake class embeddings,
enabling forgery classification and localization. The outputs from these
components are used to construct forgery prompt embeddings. Finally, we feed
these prompt embeddings into the LLM to generate textual detection responses to
assist judgment. Extensive experiments on multiple benchmarks, including FF++,
CDF2, DFD, DFDCP, and DFDC, demonstrate that our scheme surpasses
state-of-the-art methods in generalization performance, while also supporting
multi-turn dialogue capabilities.",['cs.CV'],"['Peipeng Yu', 'Jianwei Fei', 'Hui Gao', 'Xuan Feng', 'Zhihua Xia', 'Chip Hong Chang']",2025-03-19,2025-03-19
2503.15563v1,Dynamic Power Flow Analysis and Fault Characteristics: A Graph Attention Neural Network,"We propose the joint graph attention neural network (GAT), clustering with
adaptive neighbors (CAN) and probabilistic graphical model for dynamic power
flow analysis and fault characteristics. In fact, computational efficiency is
the main focus to enhance, whilst we ensure the performance accuracy at the
accepted level. Note that Machine Learning (ML) based schemes have a
requirement of sufficient labeled data during training, which is not easily
satisfied in practical applications. Also, there are unknown data due to new
arrived measurements or incompatible smart devices in complex smart grid
systems. These problems would be resolved by our proposed GAT based framework,
which models the label dependency between the network data and learns object
representations such that it could achieve the semi-supervised fault diagnosis.
To create the joint label dependency, we develop the graph construction from
the raw acquired signals by using CAN. Next, we develop the probabilistic
graphical model of Markov random field for graph representation, which supports
for the GAT based framework. We then evaluate the proposed framework in the
use-case application in smart grid and make a fair comparison to the existing
methods.",['cs.LG'],"['Tan Le', 'Van Le']",2025-03-19,2025-03-19
2503.14849v1,LogLLaMA: Transformer-based log anomaly detection with LLaMA,"Log anomaly detection refers to the task that distinguishes the anomalous log
messages from normal log messages. Transformer-based large language models
(LLMs) are becoming popular for log anomaly detection because of their superb
ability to understand complex and long language patterns. In this paper, we
propose LogLLaMA, a novel framework that leverages LLaMA2. LogLLaMA is first
finetuned on normal log messages from three large-scale datasets to learn their
patterns. After finetuning, the model is capable of generating successive log
messages given previous log messages. Our generative model is further trained
to identify anomalous log messages using reinforcement learning (RL). The
experimental results show that LogLLaMA outperforms the state-of-the-art
approaches for anomaly detection on BGL, Thunderbird, and HDFS datasets.","['cs.LG', 'cs.CL']","['Zhuoyi Yang', 'Ian G. Harris']",2025-03-19,2025-03-19
2503.14847v1,"Project Jenkins: Turning Monkey Neural Data into Robotic Arm Movement, and Back","Project Jenkins explores how neural activity in the brain can be decoded into
robotic movement and, conversely, how movement patterns can be used to generate
synthetic neural data. Using real neural data recorded from motor and premotor
cortex areas of a macaque monkey named Jenkins, we develop models for decoding
(converting brain signals into robotic arm movements) and encoding (simulating
brain activity corresponding to a given movement). For the interface between
the brain simulation and the physical world, we utilized Koch v1.1 leader and
follower robotic arms. We developed an interactive web console that allows
users to generate synthetic brain data from joystick movements in real time.
Our results are a step towards brain-controlled robotics, prosthetics, and
enhancing normal motor function. By accurately modeling brain activity, we take
a step toward flexible brain-computer interfaces that generalize beyond
predefined movements. To support the research community, we provide open source
tools for both synthetic data generation and neural decoding, fostering
reproducibility and accelerating progress. The project is available at
https://www.808robots.com/projects/jenkins","['cs.RO', 'cs.AI', 'eess.SP', 'q-bio.NC']","['Andrii Zahorodnii', 'Dima Yanovsky']",2025-03-19,2025-03-19
2503.14845v1,ClimateGS: Real-Time Climate Simulation with 3D Gaussian Style Transfer,"Adverse climate conditions pose significant challenges for autonomous
systems, demanding reliable perception and decision-making across diverse
environments. To better simulate these conditions, physically-based NeRF
rendering methods have been explored for their ability to generate realistic
scene representations. However, these methods suffer from slow rendering speeds
and long preprocessing times, making them impractical for real-time testing and
user interaction. This paper presents ClimateGS, a novel framework integrating
3D Gaussian representations with physical simulation to enable real-time
climate effects rendering. The novelty of this work is threefold: 1) developing
a linear transformation for 3D Gaussian photorealistic style transfer, enabling
direct modification of spherical harmonics across bands for efficient and
consistent style adaptation; 2) developing a joint training strategy for 3D
style transfer, combining supervised and self-supervised learning to accelerate
convergence while preserving original scene details; 3) developing a real-time
rendering method for climate simulation, integrating physics-based effects with
3D Gaussian to achieve efficient and realistic rendering. We evaluate ClimateGS
on MipNeRF360 and Tanks and Temples, demonstrating real-time rendering with
comparable or superior visual quality to SOTA 2D/3D methods, making it suitable
for interactive applications.","['cs.GR', 'cs.CV']","['Yuezhen Xie', 'Meiying Zhang', 'Qi Hao']",2025-03-19,2025-03-19
2503.14837v1,SemanticFlow: A Self-Supervised Framework for Joint Scene Flow Prediction and Instance Segmentation in Dynamic Environments,"Accurate perception of dynamic traffic scenes is crucial for high-level
autonomous driving systems, requiring robust object motion estimation and
instance segmentation. However, traditional methods often treat them as
separate tasks, leading to suboptimal performance, spatio-temporal
inconsistencies, and inefficiency in complex scenarios due to the absence of
information sharing. This paper proposes a multi-task SemanticFlow framework to
simultaneously predict scene flow and instance segmentation of full-resolution
point clouds. The novelty of this work is threefold: 1) developing a
coarse-to-fine prediction based multi-task scheme, where an initial coarse
segmentation of static backgrounds and dynamic objects is used to provide
contextual information for refining motion and semantic information through a
shared feature processing module; 2) developing a set of loss functions to
enhance the performance of scene flow estimation and instance segmentation,
while can help ensure spatial and temporal consistency of both static and
dynamic objects within traffic scenes; 3) developing a self-supervised learning
scheme, which utilizes coarse segmentation to detect rigid objects and compute
their transformation matrices between sequential frames, enabling the
generation of self-supervised labels. The proposed framework is validated on
the Argoverse and Waymo datasets, demonstrating superior performance in
instance segmentation accuracy, scene flow estimation, and computational
efficiency, establishing a new benchmark for self-supervised methods in dynamic
scene understanding.","['cs.CV', 'cs.RO']","['Yinqi Chen', 'Meiying Zhang', 'Qi Hao', 'Guang Zhou']",2025-03-19,2025-03-19
2503.14836v1,On the Robustness Tradeoff in Fine-Tuning,"Fine-tuning has become the standard practice for adapting pre-trained
(upstream) models to downstream tasks. However, the impact on model robustness
is not well understood. In this work, we characterize the robustness-accuracy
trade-off in fine-tuning. We evaluate the robustness and accuracy of fine-tuned
models over 6 benchmark datasets and 7 different fine-tuning strategies. We
observe a consistent trade-off between adversarial robustness and accuracy.
Peripheral updates such as BitFit are more effective for simple tasks--over 75%
above the average measured with area under the Pareto frontiers on CIFAR-10 and
CIFAR-100. In contrast, fine-tuning information-heavy layers, such as attention
layers via Compacter, achieves a better Pareto frontier on more complex
tasks--57.5% and 34.6% above the average on Caltech-256 and CUB-200,
respectively. Lastly, we observe that robustness of fine-tuning against
out-of-distribution data closely tracks accuracy. These insights emphasize the
need for robustness-aware fine-tuning to ensure reliable real-world
deployments.","['cs.LG', 'cs.CV']","['Kunyang Li', 'Jean-Charles Noirot Ferrand', 'Ryan Sheatsley', 'Blaine Hoak', 'Yohan Beugin', 'Eric Pauley', 'Patrick McDaniel']",2025-03-19,2025-03-19
2503.14833v1,Curiosity-Diffuser: Curiosity Guide Diffusion Models for Reliability,"One of the bottlenecks in robotic intelligence is the instability of neural
network models, which, unlike control models, lack a well-defined convergence
domain and stability. This leads to risks when applying intelligence in the
physical world. Specifically, imitation policy based on neural network may
generate hallucinations, leading to inaccurate behaviors that impact the safety
of real-world applications. To address this issue, this paper proposes the
Curiosity-Diffuser, aimed at guiding the conditional diffusion model to
generate trajectories with lower curiosity, thereby improving the reliability
of policy. The core idea is to use a Random Network Distillation (RND)
curiosity module to assess whether the model's behavior aligns with the
training data, and then minimize curiosity by classifier guidance diffusion to
reduce overgeneralization during inference. Additionally, we propose a
computationally efficient metric for evaluating the reliability of the policy,
measuring the similarity between the generated behaviors and the training
dataset, to facilitate research about reliability learning. Finally, simulation
verify the effectiveness and applicability of the proposed method to a variety
of scenarios, showing that Curiosity-Diffuser significantly improves task
performance and produces behaviors that are more similar to the training data.
The code for this work is available at: github.com/CarlDegio/Curiosity-Diffuser","['cs.RO', 'cs.AI', 'cs.LG']","['Zihao Liu', 'Xing Liu', 'Yizhai Zhang', 'Zhengxiong Liu', 'Panfeng Huang']",2025-03-19,2025-03-19
2503.14832v1,H2ST: Hierarchical Two-Sample Tests for Continual Out-of-Distribution Detection,"Task Incremental Learning (TIL) is a specialized form of Continual Learning
(CL) in which a model incrementally learns from non-stationary data streams.
Existing TIL methodologies operate under the closed-world assumption, presuming
that incoming data remains in-distribution (ID). However, in an open-world
setting, incoming samples may originate from out-of-distribution (OOD) sources,
with their task identities inherently unknown. Continually detecting OOD
samples presents several challenges for current OOD detection methods: reliance
on model outputs leads to excessive dependence on model performance, selecting
suitable thresholds is difficult, hindering real-world deployment, and binary
ID/OOD classification fails to provide task-level identification. To address
these issues, we propose a novel continual OOD detection method called the
Hierarchical Two-sample Tests (H2ST). H2ST eliminates the need for threshold
selection through hypothesis testing and utilizes feature maps to better
exploit model capabilities without excessive dependence on model performance.
The proposed hierarchical architecture enables task-level detection with
superior performance and lower overhead compared to non-hierarchical classifier
two-sample tests. Extensive experiments and analysis validate the effectiveness
of H2ST in open-world TIL scenarios and its superiority to the existing
methods. Code is available at
\href{https://github.com/YuhangLiuu/H2ST}{https://github.com/YuhangLiuu/H2ST}.","['cs.CV', 'cs.LG']","['Yuhang Liu', 'Wenjie Zhao', 'Yunhui Guo']",2025-03-19,2025-03-19
2503.14831v1,Robust Transmission of Punctured Text with Large Language Model-based Recovery,"With the recent advancements in deep learning, semantic communication which
transmits only task-oriented features, has rapidly emerged. However, since
feature extraction relies on learning-based models, its performance
fundamentally depends on the training dataset or tasks. For practical
scenarios, it is essential to design a model that demonstrates robust
performance regardless of dataset or tasks. In this correspondence, we propose
a novel text transmission model that selects and transmits only a few
characters and recovers the missing characters at the receiver using a large
language model (LLM). Additionally, we propose a novel importance character
extractor (ICE), which selects transmitted characters to enhance LLM recovery
performance. Simulations demonstrate that the proposed filter selection by ICE
outperforms random filter selection, which selects transmitted characters
randomly. Moreover, the proposed model exhibits robust performance across
different datasets and tasks and outperforms traditional bit-based
communication in low signal-to-noise ratio conditions.","['eess.SP', 'cs.LG']","['Sojeong Park', 'Hyeonho Noh', 'Hyun Jong Yang']",2025-03-19,2025-03-19
2503.14830v1,Decompositional Neural Scene Reconstruction with Generative Diffusion Prior,"Decompositional reconstruction of 3D scenes, with complete shapes and
detailed texture of all objects within, is intriguing for downstream
applications but remains challenging, particularly with sparse views as input.
Recent approaches incorporate semantic or geometric regularization to address
this issue, but they suffer significant degradation in underconstrained areas
and fail to recover occluded regions. We argue that the key to solving this
problem lies in supplementing missing information for these areas. To this end,
we propose DP-Recon, which employs diffusion priors in the form of Score
Distillation Sampling (SDS) to optimize the neural representation of each
individual object under novel views. This provides additional information for
the underconstrained areas, but directly incorporating diffusion prior raises
potential conflicts between the reconstruction and generative guidance.
Therefore, we further introduce a visibility-guided approach to dynamically
adjust the per-pixel SDS loss weights. Together these components enhance both
geometry and appearance recovery while remaining faithful to input images.
Extensive experiments across Replica and ScanNet++ demonstrate that our method
significantly outperforms SOTA methods. Notably, it achieves better object
reconstruction under 10 views than the baselines under 100 views. Our method
enables seamless text-based editing for geometry and appearance through SDS
optimization and produces decomposed object meshes with detailed UV maps that
support photorealistic Visual effects (VFX) editing. The project page is
available at https://dp-recon.github.io/.",['cs.CV'],"['Junfeng Ni', 'Yu Liu', 'Ruijie Lu', 'Zirui Zhou', 'Song-Chun Zhu', 'Yixin Chen', 'Siyuan Huang']",2025-03-19,2025-03-19
2503.14828v1,"The CLEF-2025 CheckThat! Lab: Subjectivity, Fact-Checking, Claim Normalization, and Retrieval","The CheckThat! lab aims to advance the development of innovative technologies
designed to identify and counteract online disinformation and manipulation
efforts across various languages and platforms. The first five editions focused
on key tasks in the information verification pipeline, including
check-worthiness, evidence retrieval and pairing, and verification. Since the
2023 edition, the lab has expanded its scope to address auxiliary tasks that
support research and decision-making in verification. In the 2025 edition, the
lab revisits core verification tasks while also considering auxiliary
challenges. Task 1 focuses on the identification of subjectivity (a follow-up
from CheckThat! 2024), Task 2 addresses claim normalization, Task 3 targets
fact-checking numerical claims, and Task 4 explores scientific web discourse
processing. These tasks present challenging classification and retrieval
problems at both the document and span levels, including multilingual settings.","['cs.CL', 'cs.AI', '68T50', 'I.2; I.2.7']","['Firoj Alam', 'Julia Maria Struß', 'Tanmoy Chakraborty', 'Stefan Dietze', 'Salim Hafid', 'Katerina Korre', 'Arianna Muti', 'Preslav Nakov', 'Federico Ruggeri', 'Sebastian Schellhammer', 'Vinay Setty', 'Megha Sundriyal', 'Konstantin Todorov', 'Venktesh V']",2025-03-19,2025-03-19
2503.14827v1,MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models,"Multimodal foundation models (MMFMs) play a crucial role in various
applications, including autonomous driving, healthcare, and virtual assistants.
However, several studies have revealed vulnerabilities in these models, such as
generating unsafe content by text-to-image models. Existing benchmarks on
multimodal models either predominantly assess the helpfulness of these models,
or only focus on limited perspectives such as fairness and privacy. In this
paper, we present the first unified platform, MMDT (Multimodal DecodingTrust),
designed to provide a comprehensive safety and trustworthiness evaluation for
MMFMs. Our platform assesses models from multiple perspectives, including
safety, hallucination, fairness/bias, privacy, adversarial robustness, and
out-of-distribution (OOD) generalization. We have designed various evaluation
scenarios and red teaming algorithms under different tasks for each perspective
to generate challenging data, forming a high-quality benchmark. We evaluate a
range of multimodal models using MMDT, and our findings reveal a series of
vulnerabilities and areas for improvement across these perspectives. This work
introduces the first comprehensive and unique safety and trustworthiness
evaluation platform for MMFMs, paving the way for developing safer and more
reliable MMFMs and systems. Our platform and benchmark are available at
https://mmdecodingtrust.github.io/.","['cs.CL', 'cs.AI', 'cs.CR']","['Chejian Xu', 'Jiawei Zhang', 'Zhaorun Chen', 'Chulin Xie', 'Mintong Kang', 'Yujin Potter', 'Zhun Wang', 'Zhuowen Yuan', 'Alexander Xiong', 'Zidi Xiong', 'Chenhui Zhang', 'Lingzhi Yuan', 'Yi Zeng', 'Peiyang Xu', 'Chengquan Guo', 'Andy Zhou', 'Jeffrey Ziwei Tan', 'Xuandong Zhao', 'Francesco Pinto', 'Zhen Xiang', 'Yu Gai', 'Zinan Lin', 'Dan Hendrycks', 'Bo Li', 'Dawn Song']",2025-03-19,2025-03-19
2503.16542v1,Defending Against Gradient Inversion Attacks for Biomedical Images via Learnable Data Perturbation,"The increasing need for sharing healthcare data and collaborating on clinical
research has raised privacy concerns. Health information leakage due to
malicious attacks can lead to serious problems such as misdiagnoses and patient
identification issues. Privacy-preserving machine learning (PPML) and
privacy-enhancing technologies, particularly federated learning (FL), have
emerged in recent years as innovative solutions to balance privacy protection
with data utility; however, they also suffer from inherent privacy
vulnerabilities. Gradient inversion attacks constitute major threats to data
sharing in federated learning. Researchers have proposed many defenses against
gradient inversion attacks. However, current defense methods for healthcare
data lack generalizability, i.e., existing solutions may not be applicable to
data from a broader range of populations. In addition, most existing defense
methods are tested using non-healthcare data, which raises concerns about their
applicability to real-world healthcare systems. In this study, we present a
defense against gradient inversion attacks in federated learning. We achieve
this using latent data perturbation and minimax optimization, utilizing both
general and medical image datasets. Our method is compared to two baselines,
and the results show that our approach can outperform the baselines with a
reduction of 12.5% in the attacker's accuracy in classifying reconstructed
images. The proposed method also yields an increase of over 12.4% in Mean
Squared Error (MSE) between the original and reconstructed images at the same
level of model utility of around 90% client classification accuracy. The
results suggest the potential of a generalizable defense for healthcare data.","['cs.CV', 'cs.LG']","['Shiyi Jiang', 'Farshad Firouzi', 'Krishnendu Chakrabarty']",2025-03-19,2025-03-19
2503.16541v1,Poly-FEVER: A Multilingual Fact Verification Benchmark for Hallucination Detection in Large Language Models,"Hallucinations in generative AI, particularly in Large Language Models
(LLMs), pose a significant challenge to the reliability of multilingual
applications. Existing benchmarks for hallucination detection focus primarily
on English and a few widely spoken languages, lacking the breadth to assess
inconsistencies in model performance across diverse linguistic contexts. To
address this gap, we introduce Poly-FEVER, a large-scale multilingual fact
verification benchmark specifically designed for evaluating hallucination
detection in LLMs. Poly-FEVER comprises 77,973 labeled factual claims spanning
11 languages, sourced from FEVER, Climate-FEVER, and SciFact. It provides the
first large-scale dataset tailored for analyzing hallucination patterns across
languages, enabling systematic evaluation of LLMs such as ChatGPT and the LLaMA
series. Our analysis reveals how topic distribution and web resource
availability influence hallucination frequency, uncovering language-specific
biases that impact model accuracy. By offering a multilingual benchmark for
fact verification, Poly-FEVER facilitates cross-linguistic comparisons of
hallucination detection and contributes to the development of more reliable,
language-inclusive AI systems. The dataset is publicly available to advance
research in responsible AI, fact-checking methodologies, and multilingual NLP,
promoting greater transparency and robustness in LLM performance. The proposed
Poly-FEVER is available at:
https://huggingface.co/datasets/HanzhiZhang/Poly-FEVER.",['cs.CL'],"['Hanzhi Zhang', 'Sumera Anjum', 'Heng Fan', 'Weijian Zheng', 'Yan Huang', 'Yunhe Feng']",2025-03-19,2025-03-19
2503.14824v1,Prototype Perturbation for Relaxing Alignment Constraints in Backward-Compatible Learning,"The traditional paradigm to update retrieval models requires re-computing the
embeddings of the gallery data, a time-consuming and computationally intensive
process known as backfilling. To circumvent backfilling, Backward-Compatible
Learning (BCL) has been widely explored, which aims to train a new model
compatible with the old one. Many previous works focus on effectively aligning
the embeddings of the new model with those of the old one to enhance the
backward-compatibility. Nevertheless, such strong alignment constraints would
compromise the discriminative ability of the new model, particularly when
different classes are closely clustered and hard to distinguish in the old
feature space. To address this issue, we propose to relax the constraints by
introducing perturbations to the old feature prototypes. This allows us to
align the new feature space with a pseudo-old feature space defined by these
perturbed prototypes, thereby preserving the discriminative ability of the new
model in backward-compatible learning. We have developed two approaches for
calculating the perturbations: Neighbor-Driven Prototype Perturbation (NDPP)
and Optimization-Driven Prototype Perturbation (ODPP). Particularly, they take
into account the feature distributions of not only the old but also the new
models to obtain proper perturbations along with new model updating. Extensive
experiments on the landmark and commodity datasets demonstrate that our
approaches perform favorably against state-of-the-art BCL algorithms.",['cs.CV'],"['Zikun Zhou', 'Yushuai Sun', 'Wenjie Pei', 'Xin Li', 'Yaowei Wang']",2025-03-19,2025-03-19
2503.14813v1,Scaled Supervision is an Implicit Lipschitz Regularizer,"In modern social media, recommender systems (RecSys) rely on the
click-through rate (CTR) as the standard metric to evaluate user engagement.
CTR prediction is traditionally framed as a binary classification task to
predict whether a user will interact with a given item. However, this approach
overlooks the complexity of real-world social modeling, where the user, item,
and their interactive features change dynamically in fast-paced online
environments. This dynamic nature often leads to model instability, reflected
in overfitting short-term fluctuations rather than higher-level interactive
patterns. While overfitting calls for more scaled and refined supervisions,
current solutions often rely on binary labels that overly simplify fine-grained
user preferences through the thresholding process, which significantly reduces
the richness of the supervision. Therefore, we aim to alleviate the overfitting
problem by increasing the supervision bandwidth in CTR training. Specifically,
(i) theoretically, we formulate the impact of fine-grained preferences on model
stability as a Lipschitz constrain; (ii) empirically, we discover that scaling
the supervision bandwidth can act as an implicit Lipschitz regularizer, stably
optimizing existing CTR models to achieve better generalizability. Extensive
experiments show that this scaled supervision significantly and consistently
improves the optimization process and the performance of existing CTR models,
even without the need for additional hyperparameter tuning.","['cs.LG', 'cs.IR']","['Zhongyu Ouyang', 'Chunhui Zhang', 'Yaning Jia', 'Soroush Vosoughi']",2025-03-19,2025-03-19
2503.14809v1,Learning with Expert Abstractions for Efficient Multi-Task Continuous Control,"Decision-making in complex, continuous multi-task environments is often
hindered by the difficulty of obtaining accurate models for planning and the
inefficiency of learning purely from trial and error. While precise environment
dynamics may be hard to specify, human experts can often provide high-fidelity
abstractions that capture the essential high-level structure of a task and user
preferences in the target environment. Existing hierarchical approaches often
target discrete settings and do not generalize across tasks. We propose a
hierarchical reinforcement learning approach that addresses these limitations
by dynamically planning over the expert-specified abstraction to generate
subgoals to learn a goal-conditioned policy. To overcome the challenges of
learning under sparse rewards, we shape the reward based on the optimal state
value in the abstract model. This structured decision-making process enhances
sample efficiency and facilitates zero-shot generalization. Our empirical
evaluation on a suite of procedurally generated continuous control environments
demonstrates that our approach outperforms existing hierarchical reinforcement
learning methods in terms of sample efficiency, task completion rate,
scalability to complex tasks, and generalization to novel scenarios.","['cs.LG', 'cs.AI']","['Jeff Jewett', 'Sandhya Saisubramanian']",2025-03-19,2025-03-19
2503.15562v1,Shap-MeD,"We present Shap-MeD, a text-to-3D object generative model specialized in the
biomedical domain. The objective of this study is to develop an assistant that
facilitates the 3D modeling of medical objects, thereby reducing development
time. 3D modeling in medicine has various applications, including surgical
procedure simulation and planning, the design of personalized prosthetic
implants, medical education, the creation of anatomical models, and the
development of research prototypes. To achieve this, we leverage Shap-e, an
open-source text-to-3D generative model developed by OpenAI, and fine-tune it
using a dataset of biomedical objects. Our model achieved a mean squared error
(MSE) of 0.089 in latent generation on the evaluation set, compared to Shap-e's
MSE of 0.147. Additionally, we conducted a qualitative evaluation, comparing
our model with others in the generation of biomedical objects. Our results
indicate that Shap-MeD demonstrates higher structural accuracy in biomedical
object generation.","['cs.GR', 'cs.CE', 'cs.CV']","['Nicolás Laverde', 'Melissa Robles', 'Johan Rodríguez']",2025-03-19,2025-03-19
2503.14800v1,Long Context Modeling with Ranked Memory-Augmented Retrieval,"Effective long-term memory management is crucial for language models handling
extended contexts. We introduce a novel framework that dynamically ranks memory
entries based on relevance. Unlike previous works, our model introduces a novel
relevance scoring and a pointwise re-ranking model for key-value embeddings,
inspired by learning-to-rank techniques in information retrieval. Enhanced
Ranked Memory Augmented Retrieval ERMAR achieves state-of-the-art results on
standard benchmarks.","['cs.IR', 'cs.AI', 'cs.LG']","['Ghadir Alselwi', 'Hao Xue', 'Shoaib Jameel', 'Basem Suleiman', 'Flora D. Salim', 'Imran Razzak']",2025-03-19,2025-03-19
2503.14799v1,Pruning-Based TinyML Optimization of Machine Learning Models for Anomaly Detection in Electric Vehicle Charging Infrastructure,"With the growing need for real-time processing on IoT devices, optimizing
machine learning (ML) models' size, latency, and computational efficiency is
essential. This paper investigates a pruning method for anomaly detection in
resource-constrained environments, specifically targeting Electric Vehicle
Charging Infrastructure (EVCI). Using the CICEVSE2024 dataset, we trained and
optimized three models-Multi-Layer Perceptron (MLP), Long Short-Term Memory
(LSTM), and XGBoost-through hyperparameter tuning with Optuna, further refining
them using SHapley Additive exPlanations (SHAP)-based feature selection (FS)
and unstructured pruning techniques. The optimized models achieved significant
reductions in model size and inference times, with only a marginal impact on
their performance. Notably, our findings indicate that, in the context of EVCI,
pruning and FS can enhance computational efficiency while retaining critical
anomaly detection capabilities.","['cs.LG', 'eess.SP']","['Fatemeh Dehrouyeh', 'Ibrahim Shaer', 'Soodeh Nikan', 'Firouz Badrkhani Ajaei', 'Abdallah Shami']",2025-03-19,2025-03-19
2503.14797v1,FACTS&EVIDENCE: An Interactive Tool for Transparent Fine-Grained Factual Verification of Machine-Generated Text,"With the widespread consumption of AI-generated content, there has been an
increased focus on developing automated tools to verify the factual accuracy of
such content. However, prior research and tools developed for fact verification
treat it as a binary classification or a linear regression problem. Although
this is a useful mechanism as part of automatic guardrails in systems, we argue
that such tools lack transparency in the prediction reasoning and diversity in
source evidence to provide a trustworthy user experience. We develop
Facts&Evidence - an interactive and transparent tool for user-driven
verification of complex text. The tool facilitates the intricate
decision-making involved in fact-verification, presenting its users a breakdown
of complex input texts to visualize the credibility of individual claims along
with an explanation of model decisions and attribution to multiple, diverse
evidence sources. Facts&Evidence aims to empower consumers of machine-generated
text and give them agency to understand, verify, selectively trust and use such
text.",['cs.CL'],"['Varich Boonsanong', 'Vidhisha Balachandran', 'Xiaochuang Han', 'Shangbin Feng', 'Lucy Lu Wang', 'Yulia Tsvetkov']",2025-03-19,2025-03-19
2503.14796v1,A New Benchmark for Online Learning with Budget-Balancing Constraints,"The adversarial Bandit with Knapsack problem is a multi-armed bandits problem
with budget constraints and adversarial rewards and costs. In each round, a
learner selects an action to take and observes the reward and cost of the
selected action. The goal is to maximize the sum of rewards while satisfying
the budget constraint. The classical benchmark to compare against is the best
fixed distribution over actions that satisfies the budget constraint in
expectation. Unlike its stochastic counterpart, where rewards and costs are
drawn from some fixed distribution (Badanidiyuru et al., 2018), the adversarial
BwK problem does not admit a no-regret algorithm for every problem instance due
to the ""spend-or-save"" dilemma (Immorlica et al., 2022).
  A key problem left open by existing works is whether there exists a weaker
but still meaningful benchmark to compare against such that no-regret learning
is still possible. In this work, we present a new benchmark to compare against,
motivated both by real-world applications such as autobidding and by its
underlying mathematical structure. The benchmark is based on the Earth Mover's
Distance (EMD), and we show that sublinear regret is attainable against any
strategy whose spending pattern is within EMD $o(T^2)$ of any sub-pacing
spending pattern.
  As a special case, we obtain results against the ""pacing over windows""
benchmark, where we partition time into disjoint windows of size $w$ and allow
the benchmark strategies to choose a different distribution over actions for
each window while satisfying a pacing budget constraint. Against this
benchmark, our algorithm obtains a regret bound of
$\tilde{O}(T/\sqrt{w}+\sqrt{wT})$. We also show a matching lower bound, proving
the optimality of our algorithm in this important special case. In addition, we
provide further evidence of the necessity of the EMD condition for obtaining a
sublinear regret.","['cs.LG', 'cs.GT']","['Mark Braverman', 'Jingyi Liu', 'Jieming Mao', 'Jon Schneider', 'Eric Xue']",2025-03-19,2025-03-19
2503.14795v1,The Hardness of Validating Observational Studies with Experimental Data,"Observational data is often readily available in large quantities, but can
lead to biased causal effect estimates due to the presence of unobserved
confounding. Recent works attempt to remove this bias by supplementing
observational data with experimental data, which, when available, is typically
on a smaller scale due to the time and cost involved in running a randomised
controlled trial. In this work, we prove a theorem that places fundamental
limits on this ``best of both worlds'' approach. Using the framework of
impossible inference, we show that although it is possible to use experimental
data to \emph{falsify} causal effect estimates from observational data, in
general it is not possible to \emph{validate} such estimates. Our theorem
proves that while experimental data can be used to detect bias in observational
studies, without additional assumptions on the smoothness of the correction
function, it can not be used to remove it. We provide a practical example of
such an assumption, developing a novel Gaussian Process based approach to
construct intervals which contain the true treatment effect with high
probability, both inside and outside of the support of the experimental data.
We demonstrate our methodology on both simulated and semi-synthetic datasets
and make the \href{https://github.com/Jakefawkes/Obs_and_exp_data}{code
available}.","['stat.ML', 'cs.LG', 'stat.ME']","['Jake Fawkes', ""Michael O'Riordan"", 'Athanasios Vlontzos', 'Oriol Corcoll', 'Ciarán Mark Gilligan-Lee']",2025-03-19,2025-03-19
2503.14786v1,SketchSplat: 3D Edge Reconstruction via Differentiable Multi-view Sketch Splatting,"Edges are one of the most basic parametric primitives to describe structural
information in 3D. In this paper, we study parametric 3D edge reconstruction
from calibrated multi-view images. Previous methods usually reconstruct a 3D
edge point set from multi-view 2D edge images, and then fit 3D edges to the
point set. However, noise in the point set may cause gaps among fitted edges,
and the recovered edges may not align with input multi-view images since the
edge fitting depends only on the reconstructed 3D point set. To mitigate these
problems, we propose SketchSplat, a method to reconstruct accurate, complete,
and compact 3D edges via differentiable multi-view sketch splatting. We
represent 3D edges as sketches, which are parametric lines and curves defined
by attributes including control points, scales, and opacity. During edge
reconstruction, we iteratively sample Gaussian points from a set of sketches
and rasterize the Gaussians onto 2D edge images. Then the gradient of the image
error with respect to the input 2D edge images can be back-propagated to
optimize the sketch attributes. Our method bridges 2D edge images and 3D edges
in a differentiable manner, which ensures that 3D edges align well with 2D
images and leads to accurate and complete results. We also propose a series of
adaptive topological operations and apply them along with the sketch
optimization. The topological operations help reduce the number of sketches
required while ensuring high accuracy, yielding a more compact reconstruction.
Finally, we contribute an accurate 2D edge detector that improves the
performance of both ours and existing methods. Experiments show that our method
achieves state-of-the-art accuracy, completeness, and compactness on a
benchmark CAD dataset.",['cs.CV'],"['Haiyang Ying', 'Matthias Zwicker']",2025-03-18,2025-03-18
2503.14785v1,SEEK: Self-adaptive Explainable Kernel For Nonstationary Gaussian Processes,"Gaussian processes (GPs) are powerful probabilistic models that define
flexible priors over functions, offering strong interpretability and
uncertainty quantification. However, GP models often rely on simple, stationary
kernels which can lead to suboptimal predictions and miscalibrated uncertainty
estimates, especially in nonstationary real-world applications. In this paper,
we introduce SEEK, a novel class of learnable kernels to model complex,
nonstationary functions via GPs. Inspired by artificial neurons, SEEK is
derived from first principles to ensure symmetry and positive
semi-definiteness, key properties of valid kernels. The proposed method
achieves flexible and adaptive nonstationarity by learning a mapping from a set
of base kernels. Compared to existing techniques, our approach is more
interpretable and much less prone to overfitting. We conduct comprehensive
sensitivity analyses and comparative studies to demonstrate that our approach
is not robust to only many of its design choices, but also outperforms existing
stationary/nonstationary kernels in both mean prediction accuracy and
uncertainty quantification.",['cs.LG'],"['Nima Negarandeh', 'Carlos Mora', 'Ramin Bostanabad']",2025-03-18,2025-03-18
2503.14783v1,RAT: Boosting Misclassification Detection Ability without Extra Data,"As deep neural networks(DNN) become increasingly prevalent, particularly in
high-stakes areas such as autonomous driving and healthcare, the ability to
detect incorrect predictions of models and intervene accordingly becomes
crucial for safety. In this work, we investigate the detection of misclassified
inputs for image classification models from the lens of adversarial
perturbation: we propose to use robust radius (a.k.a. input-space margin) as a
confidence metric and design two efficient estimation algorithms, RR-BS and
RR-Fast, for misclassification detection. Furthermore, we design a training
method called Radius Aware Training (RAT) to boost models' ability to identify
mistakes. Extensive experiments show our method could achieve up to 29.3%
reduction on AURC and 21.62% reduction in FPR@95TPR, compared with previous
methods.","['cs.CV', 'cs.AI', 'cs.LG']","['Ge Yan', 'Tsui-Wei Weng']",2025-03-18,2025-03-18
2503.14781v1,"Fake Runs, Real Fixes -- Analyzing xPU Performance Through Simulation","As models become larger, ML accelerators are a scarce resource whose
performance must be continually optimized to improve efficiency. Existing
performance analysis tools are coarse grained, and fail to capture model
performance at the machine-code level. In addition, these tools often do not
provide specific recommendations for optimizations. We present xPU-Shark, a
fine-grained methodology for analyzing ML models at the machine-code level that
provides actionable optimization suggestions. Our core insight is to use a
hardware-level simulator, an artifact of the hardware design process that we
can re-purpose for performance analysis. xPU-Shark captures traces from
production deployments running on accelerators and replays them in a modified
microarchitecture simulator to gain low-level insights into the model's
performance. We implement xPU-Shark for our in-house accelerator and used it to
analyze the performance of several of our production LLMs, revealing several
previously-unknown microarchitecture inefficiencies. Leveraging these insights,
we optimize a common communication collective by up to 15% and reduce token
generation latency by up to 4.1%.","['cs.PF', 'cs.DC', 'cs.LG']","['Ioannis Zarkadas', 'Amanda Tomlinson', 'Asaf Cidon', 'Baris Kasikci', 'Ofir Weisse']",2025-03-18,2025-03-18
2503.14779v1,Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution,"Single Image Super-Resolution (SISR) aims to reconstruct high-resolution (HR)
images from low-resolution (LR) inputs. Deep learning, especially Convolutional
Neural Networks (CNNs), has advanced SISR. However, increasing network depth
increases parameters, and memory usage, and slows training, which is
problematic for resource-limited devices. To address this, lightweight models
are developed to balance accuracy and efficiency. We propose the Involution &
BSConv Multi-Depth Distillation Network (IBMDN), combining Involution & BSConv
Multi-Depth Distillation Block (IBMDB) and the Contrast and High-Frequency
Attention Block (CHFAB). IBMDB integrates Involution and BSConv to balance
computational efficiency and feature extraction. CHFAB enhances high-frequency
details for better visual quality. IBMDB is compatible with other SISR
architectures and reduces complexity, improving evaluation metrics like PSNR
and SSIM. In transformer-based models, IBMDB reduces memory usage while
improving feature extraction. In GANs, it enhances perceptual quality,
balancing pixel-level accuracy with perceptual details. Our experiments show
that the method achieves high accuracy with minimal computational cost. The
code is available at GitHub.","['eess.IV', 'cs.AI', 'cs.CV']","['Akram Khatami-Rizi', 'Ahmad Mahmoudi-Aznaveh']",2025-03-18,2025-03-18
2503.14774v1,Revisiting Image Fusion for Multi-Illuminant White-Balance Correction,"White balance (WB) correction in scenes with multiple illuminants remains a
persistent challenge in computer vision. Recent methods explored fusion-based
approaches, where a neural network linearly blends multiple sRGB versions of an
input image, each processed with predefined WB presets. However, we demonstrate
that these methods are suboptimal for common multi-illuminant scenarios.
Additionally, existing fusion-based methods rely on sRGB WB datasets lacking
dedicated multi-illuminant images, limiting both training and evaluation. To
address these challenges, we introduce two key contributions. First, we propose
an efficient transformer-based model that effectively captures spatial
dependencies across sRGB WB presets, substantially improving upon linear fusion
techniques. Second, we introduce a large-scale multi-illuminant dataset
comprising over 16,000 sRGB images rendered with five different WB settings,
along with WB-corrected images. Our method achieves up to 100\% improvement
over existing techniques on our new multi-illuminant image fusion dataset.",['cs.CV'],"['David Serrano-Lozano', 'Aditya Arora', 'Luis Herranz', 'Konstantinos G. Derpanis', 'Michael S. Brown', 'Javier Vazquez-Corral']",2025-03-18,2025-03-18
2503.15561v1,Localized Physics-informed Gaussian Processes with Curriculum Training for Topology Optimization,"We introduce a simultaneous and meshfree topology optimization (TO) framework
based on physics-informed Gaussian processes (GPs). Our framework endows all
design and state variables via GP priors which have a shared, multi-output mean
function that is parametrized via a customized deep neural network (DNN). The
parameters of this mean function are estimated by minimizing a multi-component
loss function that depends on the performance metric, design constraints, and
the residuals on the state equations. Our TO approach yields well-defined
material interfaces and has a built-in continuation nature that promotes global
optimality. Other unique features of our approach include (1) its customized
DNN which, unlike fully connected feed-forward DNNs, has a localized learning
capacity that enables capturing intricate topologies and reducing residuals in
high gradient fields, (2) its loss function that leverages localized weights to
promote solution accuracy around interfaces, and (3) its use of curriculum
training to avoid local optimality.To demonstrate the power of our framework,
we validate it against commercial TO package COMSOL on three problems involving
dissipated power minimization in Stokes flow.",['cs.LG'],"['Amin Yousefpour', 'Shirin Hosseinmardi', 'Xiangyu Sun', 'Ramin Bostanabad']",2025-03-18,2025-03-18
2503.15560v1,Temporal Context Awareness: A Defense Framework Against Multi-turn Manipulation Attacks on Large Language Models,"Large Language Models (LLMs) are increasingly vulnerable to sophisticated
multi-turn manipulation attacks, where adversaries strategically build context
through seemingly benign conversational turns to circumvent safety measures and
elicit harmful or unauthorized responses. These attacks exploit the temporal
nature of dialogue to evade single-turn detection methods, representing a
critical security vulnerability with significant implications for real-world
deployments.
  This paper introduces the Temporal Context Awareness (TCA) framework, a novel
defense mechanism designed to address this challenge by continuously analyzing
semantic drift, cross-turn intention consistency and evolving conversational
patterns. The TCA framework integrates dynamic context embedding analysis,
cross-turn consistency verification, and progressive risk scoring to detect and
mitigate manipulation attempts effectively. Preliminary evaluations on
simulated adversarial scenarios demonstrate the framework's potential to
identify subtle manipulation patterns often missed by traditional detection
techniques, offering a much-needed layer of security for conversational AI
systems. In addition to outlining the design of TCA , we analyze diverse attack
vectors and their progression across multi-turn conversation, providing
valuable insights into adversarial tactics and their impact on LLM
vulnerabilities. Our findings underscore the pressing need for robust,
context-aware defenses in conversational AI systems and highlight TCA framework
as a promising direction for securing LLMs while preserving their utility in
legitimate applications. We make our implementation available to support
further research in this emerging area of AI security.","['cs.CR', 'cs.LG']","['Prashant Kulkarni', 'Assaf Namer']",2025-03-18,2025-03-18
2503.14760v1,Validation of Human Pose Estimation and Human Mesh Recovery for Extracting Clinically Relevant Motion Data from Videos,"This work aims to discuss the current landscape of kinematic analysis tools,
ranging from the state-of-the-art in sports biomechanics such as inertial
measurement units (IMUs) and retroreflective marker-based optical motion
capture (MoCap) to more novel approaches from the field of computing such as
human pose estimation and human mesh recovery. Primarily, this comparative
analysis aims to validate the use of marker-less MoCap techniques in a clinical
setting by showing that these marker-less techniques are within a reasonable
range for kinematics analysis compared to the more cumbersome and less portable
state-of-the-art tools. Not only does marker-less motion capture using human
pose estimation produce results in-line with the results of both the IMU and
MoCap kinematics but also benefits from a reduced set-up time and reduced
practical knowledge and expertise to set up. Overall, while there is still room
for improvement when it comes to the quality of the data produced, we believe
that this compromise is within the room of error that these low-speed actions
that are used in small clinical tests.",['cs.CV'],"['Kai Armstrong', 'Alexander Rodrigues', 'Alexander P. Willmott', 'Lei Zhang', 'Xujiong Ye']",2025-03-18,2025-03-18
2503.15559v1,Advanced Relay-Based Collaborative Framework for Optimizing Synchronization in Split Federated Learning over Wireless Networks,"Split Federated Learning (SFL) offers a promising approach for distributed
model training in edge computing, combining the strengths of split learning in
reducing computational demands on edge devices and enhancing data privacy, with
the role of federated aggregation to ensure model convergence and
synchronization across users. However, synchronization issues caused by user
heterogeneity have hindered the development of the framework. To optimize
synchronization efficiency among users and improve overall system performance,
we propose a collaborative SFL framework (CSFL). Based on the model's
partitioning capabilities, we design a mechanism called the collaborative relay
optimization mechanism (CROM), where the assistance provided by high-efficiency
users is seen as a relay process, with the portion of the model they compute
acting as the relay point. Wireless communication between users facilitates
real-time collaboration, allowing high-efficiency users to assist bottleneck
users in handling part of the model's computation, thereby alleviating the
computational load on bottleneck users. Simulation results show that our
proposed CSFL framework reduces synchronization delays and improves overall
system throughput while maintaining similar performance and convergence rate to
the SFL framework. This demonstrates that the collaboration not only reduces
synchronization waiting time but also accelerates model convergence.",['cs.LG'],"['Haoran Gao', 'Samuel D. Okegbile', 'Jun Cai']",2025-03-18,2025-03-18
2503.15558v1,Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning,"Physical AI systems need to perceive, understand, and perform complex actions
in the physical world. In this paper, we present the Cosmos-Reason1 models that
can understand the physical world and generate appropriate embodied decisions
(e.g., next step action) in natural language through long chain-of-thought
reasoning processes. We begin by defining key capabilities for Physical AI
reasoning, with a focus on physical common sense and embodied reasoning. To
represent physical common sense, we use a hierarchical ontology that captures
fundamental knowledge about space, time, and physics. For embodied reasoning,
we rely on a two-dimensional ontology that generalizes across different
physical embodiments. Building on these capabilities, we develop two multimodal
large language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data
and train our models in four stages: vision pre-training, general supervised
fine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL)
as the post-training. To evaluate our models, we build comprehensive benchmarks
for physical common sense and embodied reasoning according to our ontologies.
Evaluation results show that Physical AI SFT and reinforcement learning bring
significant improvements. To facilitate the development of Physical AI, we will
make our code and pre-trained models available under the NVIDIA Open Model
License at https://github.com/nvidia-cosmos/cosmos-reason1.","['cs.AI', 'cs.CV', 'cs.LG', 'cs.RO']","['NVIDIA', ':', 'Alisson Azzolini', 'Hannah Brandon', 'Prithvijit Chattopadhyay', 'Huayu Chen', 'Jinju Chu', 'Yin Cui', 'Jenna Diamond', 'Yifan Ding', 'Francesco Ferroni', 'Rama Govindaraju', 'Jinwei Gu', 'Siddharth Gururani', 'Imad El Hanafi', 'Zekun Hao', 'Jacob Huffman', 'Jingyi Jin', 'Brendan Johnson', 'Rizwan Khan', 'George Kurian', 'Elena Lantz', 'Nayeon Lee', 'Zhaoshuo Li', 'Xuan Li', 'Tsung-Yi Lin', 'Yen-Chen Lin', 'Ming-Yu Liu', 'Andrew Mathau', 'Yun Ni', 'Lindsey Pavao', 'Wei Ping', 'David W. Romero', 'Misha Smelyanskiy', 'Shuran Song', 'Lyne Tchapmi', 'Andrew Z. Wang', 'Boxin Wang', 'Haoxiang Wang', 'Fangyin Wei', 'Jiashu Xu', 'Yao Xu', 'Xiaodong Yang', 'Zhuolin Yang', 'Xiaohui Zeng', 'Zhe Zhang']",2025-03-18,2025-03-18
2503.14757v1,RETHINED: A New Benchmark and Baseline for Real-Time High-Resolution Image Inpainting On Edge Devices,"Existing image inpainting methods have shown impressive completion results
for low-resolution images. However, most of these algorithms fail at high
resolutions and require powerful hardware, limiting their deployment on edge
devices. Motivated by this, we propose the first baseline for REal-Time
High-resolution image INpainting on Edge Devices (RETHINED) that is able to
inpaint at ultra-high-resolution and can run in real-time ($\leq$ 30ms) in a
wide variety of mobile devices. A simple, yet effective novel method formed by
a lightweight Convolutional Neural Network (CNN) to recover structure, followed
by a resolution-agnostic patch replacement mechanism to provide detailed
texture. Specially our pipeline leverages the structural capacity of CNN and
the high-level detail of patch-based methods, which is a key component for
high-resolution image inpainting. To demonstrate the real application of our
method, we conduct an extensive analysis on various mobile-friendly devices and
demonstrate similar inpainting performance while being $\mathrm{100 \times
faster}$ than existing state-of-the-art methods. Furthemore, we realease
DF8K-Inpainting, the first free-form mask UHD inpainting dataset.","['cs.CV', 'cs.LG']","['Marcelo Sanchez', 'Gil Triginer', 'Ignacio Sarasua', 'Lara Raad', 'Coloma Ballester']",2025-03-18,2025-03-18
2503.14756v1,SceneEval: Evaluating Semantic Coherence in Text-Conditioned 3D Indoor Scene Synthesis,"Despite recent advances in text-conditioned 3D indoor scene generation, there
remain gaps in the evaluation of these methods. Existing metrics primarily
assess the realism of generated scenes by comparing them to a set of
ground-truth scenes, often overlooking alignment with the input text - a
critical factor in determining how effectively a method meets user
requirements. We present SceneEval, an evaluation framework designed to address
this limitation. SceneEval includes metrics for both explicit user
requirements, such as the presence of specific objects and their attributes
described in the input text, and implicit expectations, like the absence of
object collisions, providing a comprehensive assessment of scene quality. To
facilitate evaluation, we introduce SceneEval-100, a dataset of scene
descriptions with annotated ground-truth scene properties. We evaluate recent
scene generation methods using SceneEval and demonstrate its ability to provide
detailed assessments of the generated scenes, highlighting strengths and areas
for improvement across multiple dimensions. Our results show that current
methods struggle at generating scenes that meet user requirements, underscoring
the need for further research in this direction.","['cs.GR', 'cs.CV']","['Hou In Ivan Tam', 'Hou In Derek Pun', 'Austin T. Wang', 'Angel X. Chang', 'Manolis Savva']",2025-03-18,2025-03-18
2503.14755v1,Language Independent Named Entity Recognition via Orthogonal Transformation of Word Vectors,"Word embeddings have been a key building block for NLP in which models relied
heavily on word embeddings in many different tasks. In this paper, a model is
proposed based on using Bidirectional LSTM/CRF with word embeddings to perform
named entity recognition for any language. This is done by training a model on
a source language (English) and transforming word embeddings from the target
language into word embeddings of the source language by using an orthogonal
linear transformation matrix. Evaluation of the model shows that by training a
model on an English dataset the model was capable of detecting named entities
in an Arabic dataset without neither training or fine tuning the model on an
Arabic language dataset.","['cs.CL', 'cs.AI']","['Omar E. Rakha', 'Hazem M. Abbas']",2025-03-18,2025-03-18
2503.14754v1,Bayesian Modeling of Zero-Shot Classifications for Urban Flood Detection,"Street scene datasets, collected from Street View or dashboard cameras, offer
a promising means of detecting urban objects and incidents like street
flooding. However, a major challenge in using these datasets is their lack of
reliable labels: there are myriad types of incidents, many types occur rarely,
and ground-truth measures of where incidents occur are lacking. Here, we
propose BayFlood, a two-stage approach which circumvents this difficulty.
First, we perform zero-shot classification of where incidents occur using a
pretrained vision-language model (VLM). Second, we fit a spatial Bayesian model
on the VLM classifications. The zero-shot approach avoids the need to annotate
large training sets, and the Bayesian model provides frequent desiderata in
urban settings - principled measures of uncertainty, smoothing across
locations, and incorporation of external data like stormwater accumulation
zones. We comprehensively validate this two-stage approach, showing that VLMs
provide strong zero-shot signal for floods across multiple cities and time
periods, the Bayesian model improves out-of-sample prediction relative to
baseline methods, and our inferred flood risk correlates with known external
predictors of risk. Having validated our approach, we show it can be used to
improve urban flood detection: our analysis reveals 113,738 people who are at
high risk of flooding overlooked by current methods, identifies demographic
biases in existing methods, and suggests locations for new flood sensors. More
broadly, our results showcase how Bayesian modeling of zero-shot LM annotations
represents a promising paradigm because it avoids the need to collect large
labeled datasets and leverages the power of foundation models while providing
the expressiveness and uncertainty quantification of Bayesian models.","['cs.LG', 'cs.AI', 'cs.CV']","['Matt Franchi', 'Nikhil Garg', 'Wendy Ju', 'Emma Pierson']",2025-03-18,2025-03-18
2503.14751v1,LipShiFT: A Certifiably Robust Shift-based Vision Transformer,"Deriving tight Lipschitz bounds for transformer-based architectures presents
a significant challenge. The large input sizes and high-dimensional attention
modules typically prove to be crucial bottlenecks during the training process
and leads to sub-optimal results. Our research highlights practical constraints
of these methods in vision tasks. We find that Lipschitz-based margin training
acts as a strong regularizer while restricting weights in successive layers of
the model. Focusing on a Lipschitz continuous variant of the ShiftViT model, we
address significant training challenges for transformer-based architectures
under norm-constrained input setting. We provide an upper bound estimate for
the Lipschitz constants of this model using the $l_2$ norm on common image
classification datasets. Ultimately, we demonstrate that our method scales to
larger models and advances the state-of-the-art in certified robustness for
transformer-based architectures.","['cs.LG', 'cs.AI', 'cs.CV']","['Rohan Menon', 'Nicola Franco', 'Stephan Günnemann']",2025-03-18,2025-03-18
2503.14749v1,Uncertainty Distillation: Teaching Language Models to Express Semantic Confidence,"As large language models (LLMs) are increasingly used for factual
question-answering, it becomes more important for LLMs to have the capability
to communicate the likelihood that their answer is correct. For these
verbalized expressions of uncertainty to be meaningful, they should reflect the
error rates at the expressed level of confidence. However, when prompted to
express confidence, the error rates of current LLMs are inconsistent with their
communicated confidences, highlighting the need for uncertainty quantification
methods. Many prior methods calculate lexical uncertainty, estimating a model's
confidence in the specific string it generated. In some cases, however, it may
be more useful to estimate semantic uncertainty, or the model's confidence in
the answer regardless of how it is verbalized. We propose a simple procedure,
uncertainty distillation, to teach an LLM to verbalize calibrated semantic
confidences. Using held-out data to map initial uncertainty estimates to
meaningful probabilities, we create examples annotated with verbalized
probabilities for supervised fine-tuning. We demonstrate our method yields
verbalized confidences that correlate with observed error rates with a small
fine-tuned language model as well as with larger instruction-tuned models, and
find that our semantic uncertainty correlates well with lexical uncertainty on
short answers.","['cs.CL', 'cs.LG']","['Sophia Hager', 'David Mueller', 'Kevin Duh', 'Nicholas Andrews']",2025-03-18,2025-03-18
2503.15557v1,Motion Synthesis with Sparse and Flexible Keyjoint Control,"Creating expressive character animations is labor-intensive, requiring
intricate manual adjustment of animators across space and time. Previous works
on controllable motion generation often rely on a predefined set of dense
spatio-temporal specifications (e.g., dense pelvis trajectories with exact
per-frame timing), limiting practicality for animators. To process high-level
intent and intuitive control in diverse scenarios, we propose a practical
controllable motions synthesis framework that respects sparse and flexible
keyjoint signals. Our approach employs a decomposed diffusion-based motion
synthesis framework that first synthesizes keyjoint movements from sparse input
control signals and then synthesizes full-body motion based on the completed
keyjoint trajectories. The low-dimensional keyjoint movements can easily adapt
to various control signal types, such as end-effector position for diverse
goal-driven motion synthesis, or incorporate functional constraints on a subset
of keyjoints. Additionally, we introduce a time-agnostic control formulation,
eliminating the need for frame-specific timing annotations and enhancing
control flexibility. Then, the shared second stage can synthesize a natural
whole-body motion that precisely satisfies the task requirement from dense
keyjoint movements. We demonstrate the effectiveness of sparse and flexible
keyjoint control through comprehensive experiments on diverse datasets and
scenarios.","['cs.GR', 'cs.CV', 'cs.RO']","['Inwoo Hwang', 'Jinseok Bae', 'Donggeun Lim', 'Young Min Kim']",2025-03-18,2025-03-18
2503.14736v1,HandSplat: Embedding-Driven Gaussian Splatting for High-Fidelity Hand Rendering,"Existing 3D Gaussian Splatting (3DGS) methods for hand rendering rely on
rigid skeletal motion with an oversimplified non-rigid motion model, which
fails to capture fine geometric and appearance details. Additionally, they
perform densification based solely on per-point gradients and process poses
independently, ignoring spatial and temporal correlations. These limitations
lead to geometric detail loss, temporal instability, and inefficient point
distribution. To address these issues, we propose HandSplat, a novel Gaussian
Splatting-based framework that enhances both fidelity and stability for hand
rendering. To improve fidelity, we extend standard 3DGS attributes with
implicit geometry and appearance embeddings for finer non-rigid motion modeling
while preserving the static hand characteristic modeled by original 3DGS
attributes. Additionally, we introduce a local gradient-aware densification
strategy that dynamically refines Gaussian density in high-variation regions.
To improve stability, we incorporate pose-conditioned attribute regularization
to encourage attribute consistency across similar poses, mitigating temporal
artifacts. Extensive experiments on InterHand2.6M demonstrate that HandSplat
surpasses existing methods in fidelity and stability while achieving real-time
performance. We will release the code and pre-trained models upon acceptance.",['cs.CV'],"['Yilan Dong', 'Haohe Liu', 'Qing Wang', 'Jiahao Yang', 'Wenqing Wang', 'Gregory Slabaugh', 'Shanxin Yuan']",2025-03-18,2025-03-18
2503.14734v1,GR00T N1: An Open Foundation Model for Generalist Humanoid Robots,"General-purpose robots need a versatile body and an intelligent mind. Recent
advancements in humanoid robots have shown great promise as a hardware platform
for building generalist autonomy in the human world. A robot foundation model,
trained on massive and diverse data sources, is essential for enabling the
robots to reason about novel situations, robustly handle real-world
variability, and rapidly learn new tasks. To this end, we introduce GR00T N1,
an open foundation model for humanoid robots. GR00T N1 is a
Vision-Language-Action (VLA) model with a dual-system architecture. The
vision-language module (System 2) interprets the environment through vision and
language instructions. The subsequent diffusion transformer module (System 1)
generates fluid motor actions in real time. Both modules are tightly coupled
and jointly trained end-to-end. We train GR00T N1 with a heterogeneous mixture
of real-robot trajectories, human videos, and synthetically generated datasets.
We show that our generalist robot model GR00T N1 outperforms the
state-of-the-art imitation learning baselines on standard simulation benchmarks
across multiple robot embodiments. Furthermore, we deploy our model on the
Fourier GR-1 humanoid robot for language-conditioned bimanual manipulation
tasks, achieving strong performance with high data efficiency.","['cs.RO', 'cs.AI', 'cs.LG']","['NVIDIA', 'Johan Bjorck', 'Fernando Castañeda', 'Nikita Cherniadev', 'Xingye Da', 'Runyu Ding', 'Linxi ""Jim"" Fan', 'Yu Fang', 'Dieter Fox', 'Fengyuan Hu', 'Spencer Huang', 'Joel Jang', 'Zhenyu Jiang', 'Jan Kautz', 'Kaushil Kundalia', 'Lawrence Lao', 'Zhiqi Li', 'Zongyu Lin', 'Kevin Lin', 'Guilin Liu', 'Edith Llontop', 'Loic Magne', 'Ajay Mandlekar', 'Avnish Narayan', 'Soroush Nasiriany', 'Scott Reed', 'You Liang Tan', 'Guanzhi Wang', 'Zu Wang', 'Jing Wang', 'Qi Wang', 'Jiannan Xiang', 'Yuqi Xie', 'Yinzhen Xu', 'Zhenjia Xu', 'Seonghyeon Ye', 'Zhiding Yu', 'Ao Zhang', 'Hao Zhang', 'Yizhou Zhao', 'Ruijie Zheng', 'Yuke Zhu']",2025-03-18,2025-03-18
2503.14728v1,Strategic resource allocation in memory encoding: An efficiency principle shaping language processing,"How is the limited capacity of working memory efficiently used to support
human linguistic behaviors? In this paper, we investigate strategic resource
allocation as an efficiency principle for memory encoding in sentence
processing. The idea is that working memory resources are dynamically and
strategically allocated to prioritize novel and unexpected information,
enhancing their representations to make them less susceptible to memory decay
and interference. Theoretically, from a resource-rational perspective, we argue
that this efficiency principle naturally arises from two functional assumptions
about working memory, namely, its limited capacity and its noisy
representation. Empirically, through naturalistic corpus data, we find
converging evidence for strategic resource allocation in the context of
dependency locality from both the production and the comprehension side, where
non-local dependencies with less predictable antecedents are associated with
reduced locality effect. However, our results also reveal considerable
cross-linguistic variability, highlighting the need for a closer examination of
how strategic resource allocation, as a universal efficiency principle,
interacts with language-specific phrase structures.",['cs.CL'],"['Weijie Xu', 'Richard Futrell']",2025-03-18,2025-03-18
2503.14720v1,ShapeShift: Towards Text-to-Shape Arrangement Synthesis with Content-Aware Geometric Constraints,"While diffusion-based models excel at generating photorealistic images from
text, a more nuanced challenge emerges when constrained to using only a fixed
set of rigid shapes, akin to solving tangram puzzles or arranging real-world
objects to match semantic descriptions. We formalize this problem as
shape-based image generation, a new text-guided image-to-image translation task
that requires rearranging the input set of rigid shapes into non-overlapping
configurations and visually communicating the target concept. Unlike
pixel-manipulation approaches, our method, ShapeShift, explicitly parameterizes
each shape within a differentiable vector graphics pipeline, iteratively
optimizing placement and orientation through score distillation sampling from
pretrained diffusion models. To preserve arrangement clarity, we introduce a
content-aware collision resolution mechanism that applies minimal semantically
coherent adjustments when overlaps occur, ensuring smooth convergence toward
physically valid configurations. By bridging diffusion-based semantic guidance
with explicit geometric constraints, our approach yields interpretable
compositions where spatial relationships clearly embody the textual prompt.
Extensive experiments demonstrate compelling results across diverse scenarios,
with quantitative and qualitative advantages over alternative techniques.",['cs.CV'],"['Vihaan Misra', 'Peter Schaldenbrand', 'Jean Oh']",2025-03-18,2025-03-18
2503.14719v1,ViVa-SAFELAND: a New Freeware for Safe Validation of Vision-based Navigation in Aerial Vehicles,"ViVa-SAFELAND is an open source software library, aimed to test and evaluate
vision-based navigation strategies for aerial vehicles, with special interest
in autonomous landing, while complying with legal regulations and people's
safety. It consists of a collection of high definition aerial videos, focusing
on real unstructured urban scenarios, recording moving obstacles of interest,
such as cars and people. Then, an Emulated Aerial Vehicle (EAV) with a virtual
moving camera is implemented in order to ``navigate"" inside the video,
according to high-order commands. ViVa-SAFELAND provides a new, safe, simple
and fair comparison baseline to evaluate and compare different visual
navigation solutions under the same conditions, and to randomize variables
along several trials. It also facilitates the development of autonomous landing
and navigation strategies, as well as the generation of image datasets for
different training tasks. Moreover, it is useful for training either human of
autonomous pilots using deep learning. The effectiveness of the framework for
validating vision algorithms is demonstrated through two case studies,
detection of moving objects and risk assessment segmentation. To our knowledge,
this is the first safe validation framework of its kind, to test and compare
visual navigation solution for aerial vehicles, which is a crucial aspect for
urban deployment in complex real scenarios.","['cs.RO', 'cs.CV']","['Miguel S. Soriano-García', 'Diego A. Mercado-Ravell']",2025-03-18,2025-03-18
2503.14718v1,Second language Korean Universal Dependency treebank v1.2: Focus on data augmentation and annotation scheme refinement,"We expand the second language (L2) Korean Universal Dependencies (UD)
treebank with 5,454 manually annotated sentences. The annotation guidelines are
also revised to better align with the UD framework. Using this enhanced
treebank, we fine-tune three Korean language models and evaluate their
performance on in-domain and out-of-domain L2-Korean datasets. The results show
that fine-tuning significantly improves their performance across various
metrics, thus highlighting the importance of using well-tailored L2 datasets
for fine-tuning first-language-based, general-purpose language models for the
morphosyntactic analysis of L2 data.",['cs.CL'],"['Hakyung Sung', 'Gyu-Ho Shin']",2025-03-18,2025-03-18
2503.14716v1,Construction Site Scaffolding Completeness Detection Based on Mask R-CNN and Hough Transform,"Construction site scaffolding is essential for many building projects, and
ensuring its safety is crucial to prevent accidents. The safety inspector must
check the scaffolding's completeness and integrity, where most violations
occur. The inspection process includes ensuring all the components are in the
right place since workers often compromise safety for convenience and
disassemble parts such as cross braces. This paper proposes a deep
learning-based approach to detect the scaffolding and its cross braces using
computer vision. A scaffold image dataset with annotated labels is used to
train a convolutional neural network (CNN) model. With the proposed approach,
we can automatically detect the completeness of cross braces from images taken
at construction sites, without the need for manual inspection, saving a
significant amount of time and labor costs. This non-invasive and efficient
solution for detecting scaffolding completeness can help improve safety in
construction sites.","['cs.CV', 'cs.AI']","['Pei-Hsin Lin', 'Jacob J. Lin', 'Shang-Hsien Hsieh']",2025-03-18,2025-03-18
2503.15555v1,Whole-Body Image-to-Image Translation for a Virtual Scanner in a Healthcare Digital Twin,"Generating positron emission tomography (PET) images from computed tomography
(CT) scans via deep learning offers a promising pathway to reduce radiation
exposure and costs associated with PET imaging, improving patient care and
accessibility to functional imaging. Whole-body image translation presents
challenges due to anatomical heterogeneity, often limiting generalized models.
We propose a framework that segments whole-body CT images into four
regions-head, trunk, arms, and legs-and uses district-specific Generative
Adversarial Networks (GANs) for tailored CT-to-PET translation. Synthetic PET
images from each region are stitched together to reconstruct the whole-body
scan. Comparisons with a baseline non-segmented GAN and experiments with
Pix2Pix and CycleGAN architectures tested paired and unpaired scenarios.
Quantitative evaluations at district, whole-body, and lesion levels
demonstrated significant improvements with our district-specific GANs. Pix2Pix
yielded superior metrics, ensuring precise, high-quality image synthesis. By
addressing anatomical heterogeneity, this approach achieves state-of-the-art
results in whole-body CT-to-PET translation. This methodology supports
healthcare Digital Twins by enabling accurate virtual PET scans from CT data,
creating virtual imaging representations to monitor, predict, and optimize
health outcomes.","['eess.IV', 'cs.AI']","['Valerio Guarrasi', 'Francesco Di Feola', 'Rebecca Restivo', 'Lorenzo Tronchin', 'Paolo Soda']",2025-03-18,2025-03-18
2503.14710v1,Variational Autoencoded Multivariate Spatial Fay-Herriot Models,"Small area estimation models are essential for estimating population
characteristics in regions with limited sample sizes, thereby supporting policy
decisions, demographic studies, and resource allocation, among other use cases.
The spatial Fay-Herriot model is one such approach that incorporates spatial
dependence to improve estimation by borrowing strength from neighboring
regions. However, this approach often requires substantial computational
resources, limiting its scalability for high-dimensional datasets, especially
when considering multiple (multivariate) responses. This paper proposes two
methods that integrate the multivariate spatial Fay-Herriot model with spatial
random effects, learned through variational autoencoders, to efficiently
leverage spatial structure. Importantly, after training the variational
autoencoder to represent spatial dependence for a given set of geographies, it
may be used again in future modeling efforts, without the need for retraining.
Additionally, the use of the variational autoencoder to represent spatial
dependence results in extreme improvements in computational efficiency, even
for massive datasets. We demonstrate the effectiveness of our approach using
5-year period estimates from the American Community Survey over all census
tracts in California.","['stat.ML', 'cs.LG']","['Zhenhua Wang', 'Paul A. Parker', 'Scott H. Holan']",2025-03-18,2025-03-18
2503.16538v1,Leveraging Vision-Language Models for Open-Vocabulary Instance Segmentation and Tracking,"This paper introduces a novel approach that leverages the capabilities of
vision-language models (VLMs) by integrating them with established approaches
for open-vocabulary detection (OVD), instance segmentation, and tracking. We
utilize VLM-generated structured descriptions to identify visible object
instances, collect application-relevant attributes, and inform an
open-vocabulary detector to extract corresponding bounding boxes that are
passed to a video segmentation model providing precise segmentation masks and
tracking capabilities. Once initialized, this model can then directly extract
segmentation masks, allowing processing of image streams in real time with
minimal computational overhead. Tracks can be updated online as needed by
generating new structured descriptions and corresponding open-vocabulary
detections. This combines the descriptive power of VLMs with the grounding
capability of OVD and the pixel-level understanding and speed of video
segmentation. Our evaluation across datasets and robotics platforms
demonstrates the broad applicability of this approach, showcasing its ability
to extract task-specific attributes from non-standard objects in dynamic
environments.","['cs.CV', 'cs.RO']","['Bastian Pätzold', 'Jan Nogga', 'Sven Behnke']",2025-03-18,2025-03-18
2503.14709v1,Better Private Distribution Testing by Leveraging Unverified Auxiliary Data,"We extend the framework of augmented distribution testing (Aliakbarpour,
Indyk, Rubinfeld, and Silwal, NeurIPS 2024) to the differentially private
setting. This captures scenarios where a data analyst must perform hypothesis
testing tasks on sensitive data, but is able to leverage prior knowledge
(public, but possibly erroneous or untrusted) about the data distribution.
  We design private algorithms in this augmented setting for three flagship
distribution testing tasks, uniformity, identity, and closeness testing, whose
sample complexity smoothly scales with the claimed quality of the auxiliary
information. We complement our algorithms with information-theoretic lower
bounds, showing that their sample complexity is optimal (up to logarithmic
factors).","['cs.LG', 'cs.CR', 'cs.DS']","['Maryam Aliakbarpour', 'Arnav Burudgunte', 'Clément Cannone', 'Ronitt Rubinfeld']",2025-03-18,2025-03-18
2503.15554v1,A Comprehensive Study of LLM Secure Code Generation,"LLMs are widely used in software development. However, the code generated by
LLMs often contains vulnerabilities. Several secure code generation methods
have been proposed to address this issue, but their current evaluation schemes
leave several concerns unaddressed. Specifically, most existing studies
evaluate security and functional correctness separately, using different
datasets. That is, they assess vulnerabilities using security-related code
datasets while validating functionality with general code datasets. In
addition, prior research primarily relies on a single static analyzer, CodeQL,
to detect vulnerabilities in generated code, which limits the scope of security
evaluation.
  In this work, we conduct a comprehensive study to systematically assess the
improvements introduced by four state-of-the-art secure code generation
techniques. Specifically, we apply both security inspection and functionality
validation to the same generated code and evaluate these two aspects together.
We also employ three popular static analyzers and two LLMs to identify
potential vulnerabilities in the generated code. Our study reveals that
existing techniques often compromise the functionality of generated code to
enhance security. Their overall performance remains limited when evaluating
security and functionality together. In fact, many techniques even degrade the
performance of the base LLM. Our further inspection reveals that these
techniques often either remove vulnerable lines of code entirely or generate
``garbage code'' that is unrelated to the intended task. Moreover, the commonly
used static analyzer CodeQL fails to detect several vulnerabilities, further
obscuring the actual security improvements achieved by existing techniques. Our
study serves as a guideline for a more rigorous and comprehensive evaluation of
secure code generation performance in future work.","['cs.CR', 'cs.LG', 'cs.SE']","['Shih-Chieh Dai', 'Jun Xu', 'Guanhong Tao']",2025-03-18,2025-03-18
2503.14701v1,ARC-Calib: Autonomous Markerless Camera-to-Robot Calibration via Exploratory Robot Motions,"Camera-to-robot (also known as eye-to-hand) calibration is a critical
component of vision-based robot manipulation. Traditional marker-based methods
often require human intervention for system setup. Furthermore, existing
autonomous markerless calibration methods typically rely on pre-trained robot
tracking models that impede their application on edge devices and require
fine-tuning for novel robot embodiments. To address these limitations, this
paper proposes a model-based markerless camera-to-robot calibration framework,
ARC-Calib, that is fully autonomous and generalizable across diverse robots and
scenarios without requiring extensive data collection or learning. First,
exploratory robot motions are introduced to generate easily trackable
trajectory-based visual patterns in the camera's image frames. Then, a
geometric optimization framework is proposed to exploit the coplanarity and
collinearity constraints from the observed motions to iteratively refine the
estimated calibration result. Our approach eliminates the need for extra effort
in either environmental marker setup or data collection and model training,
rendering it highly adaptable across a wide range of real-world autonomous
systems. Extensive experiments are conducted in both simulation and the real
world to validate its robustness and generalizability.","['cs.RO', 'cs.CV']","['Podshara Chanrungmaneekul', 'Yiting Chen', 'Joshua T. Grace', 'Aaron M. Dollar', 'Kaiyu Hang']",2025-03-18,2025-03-18
2503.14698v1,SplatVoxel: History-Aware Novel View Streaming without Temporal Training,"We study the problem of novel view streaming from sparse-view videos, which
aims to generate a continuous sequence of high-quality, temporally consistent
novel views as new input frames arrive. However, existing novel view synthesis
methods struggle with temporal coherence and visual fidelity, leading to
flickering and inconsistency. To address these challenges, we introduce
history-awareness, leveraging previous frames to reconstruct the scene and
improve quality and stability. We propose a hybrid splat-voxel feed-forward
scene reconstruction approach that combines Gaussian Splatting to propagate
information over time, with a hierarchical voxel grid for temporal fusion.
Gaussian primitives are efficiently warped over time using a motion graph that
extends 2D tracking models to 3D motion, while a sparse voxel transformer
integrates new temporal observations in an error-aware manner. Crucially, our
method does not require training on multi-view video datasets, which are
currently limited in size and diversity, and can be directly applied to
sparse-view video streams in a history-aware manner at inference time. Our
approach achieves state-of-the-art performance in both static and streaming
scene reconstruction, effectively reducing temporal artifacts and visual
artifacts while running at interactive rates (15 fps with 350ms delay) on a
single H100 GPU. Project Page: https://19reborn.github.io/SplatVoxel/",['cs.CV'],"['Yiming Wang', 'Lucy Chai', 'Xuan Luo', 'Michael Niemeyer', 'Manuel Lagunas', 'Stephen Lombardi', 'Siyu Tang', 'Tiancheng Sun']",2025-03-18,2025-03-18
2503.14681v1,DPImageBench: A Unified Benchmark for Differentially Private Image Synthesis,"Differentially private (DP) image synthesis aims to generate artificial
images that retain the properties of sensitive images while protecting the
privacy of individual images within the dataset. Despite recent advancements,
we find that inconsistent--and sometimes flawed--evaluation protocols have been
applied across studies. This not only impedes the understanding of current
methods but also hinders future advancements.
  To address the issue, this paper introduces DPImageBench for DP image
synthesis, with thoughtful design across several dimensions: (1) Methods. We
study eleven prominent methods and systematically characterize each based on
model architecture, pretraining strategy, and privacy mechanism. (2)
Evaluation. We include nine datasets and seven fidelity and utility metrics to
thoroughly assess them. Notably, we find that a common practice of selecting
downstream classifiers based on the highest accuracy on the sensitive test set
not only violates DP but also overestimates the utility scores. DPImageBench
corrects for these mistakes. (3) Platform. Despite the methods and evaluation
protocols, DPImageBench provides a standardized interface that accommodates
current and future implementations within a unified framework. With
DPImageBench, we have several noteworthy findings. For example, contrary to the
common wisdom that pretraining on public image datasets is usually beneficial,
we find that the distributional similarity between pretraining and sensitive
images significantly impacts the performance of the synthetic images and does
not always yield improvements. In addition, adding noise to low-dimensional
features, such as the high-level characteristics of sensitive images, is less
affected by the privacy budget compared to adding noise to high-dimensional
features, like weight gradients. The former methods perform better than the
latter under a low privacy budget.","['cs.CR', 'cs.AI']","['Chen Gong', 'Kecen Li', 'Zinan Lin', 'Tianhao Wang']",2025-03-18,2025-03-18
2503.14674v1,Elevating Visual Question Answering through Implicitly Learned Reasoning Pathways in LVLMs,"Large Vision-Language Models (LVLMs) have shown remarkable progress in
various multimodal tasks, yet they often struggle with complex visual reasoning
that requires multi-step inference. To address this limitation, we propose
MF-SQ-LLaVA, a novel approach that enhances LVLMs by enabling implicit
self-questioning through end-to-end training. Our method involves augmenting
visual question answering datasets with reasoning chains consisting of
sub-question and answer pairs, and training the LVLM with a multi-task loss
that encourages the generation and answering of these intermediate steps, as
well as the prediction of the final answer. We conduct extensive experiments on
the ScienceQA and VQAv2 datasets, demonstrating that MF-SQ-LLaVA significantly
outperforms existing state-of-the-art models, including the base LLaVA and the
original SQ-LLaVA. Ablation studies further validate the contribution of each
component of our approach, and human evaluation confirms the improved accuracy
and coherence of the reasoning process enabled by our method.",['cs.CV'],"['Liu Jing', 'Amirul Rahman']",2025-03-18,2025-03-18
2503.14671v1,Generating Medically-Informed Explanations for Depression Detection using LLMs,"Early detection of depression from social media data offers a valuable
opportunity for timely intervention. However, this task poses significant
challenges, requiring both professional medical knowledge and the development
of accurate and explainable models. In this paper, we propose LLM-MTD (Large
Language Model for Multi-Task Depression Detection), a novel approach that
leverages a pre-trained large language model to simultaneously classify social
media posts for depression and generate textual explanations grounded in
medical diagnostic criteria. We train our model using a multi-task learning
framework with a combined loss function that optimizes both classification
accuracy and explanation quality. We evaluate LLM-MTD on the benchmark Reddit
Self-Reported Depression Dataset (RSDD) and compare its performance against
several competitive baseline methods, including traditional machine learning
and fine-tuned BERT. Our experimental results demonstrate that LLM-MTD achieves
state-of-the-art performance in depression detection, showing significant
improvements in AUPRC and other key metrics. Furthermore, human evaluation of
the generated explanations reveals their relevance, completeness, and medical
accuracy, highlighting the enhanced interpretability of our approach. This work
contributes a novel methodology for depression detection that combines the
power of large language models with the crucial aspect of explainability.",['cs.CL'],"['Xiangyong Chen', 'Xiaochuan Lin']",2025-03-18,2025-03-18
2503.15552v1,Personalized Attacks of Social Engineering in Multi-turn Conversations -- LLM Agents for Simulation and Detection,"The rapid advancement of conversational agents, particularly chatbots powered
by Large Language Models (LLMs), poses a significant risk of social engineering
(SE) attacks on social media platforms. SE detection in multi-turn, chat-based
interactions is considerably more complex than single-instance detection due to
the dynamic nature of these conversations. A critical factor in mitigating this
threat is understanding the mechanisms through which SE attacks operate,
specifically how attackers exploit vulnerabilities and how victims' personality
traits contribute to their susceptibility. In this work, we propose an
LLM-agentic framework, SE-VSim, to simulate SE attack mechanisms by generating
multi-turn conversations. We model victim agents with varying personality
traits to assess how psychological profiles influence susceptibility to
manipulation. Using a dataset of over 1000 simulated conversations, we examine
attack scenarios in which adversaries, posing as recruiters, funding agencies,
and journalists, attempt to extract sensitive information. Based on this
analysis, we present a proof of concept, SE-OmniGuard, to offer personalized
protection to users by leveraging prior knowledge of the victims personality,
evaluating attack strategies, and monitoring information exchanges in
conversations to identify potential SE attempts.","['cs.CR', 'cs.CL']","['Tharindu Kumarage', 'Cameron Johnson', 'Jadie Adams', 'Lin Ai', 'Matthias Kirchner', 'Anthony Hoogs', 'Joshua Garland', 'Julia Hirschberg', 'Arslan Basharat', 'Huan Liu']",2025-03-18,2025-03-18
2503.14665v2,These Magic Moments: Differentiable Uncertainty Quantification of Radiance Field Models,"This paper introduces a novel approach to uncertainty quantification for
radiance fields by leveraging higher-order moments of the rendering equation.
Uncertainty quantification is crucial for downstream tasks including view
planning and scene understanding, where safety and robustness are paramount.
However, the high dimensionality and complexity of radiance fields pose
significant challenges for uncertainty quantification, limiting the use of
these uncertainty quantification methods in high-speed decision-making. We
demonstrate that the probabilistic nature of the rendering process enables
efficient and differentiable computation of higher-order moments for radiance
field outputs, including color, depth, and semantic predictions. Our method
outperforms existing radiance field uncertainty estimation techniques while
offering a more direct, computationally efficient, and differentiable
formulation without the need for post-processing. Beyond uncertainty
quantification, we also illustrate the utility of our approach in downstream
applications such as next-best-view (NBV) selection and active ray sampling for
neural radiance field training. Extensive experiments on synthetic and
real-world scenes confirm the efficacy of our approach, which achieves
state-of-the-art performance while maintaining simplicity.","['cs.CV', 'cs.RO']","['Parker Ewen', 'Hao Chen', 'Seth Isaacson', 'Joey Wilson', 'Katherine A. Skinner', 'Ram Vasudevan']",2025-03-18,2025-03-20
2503.14663v1,Sepsyn-OLCP: An Online Learning-based Framework for Early Sepsis Prediction with Uncertainty Quantification using Conformal Prediction,"Sepsis is a life-threatening syndrome with high morbidity and mortality in
hospitals. Early prediction of sepsis plays a crucial role in facilitating
early interventions for septic patients. However, early sepsis prediction
systems with uncertainty quantification and adaptive learning are scarce. This
paper proposes Sepsyn-OLCP, a novel online learning algorithm for early sepsis
prediction by integrating conformal prediction for uncertainty quantification
and Bayesian bandits for adaptive decision-making. By combining the robustness
of Bayesian models with the statistical uncertainty guarantees of conformal
prediction methodologies, this algorithm delivers accurate and trustworthy
predictions, addressing the critical need for reliable and adaptive systems in
high-stakes healthcare applications such as early sepsis prediction. We
evaluate the performance of Sepsyn-OLCP in terms of regret in stochastic bandit
setting, the area under the receiver operating characteristic curve (AUROC),
and F-measure. Our results show that Sepsyn-OLCP outperforms existing
individual models, increasing AUROC of a neural network from 0.64 to 0.73
without retraining and high computational costs. And the model selection policy
converges to the optimal strategy in the long run. We propose a novel
reinforcement learning-based framework integrated with conformal prediction
techniques to provide uncertainty quantification for early sepsis prediction.
The proposed methodology delivers accurate and trustworthy predictions,
addressing a critical need in high-stakes healthcare applications like early
sepsis prediction.",['cs.LG'],"['Anni Zhou', 'Beyah Raheem', 'Rishikesan Kamaleswaran', 'Yao Xie']",2025-03-18,2025-03-18
2503.14662v1,ConQuer: A Framework for Concept-Based Quiz Generation,"Quizzes play a crucial role in education by reinforcing students'
understanding of key concepts and encouraging self-directed exploration.
However, compiling high-quality quizzes can be challenging and require deep
expertise and insight into specific subject matter. Although LLMs have greatly
enhanced the efficiency of quiz generation, concerns remain regarding the
quality of these AI-generated quizzes and their educational impact on students.
To address these issues, we introduce ConQuer, a concept-based quiz generation
framework that leverages external knowledge sources. We employ comprehensive
evaluation dimensions to assess the quality of the generated quizzes, using
LLMs as judges. Our experiment results demonstrate a 4.8% improvement in
evaluation scores and a 77.52% win rate in pairwise comparisons against
baseline quiz sets. Ablation studies further underscore the effectiveness of
each component in our framework. Code available at
https://github.com/sofyc/ConQuer.","['cs.CL', 'cs.AI']","['Yicheng Fu', 'Zikui Wang', 'Liuxin Yang', 'Meiqing Huo', 'Zhongdongming Dai']",2025-03-18,2025-03-18
2503.16537v1,Do Multimodal Large Language Models Understand Welding?,"This paper examines the performance of Multimodal LLMs (MLLMs) in skilled
production work, with a focus on welding. Using a novel data set of real-world
and online weld images, annotated by a domain expert, we evaluate the
performance of two state-of-the-art MLLMs in assessing weld acceptability
across three contexts: RV \& Marine, Aeronautical, and Farming. While both
models perform better on online images, likely due to prior exposure or
memorization, they also perform relatively well on unseen, real-world weld
images. Additionally, we introduce WeldPrompt, a prompting strategy that
combines Chain-of-Thought generation with in-context learning to mitigate
hallucinations and improve reasoning. WeldPrompt improves model recall in
certain contexts but exhibits inconsistent performance across others. These
results underscore the limitations and potentials of MLLMs in high-stakes
technical domains and highlight the importance of fine-tuning, domain-specific
data, and more sophisticated prompting strategies to improve model reliability.
The study opens avenues for further research into multimodal learning in
industry applications.","['cs.CL', 'cs.CV']","['Grigorii Khvatskii', 'Yong Suk Lee', 'Corey Angst', 'Maria Gibbs', 'Robert Landers', 'Nitesh V. Chawla']",2025-03-18,2025-03-18
2503.14655v1,Core-Periphery Principle Guided State Space Model for Functional Connectome Classification,"Understanding the organization of human brain networks has become a central
focus in neuroscience, particularly in the study of functional connectivity,
which plays a crucial role in diagnosing neurological disorders. Advances in
functional magnetic resonance imaging and machine learning techniques have
significantly improved brain network analysis. However, traditional machine
learning approaches struggle to capture the complex relationships between brain
regions, while deep learning methods, particularly Transformer-based models,
face computational challenges due to their quadratic complexity in
long-sequence modeling. To address these limitations, we propose a
Core-Periphery State-Space Model (CP-SSM), an innovative framework for
functional connectome classification. Specifically, we introduce Mamba, a
selective state-space model with linear complexity, to effectively capture
long-range dependencies in functional brain networks. Furthermore, inspired by
the core-periphery (CP) organization, a fundamental characteristic of brain
networks that enhances efficient information transmission, we design CP-MoE, a
CP-guided Mixture-of-Experts that improves the representation learning of brain
connectivity patterns. We evaluate CP-SSM on two benchmark fMRI datasets: ABIDE
and ADNI. Experimental results demonstrate that CP-SSM surpasses
Transformer-based models in classification performance while significantly
reducing computational complexity. These findings highlight the effectiveness
and efficiency of CP-SSM in modeling brain functional connectivity, offering a
promising direction for neuroimaging-based neurological disease diagnosis.","['q-bio.NC', 'cs.AI', 'cs.CV', 'eess.IV']","['Minheng Chen', 'Xiaowei Yu', 'Jing Zhang', 'Tong Chen', 'Chao Cao', 'Yan Zhuang', 'Yanjun Lyu', 'Lu Zhang', 'Tianming Liu', 'Dajiang Zhu']",2025-03-18,2025-03-18
2503.14654v1,A Simple Combination of Diffusion Models for Better Quality Trade-Offs in Image Denoising,"Diffusion models have garnered considerable interest in computer vision,
owing both to their capacity to synthesize photorealistic images and to their
proven effectiveness in image reconstruction tasks. However, existing
approaches fail to efficiently balance the high visual quality of diffusion
models with the low distortion achieved by previous image reconstruction
methods. Specifically, for the fundamental task of additive Gaussian noise
removal, we first illustrate an intuitive method for leveraging pretrained
diffusion models. Further, we introduce our proposed Linear Combination
Diffusion Denoiser (LCDD), which unifies two complementary inference procedures
- one that leverages the model's generative potential and another that ensures
faithful signal recovery. By exploiting the inherent structure of the denoising
samples, LCDD achieves state-of-the-art performance and offers controlled,
well-behaved trade-offs through a simple scalar hyperparameter adjustment.",['cs.CV'],"['Jonas Dornbusch', 'Emanuel Pfarr', 'Florin-Alexandru Vasluianu', 'Frank Werner', 'Radu Timofte']",2025-03-18,2025-03-18
2503.14649v2,RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving,"Retrieval-augmented generation (RAG), which combines large language models
(LLMs) with retrievals from external knowledge databases, is emerging as a
popular approach for reliable LLM serving. However, efficient RAG serving
remains an open challenge due to the rapid emergence of many RAG variants and
the substantial differences in workload characteristics across them. In this
paper, we make three fundamental contributions to advancing RAG serving. First,
we introduce RAGSchema, a structured abstraction that captures the wide range
of RAG algorithms, serving as a foundation for performance optimization.
Second, we analyze several representative RAG workloads with distinct
RAGSchema, revealing significant performance variability across these
workloads. Third, to address this variability and meet diverse performance
requirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a
system optimization framework for efficient RAG serving. Our evaluation shows
that RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in
time-to-first-token latency compared to RAG systems built on LLM-system
extensions.","['cs.IR', 'cs.AI', 'cs.CL', 'cs.DC', 'C.1; C.4; H.3']","['Wenqi Jiang', 'Suvinay Subramanian', 'Cat Graves', 'Gustavo Alonso', 'Amir Yazdanbakhsh', 'Vidushi Dadu']",2025-03-18,2025-03-21
2503.14640v1,Dynamic Accumulated Attention Map for Interpreting Evolution of Decision-Making in Vision Transformer,"Various Vision Transformer (ViT) models have been widely used for image
recognition tasks. However, existing visual explanation methods can not display
the attention flow hidden inside the inner structure of ViT models, which
explains how the final attention regions are formed inside a ViT for its
decision-making. In this paper, a novel visual explanation approach, Dynamic
Accumulated Attention Map (DAAM), is proposed to provide a tool that can
visualize, for the first time, the attention flow from the top to the bottom
through ViT networks. To this end, a novel decomposition module is proposed to
construct and store the spatial feature information by unlocking the [class]
token generated by the self-attention module of each ViT block. The module can
also obtain the channel importance coefficients by decomposing the
classification score for supervised ViT models. Because of the lack of
classification score in self-supervised ViT models, we propose dimension-wise
importance weights to compute the channel importance coefficients. Such spatial
features are linearly combined with the corresponding channel importance
coefficients, forming the attention map for each block. The dynamic attention
flow is revealed by block-wisely accumulating each attention map. The
contribution of this work focuses on visualizing the evolution dynamic of the
decision-making attention for any intermediate block inside a ViT model by
proposing a novel decomposition module and dimension-wise importance weights.
The quantitative and qualitative analysis consistently validate the
effectiveness and superior capacity of the proposed DAAM for not only
interpreting ViT models with the fully-connected layers as the classifier but
also self-supervised ViT models. The code is available at
https://github.com/ly9802/DynamicAccumulatedAttentionMap.","['cs.CV', 'cs.AI']","['Yi Liao', 'Yongsheng Gao', 'Weichuan Zhang']",2025-03-18,2025-03-18
2503.16536v1,Word2Minecraft: Generating 3D Game Levels through Large Language Models,"We present Word2Minecraft, a system that leverages large language models to
generate playable game levels in Minecraft based on structured stories. The
system transforms narrative elements-such as protagonist goals, antagonist
challenges, and environmental settings-into game levels with both spatial and
gameplay constraints. We introduce a flexible framework that allows for the
customization of story complexity, enabling dynamic level generation. The
system employs a scaling algorithm to maintain spatial consistency while
adapting key game elements. We evaluate Word2Minecraft using both metric-based
and human-based methods. Our results show that GPT-4-Turbo outperforms
GPT-4o-Mini in most areas, including story coherence and objective enjoyment,
while the latter excels in aesthetic appeal. We also demonstrate the system' s
ability to generate levels with high map enjoyment, offering a promising step
forward in the intersection of story generation and game design. We open-source
the code at https://github.com/JMZ-kk/Word2Minecraft/tree/word2mc_v0",['cs.CL'],"['Shuo Huang', 'Muhammad Umair Nasir', 'Steven James', 'Julian Togelius']",2025-03-18,2025-03-18
2503.14637v1,Reinforcement learning-based motion imitation for physiologically plausible musculoskeletal motor control,"How do humans move? The quest to understand human motion has broad
applications in numerous fields, ranging from computer animation and motion
synthesis to neuroscience, human prosthetics and rehabilitation. Although
advances in reinforcement learning (RL) have produced impressive results in
capturing human motion using simplified humanoids, controlling physiologically
accurate models of the body remains an open challenge. In this work, we present
a model-free motion imitation framework (KINESIS) to advance the understanding
of muscle-based motor control. Using a musculoskeletal model of the lower body
with 80 muscle actuators and 20 DoF, we demonstrate that KINESIS achieves
strong imitation performance on 1.9 hours of motion capture data, is
controllable by natural language through pre-trained text-to-motion generative
models, and can be fine-tuned to carry out high-level tasks such as target goal
reaching. Importantly, KINESIS generates muscle activity patterns that
correlate well with human EMG activity. The physiological plausibility makes
KINESIS a promising model for tackling challenging problems in human motor
control theory, which we highlight by investigating Bernstein's redundancy
problem in the context of locomotion. Code, videos and benchmarks will be
available at https://github.com/amathislab/Kinesis.","['cs.RO', 'cs.AI', 'cs.CV', 'cs.LG', 'q-bio.NC']","['Merkourios Simos', 'Alberto Silvio Chiappa', 'Alexander Mathis']",2025-03-18,2025-03-18
2503.14630v1,Assessing Large Language Models for Automated Feedback Generation in Learning Programming Problem Solving,"Providing effective feedback is important for student learning in programming
problem-solving. In this sense, Large Language Models (LLMs) have emerged as
potential tools to automate feedback generation. However, their reliability and
ability to identify reasoning errors in student code remain not well
understood. This study evaluates the performance of four LLMs (GPT-4o, GPT-4o
mini, GPT-4-Turbo, and Gemini-1.5-pro) on a benchmark dataset of 45 student
solutions. We assessed the models' capacity to provide accurate and insightful
feedback, particularly in identifying reasoning mistakes. Our analysis reveals
that 63\% of feedback hints were accurate and complete, while 37\% contained
mistakes, including incorrect line identification, flawed explanations, or
hallucinated issues. These findings highlight the potential and limitations of
LLMs in programming education and underscore the need for improvements to
enhance reliability and minimize risks in educational applications.","['cs.SE', 'cs.AI', 'cs.LG']","['Priscylla Silva', 'Evandro Costa']",2025-03-18,2025-03-18
2503.14626v1,An Explainable Framework for Misinformation Identification via Critical Question Answering,"Natural language misinformation detection approaches have been, to date,
largely dependent on sequence classification methods, producing opaque systems
in which the reasons behind classification as misinformation are unclear. While
an effort has been made in the area of automated fact-checking to propose
explainable approaches to the problem, this is not the case for automated
reason-checking systems. In this paper, we propose a new explainable framework
for both factual and rational misinformation detection based on the theory of
Argumentation Schemes and Critical Questions. For that purpose, we create and
release NLAS-CQ, the first corpus combining 3,566 textbook-like natural
language argumentation scheme instances and 4,687 corresponding answers to
critical questions related to these arguments. On the basis of this corpus, we
implement and validate our new framework which combines classification with
question answering to analyse arguments in search of misinformation, and
provides the explanations in form of critical questions to the human user.",['cs.CL'],"['Ramon Ruiz-Dolz', 'John Lawrence']",2025-03-18,2025-03-18
2503.14621v1,Reducing False Ventricular Tachycardia Alarms in ICU Settings: A Machine Learning Approach,"False arrhythmia alarms in intensive care units (ICUs) are a significant
challenge, contributing to alarm fatigue and potentially compromising patient
safety. Ventricular tachycardia (VT) alarms are particularly difficult to
detect accurately due to their complex nature. This paper presents a machine
learning approach to reduce false VT alarms using the VTaC dataset, a benchmark
dataset of annotated VT alarms from ICU monitors. We extract time-domain and
frequency-domain features from waveform data, preprocess the data, and train
deep learning models to classify true and false VT alarms. Our results
demonstrate high performance, with ROC-AUC scores exceeding 0.96 across various
training configurations. This work highlights the potential of machine learning
to improve the accuracy of VT alarm detection in clinical settings.","['cs.LG', 'cs.AI']","['Grace Funmilayo Farayola', 'Akinyemi Sadeeq Akintola', 'Oluwole Fagbohun', 'Chukwuka Michael Oforgu', 'Bisola Faith Kayode', 'Christian Chimezie', 'Temitope Kadri', 'Abiola Oludotun', 'Nelson Ogbeide', 'Mgbame Michael', 'Adeseye Ifaturoti', 'Toyese Oloyede']",2025-03-18,2025-03-18
2503.14620v1,Retrieval-Augmented Simulacra: Generative Agents for Up-to-date and Knowledge-Adaptive Simulations,"In the 2023 edition of the White Paper on Information and Communications, it
is estimated that the population of social networking services in Japan will
exceed 100 million by 2022, and the influence of social networking services in
Japan is growing significantly. In addition, marketing using SNS and research
on the propagation of emotions and information on SNS are being actively
conducted, creating the need for a system for predicting trends in SNS
interactions. We have already created a system that simulates the behavior of
various communities on SNS by building a virtual SNS environment in which
agents post and reply to each other in a chat community created by agents using
a LLMs. In this paper, we evaluate the impact of the search extension
generation mechanism used to create posts and replies in a virtual SNS
environment using a simulation system on the ability to generate posts and
replies. As a result of the evaluation, we confirmed that the proposed search
extension generation mechanism, which mimics human search behavior, generates
the most natural exchange.","['cs.CL', 'cs.SI']","['Hikaru Shimadzu', 'Takehito Utsuro', 'Daisuke Kitayama']",2025-03-18,2025-03-18
2503.14618v1,Anomaly-Flow: A Multi-domain Federated Generative Adversarial Network for Distributed Denial-of-Service Detection,"Distributed denial-of-service (DDoS) attacks remain a critical threat to
Internet services, causing costly disruptions. While machine learning (ML) has
shown promise in DDoS detection, current solutions struggle with multi-domain
environments where attacks must be detected across heterogeneous networks and
organizational boundaries. This limitation severely impacts the practical
deployment of ML-based defenses in real-world settings.
  This paper introduces Anomaly-Flow, a novel framework that addresses this
critical gap by combining Federated Learning (FL) with Generative Adversarial
Networks (GANs) for privacy-preserving, multi-domain DDoS detection. Our
proposal enables collaborative learning across diverse network domains while
preserving data privacy through synthetic flow generation. Through extensive
evaluation across three distinct network datasets, Anomaly-Flow achieves an
average F1-score of $0.747$, outperforming baseline models. Importantly, our
framework enables organizations to share attack detection capabilities without
exposing sensitive network data, making it particularly valuable for critical
infrastructure and privacy-sensitive sectors.
  Beyond immediate technical contributions, this work provides insights into
the challenges and opportunities in multi-domain DDoS detection, establishing a
foundation for future research in collaborative network defense systems. Our
findings have important implications for academic research and industry
practitioners working to deploy practical ML-based security solutions.","['cs.CR', 'cs.LG']","['Leonardo Henrique de Melo', 'Gustavo de Carvalho Bertoli', 'Michele Nogueira', 'Aldri Luiz dos Santos', 'Lourenço Alves Pereira Junior']",2025-03-18,2025-03-18
2503.14615v1,Unique Hard Attention: A Tale of Two Sides,"Understanding the expressive power of transformers has recently attracted
attention, as it offers insights into their abilities and limitations. Many
studies analyze unique hard attention transformers, where attention selects a
single position that maximizes the attention scores. When multiple positions
achieve the maximum score, either the rightmost or the leftmost of those is
chosen. In this paper, we highlight the importance of this seeming triviality.
Recently, finite-precision transformers with both leftmost- and rightmost-hard
attention were shown to be equivalent to Linear Temporal Logic (LTL). We show
that this no longer holds with only leftmost-hard attention -- in that case,
they correspond to a \emph{strictly weaker} fragment of LTL. Furthermore, we
show that models with leftmost-hard attention are equivalent to \emph{soft}
attention, suggesting they may better approximate real-world transformers than
right-attention models. These findings refine the landscape of transformer
expressivity and underscore the role of attention directionality.","['cs.LG', 'cs.CC', 'cs.CL', 'cs.FL']","['Selim Jerad', 'Anej Svete', 'Jiaoda Li', 'Ryan Cotterell']",2025-03-18,2025-03-18
2503.14607v1,Can Large Vision Language Models Read Maps Like a Human?,"In this paper, we introduce MapBench-the first dataset specifically designed
for human-readable, pixel-based map-based outdoor navigation, curated from
complex path finding scenarios. MapBench comprises over 1600 pixel space map
path finding problems from 100 diverse maps. In MapBench, LVLMs generate
language-based navigation instructions given a map image and a query with
beginning and end landmarks. For each map, MapBench provides Map Space Scene
Graph (MSSG) as an indexing data structure to convert between natural language
and evaluate LVLM-generated results. We demonstrate that MapBench significantly
challenges state-of-the-art LVLMs both zero-shot prompting and a
Chain-of-Thought (CoT) augmented reasoning framework that decomposes map
navigation into sequential cognitive processes. Our evaluation of both
open-source and closed-source LVLMs underscores the substantial difficulty
posed by MapBench, revealing critical limitations in their spatial reasoning
and structured decision-making capabilities. We release all the code and
dataset in https://github.com/taco-group/MapBench.",['cs.CV'],"['Shuo Xing', 'Zezhou Sun', 'Shuangyu Xie', 'Kaiyuan Chen', 'Yanjia Huang', 'Yuping Wang', 'Jiachen Li', 'Dezhen Song', 'Zhengzhong Tu']",2025-03-18,2025-03-18
2503.16535v1,Vision-Language Embodiment for Monocular Depth Estimation,"Depth estimation is a core problem in robotic perception and vision tasks,
but 3D reconstruction from a single image presents inherent uncertainties.
Current depth estimation models primarily rely on inter-image relationships for
supervised training, often overlooking the intrinsic information provided by
the camera itself. We propose a method that embodies the camera model and its
physical characteristics into a deep learning model, computing embodied scene
depth through real-time interactions with road environments. The model can
calculate embodied scene depth in real-time based on immediate environmental
changes using only the intrinsic properties of the camera, without any
additional equipment. By combining embodied scene depth with RGB image
features, the model gains a comprehensive perspective on both geometric and
visual details. Additionally, we incorporate text descriptions containing
environmental content and depth information as priors for scene understanding,
enriching the model's perception of objects. This integration of image and
language - two inherently ambiguous modalities - leverages their complementary
strengths for monocular depth estimation. The real-time nature of the embodied
language and depth prior model ensures that the model can continuously adjust
its perception and behavior in dynamic environments. Experimental results show
that the embodied depth estimation method enhances model performance across
different scenes.",['cs.CV'],"['Jinchang Zhang', 'Guoyu Lu']",2025-03-18,2025-03-18
2503.14604v1,Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives,"The evaluation of machine-generated image captions is a complex and evolving
challenge. With the advent of Multimodal Large Language Models (MLLMs), image
captioning has become a core task, increasing the need for robust and reliable
evaluation metrics. This survey provides a comprehensive overview of
advancements in image captioning evaluation, analyzing the evolution,
strengths, and limitations of existing metrics. We assess these metrics across
multiple dimensions, including correlation with human judgment, ranking
accuracy, and sensitivity to hallucinations. Additionally, we explore the
challenges posed by the longer and more detailed captions generated by MLLMs
and examine the adaptability of current metrics to these stylistic variations.
Our analysis highlights some limitations of standard evaluation approaches and
suggests promising directions for future research in image captioning
assessment.","['cs.CV', 'cs.AI', 'cs.CL']","['Sara Sarto', 'Marcella Cornia', 'Rita Cucchiara']",2025-03-18,2025-03-18
2503.14603v1,"Command R7B Arabic: A Small, Enterprise Focused, Multilingual, and Culturally Aware Arabic LLM","Building high-quality large language models (LLMs) for enterprise Arabic
applications remains challenging due to the limited availability of digitized
Arabic data. In this work, we present a data synthesis and refinement strategy
to help address this problem, namely, by leveraging synthetic data generation
and human-in-the-loop annotation to expand our Arabic training corpus. We
further present our iterative post training recipe that is essential to
achieving state-of-the-art performance in aligning the model with human
preferences, a critical aspect to enterprise use cases. The culmination of this
effort is the release of a small, 7B, open-weight model that outperforms
similarly sized peers in head-to-head comparisons and on Arabic-focused
benchmarks covering cultural knowledge, instruction following, RAG, and
contextual faithfulness.","['cs.CL', 'cs.LG']","['Yazeed Alnumay', 'Alexandre Barbet', 'Anna Bialas', 'William Darling', 'Shaan Desai', 'Joan Devassy', 'Kyle Duffy', 'Stephanie Howe', 'Olivia Lasche', 'Justin Lee', 'Anirudh Shrinivason', 'Jennifer Tracey']",2025-03-18,2025-03-18
2503.14505v1,MusicInfuser: Making Video Diffusion Listen and Dance,"We introduce MusicInfuser, an approach for generating high-quality dance
videos that are synchronized to a specified music track. Rather than attempting
to design and train a new multimodal audio-video model, we show how existing
video diffusion models can be adapted to align with musical inputs by
introducing lightweight music-video cross-attention and a low-rank adapter.
Unlike prior work requiring motion capture data, our approach fine-tunes only
on dance videos. MusicInfuser achieves high-quality music-driven video
generation while preserving the flexibility and generative capabilities of the
underlying models. We introduce an evaluation framework using Video-LLMs to
assess multiple dimensions of dance generation quality. The project page and
code are available at https://susunghong.github.io/MusicInfuser.","['cs.CV', 'cs.AI', 'cs.LG']","['Susung Hong', 'Ira Kemelmacher-Shlizerman', 'Brian Curless', 'Steven M. Seitz']",2025-03-18,2025-03-18
2503.14504v1,Aligning Multimodal LLM with Human Preference: A Survey,"Large language models (LLMs) can handle a wide variety of general tasks with
simple prompts, without the need for task-specific training. Multimodal Large
Language Models (MLLMs), built upon LLMs, have demonstrated impressive
potential in tackling complex tasks involving visual, auditory, and textual
data. However, critical issues related to truthfulness, safety, o1-like
reasoning, and alignment with human preference remain insufficiently addressed.
This gap has spurred the emergence of various alignment algorithms, each
targeting different application scenarios and optimization goals. Recent
studies have shown that alignment algorithms are a powerful approach to
resolving the aforementioned challenges. In this paper, we aim to provide a
comprehensive and systematic review of alignment algorithms for MLLMs.
Specifically, we explore four key aspects: (1) the application scenarios
covered by alignment algorithms, including general image understanding,
multi-image, video, and audio, and extended multimodal applications; (2) the
core factors in constructing alignment datasets, including data sources, model
responses, and preference annotations; (3) the benchmarks used to evaluate
alignment algorithms; and (4) a discussion of potential future directions for
the development of alignment algorithms. This work seeks to help researchers
organize current advancements in the field and inspire better alignment
methods. The project page of this paper is available at
https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.",['cs.CV'],"['Tao Yu', 'Yi-Fan Zhang', 'Chaoyou Fu', 'Junkang Wu', 'Jinda Lu', 'Kun Wang', 'Xingyu Lu', 'Yunhang Shen', 'Guibin Zhang', 'Dingjie Song', 'Yibo Yan', 'Tianlong Xu', 'Qingsong Wen', 'Zhang Zhang', 'Yan Huang', 'Liang Wang', 'Tieniu Tan']",2025-03-18,2025-03-18
2503.14503v1,The Power of Context: How Multimodality Improves Image Super-Resolution,"Single-image super-resolution (SISR) remains challenging due to the inherent
difficulty of recovering fine-grained details and preserving perceptual quality
from low-resolution inputs. Existing methods often rely on limited image
priors, leading to suboptimal results. We propose a novel approach that
leverages the rich contextual information available in multiple modalities --
including depth, segmentation, edges, and text prompts -- to learn a powerful
generative prior for SISR within a diffusion model framework. We introduce a
flexible network architecture that effectively fuses multimodal information,
accommodating an arbitrary number of input modalities without requiring
significant modifications to the diffusion process. Crucially, we mitigate
hallucinations, often introduced by text prompts, by using spatial information
from other modalities to guide regional text-based conditioning. Each
modality's guidance strength can also be controlled independently, allowing
steering outputs toward different directions, such as increasing bokeh through
depth or adjusting object prominence via segmentation. Extensive experiments
demonstrate that our model surpasses state-of-the-art generative SISR methods,
achieving superior visual quality and fidelity. See project page at
https://mmsr.kfmei.com/.","['cs.CV', 'cs.AI', 'cs.LG']","['Kangfu Mei', 'Hossein Talebi', 'Mojtaba Ardakani', 'Vishal M. Patel', 'Peyman Milanfar', 'Mauricio Delbracio']",2025-03-18,2025-03-18
2503.14501v2,Advances in 4D Generation: A Survey,"Generative artificial intelligence (AI) has made significant progress across
various domains in recent years. Building on the rapid advancements in 2D,
video, and 3D content generation fields, 4D generation has emerged as a novel
and rapidly evolving research area, attracting growing attention. 4D generation
focuses on creating dynamic 3D assets with spatiotemporal consistency based on
user input, offering greater creative freedom and richer immersive experiences.
This paper presents a comprehensive survey of the 4D generation field,
systematically summarizing its core technologies, developmental trajectory, key
challenges, and practical applications, while also exploring potential future
research directions. The survey begins by introducing various fundamental 4D
representation models, followed by a review of 4D generation frameworks built
upon these representations and the key technologies that incorporate motion and
geometry priors into 4D assets. We summarize five major challenges of 4D
generation: consistency, controllability, diversity, efficiency, and fidelity,
accompanied by an outline of existing solutions to address these issues. We
systematically analyze applications of 4D generation, spanning dynamic object
generation, scene generation, digital human synthesis, 4D editing, and
autonomous driving. Finally, we provide an in-depth discussion of the obstacles
currently hindering the development of the 4D generation. This survey offers a
clear and comprehensive overview of 4D generation, aiming to stimulate further
exploration and innovation in this rapidly evolving field. Our code is publicly
available at: https://github.com/MiaoQiaowei/Awesome-4D.",['cs.CV'],"['Qiaowei Miao', 'Kehan Li', 'Jinsheng Quan', 'Zhiyuan Min', 'Shaojie Ma', 'Yichao Xu', 'Yi Yang', 'Yawei Luo']",2025-03-18,2025-03-19
2503.14500v1,Utilization of Neighbor Information for Image Classification with Different Levels of Supervision,"We propose to bridge the gap between semi-supervised and unsupervised image
recognition with a flexible method that performs well for both generalized
category discovery (GCD) and image clustering. Despite the overlap in
motivation between these tasks, the methods themselves are restricted to a
single task -- GCD methods are reliant on the labeled portion of the data, and
deep image clustering methods have no built-in way to leverage the labels
efficiently. We connect the two regimes with an innovative approach that
Utilizes Neighbor Information for Classification (UNIC) both in the
unsupervised (clustering) and semisupervised (GCD) setting. State-of-the-art
clustering methods already rely heavily on nearest neighbors. We improve on
their results substantially in two parts, first with a sampling and cleaning
strategy where we identify accurate positive and negative neighbors, and
secondly by finetuning the backbone with clustering losses computed by sampling
both types of neighbors. We then adapt this pipeline to GCD by utilizing the
labelled images as ground truth neighbors. Our method yields state-of-the-art
results for both clustering (+3% ImageNet-100, Imagenet200) and GCD (+0.8%
ImageNet-100, +5% CUB, +2% SCars, +4% Aircraft).","['cs.CV', 'cs.LG']","['Gihan Jayatilaka', 'Abhinav Shrivastava', 'Matthew Gwilliam']",2025-03-18,2025-03-18
2503.14499v1,Measuring AI Ability to Complete Long Tasks,"Despite rapid progress on AI benchmarks, the real-world meaning of benchmark
performance remains unclear. To quantify the capabilities of AI systems in
terms of human capabilities, we propose a new metric: 50%-task-completion time
horizon. This is the time humans typically take to complete tasks that AI
models can complete with 50% success rate. We first timed humans with relevant
domain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter
tasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet
have a 50% time horizon of around 50 minutes. Furthermore, frontier AI time
horizon has been doubling approximately every seven months since 2019, though
the trend may have accelerated in 2024. The increase in AI models' time
horizons seems to be primarily driven by greater reliability and ability to
adapt to mistakes, combined with better logical reasoning and tool use
capabilities. We discuss the limitations of our results -- including their
degree of external validity -- and the implications of increased autonomy for
dangerous capabilities. If these results generalize to real-world software
tasks, extrapolation of this trend predicts that within 5 years, AI systems
will be capable of automating many software tasks that currently take humans a
month.","['cs.AI', 'cs.LG']","['Thomas Kwa', 'Ben West', 'Joel Becker', 'Amy Deng', 'Katharyn Garcia', 'Max Hasin', 'Sami Jawhar', 'Megan Kinniment', 'Nate Rush', 'Sydney Von Arx', 'Ryan Bloom', 'Thomas Broadley', 'Haoxing Du', 'Brian Goodrich', 'Nikola Jurkovic', 'Luke Harold Miles', 'Seraphina Nix', 'Tao Lin', 'Neev Parikh', 'David Rein', 'Lucas Jun Koba Sato', 'Hjalmar Wijk', 'Daniel M. Ziegler', 'Elizabeth Barnes', 'Lawrence Chan']",2025-03-18,2025-03-18
2503.14498v1,Tracking Meets Large Multimodal Models for Driving Scenario Understanding,"Large Multimodal Models (LMMs) have recently gained prominence in autonomous
driving research, showcasing promising capabilities across various emerging
benchmarks. LMMs specifically designed for this domain have demonstrated
effective perception, planning, and prediction skills. However, many of these
methods underutilize 3D spatial and temporal elements, relying mainly on image
data. As a result, their effectiveness in dynamic driving environments is
limited. We propose to integrate tracking information as an additional input to
recover 3D spatial and temporal details that are not effectively captured in
the images. We introduce a novel approach for embedding this tracking
information into LMMs to enhance their spatiotemporal understanding of driving
scenarios. By incorporating 3D tracking data through a track encoder, we enrich
visual queries with crucial spatial and temporal cues while avoiding the
computational overhead associated with processing lengthy video sequences or
extensive 3D inputs. Moreover, we employ a self-supervised approach to pretrain
the tracking encoder to provide LMMs with additional contextual information,
significantly improving their performance in perception, planning, and
prediction tasks for autonomous driving. Experimental results demonstrate the
effectiveness of our approach, with a gain of 9.5% in accuracy, an increase of
7.04 points in the ChatGPT score, and 9.4% increase in the overall score over
baseline models on DriveLM-nuScenes benchmark, along with a 3.7% final score
improvement on DriveLM-CARLA. Our code is available at
https://github.com/mbzuai-oryx/TrackingMeetsLMM","['cs.CV', 'cs.RO']","['Ayesha Ishaq', 'Jean Lahoud', 'Fahad Shahbaz Khan', 'Salman Khan', 'Hisham Cholakkal', 'Rao Muhammad Anwer']",2025-03-18,2025-03-18
2503.14495v1,Temporal Consistency for LLM Reasoning Process Error Identification,"Verification is crucial for effective mathematical reasoning. We present a
new temporal consistency method where verifiers iteratively refine their
judgments based on the previous assessment. Unlike one-round verification or
multi-model debate approaches, our method leverages consistency in a sequence
of self-reflection actions to improve verification accuracy. Empirical
evaluations across diverse mathematical process error identification benchmarks
(Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements
over baseline methods. When applied to the recent DeepSeek R1 distilled models,
our method demonstrates strong performance, enabling 7B/8B distilled models to
outperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the
distilled 14B model with our method achieves performance comparable to
Deepseek-R1. Our codes are available at
https://github.com/jcguo123/Temporal-Consistency","['cs.CL', 'cs.LG']","['Jiacheng Guo', 'Yue Wu', 'Jiahao Qiu', 'Kaixuan Huang', 'Xinzhe Juan', 'Ling Yang', 'Mengdi Wang']",2025-03-18,2025-03-18
2503.14494v1,Deeply Supervised Flow-Based Generative Models,"Flow based generative models have charted an impressive path across multiple
visual generation tasks by adhering to a simple principle: learning velocity
representations of a linear interpolant. However, we observe that training
velocity solely from the final layer output underutilizes the rich inter layer
representations, potentially impeding model convergence. To address this
limitation, we introduce DeepFlow, a novel framework that enhances velocity
representation through inter layer communication. DeepFlow partitions
transformer layers into balanced branches with deep supervision and inserts a
lightweight Velocity Refiner with Acceleration (VeRA) block between adjacent
branches, which aligns the intermediate velocity features within transformer
blocks. Powered by the improved deep supervision via the internal velocity
alignment, DeepFlow converges 8 times faster on ImageNet with equivalent
performance and further reduces FID by 2.6 while halving training time compared
to previous flow based models without a classifier free guidance. DeepFlow also
outperforms baselines in text to image generation tasks, as evidenced by
evaluations on MSCOCO and zero shot GenEval.",['cs.CV'],"['Inkyu Shin', 'Chenglin Yang', 'Liang-Chieh Chen']",2025-03-18,2025-03-18
2503.14493v2,State Space Model Meets Transformer: A New Paradigm for 3D Object Detection,"DETR-based methods, which use multi-layer transformer decoders to refine
object queries iteratively, have shown promising performance in 3D indoor
object detection. However, the scene point features in the transformer decoder
remain fixed, leading to minimal contributions from later decoder layers,
thereby limiting performance improvement. Recently, State Space Models (SSM)
have shown efficient context modeling ability with linear complexity through
iterative interactions between system states and inputs. Inspired by SSMs, we
propose a new 3D object DEtection paradigm with an interactive STate space
model (DEST). In the interactive SSM, we design a novel state-dependent SSM
parameterization method that enables system states to effectively serve as
queries in 3D indoor detection tasks. In addition, we introduce four key
designs tailored to the characteristics of point cloud and SSM: The
serialization and bidirectional scanning strategies enable bidirectional
feature interaction among scene points within the SSM. The inter-state
attention mechanism models the relationships between state points, while the
gated feed-forward network enhances inter-channel correlations. To the best of
our knowledge, this is the first method to model queries as system states and
scene points as system inputs, which can simultaneously update scene point
features and query features with linear complexity. Extensive experiments on
two challenging datasets demonstrate the effectiveness of our DEST-based
method. Our method improves the GroupFree baseline in terms of AP50 on ScanNet
V2 (+5.3) and SUN RGB-D (+3.2) datasets. Based on the VDETR baseline, Our
method sets a new SOTA on the ScanNetV2 and SUN RGB-D datasets.","['cs.CV', 'cs.AI']","['Chuxin Wang', 'Wenfei Yang', 'Xiang Liu', 'Tianzhu Zhang']",2025-03-18,2025-03-19
2503.14492v1,Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control,"We introduce Cosmos-Transfer, a conditional world generation model that can
generate world simulations based on multiple spatial control inputs of various
modalities such as segmentation, depth, and edge. In the design, the spatial
conditional scheme is adaptive and customizable. It allows weighting different
conditional inputs differently at different spatial locations. This enables
highly controllable world generation and finds use in various world-to-world
transfer use cases, including Sim2Real. We conduct extensive evaluations to
analyze the proposed model and demonstrate its applications for Physical AI,
including robotics Sim2Real and autonomous vehicle data enrichment. We further
demonstrate an inference scaling strategy to achieve real-time world generation
with an NVIDIA GB200 NVL72 rack. To help accelerate research development in the
field, we open-source our models and code at
https://github.com/nvidia-cosmos/cosmos-transfer1.","['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO']","['NVIDIA', ':', 'Hassan Abu Alhaija', 'Jose Alvarez', 'Maciej Bala', 'Tiffany Cai', 'Tianshi Cao', 'Liz Cha', 'Joshua Chen', 'Mike Chen', 'Francesco Ferroni', 'Sanja Fidler', 'Dieter Fox', 'Yunhao Ge', 'Jinwei Gu', 'Ali Hassani', 'Michael Isaev', 'Pooya Jannaty', 'Shiyi Lan', 'Tobias Lasser', 'Huan Ling', 'Ming-Yu Liu', 'Xian Liu', 'Yifan Lu', 'Alice Luo', 'Qianli Ma', 'Hanzi Mao', 'Fabio Ramos', 'Xuanchi Ren', 'Tianchang Shen', 'Shitao Tang', 'Ting-Chun Wang', 'Jay Wu', 'Jiashu Xu', 'Stella Xu', 'Kevin Xie', 'Yuchong Ye', 'Xiaodong Yang', 'Xiaohui Zeng', 'Yu Zeng']",2025-03-18,2025-03-18
2503.14489v1,Stable Virtual Camera: Generative View Synthesis with Diffusion Models,"We present Stable Virtual Camera (Seva), a generalist diffusion model that
creates novel views of a scene, given any number of input views and target
cameras. Existing works struggle to generate either large viewpoint changes or
temporally smooth samples, while relying on specific task configurations. Our
approach overcomes these limitations through simple model design, optimized
training recipe, and flexible sampling strategy that generalize across view
synthesis tasks at test time. As a result, our samples maintain high
consistency without requiring additional 3D representation-based distillation,
thus streamlining view synthesis in the wild. Furthermore, we show that our
method can generate high-quality videos lasting up to half a minute with
seamless loop closure. Extensive benchmarking demonstrates that Seva
outperforms existing methods across different datasets and settings.",['cs.CV'],"['Jensen', 'Zhou', 'Hang Gao', 'Vikram Voleti', 'Aaryaman Vasishta', 'Chun-Han Yao', 'Mark Boss', 'Philip Torr', 'Christian Rupprecht', 'Varun Jampani']",2025-03-18,2025-03-18
2503.14488v1,Engineering Scientific Assistants using Interactive Structured Induction of Programs,"We are interested in the construction of software that can act as scientific
assistants to domain specialists. It is expected that such assistants will be
needed to accelerate the identification of ways to address complex problems
requiring urgent solutions. In this paper, our focus is not on a specific
scientific problem, but on the software-engineering of such 'science
accelerators'. Recent developments in 'No Code' techniques would seem to
suggest that scientist can simply hypothesise solutions simply by conversing
with a large language model (LLM). However, for complex scientific problems,
this seems unlikely given the current state of LLM technology. What does appear
feasible is that a software engineer can use LLMs to rapidly construct programs
for use by a domain-specialist, including the specialist's requirements
expressed in natural language. We propose the design of an interactive form of
'structured' inductive programming in which a software-engineer and an LLM
collaboratively construct an 'assistant' for a scientific data analysis. The
paper describes a simple implementation called iStrucInd that adapts a '2-way
Intelligibility' protocol to implement the interaction between the software
engineer and the LLM. We test the tool on two different non-trivial scientific
data analysis tasks. Specifically, we compare the system constructed by
iStrucInd against systems constructed manually and by Low Code/No Code methods
along dimensions of: (a) program performance; (b) program quality; and (c)
programming effort. The results show iStrucInd allows a software engineer to
develop better programs faster suggesting interactive structured induction can
play a useful role in the rapid construction of scientific assistants.","['cs.AI', 'cs.SE']","['Shraddha Surana', 'Ashwin Srinivasan']",2025-03-18,2025-03-18
2503.14487v1,DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers,"Diffusion models have demonstrated remarkable success in various image
generation tasks, but their performance is often limited by the uniform
processing of inputs across varying conditions and noise levels. To address
this limitation, we propose a novel approach that leverages the inherent
heterogeneity of the diffusion process. Our method, DiffMoE, introduces a
batch-level global token pool that enables experts to access global token
distributions during training, promoting specialized expert behavior. To
unleash the full potential of the diffusion process, DiffMoE incorporates a
capacity predictor that dynamically allocates computational resources based on
noise levels and sample complexity. Through comprehensive evaluation, DiffMoE
achieves state-of-the-art performance among diffusion models on ImageNet
benchmark, substantially outperforming both dense architectures with 3x
activated parameters and existing MoE approaches while maintaining 1x activated
parameters. The effectiveness of our approach extends beyond class-conditional
generation to more challenging tasks such as text-to-image generation,
demonstrating its broad applicability across different diffusion model
applications. Project Page: https://shiml20.github.io/DiffMoE/","['cs.CV', 'cs.AI']","['Minglei Shi', 'Ziyang Yuan', 'Haotian Yang', 'Xintao Wang', 'Mingwu Zheng', 'Xin Tao', 'Wenliang Zhao', 'Wenzhao Zheng', 'Jie Zhou', 'Jiwen Lu', 'Pengfei Wan', 'Di Zhang', 'Kun Gai']",2025-03-18,2025-03-18
2503.14485v1,Lux Post Facto: Learning Portrait Performance Relighting with Conditional Video Diffusion and a Hybrid Dataset,"Video portrait relighting remains challenging because the results need to be
both photorealistic and temporally stable. This typically requires a strong
model design that can capture complex facial reflections as well as intensive
training on a high-quality paired video dataset, such as dynamic
one-light-at-a-time (OLAT). In this work, we introduce Lux Post Facto, a novel
portrait video relighting method that produces both photorealistic and
temporally consistent lighting effects. From the model side, we design a new
conditional video diffusion model built upon state-of-the-art pre-trained video
diffusion model, alongside a new lighting injection mechanism to enable precise
control. This way we leverage strong spatial and temporal generative capability
to generate plausible solutions to the ill-posed relighting problem. Our
technique uses a hybrid dataset consisting of static expression OLAT data and
in-the-wild portrait performance videos to jointly learn relighting and
temporal modeling. This avoids the need to acquire paired video data in
different lighting conditions. Our extensive experiments show that our model
produces state-of-the-art results both in terms of photorealism and temporal
consistency.","['cs.GR', 'cs.CV']","['Yiqun Mei', 'Mingming He', 'Li Ma', 'Julien Philip', 'Wenqi Xian', 'David M George', 'Xueming Yu', 'Gabriel Dedic', 'Ahmet Levent Taşel', 'Ning Yu', 'Vishal M. Patel', 'Paul Debevec']",2025-03-18,2025-03-18
2503.14484v1,Gricean Norms as a Basis for Effective Collaboration,"Effective human-AI collaboration hinges not only on the AI agent's ability to
follow explicit instructions but also on its capacity to navigate ambiguity,
incompleteness, invalidity, and irrelevance in communication. Gricean
conversational and inference norms facilitate collaboration by aligning unclear
instructions with cooperative principles. We propose a normative framework that
integrates Gricean norms and cognitive frameworks -- common ground, relevance
theory, and theory of mind -- into large language model (LLM) based agents. The
normative framework adopts the Gricean maxims of quantity, quality, relation,
and manner, along with inference, as Gricean norms to interpret unclear
instructions, which are: ambiguous, incomplete, invalid, or irrelevant. Within
this framework, we introduce Lamoids, GPT-4 powered agents designed to
collaborate with humans. To assess the influence of Gricean norms in human-AI
collaboration, we evaluate two versions of a Lamoid: one with norms and one
without. In our experiments, a Lamoid collaborates with a human to achieve
shared goals in a grid world (Doors, Keys, and Gems) by interpreting both clear
and unclear natural language instructions. Our results reveal that the Lamoid
with Gricean norms achieves higher task accuracy and generates clearer, more
accurate, and contextually relevant responses than the Lamoid without norms.
This improvement stems from the normative framework, which enhances the agent's
pragmatic reasoning, fostering effective human-AI collaboration and enabling
context-aware communication in LLM-based agents.","['cs.MA', 'cs.AI', 'cs.CL']","['Fardin Saad', 'Pradeep K. Murukannaiah', 'Munindar P. Singh']",2025-03-18,2025-03-18
2503.14483v1,Multi-view Reconstruction via SfM-guided Monocular Depth Estimation,"In this paper, we present a new method for multi-view geometric
reconstruction. In recent years, large vision models have rapidly developed,
performing excellently across various tasks and demonstrating remarkable
generalization capabilities. Some works use large vision models for monocular
depth estimation, which have been applied to facilitate multi-view
reconstruction tasks in an indirect manner. Due to the ambiguity of the
monocular depth estimation task, the estimated depth values are usually not
accurate enough, limiting their utility in aiding multi-view reconstruction. We
propose to incorporate SfM information, a strong multi-view prior, into the
depth estimation process, thus enhancing the quality of depth prediction and
enabling their direct application in multi-view geometric reconstruction.
Experimental results on public real-world datasets show that our method
significantly improves the quality of depth estimation compared to previous
monocular depth estimation works. Additionally, we evaluate the reconstruction
quality of our approach in various types of scenes including indoor,
streetscape, and aerial views, surpassing state-of-the-art MVS methods. The
code and supplementary materials are available at
https://zju3dv.github.io/murre/ .",['cs.CV'],"['Haoyu Guo', 'He Zhu', 'Sida Peng', 'Haotong Lin', 'Yunzhi Yan', 'Tao Xie', 'Wenguan Wang', 'Xiaowei Zhou', 'Hujun Bao']",2025-03-18,2025-03-18
2503.14482v1,ICE-Bench: A Unified and Comprehensive Benchmark for Image Creating and Editing,"Image generation has witnessed significant advancements in the past few
years. However, evaluating the performance of image generation models remains a
formidable challenge. In this paper, we propose ICE-Bench, a unified and
comprehensive benchmark designed to rigorously assess image generation models.
Its comprehensiveness could be summarized in the following key features: (1)
Coarse-to-Fine Tasks: We systematically deconstruct image generation into four
task categories: No-ref/Ref Image Creating/Editing, based on the presence or
absence of source images and reference images. And further decompose them into
31 fine-grained tasks covering a broad spectrum of image generation
requirements, culminating in a comprehensive benchmark. (2) Multi-dimensional
Metrics: The evaluation framework assesses image generation capabilities across
6 dimensions: aesthetic quality, imaging quality, prompt following, source
consistency, reference consistency, and controllability. 11 metrics are
introduced to support the multi-dimensional evaluation. Notably, we introduce
VLLM-QA, an innovative metric designed to assess the success of image editing
by leveraging large models. (3) Hybrid Data: The data comes from real scenes
and virtual generation, which effectively improves data diversity and
alleviates the bias problem in model evaluation. Through ICE-Bench, we conduct
a thorough analysis of existing generation models, revealing both the
challenging nature of our benchmark and the gap between current model
capabilities and real-world generation requirements. To foster further
advancements in the field, we will open-source ICE-Bench, including its
dataset, evaluation code, and models, thereby providing a valuable resource for
the research community.",['cs.CV'],"['Yulin Pan', 'Xiangteng He', 'Chaojie Mao', 'Zhen Han', 'Zeyinzi Jiang', 'Jingfeng Zhang', 'Yu Liu']",2025-03-18,2025-03-18
2503.14481v1,Don't lie to your friends: Learning what you know from collaborative self-play,"To be helpful assistants, AI agents must be aware of their own capabilities
and limitations. This includes knowing when to answer from parametric knowledge
versus using tools, when to trust tool outputs, and when to abstain or hedge.
Such capabilities are hard to teach through supervised fine-tuning because they
require constructing examples that reflect the agent's specific capabilities.
We therefore propose a radically new approach to teaching agents what they
know: \emph{collaborative self-play}. We construct multi-agent collaborations
in which the group is rewarded for collectively arriving at correct answers.
The desired meta-knowledge emerges from the incentives built into the structure
of the interaction. We focus on small societies of agents that have access to
heterogeneous tools (corpus-specific retrieval), and therefore must collaborate
to maximize their success while minimizing their effort. Experiments show that
group-level rewards for multi-agent communities can induce policies that
\emph{transfer} to improve tool use and selective prediction in settings where
individual agents are deployed in isolation.","['cs.LG', 'cs.CL']","['Jacob Eisenstein', 'Reza Aghajani', 'Adam Fisch', 'Dheeru Dua', 'Fantine Huot', 'Mirella Lapata', 'Vicky Zayats', 'Jonathan Berant']",2025-03-18,2025-03-18
2503.14478v2,Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM,"Creativity is a fundamental aspect of intelligence, involving the ability to
generate novel and appropriate solutions across diverse contexts. While Large
Language Models (LLMs) have been extensively evaluated for their creative
capabilities, the assessment of Multimodal Large Language Models (MLLMs) in
this domain remains largely unexplored. To address this gap, we introduce
Creation-MMBench, a multimodal benchmark specifically designed to evaluate the
creative capabilities of MLLMs in real-world, image-based tasks. The benchmark
comprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous
evaluation, we define instance-specific evaluation criteria for each test case,
guiding the assessment of both general response quality and factual consistency
with visual inputs. Experimental results reveal that current open-source MLLMs
significantly underperform compared to proprietary models in creative tasks.
Furthermore, our analysis demonstrates that visual fine-tuning can negatively
impact the base LLM's creative abilities. Creation-MMBench provides valuable
insights for advancing MLLM creativity and establishes a foundation for future
improvements in multimodal generative intelligence. Full data and evaluation
code is released on https://github.com/open-compass/Creation-MMBench.",['cs.CV'],"['Xinyu Fang', 'Zhijian Chen', 'Kai Lan', 'Lixin Ma', 'Shengyuan Ding', 'Yingji Liang', 'Xiangyu Zhao', 'Farong Wen', 'Zicheng Zhang', 'Guofeng Zhang', 'Haodong Duan', 'Kai Chen', 'Dahua Lin']",2025-03-18,2025-03-19
2503.14477v1,Calibrating Verbal Uncertainty as a Linear Feature to Reduce Hallucinations,"LLMs often adopt an assertive language style also when making false claims.
Such ``overconfident hallucinations'' mislead users and erode trust. Achieving
the ability to express in language the actual degree of uncertainty around a
claim is therefore of great importance. We find that ``verbal uncertainty'' is
governed by a single linear feature in the representation space of LLMs, and
show that this has only moderate correlation with the actual ``semantic
uncertainty'' of the model. We apply this insight and show that (1) the
mismatch between semantic and verbal uncertainty is a better predictor of
hallucinations than semantic uncertainty alone and (2) we can intervene on
verbal uncertainty at inference time and reduce hallucinations on short-form
answers, achieving an average relative reduction of 32%.",['cs.CL'],"['Ziwei Ji', 'Lei Yu', 'Yeskendir Koishekenov', 'Yejin Bang', 'Anthony Hartshorn', 'Alan Schelten', 'Cheng Zhang', 'Pascale Fung', 'Nicola Cancedda']",2025-03-18,2025-03-18
2503.14476v1,DAPO: An Open-Source LLM Reinforcement Learning System at Scale,"Inference scaling empowers LLMs with unprecedented reasoning ability, with
reinforcement learning as the core technique to elicit complex reasoning.
However, key technical details of state-of-the-art reasoning LLMs are concealed
(such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the
community still struggles to reproduce their RL training results. We propose
the $\textbf{D}$ecoupled Clip and $\textbf{D}$ynamic s$\textbf{A}$mpling
$\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{DAPO}$) algorithm, and
fully open-source a state-of-the-art large-scale RL system that achieves 50
points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that
withhold training details, we introduce four key techniques of our algorithm
that make large-scale LLM RL a success. In addition, we open-source our
training code, which is built on the verl framework, along with a carefully
curated and processed dataset. These components of our open-source system
enhance reproducibility and support future research in large-scale LLM RL.","['cs.LG', 'cs.CL']","['Qiying Yu', 'Zheng Zhang', 'Ruofei Zhu', 'Yufeng Yuan', 'Xiaochen Zuo', 'Yu Yue', 'Tiantian Fan', 'Gaohong Liu', 'Lingjun Liu', 'Xin Liu', 'Haibin Lin', 'Zhiqi Lin', 'Bole Ma', 'Guangming Sheng', 'Yuxuan Tong', 'Chi Zhang', 'Mofan Zhang', 'Wang Zhang', 'Hang Zhu', 'Jinhua Zhu', 'Jiaze Chen', 'Jiangjie Chen', 'Chengyi Wang', 'Hongli Yu', 'Weinan Dai', 'Yuxuan Song', 'Xiangpeng Wei', 'Hao Zhou', 'Jingjing Liu', 'Wei-Ying Ma', 'Ya-Qin Zhang', 'Lin Yan', 'Mu Qiao', 'Yonghui Wu', 'Mingxuan Wang']",2025-03-18,2025-03-18
2503.14475v1,Optimized 3D Gaussian Splatting using Coarse-to-Fine Image Frequency Modulation,"The field of Novel View Synthesis has been revolutionized by 3D Gaussian
Splatting (3DGS), which enables high-quality scene reconstruction that can be
rendered in real-time. 3DGS-based techniques typically suffer from high GPU
memory and disk storage requirements which limits their practical application
on consumer-grade devices. We propose Opti3DGS, a novel frequency-modulated
coarse-to-fine optimization framework that aims to minimize the number of
Gaussian primitives used to represent a scene, thus reducing memory and storage
demands. Opti3DGS leverages image frequency modulation, initially enforcing a
coarse scene representation and progressively refining it by modulating
frequency details in the training images. On the baseline 3DGS, we demonstrate
an average reduction of 62% in Gaussians, a 40% reduction in the training GPU
memory requirements and a 20% reduction in optimization time without
sacrificing the visual quality. Furthermore, we show that our method integrates
seamlessly with many 3DGS-based techniques, consistently reducing the number of
Gaussian primitives while maintaining, and often improving, visual quality.
Additionally, Opti3DGS inherently produces a level-of-detail scene
representation at no extra cost, a natural byproduct of the optimization
pipeline. Results and code will be made publicly available.","['cs.GR', 'cs.CV']","['Umar Farooq', 'Jean-Yves Guillemaut', 'Adrian Hilton', 'Marco Volino']",2025-03-18,2025-03-18
2503.14473v1,EnQode: Fast Amplitude Embedding for Quantum Machine Learning Using Classical Data,"Amplitude embedding (AE) is essential in quantum machine learning (QML) for
encoding classical data onto quantum circuits. However, conventional AE methods
suffer from deep, variable-length circuits that introduce high output error due
to extensive gate usage and variable error rates across samples, resulting in
noise-driven inconsistencies that degrade model accuracy. We introduce EnQode,
a fast AE technique based on symbolic representation that addresses these
limitations by clustering dataset samples and solving for cluster mean states
through a low-depth, machine-specific ansatz. Optimized to reduce physical
gates and SWAP operations, EnQode ensures all samples face consistent, low
noise levels by standardizing circuit depth and composition. With over 90%
fidelity in data mapping, EnQode enables robust, high-performance QML on noisy
intermediate-scale quantum (NISQ) devices. Our open-source solution provides a
scalable and efficient alternative for integrating classical data with quantum
models.","['quant-ph', 'cs.ET', 'cs.LG']","['Jason Han', 'Nicholas S. DiBrita', 'Younghyun Cho', 'Hengrui Luo', 'Tirthak Patel']",2025-03-18,2025-03-18
2503.14469v1,Attribution Score Alignment in Explainable Data Management,"Different attribution-scores have been proposed to quantify the relevance of
database tuples for a query answer from a database. Among them, we find Causal
Responsibility, the Shapley Value, the Banzhaf Power-Index, and the Causal
Effect. They have been analyzed in isolation, mainly in terms of computational
properties. In this work, we start an investigation into the alignment of these
scores on the basis of the queries at hand; that is, on whether they induce
compatible rankings of tuples. We are able to identify vast classes of queries
for which some pairs of scores are always aligned, and others for which they
are not. It turns out that the presence of exogenous tuples makes a crucial
difference in this regard.","['cs.DB', 'cs.AI']","['Felipe Azua', 'Leopoldo Bertossi']",2025-03-18,2025-03-18
2503.14463v1,SIR-DIFF: Sparse Image Sets Restoration with Multi-View Diffusion Model,"The computer vision community has developed numerous techniques for digitally
restoring true scene information from single-view degraded photographs, an
important yet extremely ill-posed task. In this work, we tackle image
restoration from a different perspective by jointly denoising multiple
photographs of the same scene. Our core hypothesis is that degraded images
capturing a shared scene contain complementary information that, when combined,
better constrains the restoration problem. To this end, we implement a powerful
multi-view diffusion model that jointly generates uncorrupted views by
extracting rich information from multi-view relationships. Our experiments show
that our multi-view approach outperforms existing single-view image and even
video-based methods on image deblurring and super-resolution tasks. Critically,
our model is trained to output 3D consistent images, making it a promising tool
for applications requiring robust multi-view integration, such as 3D
reconstruction or pose estimation.",['cs.CV'],"['Yucheng Mao', 'Boyang Wang', 'Nilesh Kulkarni', 'Jeong Joon Park']",2025-03-18,2025-03-18
2503.14459v1,Doubly robust identification of treatment effects from multiple environments,"Practical and ethical constraints often require the use of observational data
for causal inference, particularly in medicine and social sciences. Yet,
observational datasets are prone to confounding, potentially compromising the
validity of causal conclusions. While it is possible to correct for biases if
the underlying causal graph is known, this is rarely a feasible ask in
practical scenarios. A common strategy is to adjust for all available
covariates, yet this approach can yield biased treatment effect estimates,
especially when post-treatment or unobserved variables are present. We propose
RAMEN, an algorithm that produces unbiased treatment effect estimates by
leveraging the heterogeneity of multiple data sources without the need to know
or learn the underlying causal graph. Notably, RAMEN achieves doubly robust
identification: it can identify the treatment effect whenever the causal
parents of the treatment or those of the outcome are observed, and the node
whose parents are observed satisfies an invariance assumption. Empirical
evaluations on synthetic and real-world datasets show that our approach
outperforms existing methods.","['stat.ML', 'cs.LG', 'stat.ME']","['Piersilvio De Bartolomeis', 'Julia Kostin', 'Javier Abad', 'Yixin Wang', 'Fanny Yang']",2025-03-18,2025-03-18
2503.14456v1,"RWKV-7 ""Goose"" with Expressive Dynamic State Evolution","We present RWKV-7 ""Goose"", a new sequence modeling architecture, along with
pre-trained language models that establish a new state-of-the-art in downstream
performance at the 3 billion parameter scale on multilingual tasks, and match
current SoTA English language performance despite being trained on dramatically
fewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only
constant memory usage and constant inference time per token. RWKV-7 introduces
a newly generalized formulation of the delta rule with vector-valued gating and
in-context learning rates, as well as a relaxed value replacement rule. We show
that RWKV-7 can perform state tracking and recognize all regular languages,
while retaining parallelizability of training. This exceeds the capabilities of
Transformers under standard complexity conjectures, which are limited to
$\mathsf{TC}^0$. To demonstrate RWKV-7's language modeling capability, we also
present an extended open source 3.1 trillion token multilingual corpus, and
train four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on
this dataset.
  To foster openness, reproduction, and adoption, we release our models and
dataset component listing at https://huggingface.co/RWKV, and our training and
inference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0
License.","['cs.CL', 'cs.AI', 'cs.LG', 'I.2.0; I.2.7']","['Bo Peng', 'Ruichong Zhang', 'Daniel Goldstein', 'Eric Alcaide', 'Haowen Hou', 'Janna Lu', 'William Merrill', 'Guangyu Song', 'Kaifeng Tan', 'Saiteja Utpala', 'Nathan Wilce', 'Johan S. Wind', 'Tianyi Wu', 'Daniel Wuttke', 'Christian Zhou-Zheng']",2025-03-18,2025-03-18
2503.14453v1,Online Conformal Probabilistic Numerics via Adaptive Edge-Cloud Offloading,"Consider an edge computing setting in which a user submits queries for the
solution of a linear system to an edge processor, which is subject to
time-varying computing availability. The edge processor applies a probabilistic
linear solver (PLS) so as to be able to respond to the user's query within the
allotted time and computing budget. Feedback to the user is in the form of an
uncertainty set. Due to model misspecification, the uncertainty set obtained
via a direct application of PLS does not come with coverage guarantees with
respect to the true solution of the linear system. This work introduces a new
method to calibrate the uncertainty sets produced by PLS with the aim of
guaranteeing long-term coverage requirements. The proposed method, referred to
as online conformal prediction-PLS (OCP-PLS), assumes sporadic feedback from
cloud to edge. This enables the online calibration of uncertainty thresholds
via online conformal prediction (OCP), an online optimization method previously
studied in the context of prediction models. The validity of OCP-PLS is
verified via experiments that bring insights into trade-offs between coverage,
prediction set size, and cloud usage.","['stat.ML', 'cs.LG']","['Qiushuo Hou', 'Sangwoo Park', 'Matteo Zecchin', 'Yunlong Cai', 'Guanding Yu', 'Osvaldo Simeone']",2025-03-18,2025-03-18
2503.14448v1,Pauli Network Circuit Synthesis with Reinforcement Learning,"We introduce a Reinforcement Learning (RL)-based method for re-synthesis of
quantum circuits containing arbitrary Pauli rotations alongside Clifford
operations. By collapsing each sub-block to a compact representation and then
synthesizing it step-by-step through a learned heuristic, we obtain circuits
that are both shorter and compliant with hardware connectivity constraints. We
find that the method is fast enough and good enough to work as an optimization
procedure: in direct comparisons on 6-qubit random Pauli Networks against
state-of-the-art heuristic methods, our RL approach yields over 2x reduction in
two-qubit gate count, while executing in under 10 milliseconds per circuit. We
further integrate the method into a collect-and-re-synthesize pipeline, applied
as a Qiskit transpiler pass, where we observe average improvements of 20% in
two-qubit gate count and depth, reaching up to 60% for many instances, across
the Benchpress benchmark. These results highlight the potential of RL-driven
synthesis to significantly improve circuit quality in realistic, large-scale
quantum transpilation workloads.","['quant-ph', 'cs.AI']","['Ayushi Dubal', 'David Kremer', 'Simon Martiel', 'Victor Villar', 'Derek Wang', 'Juan Cruz-Benito']",2025-03-18,2025-03-18
2503.14445v1,Bolt3D: Generating 3D Scenes in Seconds,"We present a latent diffusion model for fast feed-forward 3D scene
generation. Given one or more images, our model Bolt3D directly samples a 3D
scene representation in less than seven seconds on a single GPU. We achieve
this by leveraging powerful and scalable existing 2D diffusion network
architectures to produce consistent high-fidelity 3D scene representations. To
train this model, we create a large-scale multiview-consistent dataset of 3D
geometry and appearance by applying state-of-the-art dense 3D reconstruction
techniques to existing multiview image datasets. Compared to prior multiview
generative models that require per-scene optimization for 3D reconstruction,
Bolt3D reduces the inference cost by a factor of up to 300 times.",['cs.CV'],"['Stanislaw Szymanowicz', 'Jason Y. Zhang', 'Pratul Srinivasan', 'Ruiqi Gao', 'Arthur Brussee', 'Aleksander Holynski', 'Ricardo Martin-Brualla', 'Jonathan T. Barron', 'Philipp Henzler']",2025-03-18,2025-03-18
2503.14443v1,EnvBench: A Benchmark for Automated Environment Setup,"Recent advances in Large Language Models (LLMs) have enabled researchers to
focus on practical repository-level tasks in software engineering domain. In
this work, we consider a cornerstone task for automating work with software
repositories-environment setup, i.e., a task of configuring a
repository-specific development environment on a system. Existing studies on
environment setup introduce innovative agentic strategies, but their evaluation
is often based on small datasets that may not capture the full range of
configuration challenges encountered in practice. To address this gap, we
introduce a comprehensive environment setup benchmark EnvBench. It encompasses
329 Python and 665 JVM-based (Java, Kotlin) repositories, with a focus on
repositories that present genuine configuration challenges, excluding projects
that can be fully configured by simple deterministic scripts. To enable further
benchmark extension and usage for model tuning, we implement two automatic
metrics: a static analysis check for missing imports in Python and a
compilation check for JVM languages. We demonstrate the applicability of our
benchmark by evaluating three environment setup approaches, including a simple
zero-shot baseline and two agentic workflows, that we test with two powerful
LLM backbones, GPT-4o and GPT-4o-mini. The best approach manages to
successfully configure 6.69% repositories for Python and 29.47% repositories
for JVM, suggesting that EnvBench remains challenging for current approaches.
Our benchmark suite is publicly available at
https://github.com/JetBrains-Research/EnvBench. The dataset and experiment
trajectories are available at https://jb.gg/envbench.","['cs.LG', 'cs.SE']","['Aleksandra Eliseeva', 'Alexander Kovrigin', 'Ilia Kholkin', 'Egor Bogomolov', 'Yaroslav Zharov']",2025-03-18,2025-03-18
2503.14442v1,Inducing Causal Structure for Interpretable Neural Networks Applied to Glucose Prediction for T1DM Patients,"Causal abstraction techniques such as Interchange Intervention Training (IIT)
have been proposed to infuse neural network with expert knowledge encoded in
causal models, but their application to real-world problems remains limited.
This article explores the application of IIT in predicting blood glucose levels
in Type 1 Diabetes Mellitus (T1DM) patients. The study utilizes an acyclic
version of the simglucose simulator approved by the FDA to train a Multi-Layer
Perceptron (MLP) model, employing IIT to impose causal relationships. Results
show that the model trained with IIT effectively abstracted the causal
structure and outperformed the standard one in terms of predictive performance
across different prediction horizons (PHs) post-meal. Furthermore, the
breakdown of the counterfactual loss can be leveraged to explain which part of
the causal mechanism are more or less effectively captured by the model. These
preliminary results suggest the potential of IIT in enhancing predictive models
in healthcare by effectively complying with expert knowledge.","['cs.LG', 'q-bio.BM']","['Ana Esponera', 'Giovanni Cinà']",2025-03-18,2025-03-18
2503.14439v1,Graph-CNNs for RF Imaging: Learning the Electric Field Integral Equations,"Radio-Frequency (RF) imaging concerns the digital recreation of the surfaces
of scene objects based on the scattered field at distributed receivers. To
solve this difficult inverse scattering problems, data-driven methods are often
employed that extract patterns from similar training examples, while offering
minimal latency. In this paper, we first provide an approximate yet fast
electromagnetic model, which is based on the electric field integral equations,
for data generation, and subsequently propose a Deep Neural Network (DNN)
architecture to learn the corresponding inverse model. A graph-attention
backbone allows for the system geometry to be passed to the DNN, where residual
convolutional layers extract features about the objects, while a UNet head
performs the final image reconstruction. Our quantitative and qualitative
evaluations on two synthetic data sets of different characteristics showcase
the performance gains of thee proposed advanced architecture and its relative
resilience to signal noise levels and various reception configurations.","['cs.LG', 'eess.SP']","['Kyriakos Stylianopoulos', 'Panagiotis Gavriilidis', 'Gabriele Gradoni', 'George C. Alexandropoulos']",2025-03-18,2025-03-18
2503.14434v1,LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as Evolutionary Optimizers,"Automated feature engineering plays a critical role in improving predictive
model performance for tabular learning tasks. Traditional automated feature
engineering methods are limited by their reliance on pre-defined
transformations within fixed, manually designed search spaces, often neglecting
domain knowledge. Recent advances using Large Language Models (LLMs) have
enabled the integration of domain knowledge into the feature engineering
process. However, existing LLM-based approaches use direct prompting or rely
solely on validation scores for feature selection, failing to leverage insights
from prior feature discovery experiments or establish meaningful reasoning
between feature generation and data-driven performance. To address these
challenges, we propose LLM-FE, a novel framework that combines evolutionary
search with the domain knowledge and reasoning capabilities of LLMs to
automatically discover effective features for tabular learning tasks. LLM-FE
formulates feature engineering as a program search problem, where LLMs propose
new feature transformation programs iteratively, and data-driven feedback
guides the search process. Our results demonstrate that LLM-FE consistently
outperforms state-of-the-art baselines, significantly enhancing the performance
of tabular prediction models across diverse classification and regression
benchmarks.","['cs.LG', 'cs.AI', 'cs.CL', 'cs.NE']","['Nikhil Abhyankar', 'Parshin Shojaee', 'Chandan K. Reddy']",2025-03-18,2025-03-18
2503.14433v1,Splintering Nonconcatenative Languages for Better Tokenization,"Common subword tokenization algorithms like BPE and UnigramLM assume that
text can be split into meaningful units by concatenative measures alone. This
is not true for languages such as Hebrew and Arabic, where morphology is
encoded in root-template patterns, or Malay and Georgian, where split affixes
are common. We present SPLINTER, a pre-processing step which rearranges text
into a linear form that better represents such nonconcatenative morphologies,
enabling meaningful contiguous segments to be found by the tokenizer. We
demonstrate SPLINTER's merit using both intrinsic measures evaluating token
vocabularies in Hebrew, Arabic, and Malay; as well as on downstream tasks using
BERT-architecture models trained for Hebrew.",['cs.CL'],"['Bar Gazit', 'Shaltiel Shmidman', 'Avi Shmidman', 'Yuval Pinter']",2025-03-18,2025-03-18
2503.14432v1,PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play,"Large language models (LLMs) are increasingly integrated with specialized
external tools, yet many tasks demand zero-shot tool usage with minimal or
noisy documentation. Existing solutions rely on manual rewriting or labeled
data for validation, making them inapplicable in true zero-shot settings. To
address these challenges, we propose PLAY2PROMPT, an automated framework that
systematically ""plays"" with each tool to explore its input-output behaviors.
Through this iterative trial-and-error process, PLAY2PROMPT refines tool
documentation and generates usage examples without any labeled data. These
examples not only guide LLM inference but also serve as validation to further
enhance tool utilization. Extensive experiments on real-world tasks demonstrate
that PLAY2PROMPT significantly improves zero-shot tool performance across both
open and closed models, offering a scalable and effective solution for
domain-specific tool integration.","['cs.CL', 'cs.AI', 'cs.LG']","['Wei Fang', 'Yang Zhang', 'Kaizhi Qian', 'James Glass', 'Yada Zhu']",2025-03-18,2025-03-18
2503.14428v1,MagicComp: Training-free Dual-Phase Refinement for Compositional Video Generation,"Text-to-video (T2V) generation has made significant strides with diffusion
models. However, existing methods still struggle with accurately binding
attributes, determining spatial relationships, and capturing complex action
interactions between multiple subjects. To address these limitations, we
propose MagicComp, a training-free method that enhances compositional T2V
generation through dual-phase refinement. Specifically, (1) During the
Conditioning Stage: We introduce the Semantic Anchor Disambiguation to
reinforces subject-specific semantics and resolve inter-subject ambiguity by
progressively injecting the directional vectors of semantic anchors into
original text embedding; (2) During the Denoising Stage: We propose Dynamic
Layout Fusion Attention, which integrates grounding priors and model-adaptive
spatial perception to flexibly bind subjects to their spatiotemporal regions
through masked attention modulation. Furthermore, MagicComp is a model-agnostic
and versatile approach, which can be seamlessly integrated into existing T2V
architectures. Extensive experiments on T2V-CompBench and VBench demonstrate
that MagicComp outperforms state-of-the-art methods, highlighting its potential
for applications such as complex prompt-based and trajectory-controllable video
generation. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/.","['cs.CV', 'cs.AI']","['Hongyu Zhang', 'Yufan Deng', 'Shenghai Yuan', 'Peng Jin', 'Zesen Cheng', 'Yian Zhao', 'Chang Liu', 'Jie Chen']",2025-03-18,2025-03-18
2503.14427v1,VisEscape: A Benchmark for Evaluating Exploration-driven Decision-making in Virtual Escape Rooms,"Escape rooms present a unique cognitive challenge that demands
exploration-driven planning: players should actively search their environment,
continuously update their knowledge based on new discoveries, and connect
disparate clues to determine which elements are relevant to their objectives.
Motivated by this, we introduce VisEscape, a benchmark of 20 virtual escape
rooms specifically designed to evaluate AI models under these challenging
conditions, where success depends not only on solving isolated puzzles but also
on iteratively constructing and refining spatial-temporal knowledge of a
dynamically changing environment. On VisEscape, we observed that even
state-of-the-art multimodal models generally fail to escape the rooms, showing
considerable variation in their levels of progress and trajectories. To address
this issue, we propose VisEscaper, which effectively integrates Memory,
Feedback, and ReAct modules, demonstrating significant improvements by
performing 3.7 times more effectively and 5.0 times more efficiently on
average.",['cs.AI'],"['Seungwon Lim', 'Sungwoong Kim', 'Jihwan Yu', 'Sungjae Lee', 'Jiwan Chung', 'Youngjae Yu']",2025-03-18,2025-03-18
2503.14421v1,ExDDV: A New Dataset for Explainable Deepfake Detection in Video,"The ever growing realism and quality of generated videos makes it
increasingly harder for humans to spot deepfake content, who need to rely more
and more on automatic deepfake detectors. However, deepfake detectors are also
prone to errors, and their decisions are not explainable, leaving humans
vulnerable to deepfake-based fraud and misinformation. To this end, we
introduce ExDDV, the first dataset and benchmark for Explainable Deepfake
Detection in Video. ExDDV comprises around 5.4K real and deepfake videos that
are manually annotated with text descriptions (to explain the artifacts) and
clicks (to point out the artifacts). We evaluate a number of vision-language
models on ExDDV, performing experiments with various fine-tuning and in-context
learning strategies. Our results show that text and click supervision are both
required to develop robust explainable models for deepfake videos, which are
able to localize and describe the observed artifacts. Our novel dataset and
code to reproduce the results are available at
https://github.com/vladhondru25/ExDDV.","['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG', 'cs.MM']","['Vlad Hondru', 'Eduard Hogea', 'Darian Onchis', 'Radu Tudor Ionescu']",2025-03-18,2025-03-18
2503.14412v1,Iffy-Or-Not: Extending the Web to Support the Critical Evaluation of Fallacious Texts,"Social platforms have expanded opportunities for deliberation with the
comments being used to inform one's opinion. However, using such information to
form opinions is challenged by unsubstantiated or false content. To enhance the
quality of opinion formation and potentially confer resistance to
misinformation, we developed Iffy-Or-Not (ION), a browser extension that seeks
to invoke critical thinking when reading texts. With three features guided by
argumentation theory, ION highlights fallacious content, suggests diverse
queries to probe them with, and offers deeper questions to consider and chat
with others about. From a user study (N=18), we found that ION encourages users
to be more attentive to the content, suggests queries that align with or are
preferable to their own, and poses thought-provoking questions that expands
their perspectives. However, some participants expressed aversion to ION due to
misalignments with their information goals and thinking predispositions.
Potential backfiring effects with ION are discussed.","['cs.HC', 'cs.AI']","['Gionnieve Lim', 'Juho Kim', 'Simon T. Perrault']",2025-03-18,2025-03-18
2503.14411v1,Unifying Text Semantics and Graph Structures for Temporal Text-attributed Graphs with Large Language Models,"Temporal graph neural networks (TGNNs) have shown remarkable performance in
temporal graph modeling. However, real-world temporal graphs often possess rich
textual information, giving rise to temporal text-attributed graphs (TTAGs).
Such combination of dynamic text semantics and evolving graph structures
introduces heightened complexity. Existing TGNNs embed texts statically and
rely heavily on encoding mechanisms that biasedly prioritize structural
information, overlooking the temporal evolution of text semantics and the
essential interplay between semantics and structures for synergistic
reinforcement. To tackle these issues, we present \textbf{{Cross}}, a novel
framework that seamlessly extends existing TGNNs for TTAG modeling. The key
idea is to employ the advanced large language models (LLMs) to extract the
dynamic semantics in text space and then generate expressive representations
unifying both semantics and structures. Specifically, we propose a Temporal
Semantics Extractor in the {Cross} framework, which empowers the LLM to offer
the temporal semantic understanding of node's evolving contexts of textual
neighborhoods, facilitating semantic dynamics. Subsequently, we introduce the
Semantic-structural Co-encoder, which collaborates with the above Extractor for
synthesizing illuminating representations by jointly considering both semantic
and structural information while encouraging their mutual reinforcement.
Extensive experimental results on four public datasets and one practical
industrial dataset demonstrate {Cross}'s significant effectiveness and
robustness.","['cs.CL', 'cs.AI']","['Siwei Zhang', 'Yun Xiong', 'Yateng Tang', 'Xi Chen', 'Zian Jia', 'Zehao Gu', 'Jiarong Xu', 'Jiawei Zhang']",2025-03-18,2025-03-18
2503.14408v1,Large Language Models for Virtual Human Gesture Selection,"Co-speech gestures convey a wide variety of meanings and play an important
role in face-to-face human interactions. These gestures significantly influence
the addressee's engagement, recall, comprehension, and attitudes toward the
speaker. Similarly, they impact interactions between humans and embodied
virtual agents. The process of selecting and animating meaningful gestures has
thus become a key focus in the design of these agents. However, automating this
gesture selection process poses a significant challenge. Prior gesture
generation techniques have varied from fully automated, data-driven methods,
which often struggle to produce contextually meaningful gestures, to more
manual approaches that require crafting specific gesture expertise and are
time-consuming and lack generalizability. In this paper, we leverage the
semantic capabilities of Large Language Models to develop a gesture selection
approach that suggests meaningful, appropriate co-speech gestures. We first
describe how information on gestures is encoded into GPT-4. Then, we conduct a
study to evaluate alternative prompting approaches for their ability to select
meaningful, contextually relevant gestures and to align them appropriately with
the co-speech utterance. Finally, we detail and demonstrate how this approach
has been implemented within a virtual agent system, automating the selection
and subsequent animation of the selected gestures for enhanced human-agent
interactions.","['cs.HC', 'cs.CL']","['Parisa Ghanad Torshizi', 'Laura B. Hensel', 'Ari Shapiro', 'Stacy C. Marsella']",2025-03-18,2025-03-18
2503.14405v1,DUNE: Distilling a Universal Encoder from Heterogeneous 2D and 3D Teachers,"Recent multi-teacher distillation methods have unified the encoders of
multiple foundation models into a single encoder, achieving competitive
performance on core vision tasks like classification, segmentation, and depth
estimation. This led us to ask: Could similar success be achieved when the pool
of teachers also includes vision models specialized in diverse tasks across
both 2D and 3D perception? In this paper, we define and investigate the problem
of heterogeneous teacher distillation, or co-distillation, a challenging
multi-teacher distillation scenario where teacher models vary significantly in
both (a) their design objectives and (b) the data they were trained on. We
explore data-sharing strategies and teacher-specific encoding, and introduce
DUNE, a single encoder excelling in 2D vision, 3D understanding, and 3D human
perception. Our model achieves performance comparable to that of its larger
teachers, sometimes even outperforming them, on their respective tasks.
Notably, DUNE surpasses MASt3R in Map-free Visual Relocalization with a much
smaller encoder.","['cs.CV', 'cs.LG']","['Mert Bulent Sariyildiz', 'Philippe Weinzaepfel', 'Thomas Lucas', 'Pau de Jorge', 'Diane Larlus', 'Yannis Kalantidis']",2025-03-18,2025-03-18
2503.14403v1,Landscape Complexity for the Empirical Risk of Generalized Linear Models: Discrimination between Structured Data,"We use the Kac-Rice formula and results from random matrix theory to obtain
the average number of critical points of a family of high-dimensional empirical
loss functions, where the data are correlated $d$-dimensional Gaussian vectors,
whose number has a fixed ratio with their dimension. The correlations are
introduced to model the existence of structure in the data, as is common in
current Machine-Learning systems. Under a technical hypothesis, our results are
exact in the large-$d$ limit, and characterize the annealed landscape
complexity, namely the logarithm of the expected number of critical points at a
given value of the loss.
  We first address in detail the landscape of the loss function of a single
perceptron and then generalize it to the case where two competing data sets
with different covariance matrices are present, with the perceptron seeking to
discriminate between them. The latter model can be applied to understand the
interplay between adversity and non-trivial data structure. For completeness,
we also treat the case of a loss function used in training Generalized Linear
Models in the presence of correlated input data.","['cs.LG', 'cond-mat.stat-mech', 'stat.ML']","['Theodoros G. Tsironis', 'Aris L. Moustakas']",2025-03-18,2025-03-18
2503.14402v1,Diffusion-based Facial Aesthetics Enhancement with 3D Structure Guidance,"Facial Aesthetics Enhancement (FAE) aims to improve facial attractiveness by
adjusting the structure and appearance of a facial image while preserving its
identity as much as possible. Most existing methods adopted deep feature-based
or score-based guidance for generation models to conduct FAE. Although these
methods achieved promising results, they potentially produced excessively
beautified results with lower identity consistency or insufficiently improved
facial attractiveness. To enhance facial aesthetics with less loss of identity,
we propose the Nearest Neighbor Structure Guidance based on Diffusion
(NNSG-Diffusion), a diffusion-based FAE method that beautifies a 2D facial
image with 3D structure guidance. Specifically, we propose to extract FAE
guidance from a nearest neighbor reference face. To allow for less change of
facial structures in the FAE process, a 3D face model is recovered by referring
to both the matched 2D reference face and the 2D input face, so that the depth
and contour guidance can be extracted from the 3D face model. Then the depth
and contour clues can provide effective guidance to Stable Diffusion with
ControlNet for FAE. Extensive experiments demonstrate that our method is
superior to previous relevant methods in enhancing facial aesthetics while
preserving facial identity.",['cs.CV'],"['Lisha Li', 'Jingwen Hou', 'Weide Liu', 'Yuming Fang', 'Jiebin Yan']",2025-03-18,2025-03-18
2503.14396v2,Technical Report: Aggregation on Learnable Manifolds for Asynchronous Federated Optimization,"In Federated Learning (FL), a primary challenge to the server-side
aggregation of client models is device heterogeneity in both loss landscape
geometry and computational capacity. This issue can be particularly pronounced
in clinical contexts where variations in data distribution (aggravated by class
imbalance), infrastructure requirements, and sample sizes are common. We
propose AsyncManifold, a novel asynchronous FL framework to address these
issues by taking advantage of underlying solution space geometry at each of the
local training, delay-correction, and aggregation stages. Our proposal is
accompanied by a convergence proof in a general form and, motivated through
exploratory studies of local behaviour, a proof-of-concept algorithm which
performs aggregation along non-linear mode connections and hence avoids
barriers to convergence that techniques based on linear interpolation will
encounter.",['cs.LG'],['Archie Licudi'],2025-03-18,2025-03-19
2503.14395v1,Weakly Supervised Spatial Implicit Neural Representation Learning for 3D MRI-Ultrasound Deformable Image Registration in HDR Prostate Brachytherapy,"Purpose: Accurate 3D MRI-ultrasound (US) deformable registration is critical
for real-time guidance in high-dose-rate (HDR) prostate brachytherapy. We
present a weakly supervised spatial implicit neural representation (SINR)
method to address modality differences and pelvic anatomy challenges.
  Methods: The framework uses sparse surface supervision from MRI/US
segmentations instead of dense intensity matching. SINR models deformations as
continuous spatial functions, with patient-specific surface priors guiding a
stationary velocity field for biologically plausible deformations. Validation
included 20 public Prostate-MRI-US-Biopsy cases and 10 institutional HDR cases,
evaluated via Dice similarity coefficient (DSC), mean surface distance (MSD),
and 95% Hausdorff distance (HD95).
  Results: The proposed method achieved robust registration. For the public
dataset, prostate DSC was $0.93 \pm 0.05$, MSD $0.87 \pm 0.10$ mm, and HD95
$1.58 \pm 0.37$ mm. For the institutional dataset, prostate CTV achieved DSC
$0.88 \pm 0.09$, MSD $1.21 \pm 0.38$ mm, and HD95 $2.09 \pm 1.48$ mm. Bladder
and rectum performance was lower due to ultrasound's limited field of view.
Visual assessments confirmed accurate alignment with minimal discrepancies.
  Conclusion: This study introduces a novel weakly supervised SINR-based
approach for 3D MRI-US deformable registration. By leveraging sparse surface
supervision and spatial priors, it achieves accurate, robust, and
computationally efficient registration, enhancing real-time image guidance in
HDR prostate brachytherapy and improving treatment precision.","['physics.med-ph', 'cs.CV']","['Jing Wang', 'Ruirui Liu', 'Yu Lei', 'Michael J. Baine', 'Tian Liu', 'Yang Lei']",2025-03-18,2025-03-18
2503.14393v1,On the clustering behavior of sliding windows,"Things can go spectacularly wrong when clustering timeseries data that has
been preprocessed with a sliding window. We highlight three surprising failures
that emerge depending on how the window size compares with the timeseries
length. In addition to computational examples, we present theoretical
explanations for each of these failure modes.",['cs.LG'],"['Boris Alexeev', 'Wenyan Luo', 'Dustin G. Mixon', 'Yan X Zhang']",2025-03-18,2025-03-18
2503.14392v1,"From ""Hallucination"" to ""Suture"": Insights from Language Philosophy to Enhance Large Language Models","This paper explores hallucination phenomena in large language models (LLMs)
through the lens of language philosophy and psychoanalysis. By incorporating
Lacan's concepts of the ""chain of signifiers"" and ""suture points,"" we propose
the Anchor-RAG framework as a novel approach to mitigate hallucinations. In
contrast to the predominant reliance on trial-and-error experiments, constant
adjustments of mathematical formulas, or resource-intensive methods that
emphasize quantity over quality, our approach returns to the fundamental
principles of linguistics to analyze the root causes of hallucinations in LLMs.
Drawing from robust theoretical foundations, we derive algorithms and models
that are not only effective in reducing hallucinations but also enhance LLM
performance and improve output quality. This paper seeks to establish a
comprehensive theoretical framework for understanding hallucinations in LLMs
and aims to challenge the prevalent ""guess-and-test"" approach and rat race
mentality in the field. We aspire to pave the way for a new era of
interpretable LLMs, offering deeper insights into the inner workings of
language-based AI systems.",['cs.CL'],['Qiantong Wang'],2025-03-18,2025-03-18
2503.14391v1,How much do LLMs learn from negative examples?,"Large language models (LLMs) undergo a three-phase training process:
unsupervised pre-training, supervised fine-tuning (SFT), and learning from
human feedback (RLHF/DPO). Notably, it is during the final phase that these
models are exposed to negative examples -- incorrect, rejected, or suboptimal
responses to queries. This paper delves into the role of negative examples in
the training of LLMs, using a likelihood-ratio (Likra) model on multiple-choice
question answering benchmarks to precisely manage the influence and the volume
of negative examples. Our findings reveal three key insights: (1) During a
critical phase in training, Likra with negative examples demonstrates a
significantly larger improvement per training example compared to SFT using
only positive examples. This leads to a sharp jump in the learning curve for
Likra unlike the smooth and gradual improvement of SFT; (2) negative examples
that are plausible but incorrect (near-misses) exert a greater influence; and
(3) while training with positive examples fails to significantly decrease the
likelihood of plausible but incorrect answers, training with negative examples
more accurately identifies them. These results indicate a potentially
significant role for negative examples in improving accuracy and reducing
hallucinations for LLMs.","['cs.CL', '68T50, 68T05', 'I.2.6; I.2.7']","['Shadi Hamdan', 'Deniz Yuret']",2025-03-18,2025-03-18
2503.14382v1,Good/Evil Reputation Judgment of Celebrities by LLMs via Retrieval Augmented Generation,"The purpose of this paper is to examine whether large language models (LLMs)
can understand what is good and evil with respect to judging good/evil
reputation of celebrities. Specifically, we first apply a large language model
(namely, ChatGPT) to the task of collecting sentences that mention the target
celebrity from articles about celebrities on Web pages. Next, the collected
sentences are categorized based on their contents by ChatGPT, where ChatGPT
assigns a category name to each of those categories. Those assigned category
names are referred to as ""aspects"" of each celebrity. Then, by applying the
framework of retrieval augmented generation (RAG), we show that the large
language model is quite effective in the task of judging good/evil reputation
of aspects and descriptions of each celebrity. Finally, also in terms of
proving the advantages of the proposed method over existing services
incorporating RAG functions, we show that the proposed method of judging
good/evil of aspects/descriptions of each celebrity significantly outperform an
existing service incorporating RAG functions.",['cs.CL'],"['Rikuto Tsuchida', 'Hibiki Yokoyama', 'Takehito Utsuro']",2025-03-18,2025-03-18
2503.14381v1,Optimizing High-Dimensional Oblique Splits,"Orthogonal-split trees perform well, but evidence suggests oblique splits can
enhance their performance. This paper explores optimizing high-dimensional
$s$-sparse oblique splits from $\{(\vec{w}, \vec{w}^{\top}\boldsymbol{X}_{i}) :
i\in \{1,\dots, n\}, \vec{w} \in \mathbb{R}^p, \| \vec{w} \|_{2} = 1, \|
\vec{w} \|_{0} \leq s \}$ for growing oblique trees, where $ s $ is a
user-defined sparsity parameter. We establish a connection between SID
convergence and $s_0$-sparse oblique splits with $s_0\ge 1$, showing that the
SID function class expands as $s_0$ increases, enabling the capture of more
complex data-generating functions such as the $s_0$-dimensional XOR function.
Thus, $s_0$ represents the unknown potential complexity of the underlying
data-generating function. Learning these complex functions requires an
$s$-sparse oblique tree with $s \geq s_0$ and greater computational resources.
This highlights a trade-off between statistical accuracy, governed by the SID
function class size depending on $s_0$, and computational cost. In contrast,
previous studies have explored the problem of SID convergence using orthogonal
splits with $ s_0 = s = 1 $, where runtime was less critical. Additionally, we
introduce a practical framework for oblique trees that integrates optimized
oblique splits alongside orthogonal splits into random forests. The proposed
approach is assessed through simulations and real-data experiments, comparing
its performance against various oblique tree models.","['stat.ML', 'cs.LG', 'math.ST', 'stat.ME', 'stat.TH']",['Chien-Ming Chi'],2025-03-18,2025-03-18
2503.14577v1,PHGNN: A Novel Prompted Hypergraph Neural Network to Diagnose Alzheimer's Disease,"The accurate diagnosis of Alzheimer's disease (AD) and prognosis of mild
cognitive impairment (MCI) conversion are crucial for early intervention.
However, existing multimodal methods face several challenges, from the
heterogeneity of input data, to underexplored modality interactions, missing
data due to patient dropouts, and limited data caused by the time-consuming and
costly data collection process. In this paper, we propose a novel Prompted
Hypergraph Neural Network (PHGNN) framework that addresses these limitations by
integrating hypergraph based learning with prompt learning. Hypergraphs capture
higher-order relationships between different modalities, while our prompt
learning approach for hypergraphs, adapted from NLP, enables efficient training
with limited data. Our model is validated through extensive experiments on the
ADNI dataset, outperforming SOTA methods in both AD diagnosis and the
prediction of MCI conversion.","['cs.LG', 'cs.AI']","['Chenyu Liu', 'Luca Rossi']",2025-03-18,2025-03-18
2503.14378v1,Impossible Videos,"Synthetic videos nowadays is widely used to complement data scarcity and
diversity of real-world videos. Current synthetic datasets primarily replicate
real-world scenarios, leaving impossible, counterfactual and anti-reality video
concepts underexplored. This work aims to answer two questions: 1) Can today's
video generation models effectively follow prompts to create impossible video
content? 2) Are today's video understanding models good enough for
understanding impossible videos? To this end, we introduce IPV-Bench, a novel
benchmark designed to evaluate and foster progress in video understanding and
generation. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing
4 domains, 14 categories. It features diverse scenes that defy physical,
biological, geographical, or social laws. Based on the taxonomy, a prompt suite
is constructed to evaluate video generation models, challenging their prompt
following and creativity capabilities. In addition, a video benchmark is
curated to assess Video-LLMs on their ability of understanding impossible
videos, which particularly requires reasoning on temporal dynamics and world
knowledge. Comprehensive evaluations reveal limitations and insights for future
directions of video models, paving the way for next-generation video models.",['cs.CV'],"['Zechen Bai', 'Hai Ci', 'Mike Zheng Shou']",2025-03-18,2025-03-18
2503.14377v1,Advancing Medical Representation Learning Through High-Quality Data,"Despite the growing scale of medical Vision-Language datasets, the impact of
dataset quality on model performance remains under-explored. We introduce
Open-PMC, a high-quality medical dataset from PubMed Central, containing 2.2
million image-text pairs, enriched with image modality annotations, subfigures,
and summarized in-text references. Notably, the in-text references provide
richer medical context, extending beyond the abstract information typically
found in captions. Through extensive experiments, we benchmark Open-PMC against
larger datasets across retrieval and zero-shot classification tasks. Our
results show that dataset quality-not just size-drives significant performance
gains. We complement our benchmark with an in-depth analysis of feature
representation. Our findings highlight the crucial role of data curation
quality in advancing multimodal medical AI. We release Open-PMC, along with the
trained models and our codebase.","['eess.IV', 'cs.CV', 'cs.LG']","['Negin Baghbanzadeh', 'Adibvafa Fallahpour', 'Yasaman Parhizkar', 'Franklin Ogidi', 'Shuvendu Roy', 'Sajad Ashkezari', 'Vahid Reza Khazaie', 'Michael Colacci', 'Ali Etemad', 'Arash Afkanpour', 'Elham Dolatabadi']",2025-03-18,2025-03-18
2503.14376v1,Tiled Flash Linear Attention: More Efficient Linear RNN and xLSTM Kernels,"Linear RNNs with gating recently demonstrated competitive performance
compared to Transformers in language modeling. Although their linear compute
scaling in sequence length offers theoretical runtime advantages over
Transformers, realizing these benefits in practice requires optimized custom
kernels, as Transformers rely on the highly efficient Flash Attention kernels.
Leveraging the chunkwise-parallel formulation of linear RNNs, Flash Linear
Attention (FLA) shows that linear RNN kernels are faster than Flash Attention,
by parallelizing over chunks of the input sequence. However, since the chunk
size of FLA is limited, many intermediate states must be materialized in GPU
memory. This leads to low arithmetic intensity and causes high memory
consumption and IO cost, especially for long-context pre-training. In this
work, we present Tiled Flash Linear Attention (TFLA), a novel kernel algorithm
for linear RNNs, that enables arbitrary large chunk sizes by introducing an
additional level of sequence parallelization within each chunk. First, we apply
TFLA to the xLSTM with matrix memory, the mLSTM. Second, we propose an mLSTM
variant with sigmoid input gate and reduced computation for even faster kernel
runtimes at equal language modeling performance. In our speed benchmarks, we
show that our new mLSTM kernels based on TFLA outperform highly optimized Flash
Attention, Linear Attention and Mamba kernels, setting a new state of the art
for efficient long-context sequence modeling primitives.","['cs.LG', 'cs.AI']","['Maximilian Beck', 'Korbinian Pöppel', 'Phillip Lippe', 'Sepp Hochreiter']",2025-03-18,2025-03-18
2503.14375v1,Evaluating Machine Learning Approaches for ASCII Art Generation,"Generating structured ASCII art using computational techniques demands a
careful interplay between aesthetic representation and computational precision,
requiring models that can effectively translate visual information into
symbolic text characters. Although Convolutional Neural Networks (CNNs) have
shown promise in this domain, the comparative performance of deep learning
architectures and classical machine learning methods remains unexplored. This
paper explores the application of contemporary ML and DL methods to generate
structured ASCII art, focusing on three key criteria: fidelity, character
classification accuracy, and output quality. We investigate deep learning
architectures, including Multilayer Perceptrons (MLPs), ResNet, and
MobileNetV2, alongside classical approaches such as Random Forests, Support
Vector Machines (SVMs) and k-Nearest Neighbors (k-NN), trained on an augmented
synthetic dataset of ASCII characters. Our results show that complex neural
network architectures often fall short in producing high-quality ASCII art,
whereas classical machine learning classifiers, despite their simplicity,
achieve performance similar to CNNs. Our findings highlight the strength of
classical methods in bridging model simplicity with output quality, offering
new insights into ASCII art synthesis and machine learning on image data with
low dimensionality.","['cs.GR', 'cs.LG']","['Sai Coumar', 'Zachary Kingston']",2025-03-18,2025-03-18
2503.14576v1,SocialJax: An Evaluation Suite for Multi-agent Reinforcement Learning in Sequential Social Dilemmas,"Social dilemmas pose a significant challenge in the field of multi-agent
reinforcement learning (MARL). Melting Pot is an extensive framework designed
to evaluate social dilemma environments, providing an evaluation protocol that
measures generalization to new social partners across various test scenarios.
However, running reinforcement learning algorithms in the official Melting Pot
environments demands substantial computational resources. In this paper, we
introduce SocialJax, a suite of sequential social dilemma environments
implemented in JAX. JAX is a high-performance numerical computing library for
Python that enables significant improvements in the operational efficiency of
SocialJax on GPUs and TPUs. Our experiments demonstrate that the training
pipeline of SocialJax achieves a 50\texttimes{} speedup in real-time
performance compared to Melting Pot's RLlib baselines. Additionally, we
validate the effectiveness of baseline algorithms within the SocialJax
environments. Finally, we use Schelling diagrams to verify the social dilemma
properties of these environments, ensuring they accurately capture the dynamics
of social dilemmas.","['cs.LG', 'cs.AI']","['Zihao Guo', 'Richard Willis', 'Shuqing Shi', 'Tristan Tomilin', 'Joel Z. Leibo', 'Yali Du']",2025-03-18,2025-03-18
2503.14369v1,C(NN)FD -- Deep Learning Modelling of Multi-Stage Axial Compressors Aerodynamics,"The field of scientific machine learning and its applications to numerical
analyses such as CFD has recently experienced a surge in interest. While its
viability has been demonstrated in different domains, it has not yet reached a
level of robustness and scalability to make it practical for industrial
applications in the turbomachinery field. The highly complex, turbulent, and
three-dimensional flows of multi-stage axial compressors for gas turbine
applications represent a remarkably challenging case. This is due to the
high-dimensionality of the regression of the flow-field from geometrical and
operational variables, and the high computational cost associated with the
large scale of the CFD domains. This paper demonstrates the development and
application of a generalized deep learning framework for predictions of the
flow field and aerodynamic performance of multi-stage axial compressors, also
potentially applicable to any type of turbomachinery. A physics-based
dimensionality reduction unlocks the potential for flow-field predictions for
large-scale domains, re-formulating the regression problem from an unstructured
to a structured one. The relevant physical equations are used to define a
multi-dimensional physical loss function. Compared to ""black-box"" approaches,
the proposed framework has the advantage of physically explainable predictions
of overall performance, as the corresponding aerodynamic drivers can be
identified on a 0D/1D/2D/3D level. An iterative architecture is employed,
improving the accuracy of the predictions, as well as estimating the associated
uncertainty. The model is trained on a series of dataset including
manufacturing and build variations, different geometries, compressor designs
and operating conditions. This demonstrates the capability to predict the
flow-field and the overall performance in a generalizable manner, with accuracy
comparable to the benchmark.","['physics.flu-dyn', 'cs.LG']","['Giuseppe Bruni', 'Sepehr Maleki', 'Senthil K Krishnababu']",2025-03-18,2025-03-18
2503.14575v1,The Exoplanet Citizen Science Pipeline: Human Factors and Machine Learning,"We present the progress of work to streamline and simplify the process of
exoplanet observation by citizen scientists. International collaborations such
as ExoClock and Exoplanet Watch enable citizen scientists to use small
telescopes to carry out transit observations. These studies provide essential
supports for space missions such as JWST and ARIEL. Contributions include
maintenance or recovery of ephemerides, follow up confirmation and transit time
variations. Ongoing observation programs benefit from a large pool of
observers, with a wide variety of experience levels. Our projects work closely
with these communities to streamline their observation pipelines and enable
wider participation. Two complementary approaches are taken: Star Guide applies
human-centric design and community consultation to identify points of friction
within existing systems and provide complementary online tools and resources to
reduce barriers to entry to the observing community. Machine Learning is used
to accelerate data processing and automate steps which are currently manual,
providing a streamlined tool for citizen science and a scalable solution for
large-scale archival research.","['astro-ph.IM', 'astro-ph.EP', 'cs.LG']","['Oisín Creaner', 'Anna Preis', 'Cormac Ryan', 'Nika Gorchakova']",2025-03-18,2025-03-18
2503.14359v1,ImViD: Immersive Volumetric Videos for Enhanced VR Engagement,"User engagement is greatly enhanced by fully immersive multi-modal
experiences that combine visual and auditory stimuli. Consequently, the next
frontier in VR/AR technologies lies in immersive volumetric videos with
complete scene capture, large 6-DoF interaction space, multi-modal feedback,
and high resolution & frame-rate contents. To stimulate the reconstruction of
immersive volumetric videos, we introduce ImViD, a multi-view, multi-modal
dataset featuring complete space-oriented data capture and various
indoor/outdoor scenarios. Our capture rig supports multi-view video-audio
capture while on the move, a capability absent in existing datasets,
significantly enhancing the completeness, flexibility, and efficiency of data
capture.
  The captured multi-view videos (with synchronized audios) are in 5K
resolution at 60FPS, lasting from 1-5 minutes, and include rich
foreground-background elements, and complex dynamics. We benchmark existing
methods using our dataset and establish a base pipeline for constructing
immersive volumetric videos from multi-view audiovisual inputs for 6-DoF
multi-modal immersive VR experiences. The benchmark and the reconstruction and
interaction results demonstrate the effectiveness of our dataset and baseline
method, which we believe will stimulate future research on immersive volumetric
video production.",['cs.CV'],"['Zhengxian Yang', 'Shi Pan', 'Shengqi Wang', 'Haoxiang Wang', 'Li Lin', 'Guanjun Li', 'Zhengqi Wen', 'Borong Lin', 'Jianhua Tao', 'Tao Yu']",2025-03-18,2025-03-18
2503.14358v1,RFMI: Estimating Mutual Information on Rectified Flow for Text-to-Image Alignment,"Rectified Flow (RF) models trained with a Flow matching framework have
achieved state-of-the-art performance on Text-to-Image (T2I) conditional
generation. Yet, multiple benchmarks show that synthetic images can still
suffer from poor alignment with the prompt, i.e., images show wrong attribute
binding, subject positioning, numeracy, etc. While the literature offers many
methods to improve T2I alignment, they all consider only Diffusion Models, and
require auxiliary datasets, scoring models, and linguistic analysis of the
prompt. In this paper we aim to address these gaps. First, we introduce RFMI, a
novel Mutual Information (MI) estimator for RF models that uses the pre-trained
model itself for the MI estimation. Then, we investigate a self-supervised
fine-tuning approach for T2I alignment based on RFMI that does not require
auxiliary information other than the pre-trained model itself. Specifically, a
fine-tuning set is constructed by selecting synthetic images generated from the
pre-trained RF model and having high point-wise MI between images and prompts.
Our experiments on MI estimation benchmarks demonstrate the validity of RFMI,
and empirical fine-tuning on SD3.5-Medium confirms the effectiveness of RFMI
for improving T2I alignment while maintaining image quality.","['cs.CV', 'cs.LG']","['Chao Wang', 'Giulio Franzese', 'Alessandro Finamore', 'Pietro Michiardi']",2025-03-18,2025-03-18
2503.14357v1,Wasserstein-based Kernels for Clustering: Application to Power Distribution Graphs,"Many data clustering applications must handle objects that cannot be
represented as vector data. In this context, the bag-of-vectors representation
can be leveraged to describe complex objects through discrete distributions,
and the Wasserstein distance can effectively measure the dissimilarity between
them. Additionally, kernel methods can be used to embed data into feature
spaces that are easier to analyze. Despite significant progress in data
clustering, a method that simultaneously accounts for distributional and
vectorial dissimilarity measures is still lacking. To tackle this gap, this
work explores kernel methods and Wasserstein distance metrics to develop a
computationally tractable clustering framework. The compositional properties of
kernels allow the simultaneous handling of different metrics, enabling the
integration of both vectors and discrete distributions for object
representation. This approach is flexible enough to be applied in various
domains, such as graph analysis and image processing. The framework consists of
three main components. First, we efficiently approximate pairwise Wasserstein
distances using multiple reference distributions. Second, we employ kernel
functions based on Wasserstein distances and present ways of composing kernels
to express different types of information. Finally, we use the kernels to
cluster data and evaluate the quality of the results using scalable and
distance-agnostic validity indices. A case study involving two datasets of 879
and 34,920 power distribution graphs demonstrates the framework's effectiveness
and efficiency.","['cs.LG', 'stat.AP']","['Alfredo Oneto', 'Blazhe Gjorgiev', 'Giovanni Sansavini']",2025-03-18,2025-03-18
2503.14574v1,Sequence Analysis Using the Bezier Curve,"The analysis of sequences (e.g., protein, DNA, and SMILES string) is
essential for disease diagnosis, biomaterial engineering, genetic engineering,
and drug discovery domains. Conventional analytical methods focus on
transforming sequences into numerical representations for applying machine
learning/deep learning-based sequence characterization. However, their efficacy
is constrained by the intrinsic nature of deep learning (DL) models, which tend
to exhibit suboptimal performance when applied to tabular data. An alternative
group of methodologies endeavors to convert biological sequences into image
forms by applying the concept of Chaos Game Representation (CGR). However, a
noteworthy drawback of these methods lies in their tendency to map individual
elements of the sequence onto a relatively small subset of designated pixels
within the generated image. The resulting sparse image representation may not
adequately encapsulate the comprehensive sequence information, potentially
resulting in suboptimal predictions. In this study, we introduce a novel
approach to transform sequences into images using the B\'ezier curve concept
for element mapping. Mapping the elements onto a curve enhances the sequence
information representation in the respective images, hence yielding better
DL-based classification performance. We employed different sequence datasets to
validate our system by using different classification tasks, and the results
illustrate that our B\'ezier curve method is able to achieve good performance
for all the tasks.","['q-bio.QM', 'cs.LG']","['Taslim Murad', 'Sarwan Ali', 'Murray Patterson']",2025-03-18,2025-03-18
2503.14356v1,"Benchmarking community drug response prediction models: datasets, models, tools, and metrics for cross-dataset generalization analysis","Deep learning (DL) and machine learning (ML) models have shown promise in
drug response prediction (DRP), yet their ability to generalize across datasets
remains an open question, raising concerns about their real-world
applicability. Due to the lack of standardized benchmarking approaches, model
evaluations and comparisons often rely on inconsistent datasets and evaluation
criteria, making it difficult to assess true predictive capabilities. In this
work, we introduce a benchmarking framework for evaluating cross-dataset
prediction generalization in DRP models. Our framework incorporates five
publicly available drug screening datasets, six standardized DRP models, and a
scalable workflow for systematic evaluation. To assess model generalization, we
introduce a set of evaluation metrics that quantify both absolute performance
(e.g., predictive accuracy across datasets) and relative performance (e.g.,
performance drop compared to within-dataset results), enabling a more
comprehensive assessment of model transferability. Our results reveal
substantial performance drops when models are tested on unseen datasets,
underscoring the importance of rigorous generalization assessments. While
several models demonstrate relatively strong cross-dataset generalization, no
single model consistently outperforms across all datasets. Furthermore, we
identify CTRPv2 as the most effective source dataset for training, yielding
higher generalization scores across target datasets. By sharing this
standardized evaluation framework with the community, our study aims to
establish a rigorous foundation for model comparison, and accelerate the
development of robust DRP models for real-world applications.","['cs.LG', 'q-bio.QM']","['Alexander Partin', 'Priyanka Vasanthakumari', 'Oleksandr Narykov', 'Andreas Wilke', 'Natasha Koussa', 'Sara E. Jones', 'Yitan Zhu', 'Jamie C. Overbeek', 'Rajeev Jain', 'Gayara Demini Fernando', 'Cesar Sanchez-Villalobos', 'Cristina Garcia-Cardona', 'Jamaludin Mohd-Yusof', 'Nicholas Chia', 'Justin M. Wozniak', 'Souparno Ghosh', 'Ranadip Pal', 'Thomas S. Brettin', 'M. Ryan Weil', 'Rick L. Stevens']",2025-03-18,2025-03-18
2503.14355v1,MAST-Pro: Dynamic Mixture-of-Experts for Adaptive Segmentation of Pan-Tumors with Knowledge-Driven Prompts,"Accurate tumor segmentation is crucial for cancer diagnosis and treatment.
While foundation models have advanced general-purpose segmentation, existing
methods still struggle with: (1) limited incorporation of medical priors, (2)
imbalance between generic and tumor-specific features, and (3) high
computational costs for clinical adaptation. To address these challenges, we
propose MAST-Pro (Mixture-of-experts for Adaptive Segmentation of pan-Tumors
with knowledge-driven Prompts), a novel framework that integrates dynamic
Mixture-of-Experts (D-MoE) and knowledge-driven prompts for pan-tumor
segmentation. Specifically, text and anatomical prompts provide domain-specific
priors, guiding tumor representation learning, while D-MoE dynamically selects
experts to balance generic and tumor-specific feature learning, improving
segmentation accuracy across diverse tumor types. To enhance efficiency, we
employ Parameter-Efficient Fine-Tuning (PEFT), optimizing MAST-Pro with
significantly reduced computational overhead. Experiments on multi-anatomical
tumor datasets demonstrate that MAST-Pro outperforms state-of-the-art
approaches, achieving up to a 5.20% improvement in average DSC while reducing
trainable parameters by 91.04%, without compromising accuracy.",['cs.CV'],"['Runqi Meng', 'Sifan Song', 'Pengfei Jin', 'Yujin Oh', 'Lin Teng', 'Yulin Wang', 'Yiqun Sun', 'Ling Chen', 'Xiang Li', 'Quanzheng Li', 'Ning Guo', 'Dinggang Shen']",2025-03-18,2025-03-18
2503.14354v1,Retrospective: A CORDIC Based Configurable Activation Function for NN Applications,"A CORDIC-based configuration for the design of Activation Functions (AF) was
previously suggested to accelerate ASIC hardware design for
resource-constrained systems by providing functional reconfigurability. Since
its introduction, this new approach for neural network acceleration has gained
widespread popularity, influencing numerous designs for activation functions in
both academic and commercial AI processors. In this retrospective analysis, we
explore the foundational aspects of this initiative, summarize key developments
over recent years, and introduce the DA-VINCI AF tailored for the evolving
needs of AI applications. This new generation of dynamically configurable and
precision-adjustable activation function cores promise greater adaptability for
a range of activation functions in AI workloads, including Swish, SoftMax,
SeLU, and GeLU, utilizing the Shift-and-Add CORDIC technique. The previously
presented design has been optimized for MAC, Sigmoid, and Tanh functionalities
and incorporated into ReLU AFs, culminating in an accumulative NEURIC compute
unit. These enhancements position NEURIC as a fundamental component in the
resource-efficient vector engine for the realization of AI accelerators that
focus on DNNs, RNNs/LSTMs, and Transformers, achieving a quality of results
(QoR) of 98.5%.","['cs.AR', 'cs.AI', 'cs.CV', 'cs.ET', 'eess.IV']","['Omkar Kokane', 'Gopal Raut', 'Salim Ullah', 'Mukul Lokhande', 'Adam Teman', 'Akash Kumar', 'Santosh Kumar Vishvakarma']",2025-03-18,2025-03-18
2503.14353v1,Unified Analysis of Decentralized Gradient Descent: a Contraction Mapping Framework,"The decentralized gradient descent (DGD) algorithm, and its sibling,
diffusion, are workhorses in decentralized machine learning, distributed
inference and estimation, and multi-agent coordination. We propose a novel,
principled framework for the analysis of DGD and diffusion for strongly convex,
smooth objectives, and arbitrary undirected topologies, using contraction
mappings coupled with a result called the mean Hessian theorem (MHT). The use
of these tools yields tight convergence bounds, both in the noise-free and
noisy regimes. While these bounds are qualitatively similar to results found in
the literature, our approach using contractions together with the MHT decouples
the algorithm dynamics (how quickly the algorithm converges to its fixed point)
from its asymptotic convergence properties (how far the fixed point is from the
global optimum). This yields a simple, intuitive analysis that is accessible to
a broader audience. Extensions are provided to multiple local gradient updates,
time-varying step sizes, noisy gradients (stochastic DGD and diffusion),
communication noise, and random topologies.","['eess.SP', 'cs.DC', 'cs.LG']","['Erik G. Larsson', 'Nicolo Michelusi']",2025-03-18,2025-03-18
2503.14350v2,VEGGIE: Instructional Editing and Reasoning of Video Concepts with Grounded Generation,"Recent video diffusion models have enhanced video editing, but it remains
challenging to handle instructional editing and diverse tasks (e.g., adding,
removing, changing) within a unified framework. In this paper, we introduce
VEGGIE, a Video Editor with Grounded Generation from Instructions, a simple
end-to-end framework that unifies video concept editing, grounding, and
reasoning based on diverse user instructions. Specifically, given a video and
text query, VEGGIE first utilizes an MLLM to interpret user intentions in
instructions and ground them to the video contexts, generating frame-specific
grounded task queries for pixel-space responses. A diffusion model then renders
these plans and generates edited videos that align with user intent. To support
diverse tasks and complex instructions, we employ a curriculum learning
strategy: first aligning the MLLM and video diffusion model with large-scale
instructional image editing data, followed by end-to-end fine-tuning on
high-quality multitask video data. Additionally, we introduce a novel data
synthesis pipeline to generate paired instructional video editing data for
model training. It transforms static image data into diverse, high-quality
video editing samples by leveraging Image-to-Video models to inject dynamics.
VEGGIE shows strong performance in instructional video editing with different
editing skills, outperforming the best instructional baseline as a versatile
model, while other models struggle with multi-tasking. VEGGIE also excels in
video object grounding and reasoning segmentation, where other baselines fail.
We further reveal how the multiple tasks help each other and highlight
promising applications like zero-shot multimodal instructional and in-context
video editing.","['cs.CV', 'cs.AI', 'cs.CL']","['Shoubin Yu', 'Difan Liu', 'Ziqiao Ma', 'Yicong Hong', 'Yang Zhou', 'Hao Tan', 'Joyce Chai', 'Mohit Bansal']",2025-03-18,2025-03-19
2503.16534v1,Gender and content bias in Large Language Models: a case study on Google Gemini 2.0 Flash Experimental,"This study evaluates the biases in Gemini 2.0 Flash Experimental, a
state-of-the-art large language model (LLM) developed by Google, focusing on
content moderation and gender disparities. By comparing its performance to
ChatGPT-4o, examined in a previous work of the author, the analysis highlights
some differences in ethical moderation practices. Gemini 2.0 demonstrates
reduced gender bias, notably with female-specific prompts achieving a
substantial rise in acceptance rates compared to results obtained by
ChatGPT-4o. It adopts a more permissive stance toward sexual content and
maintains relatively high acceptance rates for violent prompts, including
gender-specific cases. Despite these changes, whether they constitute an
improvement is debatable. While gender bias has been reduced, this reduction
comes at the cost of permitting more violent content toward both males and
females, potentially normalizing violence rather than mitigating harm.
Male-specific prompts still generally receive higher acceptance rates than
female-specific ones. These findings underscore the complexities of aligning AI
systems with ethical standards, highlighting progress in reducing certain
biases while raising concerns about the broader implications of the model's
permissiveness. Ongoing refinements are essential to achieve moderation
practices that ensure transparency, fairness, and inclusivity without
amplifying harmful content.","['cs.CL', 'cs.AI', 'cs.CY', 'cs.HC']",['Roberto Balestri'],2025-03-18,2025-03-18
2503.14346v1,3D Densification for Multi-Map Monocular VSLAM in Endoscopy,"Multi-map Sparse Monocular visual Simultaneous Localization and Mapping
applied to monocular endoscopic sequences has proven efficient to robustly
recover tracking after the frequent losses in endoscopy due to motion blur,
temporal occlusion, tools interaction or water jets. The sparse multi-maps are
adequate for robust camera localization, however they are very poor for
environment representation, they are noisy, with a high percentage of
inaccurately reconstructed 3D points, including significant outliers, and more
importantly with an unacceptable low density for clinical applications.
  We propose a method to remove outliers and densify the maps of the state of
the art for sparse endoscopy multi-map CudaSIFT-SLAM. The NN LightDepth for
up-to-scale depth dense predictions are aligned with the sparse CudaSIFT
submaps by means of the robust to spurious LMedS. Our system mitigates the
inherent scale ambiguity in monocular depth estimation while filtering
outliers, leading to reliable densified 3D maps.
  We provide experimental evidence of accurate densified maps 4.15 mm RMS
accuracy at affordable computing time in the C3VD phantom colon dataset. We
report qualitative results on the real colonoscopy from the Endomapper dataset.",['cs.CV'],"['X. Anadón', 'Javier Rodríguez-Puigvert', 'J. M. M. Montiel']",2025-03-18,2025-03-18
2503.14345v2,MoonCast: High-Quality Zero-Shot Podcast Generation,"Recent advances in text-to-speech synthesis have achieved notable success in
generating high-quality short utterances for individual speakers. However,
these systems still face challenges when extending their capabilities to long,
multi-speaker, and spontaneous dialogues, typical of real-world scenarios such
as podcasts. These limitations arise from two primary challenges: 1) long
speech: podcasts typically span several minutes, exceeding the upper limit of
most existing work; 2) spontaneity: podcasts are marked by their spontaneous,
oral nature, which sharply contrasts with formal, written contexts; existing
works often fall short in capturing this spontaneity. In this paper, we propose
MoonCast, a solution for high-quality zero-shot podcast generation, aiming to
synthesize natural podcast-style speech from text-only sources (e.g., stories,
technical reports, news in TXT, PDF, or Web URL formats) using the voices of
unseen speakers. To generate long audio, we adopt a long-context language
model-based audio modeling approach utilizing large-scale long-context speech
data. To enhance spontaneity, we utilize a podcast generation module to
generate scripts with spontaneous details, which have been empirically shown to
be as crucial as the text-to-speech modeling itself. Experiments demonstrate
that MoonCast outperforms baselines, with particularly notable improvements in
spontaneity and coherence.","['eess.AS', 'cs.AI', 'cs.CL', 'cs.LG', 'cs.SD']","['Zeqian Ju', 'Dongchao Yang', 'Jianwei Yu', 'Kai Shen', 'Yichong Leng', 'Zhengtao Wang', 'Xu Tan', 'Xinyu Zhou', 'Tao Qin', 'Xiangyang Li']",2025-03-18,2025-03-19
2503.14343v1,Multi-Prototype Embedding Refinement for Semi-Supervised Medical Image Segmentation,"Medical image segmentation aims to identify anatomical structures at the
voxel-level. Segmentation accuracy relies on distinguishing voxel differences.
Compared to advancements achieved in studies of the inter-class variance, the
intra-class variance receives less attention. Moreover, traditional linear
classifiers, limited by a single learnable weight per class, struggle to
capture this finer distinction. To address the above challenges, we propose a
Multi-Prototype-based Embedding Refinement method for semi-supervised medical
image segmentation. Specifically, we design a multi-prototype-based
classification strategy, rethinking the segmentation from the perspective of
structural relationships between voxel embeddings. The intra-class variations
are explored by clustering voxels along the distribution of multiple prototypes
in each class. Next, we introduce a consistency constraint to alleviate the
limitation of linear classifiers. This constraint integrates different
classification granularities from a linear classifier and the proposed
prototype-based classifier. In the thorough evaluation on two popular
benchmarks, our method achieves superior performance compared with
state-of-the-art methods. Code is available at
https://github.com/Briley-byl123/MPER.","['eess.IV', 'cs.CV']","['Yali Bi', 'Enyu Che', 'Yinan Chen', 'Yuanpeng He', 'Jingwei Qu']",2025-03-18,2025-03-18
