[
  {
    "id": "2503.17363v1",
    "title": "Dancing with Critiques: Enhancing LLM Reasoning with Stepwise Natural Language Self-Critique",
    "abstract": "Enhancing the reasoning capabilities of large language models (LLMs),\nparticularly for complex tasks requiring multi-step logical deductions, remains\na significant challenge. Traditional inference time scaling methods utilize\nscalar reward signals from process reward models to evaluate candidate\nreasoning steps, but these scalar rewards lack the nuanced qualitative\ninformation essential for understanding and justifying each step. In this\npaper, we propose a novel inference-time scaling approach -- stepwise natural\nlanguage self-critique (PANEL), which employs self-generated natural language\ncritiques as feedback to guide the step-level search process. By generating\nrich, human-readable critiques for each candidate reasoning step, PANEL retains\nessential qualitative information, facilitating better-informed decision-making\nduring inference. This approach bypasses the need for task-specific verifiers\nand the associated training overhead, making it broadly applicable across\ndiverse tasks. Experimental results on challenging reasoning benchmarks,\nincluding AIME and GPQA, demonstrate that PANEL significantly enhances\nreasoning performance, outperforming traditional scalar reward-based methods.\nOur code is available at https://github.com/puddingyeah/PANEL to support and\nencourage future research in this promising field.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Yansi Li",
      "Jiahao Xu",
      "Tian Liang",
      "Xingyu Chen",
      "Zhiwei He",
      "Qiuzhi Liu",
      "Rui Wang",
      "Zhuosheng Zhang",
      "Zhaopeng Tu",
      "Haitao Mi",
      "Dong Yu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17361v1",
    "title": "Gumbel-Softmax Flow Matching with Straight-Through Guidance for Controllable Biological Sequence Generation",
    "abstract": "Flow matching in the continuous simplex has emerged as a promising strategy\nfor DNA sequence design, but struggles to scale to higher simplex dimensions\nrequired for peptide and protein generation. We introduce Gumbel-Softmax Flow\nand Score Matching, a generative framework on the simplex based on a novel\nGumbel-Softmax interpolant with a time-dependent temperature. Using this\ninterpolant, we introduce Gumbel-Softmax Flow Matching by deriving a\nparameterized velocity field that transports from smooth categorical\ndistributions to distributions concentrated at a single vertex of the simplex.\nWe alternatively present Gumbel-Softmax Score Matching which learns to regress\nthe gradient of the probability density. Our framework enables high-quality,\ndiverse generation and scales efficiently to higher-dimensional simplices. To\nenable training-free guidance, we propose Straight-Through Guided Flows\n(STGFlow), a classifier-based guidance method that leverages straight-through\nestimators to steer the unconditional velocity field toward optimal vertices of\nthe simplex. STGFlow enables efficient inference-time guidance using\nclassifiers pre-trained on clean sequences, and can be used with any discrete\nflow method. Together, these components form a robust framework for\ncontrollable de novo sequence generation. We demonstrate state-of-the-art\nperformance in conditional DNA promoter design, sequence-only protein\ngeneration, and target-binding peptide design for rare disease treatment.",
    "categories": [
      "cs.LG",
      "q-bio.BM"
    ],
    "authors": [
      "Sophia Tang",
      "Yinuo Zhang",
      "Alexander Tong",
      "Pranam Chatterjee"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17359v1",
    "title": "Position: Interactive Generative Video as Next-Generation Game Engine",
    "abstract": "Modern game development faces significant challenges in creativity and cost\ndue to predetermined content in traditional game engines. Recent breakthroughs\nin video generation models, capable of synthesizing realistic and interactive\nvirtual environments, present an opportunity to revolutionize game creation. In\nthis position paper, we propose Interactive Generative Video (IGV) as the\nfoundation for Generative Game Engines (GGE), enabling unlimited novel content\ngeneration in next-generation gaming. GGE leverages IGV's unique strengths in\nunlimited high-quality content synthesis, physics-aware world modeling,\nuser-controlled interactivity, long-term memory capabilities, and causal\nreasoning. We present a comprehensive framework detailing GGE's core modules\nand a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work\ncharts a new course for game development in the AI era, envisioning a future\nwhere AI-powered generative systems fundamentally reshape how games are created\nand experienced.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jiwen Yu",
      "Yiran Qin",
      "Haoxuan Che",
      "Quande Liu",
      "Xintao Wang",
      "Pengfei Wan",
      "Di Zhang",
      "Xihui Liu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17358v1",
    "title": "Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image",
    "abstract": "In many robotics and VR/AR applications, fast camera motions cause a high\nlevel of motion blur, causing existing camera pose estimation methods to fail.\nIn this work, we propose a novel framework that leverages motion blur as a rich\ncue for motion estimation rather than treating it as an unwanted artifact. Our\napproach works by predicting a dense motion flow field and a monocular depth\nmap directly from a single motion-blurred image. We then recover the\ninstantaneous camera velocity by solving a linear least squares problem under\nthe small motion assumption. In essence, our method produces an IMU-like\nmeasurement that robustly captures fast and aggressive camera movements. To\ntrain our model, we construct a large-scale dataset with realistic synthetic\nmotion blur derived from ScanNet++v2 and further refine our model by training\nend-to-end on real data using our fully differentiable pipeline. Extensive\nevaluations on real-world benchmarks demonstrate that our method achieves\nstate-of-the-art angular and translational velocity estimates, outperforming\ncurrent methods like MASt3R and COLMAP.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jerred Chen",
      "Ronald Clark"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17355v1",
    "title": "Glivenko-Cantelli for $f$-divergence",
    "abstract": "We extend the celebrated Glivenko-Cantelli theorem, sometimes called the\nfundamental theorem of statistics, from its standard setting of total variation\ndistance to all $f$-divergences. A key obstacle in this endeavor is to define\n$f$-divergence on a subcollection of a $\\sigma$-algebra that forms a\n$\\pi$-system but not a $\\sigma$-subalgebra. This is a side contribution of our\nwork. We will show that this notion of $f$-divergence on the $\\pi$-system of\nrays preserves nearly all known properties of standard $f$-divergence, yields a\nnovel integral representation of the Kolmogorov-Smirnov distance, and has a\nGlivenko-Cantelli theorem.",
    "categories": [
      "math.ST",
      "cs.LG",
      "stat.TH",
      "60B10, 60F15, 60F25"
    ],
    "authors": [
      "Haoming Wang",
      "Lek-Heng Lim"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17354v1",
    "title": "HCAST: Human-Calibrated Autonomy Software Tasks",
    "abstract": "To understand and predict the societal impacts of highly autonomous AI\nsystems, we need benchmarks with grounding, i.e., metrics that directly connect\nAI performance to real-world effects we care about. We present HCAST\n(Human-Calibrated Autonomy Software Tasks), a benchmark of 189 machine learning\nengineering, cybersecurity, software engineering, and general reasoning tasks.\nWe collect 563 human baselines (totaling over 1500 hours) from people skilled\nin these domains, working under identical conditions as AI agents, which lets\nus estimate that HCAST tasks take humans between one minute and 8+ hours.\nMeasuring the time tasks take for humans provides an intuitive metric for\nevaluating AI capabilities, helping answer the question \"can an agent be\ntrusted to complete a task that would take a human X hours?\" We evaluate the\nsuccess rates of AI agents built on frontier foundation models, and we find\nthat current agents succeed 70-80% of the time on tasks that take humans less\nthan one hour, and less than 20% of the time on tasks that take humans more\nthan 4 hours.",
    "categories": [
      "cs.AI",
      "I.2.0"
    ],
    "authors": [
      "David Rein",
      "Joel Becker",
      "Amy Deng",
      "Seraphina Nix",
      "Chris Canal",
      "Daniel O'Connel",
      "Pip Arnott",
      "Ryan Bloom",
      "Thomas Broadley",
      "Katharyn Garcia",
      "Brian Goodrich",
      "Max Hasin",
      "Sami Jawhar",
      "Megan Kinniment",
      "Thomas Kwa",
      "Aron Lajko",
      "Nate Rush",
      "Lucas Jun Koba Sato",
      "Sydney Von Arx",
      "Ben West",
      "Lawrence Chan",
      "Elizabeth Barnes"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17353v1",
    "title": "NdLinear Is All You Need for Representation Learning",
    "abstract": "Many high-impact machine learning tasks involve multi-dimensional data (e.g.,\nimages, volumetric medical scans, multivariate time-series). Yet, most neural\narchitectures flatten inputs, discarding critical cross-dimension information.\nWe introduce NdLinear, a novel linear transformation that preserves these\nstructures without extra overhead. By operating separately along each\ndimension, NdLinear captures dependencies that standard fully connected layers\noverlook. Extensive experiments across convolutional, recurrent, and\ntransformer-based networks show significant improvements in representational\npower and parameter efficiency. Crucially, NdLinear serves as a foundational\nbuilding block for large-scale foundation models by operating on any unimodal\nor multimodal data in its native form. This removes the need for flattening or\nmodality-specific preprocessing. Ndlinear rethinks core architectural\npriorities beyond attention, enabling more expressive, context-aware models at\nscale. We propose NdLinear as a drop-in replacement for standard linear layers\n-- marking an important step toward next-generation neural architectures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Alex Reneau",
      "Jerry Yao-Chieh Hu",
      "Zhongfang Zhuang",
      "Ting-Chun Liu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17352v1",
    "title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement",
    "abstract": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex\nreasoning abilities in large language models (LLMs), including sophisticated\nbehaviors such as self-verification and self-correction, can be achieved by RL\nwith verifiable rewards and significantly improves model performance on\nchallenging tasks such as AIME. Motivated by these findings, our study\ninvestigates whether similar reasoning capabilities can be successfully\nintegrated into large vision-language models (LVLMs) and assesses their impact\non challenging multimodal reasoning tasks. We consider an approach that\niteratively leverages supervised fine-tuning (SFT) on lightweight training data\nand Reinforcement Learning (RL) to further improve model generalization.\nInitially, reasoning capabilities were distilled from pure-text R1 models by\ngenerating reasoning steps using high-quality captions of the images sourced\nfrom diverse visual datasets. Subsequently, iterative RL training further\nenhance reasoning skills, with each iteration's RL-improved model generating\nrefined SFT datasets for the next round. This iterative process yielded\nOpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on\nchallenging benchmarks such as MathVista, MathVerse, and MathVision,\ndemonstrating the potential of our strategy for robust vision-language\nreasoning. The code, model and data are held at\nhttps://github.com/yihedeng9/OpenVLThinker.",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "authors": [
      "Yihe Deng",
      "Hritik Bansal",
      "Fan Yin",
      "Nanyun Peng",
      "Wei Wang",
      "Kai-Wei Chang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17351v1",
    "title": "Time-Series U-Net with Recurrence for Noise-Robust Imaging Photoplethysmography",
    "abstract": "Remote estimation of vital signs enables health monitoring for situations in\nwhich contact-based devices are either not available, too intrusive, or too\nexpensive. In this paper, we present a modular, interpretable pipeline for\npulse signal estimation from video of the face that achieves state-of-the-art\nresults on publicly available datasets.Our imaging photoplethysmography (iPPG)\nsystem consists of three modules: face and landmark detection, time-series\nextraction, and pulse signal/pulse rate estimation. Unlike many deep learning\nmethods that make use of a single black-box model that maps directly from input\nvideo to output signal or heart rate, our modular approach enables each of the\nthree parts of the pipeline to be interpreted individually. The pulse signal\nestimation module, which we call TURNIP (Time-Series U-Net with Recurrence for\nNoise-Robust Imaging Photoplethysmography), allows the system to faithfully\nreconstruct the underlying pulse signal waveform and uses it to measure heart\nrate and pulse rate variability metrics, even in the presence of motion. When\nparts of the face are occluded due to extreme head poses, our system explicitly\ndetects such \"self-occluded\" regions and maintains estimation robustness\ndespite the missing information. Our algorithm provides reliable heart rate\nestimates without the need for specialized sensors or contact with the skin,\noutperforming previous iPPG methods on both color (RGB) and near-infrared (NIR)\ndatasets.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Vineet R. Shenoy",
      "Shaoju Wu",
      "Armand Comas",
      "Tim K. Marks",
      "Suhas Lohit",
      "Hassan Mansour"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17350v1",
    "title": "Decouple and Track: Benchmarking and Improving Video Diffusion Transformers for Motion Transfer",
    "abstract": "The motion transfer task involves transferring motion from a source video to\nnewly generated videos, requiring the model to decouple motion from appearance.\nPrevious diffusion-based methods primarily rely on separate spatial and\ntemporal attention mechanisms within 3D U-Net. In contrast, state-of-the-art\nvideo Diffusion Transformers (DiT) models use 3D full attention, which does not\nexplicitly separate temporal and spatial information. Thus, the interaction\nbetween spatial and temporal dimensions makes decoupling motion and appearance\nmore challenging for DiT models. In this paper, we propose DeT, a method that\nadapts DiT models to improve motion transfer ability. Our approach introduces a\nsimple yet effective temporal kernel to smooth DiT features along the temporal\ndimension, facilitating the decoupling of foreground motion from background\nappearance. Meanwhile, the temporal kernel effectively captures temporal\nvariations in DiT features, which are closely related to motion. Moreover, we\nintroduce explicit supervision along dense trajectories in the latent feature\nspace to further enhance motion consistency. Additionally, we present MTBench,\na general and challenging benchmark for motion transfer. We also introduce a\nhybrid motion fidelity metric that considers both the global and local motion\nsimilarity. Therefore, our work provides a more comprehensive evaluation than\nprevious works. Extensive experiments on MTBench demonstrate that DeT achieves\nthe best trade-off between motion fidelity and edit fidelity.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Qingyu Shi",
      "Jianzong Wu",
      "Jinbin Bai",
      "Jiangning Zhang",
      "Lu Qi",
      "Xiangtai Li",
      "Yunhai Tong"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17349v1",
    "title": "Beyond Semantics: Rediscovering Spatial Awareness in Vision-Language Models",
    "abstract": "Vision-Language Models (VLMs) excel at identifying and describing objects but\nstruggle with spatial reasoning such as accurately understanding the relative\npositions of objects. Inspired by the dual-pathway (ventral-dorsal) model of\nhuman vision, we investigate why VLMs fail spatial tasks despite strong object\nrecognition capabilities. Our interpretability-driven analysis reveals a\ncritical underlying cause: vision embeddings in VLMs are treated primarily as\nsemantic ``bag-of-tokens,\" overshadowing subtle yet crucial positional cues due\nto their disproportionately large embedding norms. We validate this insight\nthrough extensive diagnostic experiments, demonstrating minimal performance\nimpact when token orders or fine-grained spatial details are removed. Guided by\nthese findings, we propose simple, interpretable interventions, including\nnormalizing vision embedding norms and extracting mid-layer spatially rich\nfeatures, to restore spatial awareness. Empirical results on both our synthetic\ndata and standard benchmarks demonstrate improved spatial reasoning\ncapabilities, highlighting the value of interpretability-informed design\nchoices. Our study not only uncovers fundamental limitations in current VLM\narchitectures but also provides actionable insights for enhancing structured\nperception of visual scenes.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jianing Qi",
      "Jiawei Liu",
      "Hao Tang",
      "Zhigang Zhu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17347v1",
    "title": "Dereflection Any Image with Diffusion Priors and Diversified Data",
    "abstract": "Reflection removal of a single image remains a highly challenging task due to\nthe complex entanglement between target scenes and unwanted reflections.\nDespite significant progress, existing methods are hindered by the scarcity of\nhigh-quality, diverse data and insufficient restoration priors, resulting in\nlimited generalization across various real-world scenarios. In this paper, we\npropose Dereflection Any Image, a comprehensive solution with an efficient data\npreparation pipeline and a generalizable model for robust reflection removal.\nFirst, we introduce a dataset named Diverse Reflection Removal (DRR) created by\nrandomly rotating reflective mediums in target scenes, enabling variation of\nreflection angles and intensities, and setting a new benchmark in scale,\nquality, and diversity. Second, we propose a diffusion-based framework with\none-step diffusion for deterministic outputs and fast inference. To ensure\nstable learning, we design a three-stage progressive training strategy,\nincluding reflection-invariant finetuning to encourage consistent outputs\nacross varying reflection patterns that characterize our dataset. Extensive\nexperiments show that our method achieves SOTA performance on both common\nbenchmarks and challenging in-the-wild images, showing superior generalization\nacross diverse real-world scenes.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jichen Hu",
      "Chen Yang",
      "Zanwei Zhou",
      "Jiemin Fang",
      "Xiaokang Yang",
      "Qi Tian",
      "Wei Shen"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17340v1",
    "title": "Align Your Rhythm: Generating Highly Aligned Dance Poses with Gating-Enhanced Rhythm-Aware Feature Representation",
    "abstract": "Automatically generating natural, diverse and rhythmic human dance movements\ndriven by music is vital for virtual reality and film industries. However,\ngenerating dance that naturally follows music remains a challenge, as existing\nmethods lack proper beat alignment and exhibit unnatural motion dynamics. In\nthis paper, we propose Danceba, a novel framework that leverages gating\nmechanism to enhance rhythm-aware feature representation for music-driven dance\ngeneration, which achieves highly aligned dance poses with enhanced rhythmic\nsensitivity. Specifically, we introduce Phase-Based Rhythm Extraction (PRE) to\nprecisely extract rhythmic information from musical phase data, capitalizing on\nthe intrinsic periodicity and temporal structures of music. Additionally, we\npropose Temporal-Gated Causal Attention (TGCA) to focus on global rhythmic\nfeatures, ensuring that dance movements closely follow the musical rhythm. We\nalso introduce Parallel Mamba Motion Modeling (PMMM) architecture to separately\nmodel upper and lower body motions along with musical features, thereby\nimproving the naturalness and diversity of generated dance movements. Extensive\nexperiments confirm that Danceba outperforms state-of-the-art methods,\nachieving significantly better rhythmic alignment and motion diversity. Project\npage: https://danceba.github.io/ .",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "authors": [
      "Congyi Fan",
      "Jian Guan",
      "Xuanjia Zhao",
      "Dongli Xu",
      "Youtian Lin",
      "Tong Ye",
      "Pengming Feng",
      "Haiwei Pan"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17339v1",
    "title": "Can AI expose tax loopholes? Towards a new generation of legal policy assistants",
    "abstract": "The legislative process is the backbone of a state built on solid\ninstitutions. Yet, due to the complexity of laws -- particularly tax law --\npolicies may lead to inequality and social tensions. In this study, we\nintroduce a novel prototype system designed to address the issues of tax\nloopholes and tax avoidance. Our hybrid solution integrates a natural language\ninterface with a domain-specific language tailored for planning. We demonstrate\non a case study how tax loopholes and avoidance schemes can be exposed. We\nconclude that our prototype can help enhance social welfare by systematically\nidentifying and addressing tax gaps stemming from loopholes.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "authors": [
      "Peter Fratrič",
      "Nils Holzenberger",
      "David Restrepo Amariles"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17338v1",
    "title": "Capturing Individual Human Preferences with Reward Features",
    "abstract": "Reinforcement learning from human feedback usually models preferences using a\nreward model that does not distinguish between people. We argue that this is\nunlikely to be a good design choice in contexts with high potential for\ndisagreement, like in the training of large language models. We propose a\nmethod to specialise a reward model to a person or group of people. Our\napproach builds on the observation that individual preferences can be captured\nas a linear combination of a set of general reward features. We show how to\nlearn such features and subsequently use them to quickly adapt the reward model\nto a specific individual, even if their preferences are not reflected in the\ntraining data. We present experiments with large language models comparing the\nproposed architecture with a non-adaptive reward model and also adaptive\ncounterparts, including models that do in-context personalisation. Depending on\nhow much disagreement there is in the training data, our model either\nsignificantly outperforms the baselines or matches their performance with a\nsimpler architecture and more stable training.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "André Barreto",
      "Vincent Dumoulin",
      "Yiran Mao",
      "Nicolas Perez-Nieves",
      "Bobak Shahriari",
      "Yann Dauphin",
      "Doina Precup",
      "Hugo Larochelle"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17336v1",
    "title": "Efficient Intent-Based Filtering for Multi-Party Conversations Using Knowledge Distillation from LLMs",
    "abstract": "Large language models (LLMs) have showcased remarkable capabilities in\nconversational AI, enabling open-domain responses in chat-bots, as well as\nadvanced processing of conversations like summarization, intent classification,\nand insights generation. However, these models are resource-intensive,\ndemanding substantial memory and computational power. To address this, we\npropose a cost-effective solution that filters conversational snippets of\ninterest for LLM processing, tailored to the target downstream application,\nrather than processing every snippet. In this work, we introduce an innovative\napproach that leverages knowledge distillation from LLMs to develop an\nintent-based filter for multi-party conversations, optimized for compute power\nconstrained environments. Our method combines different strategies to create a\ndiverse multi-party conversational dataset, that is annotated with the target\nintents and is then used to fine-tune the MobileBERT model for multi-label\nintent classification. This model achieves a balance between efficiency and\nperformance, effectively filtering conversation snippets based on their\nintents. By passing only the relevant snippets to the LLM for further\nprocessing, our approach significantly reduces overall operational costs\ndepending on the intents and the data distribution as demonstrated in our\nexperiments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Reem Gody",
      "Mohamed Abdelghaffar",
      "Mohammed Jabreel",
      "Ahmed Tawfik"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17332v1",
    "title": "CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities",
    "abstract": "Large language model (LLM) agents are increasingly capable of autonomously\nconducting cyberattacks, posing significant threats to existing applications.\nThis growing risk highlights the urgent need for a real-world benchmark to\nevaluate the ability of LLM agents to exploit web application vulnerabilities.\nHowever, existing benchmarks fall short as they are limited to abstracted\nCapture the Flag competitions or lack comprehensive coverage. Building a\nbenchmark for real-world vulnerabilities involves both specialized expertise to\nreproduce exploits and a systematic approach to evaluating unpredictable\nthreats. To address this challenge, we introduce CVE-Bench, a real-world\ncybersecurity benchmark based on critical-severity Common Vulnerabilities and\nExposures. In CVE-Bench, we design a sandbox framework that enables LLM agents\nto exploit vulnerable web applications in scenarios that mimic real-world\nconditions, while also providing effective evaluation of their exploits. Our\nevaluation shows that the state-of-the-art agent framework can resolve up to\n13% of vulnerabilities.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "I.2.1; I.2.7"
    ],
    "authors": [
      "Yuxuan Zhu",
      "Antony Kellermann",
      "Dylan Bowman",
      "Philip Li",
      "Akul Gupta",
      "Adarsh Danda",
      "Richard Fang",
      "Conner Jensen",
      "Eric Ihli",
      "Jason Benn",
      "Jet Geronimo",
      "Avi Dhir",
      "Sudhit Rao",
      "Kaicheng Yu",
      "Twm Stone",
      "Daniel Kang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17331v1",
    "title": "A Topological Data Analysis Framework for Quantifying Necrosis in Glioblastomas",
    "abstract": "In this paper, we introduce a shape descriptor that we call \"interior\nfunction\". This is a Topological Data Analysis (TDA) based descriptor that\nrefines previous descriptors for image analysis. Using this concept, we define\nsubcomplex lacunarity, a new index that quantifies geometric characteristics of\nnecrosis in tumors such as conglomeration. Building on this framework, we\npropose a set of indices to analyze necrotic morphology and construct a diagram\nthat captures the distinct structural and geometric properties of necrotic\nregions in tumors. We present an application of this framework in the study of\nMRIs of Glioblastomas (GB). Using cluster analysis, we identify four distinct\nsubtypes of Glioblastomas that reflect geometric properties of necrotic\nregions.",
    "categories": [
      "math.AT",
      "cs.CV"
    ],
    "authors": [
      "Francisco Tellez",
      "Enrique Torres-Giese"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17329v1",
    "title": "Predicting Potential Customer Support Needs and Optimizing Search Ranking in a Two-Sided Marketplace",
    "abstract": "Airbnb is an online marketplace that connects hosts and guests to unique\nstays and experiences. When guests stay at homes booked on Airbnb, there are a\nsmall fraction of stays that lead to support needed from Airbnb's Customer\nSupport (CS), which may cause inconvenience to guests and hosts and require\nAirbnb resources to resolve. In this work, we show that instances where CS\nsupport is needed may be predicted based on hosts and guests behavior. We build\na model to predict the likelihood of CS support needs for each match of guest\nand host. The model score is incorporated into Airbnb's search ranking\nalgorithm as one of the many factors. The change promotes more reliable matches\nin search results and significantly reduces bookings that require CS support.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Do-kyum Kim",
      "Han Zhao",
      "Huiji Gao",
      "Liwei He",
      "Malay Haldar",
      "Sanjeev Katariya"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17316v1",
    "title": "Pow3R: Empowering Unconstrained 3D Reconstruction with Camera and Scene Priors",
    "abstract": "We present Pow3r, a novel large 3D vision regression model that is highly\nversatile in the input modalities it accepts. Unlike previous feed-forward\nmodels that lack any mechanism to exploit known camera or scene priors at test\ntime, Pow3r incorporates any combination of auxiliary information such as\nintrinsics, relative pose, dense or sparse depth, alongside input images,\nwithin a single network. Building upon the recent DUSt3R paradigm, a\ntransformer-based architecture that leverages powerful pre-training, our\nlightweight and versatile conditioning acts as additional guidance for the\nnetwork to predict more accurate estimates when auxiliary information is\navailable. During training we feed the model with random subsets of modalities\nat each iteration, which enables the model to operate under different levels of\nknown priors at test time. This in turn opens up new capabilities, such as\nperforming inference in native image resolution, or point-cloud completion. Our\nexperiments on 3D reconstruction, depth completion, multi-view depth\nprediction, multi-view stereo, and multi-view pose estimation tasks yield\nstate-of-the-art results and confirm the effectiveness of Pow3r at exploiting\nall available information. The project webpage is\nhttps://europe.naverlabs.com/pow3r.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Wonbong Jang",
      "Philippe Weinzaepfel",
      "Vincent Leroy",
      "Lourdes Agapito",
      "Jerome Revaud"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17309v1",
    "title": "LLM+MAP: Bimanual Robot Task Planning using Large Language Models and Planning Domain Definition Language",
    "abstract": "Bimanual robotic manipulation provides significant versatility, but also\npresents an inherent challenge due to the complexity involved in the spatial\nand temporal coordination between two hands. Existing works predominantly focus\non attaining human-level manipulation skills for robotic hands, yet little\nattention has been paid to task planning on long-horizon timescales. With their\noutstanding in-context learning and zero-shot generation abilities, Large\nLanguage Models (LLMs) have been applied and grounded in diverse robotic\nembodiments to facilitate task planning. However, LLMs still suffer from errors\nin long-horizon reasoning and from hallucinations in complex robotic tasks,\nlacking a guarantee of logical correctness when generating the plan. Previous\nworks, such as LLM+P, extended LLMs with symbolic planners. However, none have\nbeen successfully applied to bimanual robots. New challenges inevitably arise\nin bimanual manipulation, necessitating not only effective task decomposition\nbut also efficient task allocation. To address these challenges, this paper\nintroduces LLM+MAP, a bimanual planning framework that integrates LLM reasoning\nand multi-agent planning, automating effective and efficient bimanual task\nplanning. We conduct simulated experiments on various long-horizon manipulation\ntasks of differing complexity. Our method is built using GPT-4o as the backend,\nand we compare its performance against plans generated directly by LLMs,\nincluding GPT-4o, V3 and also recent strong reasoning models o1 and R1. By\nanalyzing metrics such as planning time, success rate, group debits, and\nplanning-step reduction rate, we demonstrate the superior performance of\nLLM+MAP, while also providing insights into robotic reasoning. Code is\navailable at https://github.com/Kchu/LLM-MAP.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Kun Chu",
      "Xufeng Zhao",
      "Cornelius Weber",
      "Stefan Wermter"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17308v1",
    "title": "On Quantum Perceptron Learning via Quantum Search",
    "abstract": "With the growing interest in quantum machine learning, the perceptron -- a\nfundamental building block in traditional machine learning -- has emerged as a\nvaluable model for exploring quantum advantages. Two quantum perceptron\nalgorithms based on Grover's search, were developed in arXiv:1602.04799 to\naccelerate training and improve statistical efficiency in perceptron learning.\nThis paper points out and corrects a mistake in the proof of Theorem 2 in\narXiv:1602.04799. Specifically, we show that the probability of sampling from a\nnormal distribution for a $D$-dimensional hyperplane that perfectly classifies\nthe data scales as $\\Omega(\\gamma^{D})$ instead of $\\Theta({\\gamma})$, where\n$\\gamma$ is the margin. We then revisit two well-established linear programming\nalgorithms -- the ellipsoid method and the cutting plane random walk algorithm\n-- in the context of perceptron learning, and show how quantum search\nalgorithms can be leveraged to enhance the overall complexity. Specifically,\nboth algorithms gain a sub-linear speed-up $O(\\sqrt{N})$ in the number of data\npoints $N$ as a result of Grover's algorithm and an additional $O(D^{1.5})$\nspeed-up is possible for cutting plane random walk algorithm employing quantum\nwalk search.",
    "categories": [
      "quant-ph",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Xiaoyu Sun",
      "Mathieu Roget",
      "Giuseppe Di Molfetta",
      "Hachem Kadri"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17299v1",
    "title": "Preference-Guided Diffusion for Multi-Objective Offline Optimization",
    "abstract": "Offline multi-objective optimization aims to identify Pareto-optimal\nsolutions given a dataset of designs and their objective values. In this work,\nwe propose a preference-guided diffusion model that generates Pareto-optimal\ndesigns by leveraging a classifier-based guidance mechanism. Our guidance\nclassifier is a preference model trained to predict the probability that one\ndesign dominates another, directing the diffusion model toward optimal regions\nof the design space. Crucially, this preference model generalizes beyond the\ntraining distribution, enabling the discovery of Pareto-optimal solutions\noutside the observed dataset. We introduce a novel diversity-aware preference\nguidance, augmenting Pareto dominance preference with diversity criteria. This\nensures that generated solutions are optimal and well-distributed across the\nobjective space, a capability absent in prior generative methods for offline\nmulti-objective optimization. We evaluate our approach on various continuous\noffline multi-objective optimization tasks and find that it consistently\noutperforms other inverse/generative approaches while remaining competitive\nwith forward/surrogate-based optimization methods. Our results highlight the\neffectiveness of classifier-guided diffusion models in generating diverse and\nhigh-quality solutions that approximate the Pareto front well.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yashas Annadani",
      "Syrine Belakaria",
      "Stefano Ermon",
      "Stefan Bauer",
      "Barbara E Engelhardt"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17290v1",
    "title": "Calibration Strategies for Robust Causal Estimation: Theoretical and Empirical Insights on Propensity Score Based Estimators",
    "abstract": "The partitioning of data for estimation and calibration critically impacts\nthe performance of propensity score based estimators like inverse probability\nweighting (IPW) and double/debiased machine learning (DML) frameworks. We\nextend recent advances in calibration techniques for propensity score\nestimation, improving the robustness of propensity scores in challenging\nsettings such as limited overlap, small sample sizes, or unbalanced data. Our\ncontributions are twofold: First, we provide a theoretical analysis of the\nproperties of calibrated estimators in the context of DML. To this end, we\nrefine existing calibration frameworks for propensity score models, with a\nparticular emphasis on the role of sample-splitting schemes in ensuring valid\ncausal inference. Second, through extensive simulations, we show that\ncalibration reduces variance of inverse-based propensity score estimators while\nalso mitigating bias in IPW, even in small-sample regimes. Notably, calibration\nimproves stability for flexible learners (e.g., gradient boosting) while\npreserving the doubly robust properties of DML. A key insight is that, even\nwhen methods perform well without calibration, incorporating a calibration step\ndoes not degrade performance, provided that an appropriate sample-splitting\napproach is chosen.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "econ.EM",
      "stat.ME"
    ],
    "authors": [
      "Jan Rabenseifner",
      "Sven Klaassen",
      "Jannis Kueck",
      "Philipp Bach"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17289v1",
    "title": "3D Neural Operator-Based Flow Surrogates around 3D geometries: Signed Distance Functions and Derivative Constraints",
    "abstract": "Accurate modeling of fluid dynamics around complex geometries is critical for\napplications such as aerodynamic optimization and biomedical device design.\nWhile advancements in numerical methods and high-performance computing have\nimproved simulation capabilities, the computational cost of high-fidelity 3D\nflow simulations remains a significant challenge. Scientific machine learning\n(SciML) offers an efficient alternative, enabling rapid and reliable flow\npredictions. In this study, we evaluate Deep Operator Networks (DeepONet) and\nGeometric-DeepONet, a variant that incorporates geometry information via signed\ndistance functions (SDFs), on steady-state 3D flow over complex objects. Our\ndataset consists of 1,000 high-fidelity simulations spanning Reynolds numbers\nfrom 10 to 1,000, enabling comprehensive training and evaluation across a range\nof flow regimes. To assess model generalization, we test our models on a random\nand extrapolatory train-test splitting. Additionally, we explore a\nderivative-informed training strategy that augments standard loss functions\nwith velocity gradient penalties and incompressibility constraints, improving\nphysics consistency in 3D flow prediction. Our results show that\nGeometric-DeepONet improves boundary-layer accuracy by up to 32% compared to\nstandard DeepONet. Moreover, incorporating derivative constraints enhances\ngradient accuracy by 25% in interpolation tasks and up to 45% in extrapolatory\ntest scenarios, suggesting significant improvement in generalization\ncapabilities to unseen 3D Reynolds numbers.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Ali Rabeh",
      "Adarsh Krishnamurthy",
      "Baskar Ganapathysubramanian"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17288v1",
    "title": "Exploring a Principled Framework for Deep Subspace Clustering",
    "abstract": "Subspace clustering is a classical unsupervised learning task, built on a\nbasic assumption that high-dimensional data can be approximated by a union of\nsubspaces (UoS). Nevertheless, the real-world data are often deviating from the\nUoS assumption. To address this challenge, state-of-the-art deep subspace\nclustering algorithms attempt to jointly learn UoS representations and\nself-expressive coefficients. However, the general framework of the existing\nalgorithms suffers from a catastrophic feature collapse and lacks a theoretical\nguarantee to learn desired UoS representation. In this paper, we present a\nPrincipled fRamewOrk for Deep Subspace Clustering (PRO-DSC), which is designed\nto learn structured representations and self-expressive coefficients in a\nunified manner. Specifically, in PRO-DSC, we incorporate an effective\nregularization on the learned representations into the self-expressive model,\nprove that the regularized self-expressive model is able to prevent feature\nspace collapse, and demonstrate that the learned optimal representations under\ncertain condition lie on a union of orthogonal subspaces. Moreover, we provide\na scalable and efficient approach to implement our PRO-DSC and conduct\nextensive experiments to verify our theoretical findings and demonstrate the\nsuperior performance of our proposed deep subspace clustering approach. The\ncode is available at https://github.com/mengxianghan123/PRO-DSC.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Xianghan Meng",
      "Zhiyuan Huang",
      "Wei He",
      "Xianbiao Qi",
      "Rong Xiao",
      "Chun-Guang Li"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17287v1",
    "title": "FastCuRL: Curriculum Reinforcement Learning with Progressive Context Extension for Efficient Training R1-like Reasoning Models",
    "abstract": "In this paper, we propose \\textbf{\\textsc{FastCuRL}}, a simple yet efficient\n\\textbf{Cu}rriculum \\textbf{R}einforcement \\textbf{L}earning approach with\ncontext window extending strategy to accelerate the reinforcement learning\ntraining efficiency for R1-like reasoning models while enhancing their\nperformance in tackling complex reasoning tasks with long chain-of-thought\nrationales, particularly with a 1.5B parameter language model.\n\\textbf{\\textsc{FastCuRL}} consists of two main procedures: length-aware\ntraining data segmentation and context window extension training. Specifically,\nthe former first splits the original training data into three different levels\nby the input prompt length, and then the latter leverages segmented training\ndatasets with a progressively increasing context window length to train the\nreasoning model. Experimental results demonstrate that\n\\textbf{\\textsc{FastCuRL}}-1.5B-Preview surpasses DeepScaleR-1.5B-Preview\nacross all five datasets (including MATH 500, AIME 2024, AMC 2023, Minerva\nMath, and OlympiadBench) while only utilizing 50\\% of training steps.\nFurthermore, all training stages for FastCuRL-1.5B-Preview are completed using\njust a single node with 8 GPUs.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Mingyang Song",
      "Mao Zheng",
      "Zheng Li",
      "Wenjie Yang",
      "Xuan Luo",
      "Yue Pan",
      "Feng Zhang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17286v1",
    "title": "Offline Model-Based Optimization: Comprehensive Review",
    "abstract": "Offline optimization is a fundamental challenge in science and engineering,\nwhere the goal is to optimize black-box functions using only offline datasets.\nThis setting is particularly relevant when querying the objective function is\nprohibitively expensive or infeasible, with applications spanning protein\nengineering, material discovery, neural architecture search, and beyond. The\nmain difficulty lies in accurately estimating the objective landscape beyond\nthe available data, where extrapolations are fraught with significant epistemic\nuncertainty. This uncertainty can lead to objective hacking(reward hacking),\nexploiting model inaccuracies in unseen regions, or other spurious\noptimizations that yield misleadingly high performance estimates outside the\ntraining distribution. Recent advances in model-based optimization(MBO) have\nharnessed the generalization capabilities of deep neural networks to develop\noffline-specific surrogate and generative models. Trained with carefully\ndesigned strategies, these models are more robust against out-of-distribution\nissues, facilitating the discovery of improved designs. Despite its growing\nimpact in accelerating scientific discovery, the field lacks a comprehensive\nreview. To bridge this gap, we present the first thorough review of offline\nMBO. We begin by formalizing the problem for both single-objective and\nmulti-objective settings and by reviewing recent benchmarks and evaluation\nmetrics. We then categorize existing approaches into two key areas: surrogate\nmodeling, which emphasizes accurate function approximation in\nout-of-distribution regions, and generative modeling, which explores\nhigh-dimensional design spaces to identify high-performing designs. Finally, we\nexamine the key challenges and propose promising directions for advancement in\nthis rapidly evolving field including safe control of superintelligent systems.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Minsu Kim",
      "Jiayao Gu",
      "Ye Yuan",
      "Taeyoung Yun",
      "Zixuan Liu",
      "Yoshua Bengio",
      "Can Chen"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17285v1",
    "title": "An Iterative Feedback Mechanism for Improving Natural Language Class Descriptions in Open-Vocabulary Object Detection",
    "abstract": "Recent advances in open-vocabulary object detection models will enable\nAutomatic Target Recognition systems to be sustainable and repurposed by\nnon-technical end-users for a variety of applications or missions. New, and\npotentially nuanced, classes can be defined with natural language text\ndescriptions in the field, immediately before runtime, without needing to\nretrain the model. We present an approach for improving non-technical users'\nnatural language text descriptions of their desired targets of interest, using\na combination of analysis techniques on the text embeddings, and proper\ncombinations of embeddings for contrastive examples. We quantify the\nimprovement that our feedback mechanism provides by demonstrating performance\nwith multiple publicly-available open-vocabulary object detection models.",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "authors": [
      "Louis Y. Kim",
      "Michelle Karker",
      "Victoria Valledor",
      "Seiyoung C. Lee",
      "Karl F. Brzoska",
      "Margaret Duff",
      "Anthony Palladino"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17279v1",
    "title": "CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement",
    "abstract": "The meaning conveyed by a sentence often depends on the context in which it\nappears. Despite the progress of sentence embedding methods, it remains unclear\nhow to best modify a sentence embedding conditioned on its context. To address\nthis problem, we propose Condition-Aware Sentence Embeddings (CASE), an\nefficient and accurate method to create an embedding for a sentence under a\ngiven condition. First, CASE creates an embedding for the condition using a\nLarge Language Model (LLM), where the sentence influences the attention scores\ncomputed for the tokens in the condition during pooling. Next, a supervised\nnonlinear projection is learned to reduce the dimensionality of the LLM-based\ntext embeddings. We show that CASE significantly outperforms previously\nproposed Conditional Semantic Textual Similarity (C-STS) methods on an existing\nstandard benchmark dataset. We find that subtracting the condition embedding\nconsistently improves the C-STS performance of LLM-based text embeddings.\nMoreover, we propose a supervised dimensionality reduction method that not only\nreduces the dimensionality of LLM-based embeddings but also significantly\nimproves their performance.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Gaifan Zhang",
      "Yi Zhou",
      "Danushka Bollegala"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17276v1",
    "title": "HyperNVD: Accelerating Neural Video Decomposition via Hypernetworks",
    "abstract": "Decomposing a video into a layer-based representation is crucial for easy\nvideo editing for the creative industries, as it enables independent editing of\nspecific layers. Existing video-layer decomposition models rely on implicit\nneural representations (INRs) trained independently for each video, making the\nprocess time-consuming when applied to new videos. Noticing this limitation, we\npropose a meta-learning strategy to learn a generic video decomposition model\nto speed up the training on new videos. Our model is based on a hypernetwork\narchitecture which, given a video-encoder embedding, generates the parameters\nfor a compact INR-based neural video decomposition model. Our strategy\nmitigates the problem of single-video overfitting and, importantly, shortens\nthe convergence of video decomposition on new, unseen videos. Our code is\navailable at: https://hypernvd.github.io/",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Maria Pilligua",
      "Danna Xue",
      "Javier Vazquez-Corral"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17275v1",
    "title": "Vision Transformer Based Semantic Communications for Next Generation Wireless Networks",
    "abstract": "In the evolving landscape of 6G networks, semantic communications are poised\nto revolutionize data transmission by prioritizing the transmission of semantic\nmeaning over raw data accuracy. This paper presents a Vision Transformer\n(ViT)-based semantic communication framework that has been deliberately\ndesigned to achieve high semantic similarity during image transmission while\nsimultaneously minimizing the demand for bandwidth. By equipping ViT as the\nencoder-decoder framework, the proposed architecture can proficiently encode\nimages into a high semantic content at the transmitter and precisely\nreconstruct the images, considering real-world fading and noise consideration\nat the receiver. Building on the attention mechanisms inherent to ViTs, our\nmodel outperforms Convolution Neural Network (CNNs) and Generative Adversarial\nNetworks (GANs) tailored for generating such images. The architecture based on\nthe proposed ViT network achieves the Peak Signal-to-noise Ratio (PSNR) of 38\ndB, which is higher than other Deep Learning (DL) approaches in maintaining\nsemantic similarity across different communication environments. These findings\nestablish our ViT-based approach as a significant breakthrough in semantic\ncommunications.",
    "categories": [
      "eess.IV",
      "cs.CV",
      "eess.SP"
    ],
    "authors": [
      "Muhammad Ahmed Mohsin",
      "Muhammad Jazib",
      "Zeeshan Alam",
      "Muhmmad Farhan Khan",
      "Muhammad Saad",
      "Muhammad Ali Jamshed"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17272v1",
    "title": "Revisiting End To End Sparse Autoencoder Training -- A Short Finetune is All You Need",
    "abstract": "Sparse autoencoders (SAEs) are widely used for interpreting language model\nactivations. A key evaluation metric is the increase in cross-entropy loss when\nreplacing model activations with SAE reconstructions. Typically, SAEs are\ntrained solely on mean squared error (MSE) using precomputed, shuffled\nactivations. Recent work introduced training SAEs directly with a combination\nof KL divergence and MSE (\"end-to-end\" SAEs), significantly improving\nreconstruction accuracy at the cost of substantially increased computation,\nwhich has limited their widespread adoption. We propose a brief KL+MSE\nfine-tuning step applied only to the final 25M training tokens (just a few\npercent of typical training budgets) that achieves comparable improvements,\nreducing the cross-entropy loss gap by 20-50%, while incurring minimal\nadditional computational cost. We further find that multiple fine-tuning\nmethods (KL fine-tuning, LoRA adapters, linear adapters) yield similar,\nnon-additive cross-entropy improvements, suggesting a common, easily\ncorrectable error source in MSE-trained SAEs. We demonstrate a straightforward\nmethod for effectively transferring hyperparameters and sparsity penalties\ndespite scale differences between KL and MSE losses. While both ReLU and TopK\nSAEs see significant cross-entropy loss improvements, evaluations on supervised\nSAEBench metrics yield mixed results, suggesting practical benefits depend on\nboth SAE architecture and the specific downstream task. Nonetheless, our method\noffers meaningful improvements in interpretability applications such as circuit\nanalysis with minor additional cost.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Adam Karvonen"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17269v1",
    "title": "Recovering Pulse Waves from Video Using Deep Unrolling and Deep Equilibrium Models",
    "abstract": "Camera-based monitoring of vital signs, also known as imaging\nphotoplethysmography (iPPG), has seen applications in driver-monitoring,\nperfusion assessment in surgical settings, affective computing, and more. iPPG\ninvolves sensing the underlying cardiac pulse from video of the skin and\nestimating vital signs such as the heart rate or a full pulse waveform. Some\nprevious iPPG methods impose model-based sparse priors on the pulse signals and\nuse iterative optimization for pulse wave recovery, while others use end-to-end\nblack-box deep learning methods. In contrast, we introduce methods that combine\nsignal processing and deep learning methods in an inverse problem framework.\nOur methods estimate the underlying pulse signal and heart rate from facial\nvideo by learning deep-network-based denoising operators that leverage deep\nalgorithm unfolding and deep equilibrium models. Experiments show that our\nmethods can denoise an acquired signal from the face and infer the correct\nunderlying pulse rate, achieving state-of-the-art heart rate estimation\nperformance on well-known benchmarks, all with less than one-fifth the number\nof learnable parameters as the closest competing method.",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "authors": [
      "Vineet R Shenoy",
      "Suhas Lohit",
      "Hassan Mansour",
      "Rama Chellappa",
      "Tim K. Marks"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17267v1",
    "title": "Physical Plausibility-aware Trajectory Prediction via Locomotion Embodiment",
    "abstract": "Humans can predict future human trajectories even from momentary observations\nby using human pose-related cues. However, previous Human Trajectory Prediction\n(HTP) methods leverage the pose cues implicitly, resulting in implausible\npredictions. To address this, we propose Locomotion Embodiment, a framework\nthat explicitly evaluates the physical plausibility of the predicted trajectory\nby locomotion generation under the laws of physics. While the plausibility of\nlocomotion is learned with an indifferentiable physics simulator, it is\nreplaced by our differentiable Locomotion Value function to train an HTP\nnetwork in a data-driven manner. In particular, our proposed Embodied\nLocomotion loss is beneficial for efficiently training a stochastic HTP network\nusing multiple heads. Furthermore, the Locomotion Value filter is proposed to\nfilter out implausible trajectories at inference. Experiments demonstrate that\nour method enhances even the state-of-the-art HTP methods across diverse\ndatasets and problem settings. Our code is available at:\nhttps://github.com/ImIntheMiddle/EmLoco.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Hiromu Taketsugu",
      "Takeru Oba",
      "Takahiro Maeda",
      "Shohei Nobuhara",
      "Norimichi Ukita"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17265v1",
    "title": "Learning to Solve Related Linear Systems",
    "abstract": "Solving multiple parametrised related systems is an essential component of\nmany numerical tasks. Borrowing strength from the solved systems and learning\nwill make this process faster. In this work, we propose a novel probabilistic\nlinear solver over the parameter space. This leverages information from the\nsolved linear systems in a regression setting to provide an efficient posterior\nmean and covariance. We advocate using this as companion regression model for\nthe preconditioned conjugate gradient method, and discuss the favourable\nproperties of the posterior mean and covariance as the initial guess and\npreconditioner. We also provide several design choices for this companion\nsolver. Numerical experiments showcase the benefits of using our novel solver\nin a hyperparameter optimisation problem.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "cs.NA",
      "math.NA"
    ],
    "authors": [
      "Disha Hegde",
      "Jon Cockayne"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17262v1",
    "title": "Unsupervised Joint Learning of Optical Flow and Intensity with Event Cameras",
    "abstract": "Event cameras rely on motion to obtain information about scene appearance. In\nother words, for event cameras, motion and appearance are seen both or neither,\nwhich are encoded in the output event stream. Previous works consider\nrecovering these two visual quantities as separate tasks, which does not fit\nwith the nature of event cameras and neglects the inherent relations between\nboth tasks. In this paper, we propose an unsupervised learning framework that\njointly estimates optical flow (motion) and image intensity (appearance), with\na single network. Starting from the event generation model, we newly derive the\nevent-based photometric error as a function of optical flow and image\nintensity, which is further combined with the contrast maximization framework,\nyielding a comprehensive loss function that provides proper constraints for\nboth flow and intensity estimation. Exhaustive experiments show that our model\nachieves state-of-the-art performance for both optical flow (achieves 20% and\n25% improvement in EPE and AE respectively in the unsupervised learning\ncategory) and intensity estimation (produces competitive results with other\nbaselines, particularly in high dynamic range scenarios). Last but not least,\nour model achieves shorter inference time than all the other optical flow\nmodels and many of the image reconstruction models, while they output only one\nquantity. Project page: https://github.com/tub-rip/e2fai",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "authors": [
      "Shuang Guo",
      "Friedhelm Hamann",
      "Guillermo Gallego"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17261v1",
    "title": "Cross-Modal Interactive Perception Network with Mamba for Lung Tumor Segmentation in PET-CT Images",
    "abstract": "Lung cancer is a leading cause of cancer-related deaths globally. PET-CT is\ncrucial for imaging lung tumors, providing essential metabolic and anatomical\ninformation, while it faces challenges such as poor image quality, motion\nartifacts, and complex tumor morphology. Deep learning-based models are\nexpected to address these problems, however, existing small-scale and private\ndatasets limit significant performance improvements for these methods. Hence,\nwe introduce a large-scale PET-CT lung tumor segmentation dataset, termed\nPCLT20K, which comprises 21,930 pairs of PET-CT images from 605 patients.\nFurthermore, we propose a cross-modal interactive perception network with Mamba\n(CIPA) for lung tumor segmentation in PET-CT images. Specifically, we design a\nchannel-wise rectification module (CRM) that implements a channel state space\nblock across multi-modal features to learn correlated representations and helps\nfilter out modality-specific noise. A dynamic cross-modality interaction module\n(DCIM) is designed to effectively integrate position and context information,\nwhich employs PET images to learn regional position information and serves as a\nbridge to assist in modeling the relationships between local features of CT\nimages. Extensive experiments on a comprehensive benchmark demonstrate the\neffectiveness of our CIPA compared to the current state-of-the-art segmentation\nmethods. We hope our research can provide more exploration opportunities for\nmedical image segmentation. The dataset and code are available at\nhttps://github.com/mj129/CIPA.",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Jie Mei",
      "Chenyu Lin",
      "Yu Qiu",
      "Yaonan Wang",
      "Hui Zhang",
      "Ziyang Wang",
      "Dong Dai"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17252v1",
    "title": "On Privately Estimating a Single Parameter",
    "abstract": "We investigate differentially private estimators for individual parameters\nwithin larger parametric models. While generic private estimators exist, the\nestimators we provide repose on new local notions of estimand stability, and\nthese notions allow procedures that provide private certificates of their own\nstability. By leveraging these private certificates, we provide computationally\nand statistical efficient mechanisms that release private statistics that are,\nat least asymptotically in the sample size, essentially unimprovable: they\nachieve instance optimal bounds. Additionally, we investigate the practicality\nof the algorithms both in simulated data and in real-world data from the\nAmerican Community Survey and US Census, highlighting scenarios in which the\nnew procedures are successful and identifying areas for future work.",
    "categories": [
      "cs.LG",
      "cs.CR",
      "math.ST",
      "stat.TH"
    ],
    "authors": [
      "Hilal Asi",
      "John C. Duchi",
      "Kunal Talwar"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17251v1",
    "title": "Breaking the Symmetries of Indistinguishable Objects",
    "abstract": "Indistinguishable objects often occur when modelling problems in constraint\nprogramming, as well as in other related paradigms. They occur when objects can\nbe viewed as being drawn from a set of unlabelled objects, and the only\noperation allowed on them is equality testing. For example, the golfers in the\nsocial golfer problem are indistinguishable. If we do label the golfers, then\nany relabelling of the golfers in one solution gives another valid solution.\nTherefore, we can regard the symmetric group of size $n$ as acting on a set of\n$n$ indistinguishable objects. In this paper, we show how we can break the\nsymmetries resulting from indistinguishable objects. We show how symmetries on\nindistinguishable objects can be defined properly in complex types, for example\nin a matrix indexed by indistinguishable objects. We then show how the\nresulting symmetries can be broken correctly. In Essence, a high-level\nmodelling language, indistinguishable objects are encapsulated in \"unnamed\ntypes\". We provide an implementation of complete symmetry breaking for unnamed\ntypes in Essence.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Ozgur Akgun",
      "Mun See Chang",
      "Ian P. Gent",
      "Christopher Jefferson"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17247v1",
    "title": "KL3M Tokenizers: A Family of Domain-Specific and Character-Level Tokenizers for Legal, Financial, and Preprocessing Applications",
    "abstract": "We present the KL3M tokenizers, a family of specialized tokenizers for legal,\nfinancial, and governmental text. Despite established work on tokenization,\nspecialized tokenizers for professional domains remain understudied. Our paper\noffers two main contributions to this area.\n  First, we introduce domain-specific BPE tokenizers for legal, financial, and\ngovernmental text. Our kl3m-004-128k-cased tokenizer uses 9-17% fewer tokens\nthan GPT-4o and Llama3 for domain-specific documents, despite having a smaller\nvocabulary. For specialized terminology, our cased tokenizer is even more\nefficient, using up to 83% fewer tokens for legal terms and 39% fewer tokens\nfor financial terms.\n  Second, we develop character-level BPE tokenizers (4K, 8K, and 16K vocabulary\nsizes) for text correction tasks like OCR post-processing. These tokenizers\nkeep consistent token boundaries between error-containing and correct text,\nmaking it easier for models to learn correction patterns.\n  These tokenizers help professional applications by fitting more text in\ncontext windows, reducing computational needs, and preserving the meaning of\ndomain-specific terms. Our analysis shows these efficiency gains directly\nbenefit the processing of long legal and financial documents. We release all\ntokenizers and code through GitHub and Hugging Face to support further research\nin specialized tokenization.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Michael J Bommarito",
      "Daniel Martin Katz",
      "Jillian Bommarito"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17244v1",
    "title": "Deep End-to-End Posterior ENergy (DEEPEN) for image recovery",
    "abstract": "Current end-to-end (E2E) and plug-and-play (PnP) image reconstruction\nalgorithms approximate the maximum a posteriori (MAP) estimate but cannot offer\nsampling from the posterior distribution, like diffusion models. By contrast,\nit is challenging for diffusion models to be trained in an E2E fashion. This\npaper introduces a Deep End-to-End Posterior ENergy (DEEPEN) framework, which\nenables MAP estimation as well as sampling. We learn the parameters of the\nposterior, which is the sum of the data consistency error and the negative\nlog-prior distribution, using maximum likelihood optimization in an E2E\nfashion. The proposed approach does not require algorithm unrolling, and hence\nhas a smaller computational and memory footprint than current E2E methods,\nwhile it does not require contraction constraints typically needed by current\nPnP methods. Our results demonstrate that DEEPEN offers improved performance\nthan current E2E and PnP models in the MAP setting, while it also offers faster\nsampling compared to diffusion models. In addition, the learned energy-based\nmodel is observed to be more robust to changes in image acquisition settings.",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Jyothi Rikhab Chand",
      "Mathews Jacob"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17239v1",
    "title": "SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging",
    "abstract": "Fine-tuning large language models (LLMs) on downstream tasks can\ninadvertently erode their safety alignment, even for benign fine-tuning\ndatasets. We address this challenge by proposing SafeMERGE, a post-fine-tuning\nframework that preserves safety while maintaining task utility. It achieves\nthis by selectively merging fine-tuned and safety-aligned model layers only\nwhen those deviate from safe behavior, measured by a cosine similarity\ncriterion. We evaluate SafeMERGE against other fine-tuning- and\npost-fine-tuning-stage approaches for Llama-2-7B-Chat and Qwen-2-7B-Instruct\nmodels on GSM8K and PubMedQA tasks while exploring different merging\nstrategies. We find that SafeMERGE consistently reduces harmful outputs\ncompared to other baselines without significantly sacrificing performance,\nsometimes even enhancing it. The results suggest that our selective,\nsubspace-guided, and per-layer merging method provides an effective safeguard\nagainst the inadvertent loss of safety in fine-tuned LLMs while outperforming\nsimpler post-fine-tuning-stage defenses.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Aladin Djuhera",
      "Swanand Ravindra Kadhe",
      "Farhan Ahmed",
      "Syed Zawad",
      "Holger Boche"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17238v1",
    "title": "Slide-Level Prompt Learning with Vision Language Models for Few-Shot Multiple Instance Learning in Histopathology",
    "abstract": "In this paper, we address the challenge of few-shot classification in\nhistopathology whole slide images (WSIs) by utilizing foundational\nvision-language models (VLMs) and slide-level prompt learning. Given the\ngigapixel scale of WSIs, conventional multiple instance learning (MIL) methods\nrely on aggregation functions to derive slide-level (bag-level) predictions\nfrom patch representations, which require extensive bag-level labels for\ntraining. In contrast, VLM-based approaches excel at aligning visual embeddings\nof patches with candidate class text prompts but lack essential pathological\nprior knowledge. Our method distinguishes itself by utilizing pathological\nprior knowledge from language models to identify crucial local tissue types\n(patches) for WSI classification, integrating this within a VLM-based MIL\nframework. Our approach effectively aligns patch images with tissue types, and\nwe fine-tune our model via prompt learning using only a few labeled WSIs per\ncategory. Experimentation on real-world pathological WSI datasets and ablation\nstudies highlight our method's superior performance over existing MIL- and\nVLM-based methods in few-shot WSI classification tasks. Our code is publicly\navailable at https://github.com/LTS5/SLIP.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Devavrat Tomar",
      "Guillaume Vray",
      "Dwarikanath Mahapatra",
      "Sudipta Roy",
      "Jean-Philippe Thiran",
      "Behzad Bozorgtabar"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17237v1",
    "title": "Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID",
    "abstract": "Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal\ninfrared video is inherently challenging due to low contrast, environmental\nnoise, and small target sizes. This paper provides a straightforward approach\nto address multi-UAV tracking in thermal infrared video, leveraging recent\nadvances in detection and tracking. Instead of relying on the YOLOv5 with the\nDeepSORT pipeline, we present a tracking framework built on YOLOv12 and\nBoT-SORT, enhanced with tailored training and inference strategies. We evaluate\nour approach following the metrics from the 4th Anti-UAV Challenge and\ndemonstrate competitive performance. Notably, we achieve strong results without\nusing contrast enhancement or temporal information fusion to enrich UAV\nfeatures, highlighting our approach as a \"Strong Baseline\" for the multi-UAV\ntracking task. We provide implementation details, in-depth experimental\nanalysis, and a discussion of potential improvements. The code is available at\nhttps://github.com/wish44165/YOLOv12-BoT-SORT-ReID .",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Yu-Hsi Chen"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17231v1",
    "title": "LoGoFair: Post-Processing for Local and Global Fairness in Federated Learning",
    "abstract": "Federated learning (FL) has garnered considerable interest for its capability\nto learn from decentralized data sources. Given the increasing application of\nFL in decision-making scenarios, addressing fairness issues across different\nsensitive groups (e.g., female, male) in FL is crucial. Current research often\nfocuses on facilitating fairness at each client's data (local fairness) or\nwithin the entire dataset across all clients (global fairness). However,\nexisting approaches that focus exclusively on either local or global fairness\nfail to address two key challenges: (\\textbf{CH1}) Under statistical\nheterogeneity, global fairness does not imply local fairness, and vice versa.\n(\\textbf{CH2}) Achieving fairness under model-agnostic setting. To tackle the\naforementioned challenges, this paper proposes a novel post-processing\nframework for achieving both Local and Global Fairness in the FL context,\nnamely LoGoFair. To address CH1, LoGoFair endeavors to seek the Bayes optimal\nclassifier under local and global fairness constraints, which strikes the\noptimal accuracy-fairness balance in the probabilistic sense. To address CH2,\nLoGoFair employs a model-agnostic federated post-processing procedure that\nenables clients to collaboratively optimize global fairness while ensuring\nlocal fairness, thereby achieving the optimal fair classifier within FL.\nExperimental results on three real-world datasets further illustrate the\neffectiveness of the proposed LoGoFair framework.",
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "authors": [
      "Li Zhang",
      "Chaochao Chen",
      "Zhongxuan Han",
      "Qiyong Zhong",
      "Xiaolin Zheng"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17229v1",
    "title": "FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs",
    "abstract": "Large Language Models (LLMs) frequently generate hallucinated content, posing\nsignificant challenges for applications where factuality is crucial. While\nexisting hallucination detection methods typically operate at the sentence\nlevel or passage level, we propose FactSelfCheck, a novel black-box\nsampling-based method that enables fine-grained fact-level detection. Our\napproach represents text as knowledge graphs consisting of facts in the form of\ntriples. Through analyzing factual consistency across multiple LLM responses,\nwe compute fine-grained hallucination scores without requiring external\nresources or training data. Our evaluation demonstrates that FactSelfCheck\nperforms competitively with leading sampling-based methods while providing more\ndetailed insights. Most notably, our fact-level approach significantly improves\nhallucination correction, achieving a 35% increase in factual content compared\nto the baseline, while sentence-level SelfCheckGPT yields only an 8%\nimprovement. The granular nature of our detection enables more precise\nidentification and correction of hallucinated content.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Albert Sawczyn",
      "Jakub Binkowski",
      "Denis Janiak",
      "Bogdan Gabrys",
      "Tomasz Kajdanowicz"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17226v1",
    "title": "Leveraging Text-to-Image Generation for Handling Spurious Correlation",
    "abstract": "Deep neural networks trained with Empirical Risk Minimization (ERM) perform\nwell when both training and test data come from the same domain, but they often\nfail to generalize to out-of-distribution samples. In image classification,\nthese models may rely on spurious correlations that often exist between labels\nand irrelevant features of images, making predictions unreliable when those\nfeatures do not exist. We propose a technique to generate training samples with\ntext-to-image (T2I) diffusion models for addressing the spurious correlation\nproblem. First, we compute the best describing token for the visual features\npertaining to the causal components of samples by a textual inversion\nmechanism. Then, leveraging a language segmentation method and a diffusion\nmodel, we generate new samples by combining the causal component with the\nelements from other classes. We also meticulously prune the generated samples\nbased on the prediction probabilities and attribution scores of the ERM model\nto ensure their correct composition for our objective. Finally, we retrain the\nERM model on our augmented dataset. This process reduces the model's reliance\non spurious correlations by learning from carefully crafted samples for in\nwhich this correlation does not exist. Our experiments show that across\ndifferent benchmarks, our technique achieves better worst-group accuracy than\nthe existing state-of-the-art methods.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Aryan Yazdan Parast",
      "Basim Azam",
      "Naveed Akhtar"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17224v1",
    "title": "Neuro-Symbolic Scene Graph Conditioning for Synthetic Image Dataset Generation",
    "abstract": "As machine learning models increase in scale and complexity, obtaining\nsufficient training data has become a critical bottleneck due to acquisition\ncosts, privacy constraints, and data scarcity in specialised domains. While\nsynthetic data generation has emerged as a promising alternative, a notable\nperformance gap remains compared to models trained on real data, particularly\nas task complexity grows. Concurrently, Neuro-Symbolic methods, which combine\nneural networks' learning strengths with symbolic reasoning's structured\nrepresentations, have demonstrated significant potential across various\ncognitive tasks. This paper explores the utility of Neuro-Symbolic conditioning\nfor synthetic image dataset generation, focusing specifically on improving the\nperformance of Scene Graph Generation models. The research investigates whether\nstructured symbolic representations in the form of scene graphs can enhance\nsynthetic data quality through explicit encoding of relational constraints. The\nresults demonstrate that Neuro-Symbolic conditioning yields significant\nimprovements of up to +2.59% in standard Recall metrics and +2.83% in No Graph\nConstraint Recall metrics when used for dataset augmentation. These findings\nestablish that merging Neuro-Symbolic and generative approaches produces\nsynthetic data with complementary structural information that enhances model\nperformance when combined with real data, providing a novel approach to\novercome data scarcity limitations even for complex visual reasoning tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Giacomo Savazzi",
      "Eugenio Lomurno",
      "Cristian Sbrolli",
      "Agnese Chiatti",
      "Matteo Matteucci"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17222v1",
    "title": "Automating Adjudication of Cardiovascular Events Using Large Language Models",
    "abstract": "Cardiovascular events, such as heart attacks and strokes, remain a leading\ncause of mortality globally, necessitating meticulous monitoring and\nadjudication in clinical trials. This process, traditionally performed manually\nby clinical experts, is time-consuming, resource-intensive, and prone to\ninter-reviewer variability, potentially introducing bias and hindering trial\nprogress. This study addresses these critical limitations by presenting a novel\nframework for automating the adjudication of cardiovascular events in clinical\ntrials using Large Language Models (LLMs). We developed a two-stage approach:\nfirst, employing an LLM-based pipeline for event information extraction from\nunstructured clinical data and second, using an LLM-based adjudication process\nguided by a Tree of Thoughts approach and clinical endpoint committee (CEC)\nguidelines. Using cardiovascular event-specific clinical trial data, the\nframework achieved an F1-score of 0.82 for event extraction and an accuracy of\n0.68 for adjudication. Furthermore, we introduce the CLEART score, a novel,\nautomated metric specifically designed for evaluating the quality of\nAI-generated clinical reasoning in adjudicating cardiovascular events. This\napproach demonstrates significant potential for substantially reducing\nadjudication time and costs while maintaining high-quality, consistent, and\nauditable outcomes in clinical trials. The reduced variability and enhanced\nstandardization also allow for faster identification and mitigation of risks\nassociated with cardiovascular therapies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Sonish Sivarajkumar",
      "Kimia Ameri",
      "Chuqin Li",
      "Yanshan Wang",
      "Min Jiang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17221v1",
    "title": "UniCon: Unidirectional Information Flow for Effective Control of Large-Scale Diffusion Models",
    "abstract": "We introduce UniCon, a novel architecture designed to enhance control and\nefficiency in training adapters for large-scale diffusion models. Unlike\nexisting methods that rely on bidirectional interaction between the diffusion\nmodel and control adapter, UniCon implements a unidirectional flow from the\ndiffusion network to the adapter, allowing the adapter alone to generate the\nfinal output. UniCon reduces computational demands by eliminating the need for\nthe diffusion model to compute and store gradients during adapter training. Our\nresults indicate that UniCon reduces GPU memory usage by one-third and\nincreases training speed by 2.3 times, while maintaining the same adapter\nparameter size. Additionally, without requiring extra computational resources,\nUniCon enables the training of adapters with double the parameter volume of\nexisting ControlNets. In a series of image conditional generation tasks, UniCon\nhas demonstrated precise responsiveness to control inputs and exceptional\ngeneration capabilities.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Fanghua Yu",
      "Jinjin Gu",
      "Jinfan Hu",
      "Zheyuan Li",
      "Chao Dong"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17214v1",
    "title": "ML-Based Bidding Price Prediction for Pay-As-Bid Ancillary Services Markets: A Use Case in the German Control Reserve Market",
    "abstract": "The increasing integration of renewable energy sources has led to greater\nvolatility and unpredictability in electricity generation, posing challenges to\ngrid stability. Ancillary service markets, such as the German control reserve\nmarket, allow industrial consumers and producers to offer flexibility in their\npower consumption or generation, contributing to grid stability while earning\nadditional income. However, many participants use simple bidding strategies\nthat may not maximize their revenues. This paper presents a methodology for\nforecasting bidding prices in pay-as-bid ancillary service markets, focusing on\nthe German control reserve market. We evaluate various machine learning models,\nincluding Support Vector Regression, Decision Trees, and k-Nearest Neighbors,\nand compare their performance against benchmark models. To address the\nasymmetry in the revenue function of pay-as-bid markets, we introduce an offset\nadjustment technique that enhances the practical applicability of the\nforecasting models. Our analysis demonstrates that the proposed approach\nimproves potential revenues by 27.43 % to 37.31 % compared to baseline models.\nWhen analyzing the relationship between the model forecasting errors and the\nrevenue, a negative correlation is measured for three markets; according to the\nresults, a reduction of 1 EUR/MW model price forecasting error (MAE)\nstatistically leads to a yearly revenue increase between 483 EUR/MW and 3,631\nEUR/MW. The proposed methodology enables industrial participants to optimize\ntheir bidding strategies, leading to increased earnings and contributing to the\nefficiency and stability of the electrical grid.",
    "categories": [
      "cs.LG",
      "cs.CE",
      "stat.AP",
      "stat.ML"
    ],
    "authors": [
      "Vincent Bezold",
      "Lukas Baur",
      "Alexander Sauer"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17213v1",
    "title": "PP-DocLayout: A Unified Document Layout Detection Model to Accelerate Large-Scale Data Construction",
    "abstract": "Document layout analysis is a critical preprocessing step in document\nintelligence, enabling the detection and localization of structural elements\nsuch as titles, text blocks, tables, and formulas. Despite its importance,\nexisting layout detection models face significant challenges in generalizing\nacross diverse document types, handling complex layouts, and achieving\nreal-time performance for large-scale data processing. To address these\nlimitations, we present PP-DocLayout, which achieves high precision and\nefficiency in recognizing 23 types of layout regions across diverse document\nformats. To meet different needs, we offer three models of varying scales.\nPP-DocLayout-L is a high-precision model based on the RT-DETR-L detector,\nachieving 90.4% mAP@0.5 and an end-to-end inference time of 13.4 ms per page on\na T4 GPU. PP-DocLayout-M is a balanced model, offering 75.2% mAP@0.5 with an\ninference time of 12.7 ms per page on a T4 GPU. PP-DocLayout-S is a\nhigh-efficiency model designed for resource-constrained environments and\nreal-time applications, with an inference time of 8.1 ms per page on a T4 GPU\nand 14.5 ms on a CPU. This work not only advances the state of the art in\ndocument layout analysis but also provides a robust solution for constructing\nhigh-quality training data, enabling advancements in document intelligence and\nmultimodal AI systems. Code and models are available at\nhttps://github.com/PaddlePaddle/PaddleX .",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Ting Sun",
      "Cheng Cui",
      "Yuning Du",
      "Yi Liu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17212v1",
    "title": "A Deep Learning Framework for Visual Attention Prediction and Analysis of News Interfaces",
    "abstract": "News outlets' competition for attention in news interfaces has highlighted\nthe need for demographically-aware saliency prediction models. Despite recent\nadvancements in saliency detection applied to user interfaces (UI), existing\ndatasets are limited in size and demographic representation. We present a deep\nlearning framework that enhances the SaRa (Saliency Ranking) model with\nDeepGaze IIE, improving Salient Object Ranking (SOR) performance by 10.7%. Our\nframework optimizes three key components: saliency map generation, grid segment\nscoring, and map normalization. Through a two-fold experiment using\neye-tracking (30 participants) and mouse-tracking (375 participants aged\n13--70), we analyze attention patterns across demographic groups. Statistical\nanalysis reveals significant age-based variations (p < 0.05, {\\epsilon^2} =\n0.042), with older users (36--70) engaging more with textual content and\nyounger users (13--35) interacting more with images. Mouse-tracking data\nclosely approximates eye-tracking behavior (sAUC = 0.86) and identifies UI\nelements that immediately stand out, validating its use in large-scale studies.\nWe conclude that saliency studies should prioritize gathering data from a\nlarger, demographically representative sample and report exact demographic\ndistributions.",
    "categories": [
      "cs.CV",
      "cs.HC"
    ],
    "authors": [
      "Matthew Kenely",
      "Dylan Seychell",
      "Carl James Debono",
      "Chris Porter"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17211v1",
    "title": "A Language Anchor-Guided Method for Robust Noisy Domain Generalization",
    "abstract": "Real-world machine learning applications often struggle with two major\nchallenges: distribution shift and label noise. Models tend to overfit by\nfocusing on redundant and uninformative features in the training data, which\nmakes it hard for them to generalize to the target domain. Noisy data worsens\nthis problem by causing further overfitting to the noise, meaning that existing\nmethods often fail to tell the difference between true, invariant features and\nmisleading, spurious ones. To tackle these issues, we introduce Anchor\nAlignment and Adaptive Weighting (A3W). This new algorithm uses sample\nreweighting guided by natural language processing (NLP) anchors to extract more\nrepresentative features. In simple terms, A3W leverages semantic\nrepresentations from natural language models as a source of domain-invariant\nprior knowledge. Additionally, it employs a weighted loss function that adjusts\neach sample's contribution based on its similarity to the corresponding NLP\nanchor. This adjustment makes the model more robust to noisy labels. Extensive\nexperiments on standard benchmark datasets show that A3W consistently\noutperforms state-of-the-art domain generalization methods, offering\nsignificant improvements in both accuracy and robustness across different\ndatasets and noise levels.",
    "categories": [
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Zilin Dai",
      "Lehong Wang",
      "Fangzhou Lin",
      "Yidong Wang",
      "Zhigang Li",
      "Kazunori D Yamada",
      "Ziming Zhang",
      "Wang Lu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17198v1",
    "title": "Jailbreaking the Non-Transferable Barrier via Test-Time Data Disguising",
    "abstract": "Non-transferable learning (NTL) has been proposed to protect model\nintellectual property (IP) by creating a \"non-transferable barrier\" to restrict\ngeneralization from authorized to unauthorized domains. Recently, well-designed\nattack, which restores the unauthorized-domain performance by fine-tuning NTL\nmodels on few authorized samples, highlights the security risks of NTL-based\napplications. However, such attack requires modifying model weights, thus being\ninvalid in the black-box scenario. This raises a critical question: can we\ntrust the security of NTL models deployed as black-box systems? In this work,\nwe reveal the first loophole of black-box NTL models by proposing a novel\nattack method (dubbed as JailNTL) to jailbreak the non-transferable barrier\nthrough test-time data disguising. The main idea of JailNTL is to disguise\nunauthorized data so it can be identified as authorized by the NTL model,\nthereby bypassing the non-transferable barrier without modifying the NTL model\nweights. Specifically, JailNTL encourages unauthorized-domain disguising in two\nlevels, including: (i) data-intrinsic disguising (DID) for eliminating domain\ndiscrepancy and preserving class-related content at the input-level, and (ii)\nmodel-guided disguising (MGD) for mitigating output-level statistics difference\nof the NTL model. Empirically, when attacking state-of-the-art (SOTA) NTL\nmodels in the black-box scenario, JailNTL achieves an accuracy increase of up\nto 55.7% in the unauthorized domain by using only 1% authorized samples,\nlargely exceeding existing SOTA white-box attacks.",
    "categories": [
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Yongli Xiang",
      "Ziming Hong",
      "Lina Yao",
      "Dadong Wang",
      "Tongliang Liu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17197v1",
    "title": "FreeUV: Ground-Truth-Free Realistic Facial UV Texture Recovery via Cross-Assembly Inference Strategy",
    "abstract": "Recovering high-quality 3D facial textures from single-view 2D images is a\nchallenging task, especially under constraints of limited data and complex\nfacial details such as makeup, wrinkles, and occlusions. In this paper, we\nintroduce FreeUV, a novel ground-truth-free UV texture recovery framework that\neliminates the need for annotated or synthetic UV data. FreeUV leverages\npre-trained stable diffusion model alongside a Cross-Assembly inference\nstrategy to fulfill this objective. In FreeUV, separate networks are trained\nindependently to focus on realistic appearance and structural consistency, and\nthese networks are combined during inference to generate coherent textures. Our\napproach accurately captures intricate facial features and demonstrates robust\nperformance across diverse poses and occlusions. Extensive experiments validate\nFreeUV's effectiveness, with results surpassing state-of-the-art methods in\nboth quantitative and qualitative metrics. Additionally, FreeUV enables new\napplications, including local editing, facial feature interpolation, and\nmulti-view texture recovery. By reducing data requirements, FreeUV offers a\nscalable solution for generating high-fidelity 3D facial textures suitable for\nreal-world scenarios.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Xingchao Yang",
      "Takafumi Taketomi",
      "Yuki Endo",
      "Yoshihiro Kanamori"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17195v1",
    "title": "TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided Subspace Partitioning",
    "abstract": "Model customization requires high-quality and diverse datasets, but acquiring\nsuch data remains challenging and costly. Although large language models (LLMs)\ncan synthesize training data, current approaches are constrained by limited\nseed data, model bias and insufficient control over the generation process,\nresulting in limited diversity and biased distribution with the increase of\ndata scales. To tackle this challenge, we present TreeSynth, a tree-guided\nsubspace-based data synthesis framework that recursively partitions the entire\ndata space into hierar-chical subspaces, enabling comprehensive and diverse\nscaling of data synthesis. Briefly, given a task-specific description, we\nconstruct a data space partitioning tree by iteratively executing criteria\ndetermination and subspace coverage steps. This hierarchically divides the\nwhole space (i.e., root node) into mutually exclusive and complementary atomic\nsubspaces (i.e., leaf nodes). By collecting synthesized data according to the\nattributes of each leaf node, we obtain a diverse dataset that fully covers the\ndata space. Empirically, our extensive experiments demonstrate that TreeSynth\nsurpasses both human-designed datasets and the state-of-the-art data synthesis\nbaselines, achieving maximum improvements of 45.2% in data diversity and 17.6%\nin downstream task performance across various models and tasks. Hopefully,\nTreeSynth provides a scalable solution to synthesize diverse and comprehensive\ndatasets from scratch without human intervention.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Sheng Wang",
      "Pengan Chen",
      "Jingqi Zhou",
      "Qintong Li",
      "Jingwei Dong",
      "Jiahui Gao",
      "Boyang Xue",
      "Jiyue Jiang",
      "Lingpeng Kong",
      "Chuan Wu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17194v1",
    "title": "Curriculum RL meets Monte Carlo Planning: Optimization of a Real World Container Management Problem",
    "abstract": "In this work, we augment reinforcement learning with an inference-time\ncollision model to ensure safe and efficient container management in a\nwaste-sorting facility with limited processing capacity. Each container has two\noptimal emptying volumes that trade off higher throughput against overflow\nrisk. Conventional reinforcement learning (RL) approaches struggle under\ndelayed rewards, sparse critical events, and high-dimensional uncertainty --\nfailing to consistently balance higher-volume empties with the risk of\nsafety-limit violations. To address these challenges, we propose a hybrid\nmethod comprising: (1) a curriculum-learning pipeline that incrementally trains\na PPO agent to handle delayed rewards and class imbalance, and (2) an offline\npairwise collision model used at inference time to proactively avert collisions\nwith minimal online cost. Experimental results show that our targeted\ninference-time collision checks significantly improve collision avoidance,\nreduce safety-limit violations, maintain high throughput, and scale effectively\nacross varying container-to-PU ratios. These findings offer actionable\nguidelines for designing safe and efficient container-management systems in\nreal-world facilities.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Abhijeet Pendyala",
      "Tobias Glasmachers"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17193v1",
    "title": "MSCA-Net:Multi-Scale Context Aggregation Network for Infrared Small Target Detection",
    "abstract": "Detecting infrared small targets in complex backgrounds remains a challenging\ntask because of the low contrast and high noise levels inherent in infrared\nimages. These factors often lead to the loss of crucial details during feature\nextraction. Moreover, existing detection methods have limitations in adequately\nintegrating global and local information, which constrains the efficiency and\naccuracy of infrared small target detection. To address these challenges, this\npaper proposes a novel network architecture named MSCA-Net, which integrates\nthree key components: Multi-Scale Enhanced Detection Attention\nmechanism(MSEDA), Positional Convolutional Block Attention Module (PCBAM), and\nChannel Aggregation Block (CAB). Specifically, MSEDA employs a multi-scale\nfeature fusion attention mechanism to adaptively aggregate information across\ndifferent scales, enriching feature representation. PCBAM captures the\ncorrelation between global and local features through a correlation\nmatrix-based strategy, enabling deep feature interaction. Moreover, CAB\nredistributes input feature channels, facilitating the efficient transmission\nof beneficial features and further enhancing the model detection capability in\ncomplex backgrounds. The experimental results demonstrate that MSCA-Net\nachieves outstanding small target detection performance in complex backgrounds.\nSpecifically, it attains mIoU scores of 78.43\\%, 94.56\\%, and 67.08\\% on the\nNUAA-SIRST, NUDT-SIRST, and IRTSD-1K datasets, respectively, underscoring its\neffectiveness and strong potential for real-world applications.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Xiaojin Lu",
      "Taoran yue",
      "Jiaxi cai",
      "Shibing Chu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17184v1",
    "title": "D2Fusion: Dual-domain Fusion with Feature Superposition for Deepfake Detection",
    "abstract": "Deepfake detection is crucial for curbing the harm it causes to society.\nHowever, current Deepfake detection methods fail to thoroughly explore artifact\ninformation across different domains due to insufficient intrinsic\ninteractions. These interactions refer to the fusion and coordination after\nfeature extraction processes across different domains, which are crucial for\nrecognizing complex forgery clues. Focusing on more generalized Deepfake\ndetection, in this work, we introduce a novel bi-directional attention module\nto capture the local positional information of artifact clues from the spatial\ndomain. This enables accurate artifact localization, thus addressing the coarse\nprocessing with artifact features. To further address the limitation that the\nproposed bi-directional attention module may not well capture global subtle\nforgery information in the artifact feature (e.g., textures or edges), we\nemploy a fine-grained frequency attention module in the frequency domain. By\ndoing so, we can obtain high-frequency information in the fine-grained\nfeatures, which contains the global and subtle forgery information. Although\nthese features from the diverse domains can be effectively and independently\nimproved, fusing them directly does not effectively improve the detection\nperformance. Therefore, we propose a feature superposition strategy that\ncomplements information from spatial and frequency domains. This strategy turns\nthe feature components into the form of wave-like tokens, which are updated\nbased on their phase, such that the distinctions between authentic and artifact\nfeatures can be amplified. Our method demonstrates significant improvements\nover state-of-the-art (SOTA) methods on five public Deepfake datasets in\ncapturing abnormalities across different manipulated operations and real-life.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Xueqi Qiu",
      "Xingyu Miao",
      "Fan Wan",
      "Haoran Duan",
      "Tejal Shah",
      "Varun Ojhab",
      "Yang Longa",
      "Rajiv Ranjan"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17182v1",
    "title": "Radar-Guided Polynomial Fitting for Metric Depth Estimation",
    "abstract": "We propose PolyRad, a novel radar-guided depth estimation method that\nintroduces polynomial fitting to transform scaleless depth predictions from\npretrained monocular depth estimation (MDE) models into metric depth maps.\nUnlike existing approaches that rely on complex architectures or expensive\nsensors, our method is grounded in a simple yet fundamental insight: using\npolynomial coefficients predicted from cheap, ubiquitous radar data to\nadaptively adjust depth predictions non-uniformly across depth ranges. Although\nMDE models often infer reasonably accurate local depth structure within each\nobject or local region, they may misalign these regions relative to one\nanother, making a linear scale-and-shift transformation insufficient given\nthree or more of these regions. In contrast, PolyRad generalizes beyond linear\ntransformations and is able to correct such misalignments by introducing\ninflection points. Importantly, our polynomial fitting framework preserves\nstructural consistency through a novel training objective that enforces\nmonotonicity via first-derivative regularization. PolyRad achieves\nstate-of-the-art performance on the nuScenes, ZJU-4DRadarCam, and View-of-Delft\ndatasets, outperforming existing methods by 30.3% in MAE and 37.2% in RMSE.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Patrick Rim",
      "Hyoungseob Park",
      "Vadim Ezhov",
      "Jeffrey Moon",
      "Alex Wong"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17181v1",
    "title": "LLMs Love Python: A Study of LLMs' Bias for Programming Languages and Libraries",
    "abstract": "Programming language and library choices are crucial to software reliability\nand security. Poor or inconsistent choices can lead to increased technical\ndebt, security vulnerabilities, and even catastrophic failures in\nsafety-critical systems. As Large Language Models (LLMs) play an increasing\nrole in code generation, it is essential to understand how they make these\ndecisions. However, little is known about their preferences when selecting\nprogramming languages and libraries for different coding tasks. To fill this\ngap, this study provides the first in-depth investigation into LLM preferences\nfor programming languages and libraries used when generating code. We assess\nthe preferences of eight diverse LLMs by prompting them to complete various\ncoding tasks, including widely-studied benchmarks and the more practical task\nof generating the initial structural code for new projects (a crucial step that\noften determines a project's language or library choices).\n  Our findings reveal that LLMs heavily favour Python when solving\nlanguage-agnostic problems, using it in 90%-97% of cases for benchmark tasks.\nEven when generating initial project code where Python is not a suitable\nlanguage, it remains the most-used language in 58% of instances. Moreover, LLMs\ncontradict their own language recommendations in 83% of project initialisation\ntasks, raising concerns about their reliability in guiding language selection.\nSimilar biases toward well-established libraries further create serious\ndiscoverability challenges for newer open-source projects. These results\nhighlight the need to improve LLMs' adaptability to diverse programming\ncontexts and to develop mechanisms for mitigating programming language and\nlibrary bias.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Lukas Twist",
      "Jie M. Zhang",
      "Mark Harman",
      "Don Syme",
      "Joost Noppen",
      "Detlef Nauck"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17175v1",
    "title": "Which2comm: An Efficient Collaborative Perception Framework for 3D Object Detection",
    "abstract": "Collaborative perception allows real-time inter-agent information exchange\nand thus offers invaluable opportunities to enhance the perception capabilities\nof individual agents. However, limited communication bandwidth in practical\nscenarios restricts the inter-agent data transmission volume, consequently\nresulting in performance declines in collaborative perception systems. This\nimplies a trade-off between perception performance and communication cost. To\naddress this issue, we propose Which2comm, a novel multi-agent 3D object\ndetection framework leveraging object-level sparse features. By integrating\nsemantic information of objects into 3D object detection boxes, we introduce\nsemantic detection boxes (SemDBs). Innovatively transmitting these\ninformation-rich object-level sparse features among agents not only\nsignificantly reduces the demanding communication volume, but also improves 3D\nobject detection performance. Specifically, a fully sparse network is\nconstructed to extract SemDBs from individual agents; a temporal fusion\napproach with a relative temporal encoding mechanism is utilized to obtain the\ncomprehensive spatiotemporal features. Extensive experiments on the V2XSet and\nOPV2V datasets demonstrate that Which2comm consistently outperforms other\nstate-of-the-art methods on both perception performance and communication cost,\nexhibiting better robustness to real-world latency. These results present that\nfor multi-agent collaborative 3D object detection, transmitting only\nobject-level sparse features is sufficient to achieve high-precision and robust\nperformance.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Duanrui Yu",
      "Jing You",
      "Xin Pei",
      "Anqi Qu",
      "Dingyu Wang",
      "Shaocheng Jia"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17173v1",
    "title": "Robustness of deep learning classification to adversarial input on GPUs: asynchronous parallel accumulation is a source of vulnerability",
    "abstract": "The ability of machine learning (ML) classification models to resist small,\ntargeted input perturbations - known as adversarial attacks - is a key measure\nof their safety and reliability. We show that floating-point non associativity\n(FPNA) coupled with asynchronous parallel programming on GPUs is sufficient to\nresult in misclassification, without any perturbation to the input.\nAdditionally, we show this misclassification is particularly significant for\ninputs close to the decision boundary and that standard adversarial robustness\nresults may be overestimated up to 4.6% when not considering machine-level\ndetails. We first study a linear classifier, before focusing on standard Graph\nNeural Network (GNN) architectures and datasets. We present a novel black-box\nattack using Bayesian optimization to determine external workloads that bias\nthe output of reductions on GPUs and reliably lead to misclassification.\nMotivated by these results, we present a new learnable permutation (LP)\ngradient-based approach, to learn floating point operation orderings that lead\nto misclassifications, making the assumption that any reduction or permutation\nordering is possible. This LP approach provides a worst-case estimate in a\ncomputationally efficient manner, avoiding the need to run identical\nexperiments tens of thousands of times over a potentially large set of possible\nGPU states or architectures. Finally, we investigate parallel reduction\nordering across different GPU architectures for a reduction under three\nconditions: (1) executing external background workloads, (2) utilizing\nmulti-GPU virtualization, and (3) applying power capping. Our results\ndemonstrate that parallel reduction ordering varies significantly across\narchitectures under the first two conditions. The results and methods developed\nhere can help to include machine-level considerations into adversarial\nrobustness assessments.",
    "categories": [
      "cs.LG",
      "cs.DC",
      "I.2.11; B.8.1"
    ],
    "authors": [
      "Sanjif Shanmugavelu",
      "Mathieu Taillefumier",
      "Christopher Culver",
      "Vijay Ganesh",
      "Oscar Hernandez",
      "Ada Sedova"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17172v1",
    "title": "Principal Eigenvalue Regularization for Improved Worst-Class Certified Robustness of Smoothed Classifiers",
    "abstract": "Recent studies have identified a critical challenge in deep neural networks\n(DNNs) known as ``robust fairness\", where models exhibit significant\ndisparities in robust accuracy across different classes. While prior work has\nattempted to address this issue in adversarial robustness, the study of\nworst-class certified robustness for smoothed classifiers remains unexplored.\nOur work bridges this gap by developing a PAC-Bayesian bound for the\nworst-class error of smoothed classifiers. Through theoretical analysis, we\ndemonstrate that the largest eigenvalue of the smoothed confusion matrix\nfundamentally influences the worst-class error of smoothed classifiers. Based\non this insight, we introduce a regularization method that optimizes the\nlargest eigenvalue of smoothed confusion matrix to enhance worst-class accuracy\nof the smoothed classifier and further improve its worst-class certified\nrobustness. We provide extensive experimental validation across multiple\ndatasets and model architectures to demonstrate the effectiveness of our\napproach.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Gaojie Jin",
      "Tianjin Huang",
      "Ronghui Mu",
      "Xiaowei Huang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17171v1",
    "title": "Generative adversarial framework to calibrate excursion set models for the 3D morphology of all-solid-state battery cathodes",
    "abstract": "This paper presents a computational method for generating virtual 3D\nmorphologies of functional materials using low-parametric stochastic geometry\nmodels, i.e., digital twins, calibrated with 2D microscopy images. These\ndigital twins allow systematic parameter variations to simulate various\nmorphologies, that can be deployed for virtual materials testing by means of\nspatially resolved numerical simulations of macroscopic properties. Generative\nadversarial networks (GANs) have gained popularity for calibrating models to\ngenerate realistic 3D morphologies. However, GANs often comprise of numerous\nuninterpretable parameters make systematic variation of morphologies for\nvirtual materials testing challenging. In contrast, low-parametric stochastic\ngeometry models (e.g., based on Gaussian random fields) enable targeted\nvariation but may struggle to mimic complex morphologies. Combining GANs with\nadvanced stochastic geometry models (e.g., excursion sets of more general\nrandom fields) addresses these limitations, allowing model calibration solely\nfrom 2D image data. This approach is demonstrated by generating a digital twin\nof all-solid-state battery (ASSB) cathodes. Since the digital twins are\nparametric, they support systematic exploration of structural scenarios and\ntheir macroscopic properties. The proposed method facilitates simulation\nstudies for optimizing 3D morphologies, benefiting not only ASSB cathodes but\nalso other materials with similar structures.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Orkun Furat",
      "Sabrina Weber",
      "Johannes Schubert",
      "René Rekers",
      "Maximilian Luczak",
      "Erik Glatt",
      "Andreas Wiegmann",
      "Jürgen Janek",
      "Anja Bielefeld",
      "Volker Schmidt"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17168v1",
    "title": "Hi-ALPS -- An Experimental Robustness Quantification of Six LiDAR-based Object Detection Systems for Autonomous Driving",
    "abstract": "Light Detection and Ranging (LiDAR) is an essential sensor technology for\nautonomous driving as it can capture high-resolution 3D data. As 3D object\ndetection systems (OD) can interpret such point cloud data, they play a key\nrole in the driving decisions of autonomous vehicles. Consequently, such 3D OD\nmust be robust against all types of perturbations and must therefore be\nextensively tested. One approach is the use of adversarial examples, which are\nsmall, sometimes sophisticated perturbations in the input data that change,\ni.e., falsify, the prediction of the OD. These perturbations are carefully\ndesigned based on the weaknesses of the OD. The robustness of the OD cannot be\nquantified with adversarial examples in general, because if the OD is\nvulnerable to a given attack, it is unclear whether this is due to the\nrobustness of the OD or whether the attack algorithm produces particularly\nstrong adversarial examples. The contribution of this work is Hi-ALPS --\nHierarchical Adversarial-example-based LiDAR Perturbation Level System, where\nhigher robustness of the OD is required to withstand the perturbations as the\nperturbation levels increase. In doing so, the Hi-ALPS levels successively\nimplement a heuristic followed by established adversarial example approaches.\nIn a series of comprehensive experiments using Hi-ALPS, we quantify the\nrobustness of six state-of-the-art 3D OD under different types of\nperturbations. The results of the experiments show that none of the OD is\nrobust against all Hi-ALPS levels; an important factor for the ranking is that\nhuman observers can still correctly recognize the perturbed objects, as the\nrespective perturbations are small. To increase the robustness of the OD, we\ndiscuss the applicability of state-of-the-art countermeasures. In addition, we\nderive further suggestions for countermeasures based on our experimental\nresults.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Alexandra Arzberger",
      "Ramin Tavakoli Kolagari"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17167v1",
    "title": "DiTEC-WDN: A Large-Scale Dataset of Water Distribution Network Scenarios under Diverse Hydraulic Conditions",
    "abstract": "Privacy restrictions hinder the sharing of real-world Water Distribution\nNetwork (WDN) models, limiting the application of emerging data-driven machine\nlearning, which typically requires extensive observations. To address this\nchallenge, we propose the dataset DiTEC-WDN that comprises 36,000 unique\nscenarios simulated over either short-term (24 hours) or long-term (1 year)\nperiods. We constructed this dataset using an automated pipeline that optimizes\ncrucial parameters (e.g., pressure, flow rate, and demand patterns),\nfacilitates large-scale simulations, and records discrete, synthetic but\nhydraulically realistic states under standard conditions via rule validation\nand post-hoc analysis. With a total of 228 million generated graph-based\nstates, DiTEC-WDN can support a variety of machine-learning tasks, including\ngraph-level, node-level, and link-level regression, as well as time-series\nforecasting. This contribution, released under a public license, encourages\nopen scientific research in the critical water sector, eliminates the risk of\nexposing sensitive data, and fulfills the need for a large-scale water\ndistribution network benchmark for study comparisons and scenario analysis.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Huy Truong",
      "Andrés Tello",
      "Alexander Lazovik",
      "Victoria Degeler"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17162v1",
    "title": "CoRLD: Contrastive Representation Learning Of Deformable Shapes In Images",
    "abstract": "Deformable shape representations, parameterized by deformations relative to a\ngiven template, have proven effective for improved image analysis tasks.\nHowever, their broader applicability is hindered by two major challenges.\nFirst, existing methods mainly rely on a known template during testing, which\nis impractical and limits flexibility. Second, they often struggle to capture\nfine-grained, voxel-level distinctions between similar shapes (e.g., anatomical\nvariations among healthy individuals, those with mild cognitive impairment, and\ndiseased states). To address these limitations, we propose a novel framework -\nContrastive Representation Learning of Deformable shapes (CoRLD) in learned\ndeformation spaces and demonstrate its effectiveness in the context of image\nclassification. Our CoRLD leverages a class-aware contrastive supervised\nlearning objective in latent deformation spaces, promoting proximity among\nrepresentations of similar classes while ensuring separation of dissimilar\ngroups. In contrast to previous deep learning networks that require a reference\nimage as input to predict deformation changes, our approach eliminates this\ndependency. Instead, template images are utilized solely as ground truth in the\nloss function during the training process, making our model more flexible and\ngeneralizable to a wide range of medical applications. We validate CoRLD on\ndiverse datasets, including real brain magnetic resonance imaging (MRIs) and\nadrenal shapes derived from computed tomography (CT) scans. Experimental\nresults show that our model effectively extracts deformable shape features,\nwhich can be easily integrated with existing classifiers to substantially boost\nthe classification accuracy. Our code is available at GitHub.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Tonmoy Hossain ana Miaomiao Zhang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17155v1",
    "title": "D2C: Unlocking the Potential of Continuous Autoregressive Image Generation with Discrete Tokens",
    "abstract": "In the domain of image generation, latent-based generative models occupy a\ndominant status; however, these models rely heavily on image tokenizer. To meet\nmodeling requirements, autoregressive models possessing the characteristics of\nscalability and flexibility embrace a discrete-valued tokenizer, but face the\nchallenge of poor image generation quality. In contrast, diffusion models take\nadvantage of the continuous-valued tokenizer to achieve better generation\nquality but are subject to low efficiency and complexity. The existing hybrid\nmodels are mainly to compensate for information loss and simplify the diffusion\nlearning process. The potential of merging discrete-valued and\ncontinuous-valued tokens in the field of image generation has not yet been\nexplored. In this paper, we propose D2C, a novel two-stage method to enhance\nmodel generation capacity. In the first stage, the discrete-valued tokens\nrepresenting coarse-grained image features are sampled by employing a small\ndiscrete-valued generator. Then in the second stage, the continuous-valued\ntokens representing fine-grained image features are learned conditioned on the\ndiscrete token sequence. In addition, we design two kinds of fusion modules for\nseamless interaction. On the ImageNet-256 benchmark, extensive experiment\nresults validate that our model achieves superior performance compared with\nseveral continuous-valued and discrete-valued generative models on the\nclass-conditional image generation tasks.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Panpan Wang",
      "Liqiang Niu",
      "Fandong Meng",
      "Jinan Xu",
      "Yufeng Chen",
      "Jie Zhou"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17153v1",
    "title": "Enhancing Steering Estimation with Semantic-Aware GNNs",
    "abstract": "Steering estimation is a critical task in autonomous driving, traditionally\nrelying on 2D image-based models. In this work, we explore the advantages of\nincorporating 3D spatial information through hybrid architectures that combine\n3D neural network models with recurrent neural networks (RNNs) for temporal\nmodeling, using LiDAR-based point clouds as input. We systematically evaluate\nfour hybrid 3D models, all of which outperform the 2D-only baseline, with the\nGraph Neural Network (GNN) - RNN model yielding the best results.\n  To reduce reliance on LiDAR, we leverage a pretrained unified model to\nestimate depth from monocular images, reconstructing pseudo-3D point clouds. We\nthen adapt the GNN-RNN model, originally designed for LiDAR-based point clouds,\nto work with these pseudo-3D representations, achieving comparable or even\nsuperior performance compared to the LiDAR-based model. Additionally, the\nunified model provides semantic labels for each point, enabling a more\nstructured scene representation. To further optimize graph construction, we\nintroduce an efficient connectivity strategy where connections are\npredominantly formed between points of the same semantic class, with only 20\\%\nof inter-class connections retained. This targeted approach reduces graph\ncomplexity and computational cost while preserving critical spatial\nrelationships.\n  Finally, we validate our approach on the KITTI dataset, achieving a 71%\nimprovement over 2D-only models. Our findings highlight the advantages of 3D\nspatial information and efficient graph construction for steering estimation,\nwhile maintaining the cost-effectiveness of monocular images and avoiding the\nexpense of LiDAR-based systems.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Fouad Makiyeh",
      "Huy-Dung Nguyen",
      "Patrick Chareyre",
      "Ramin Hasani",
      "Marc Blanchon",
      "Daniela Rus"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17142v1",
    "title": "Not Only Text: Exploring Compositionality of Visual Representations in Vision-Language Models",
    "abstract": "Vision-Language Models (VLMs) learn a shared feature space for text and\nimages, enabling the comparison of inputs of different modalities. While prior\nworks demonstrated that VLMs organize natural language representations into\nregular structures encoding composite meanings, it remains unclear if\ncompositional patterns also emerge in the visual embedding space. In this work,\nwe investigate compositionality in the image domain, where the analysis of\ncompositional properties is challenged by noise and sparsity of visual data. We\naddress these problems and propose a framework, called Geodesically\nDecomposable Embeddings (GDE), that approximates image representations with\ngeometry-aware compositional structures in the latent space. We demonstrate\nthat visual embeddings of pre-trained VLMs exhibit a compositional arrangement,\nand evaluate the effectiveness of this property in the tasks of compositional\nclassification and group robustness. GDE achieves stronger performance in\ncompositional classification compared to its counterpart method that assumes\nlinear geometry of the latent space. Notably, it is particularly effective for\ngroup robustness, where we achieve higher results than task-specific solutions.\nOur results indicate that VLMs can automatically develop a human-like form of\ncompositional reasoning in the visual domain, making their underlying processes\nmore interpretable. Code is available at\nhttps://github.com/BerasiDavide/vlm_image_compositionality.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Davide Berasi",
      "Matteo Farina",
      "Massimiliano Mancini",
      "Elisa Ricci",
      "Nicola Strisciuglio"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17141v1",
    "title": "HiFi-Stream: Streaming Speech Enhancement with Generative Adversarial Networks",
    "abstract": "Speech Enhancement techniques have become core technologies in mobile devices\nand voice software simplifying downstream speech tasks. Still, modern Deep\nLearning (DL) solutions often require high amount of computational resources\nwhat makes their usage on low-resource devices challenging. We present\nHiFi-Stream, an optimized version of recently published HiFi++ model. Our\nexperiments demonstrate that HiFiStream saves most of the qualities of the\noriginal model despite its size and computational complexity: the lightest\nversion has only around 490k parameters which is 3.5x reduction in comparison\nto the original HiFi++ making it one of the smallest and fastest models\navailable. The model is evaluated in streaming setting where it demonstrates\nits superior performance in comparison to modern baselines.",
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "authors": [
      "Ekaterina Dmitrieva",
      "Maksim Kaledin"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17140v1",
    "title": "Adiabatic Fine-Tuning of Neural Quantum States Enables Detection of Phase Transitions in Weight Space",
    "abstract": "Neural quantum states (NQS) have emerged as a powerful tool for approximating\nquantum wavefunctions using deep learning. While these models achieve\nremarkable accuracy, understanding how they encode physical information remains\nan open challenge. In this work, we introduce adiabatic fine-tuning, a scheme\nthat trains NQS across a phase diagram, leading to strongly correlated weight\nrepresentations across different models. This correlation in weight space\nenables the detection of phase transitions in quantum systems by analyzing the\ntrained network weights alone. We validate our approach on the transverse field\nIsing model and the J1-J2 Heisenberg model, demonstrating that phase\ntransitions manifest as distinct structures in weight space. Our results\nestablish a connection between physical phase transitions and the geometry of\nneural network parameters, opening new directions for the interpretability of\nmachine learning models in physics.",
    "categories": [
      "quant-ph",
      "cs.LG"
    ],
    "authors": [
      "Vinicius Hernandes",
      "Thomas Spriggs",
      "Saqar Khaleefah",
      "Eliska Greplova"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17138v1",
    "title": "Structure Is Not Enough: Leveraging Behavior for Neural Network Weight Reconstruction",
    "abstract": "The weights of neural networks (NNs) have recently gained prominence as a new\ndata modality in machine learning, with applications ranging from accuracy and\nhyperparameter prediction to representation learning or weight generation. One\napproach to leverage NN weights involves training autoencoders (AEs), using\ncontrastive and reconstruction losses. This allows such models to be applied to\na wide variety of downstream tasks, and they demonstrate strong predictive\nperformance and low reconstruction error. However, despite the low\nreconstruction error, these AEs reconstruct NN models with deteriorated\nperformance compared to the original ones, limiting their usability with regard\nto model weight generation. In this paper, we identify a limitation of\nweight-space AEs, specifically highlighting that a structural loss, that uses\nthe Euclidean distance between original and reconstructed weights, fails to\ncapture some features critical for reconstructing high-performing models. We\nanalyze the addition of a behavioral loss for training AEs in weight space,\nwhere we compare the output of the reconstructed model with that of the\noriginal one, given some common input. We show a strong synergy between\nstructural and behavioral signals, leading to increased performance in all\ndownstream tasks evaluated, in particular NN weights reconstruction and\ngeneration.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Léo Meynent",
      "Ivan Melev",
      "Konstantin Schürholt",
      "Göran Kauermann",
      "Damian Borth"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17136v1",
    "title": "CoKe: Customizable Fine-Grained Story Evaluation via Chain-of-Keyword Rationalization",
    "abstract": "Evaluating creative text such as human-written stories using language models\nhas always been a challenging task -- owing to the subjectivity of\nmulti-annotator ratings. To mimic the thinking process of humans, chain of\nthought (CoT) generates free-text explanations that help guide a model's\npredictions and Self-Consistency (SC) marginalizes predictions over multiple\ngenerated explanations. In this study, we discover that the widely-used\nself-consistency reasoning methods cause suboptimal results due to an objective\nmismatch between generating 'fluent-looking' explanations vs. actually leading\nto a good rating prediction for an aspect of a story. To overcome this\nchallenge, we propose $\\textbf{C}$hain-$\\textbf{o}$f-$\\textbf{Ke}$ywords\n(CoKe), that generates a sequence of keywords $\\textit{before}$ generating a\nfree-text rationale, that guide the rating prediction of our evaluation\nlanguage model. Then, we generate a diverse set of such keywords, and aggregate\nthe scores corresponding to these generations. On the StoryER dataset, CoKe\nbased on our small fine-tuned evaluation models not only reach human-level\nperformance and significantly outperform GPT-4 with a 2x boost in correlation\nwith human annotators, but also requires drastically less number of parameters.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Brihi Joshi",
      "Sriram Venkatapathy",
      "Mohit Bansal",
      "Nanyun Peng",
      "Haw-Shiuan Chang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17132v1",
    "title": "Temporal-Guided Spiking Neural Networks for Event-Based Human Action Recognition",
    "abstract": "This paper explores the promising interplay between spiking neural networks\n(SNNs) and event-based cameras for privacy-preserving human action recognition\n(HAR). The unique feature of event cameras in capturing only the outlines of\nmotion, combined with SNNs' proficiency in processing spatiotemporal data\nthrough spikes, establishes a highly synergistic compatibility for event-based\nHAR. Previous studies, however, have been limited by SNNs' ability to process\nlong-term temporal information, essential for precise HAR. In this paper, we\nintroduce two novel frameworks to address this: temporal segment-based SNN\n(\\textit{TS-SNN}) and 3D convolutional SNN (\\textit{3D-SNN}). The\n\\textit{TS-SNN} extracts long-term temporal information by dividing actions\ninto shorter segments, while the \\textit{3D-SNN} replaces 2D spatial elements\nwith 3D components to facilitate the transmission of temporal information. To\npromote further research in event-based HAR, we create a dataset,\n\\textit{FallingDetection-CeleX}, collected using the high-resolution CeleX-V\nevent camera $(1280 \\times 800)$, comprising 7 distinct actions. Extensive\nexperimental results show that our proposed frameworks surpass state-of-the-art\nSNN methods on our newly collected dataset and three other neuromorphic\ndatasets, showcasing their effectiveness in handling long-range temporal\ninformation for event-based HAR.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.NE"
    ],
    "authors": [
      "Siyuan Yang",
      "Shilin Lu",
      "Shizheng Wang",
      "Meng Hwa Er",
      "Zengwei Zheng",
      "Alex C. Kot"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17126v1",
    "title": "Modifying Large Language Model Post-Training for Diverse Creative Writing",
    "abstract": "As creative writing tasks do not have singular correct answers, large\nlanguage models (LLMs) trained to perform these tasks should be able to\ngenerate diverse valid outputs. However, LLM post-training often focuses on\nimproving generation quality but neglects to facilitate output diversity.\nHence, in creative writing generation, we investigate post-training approaches\nto promote both output diversity and quality. Our core idea is to include\ndeviation -- the degree of difference between a training sample and all other\nsamples with the same prompt -- in the training objective to facilitate\nlearning from rare high-quality instances. By adopting our approach to direct\npreference optimization (DPO) and odds ratio preference optimization (ORPO), we\ndemonstrate that we can promote the output diversity of trained models while\nminimally decreasing quality. Our best model with 8B parameters could achieve\non-par diversity as a human-created dataset while having output quality similar\nto the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We\nfurther validate our approaches with a human evaluation, an ablation, and a\ncomparison to an existing diversification approach, DivPO.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "John Joon Young Chung",
      "Vishakh Padmakumar",
      "Melissa Roemmele",
      "Yuqian Sun",
      "Max Kreminski"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17125v1",
    "title": "Leveraging Language Models for Out-of-Distribution Recovery in Reinforcement Learning",
    "abstract": "Deep Reinforcement Learning (DRL) has demonstrated strong performance in\nrobotic control but remains susceptible to out-of-distribution (OOD) states,\noften resulting in unreliable actions and task failure. While previous methods\nhave focused on minimizing or preventing OOD occurrences, they largely neglect\nrecovery once an agent encounters such states. Although the latest research has\nattempted to address this by guiding agents back to in-distribution states,\ntheir reliance on uncertainty estimation hinders scalability in complex\nenvironments. To overcome this limitation, we introduce Language Models for\nOut-of-Distribution Recovery (LaMOuR), which enables recovery learning without\nrelying on uncertainty estimation. LaMOuR generates dense reward codes that\nguide the agent back to a state where it can successfully perform its original\ntask, leveraging the capabilities of LVLMs in image description, logical\nreasoning, and code generation. Experimental results show that LaMOuR\nsubstantially enhances recovery efficiency across diverse locomotion tasks and\neven generalizes effectively to complex environments, including humanoid\nlocomotion and mobile manipulation, where existing methods struggle. The code\nand supplementary materials are available at\n\\href{https://lamour-rl.github.io/}{https://lamour-rl.github.io/}.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Chan Kim",
      "Seung-Woo Seo",
      "Seong-Woo Kim"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17122v1",
    "title": "R-LiViT: A LiDAR-Visual-Thermal Dataset Enabling Vulnerable Road User Focused Roadside Perception",
    "abstract": "In autonomous driving, the integration of roadside perception systems is\nessential for overcoming occlusion challenges and enhancing the safety of\nVulnerable Road Users (VRUs). While LiDAR and visual (RGB) sensors are commonly\nused, thermal imaging remains underrepresented in datasets, despite its\nacknowledged advantages for VRU detection in extreme lighting conditions. In\nthis paper, we present R-LiViT, the first dataset to combine LiDAR, RGB, and\nthermal imaging from a roadside perspective, with a strong focus on VRUs.\nR-LiViT captures three intersections during both day and night, ensuring a\ndiverse dataset. It includes 10,000 LiDAR frames and 2,400 temporally and\nspatially aligned RGB and thermal images across over 150 traffic scenarios,\nwith 6 and 8 annotated classes respectively, providing a comprehensive resource\nfor tasks such as object detection and tracking. The dataset1 and the code for\nreproducing our evaluation results2 are made publicly available.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jonas Mirlach",
      "Lei Wan",
      "Andreas Wiedholz",
      "Hannan Ejaz Keen",
      "Andreas Eich"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17117v1",
    "title": "A New Statistical Model of Star Speckles for Learning to Detect and Characterize Exoplanets in Direct Imaging Observations",
    "abstract": "The search for exoplanets is an active field in astronomy, with direct\nimaging as one of the most challenging methods due to faint exoplanet signals\nburied within stronger residual starlight. Successful detection requires\nadvanced image processing to separate the exoplanet signal from this nuisance\ncomponent. This paper presents a novel statistical model that captures nuisance\nfluctuations using a multi-scale approach, leveraging problem symmetries and a\njoint spectral channel representation grounded in physical principles. Our\nmodel integrates into an interpretable, end-to-end learnable framework for\nsimultaneous exoplanet detection and flux estimation. The proposed algorithm is\nevaluated against the state of the art using datasets from the SPHERE\ninstrument operating at the Very Large Telescope (VLT). It significantly\nimproves the precision-recall trade-off, notably on challenging datasets that\nare otherwise unusable by astronomers. The proposed approach is computationally\nefficient, robust to varying data quality, and well suited for large-scale\nobservational surveys.",
    "categories": [
      "astro-ph.IM",
      "astro-ph.EP",
      "cs.CV",
      "cs.LG",
      "stat.AP"
    ],
    "authors": [
      "Théo Bodrito",
      "Olivier Flasseur",
      "Julien Mairal",
      "Jean Ponce",
      "Maud Langlois",
      "Anne-Marie Lagrange"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17116v1",
    "title": "The CASTLE 2024 Dataset: Advancing the Art of Multimodal Understanding",
    "abstract": "Egocentric video has seen increased interest in recent years, as it is used\nin a range of areas. However, most existing datasets are limited to a single\nperspective. In this paper, we present the CASTLE 2024 dataset, a multimodal\ncollection containing ego- and exo-centric (i.e., first- and third-person\nperspective) video and audio from 15 time-aligned sources, as well as other\nsensor streams and auxiliary data. The dataset was recorded by volunteer\nparticipants over four days in a fixed location and includes the point of view\nof 10 participants, with an additional 5 fixed cameras providing an exocentric\nperspective. The entire dataset contains over 600 hours of UHD video recorded\nat 50 frames per second. In contrast to other datasets, CASTLE 2024 does not\ncontain any partial censoring, such as blurred faces or distorted audio. The\ndataset is available via https://castle-dataset.github.io/.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV",
      "cs.IR"
    ],
    "authors": [
      "Luca Rossetto",
      "Werner Bailer",
      "Duc-Tien Dang-Nguyen",
      "Graham Healy",
      "Björn Þór Jónsson",
      "Onanong Kongmeesub",
      "Hoang-Bao Le",
      "Stevan Rudinac",
      "Klaus Schöffmann",
      "Florian Spiess",
      "Allie Tran",
      "Minh-Triet Tran",
      "Quang-Linh Tran",
      "Cathal Gurrin"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17110v1",
    "title": "Beyond Accuracy: What Matters in Designing Well-Behaved Models?",
    "abstract": "Deep learning has become an essential part of computer vision, with deep\nneural networks (DNNs) excelling in predictive performance. However, they often\nfall short in other critical quality dimensions, such as robustness,\ncalibration, or fairness. While existing studies have focused on a subset of\nthese quality dimensions, none have explored a more general form of\n\"well-behavedness\" of DNNs. With this work, we address this gap by\nsimultaneously studying nine different quality dimensions for image\nclassification. Through a large-scale study, we provide a bird's-eye view by\nanalyzing 326 backbone models and how different training paradigms and model\narchitectures affect the quality dimensions. We reveal various new insights\nsuch that (i) vision-language models exhibit high fairness on ImageNet-1k\nclassification and strong robustness against domain changes; (ii)\nself-supervised learning is an effective training paradigm to improve almost\nall considered quality dimensions; and (iii) the training dataset size is a\nmajor driver for most of the quality dimensions. We conclude our study by\nintroducing the QUBA score (Quality Understanding Beyond Accuracy), a novel\nmetric that ranks models across multiple dimensions of quality, enabling\ntailored recommendations based on specific user needs.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Robin Hesse",
      "Doğukan Bağcı",
      "Bernt Schiele",
      "Simone Schaub-Meyer",
      "Stefan Roth"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17109v1",
    "title": "Missing Target-Relevant Information Prediction with World Model for Accurate Zero-Shot Composed Image Retrieval",
    "abstract": "Zero-Shot Composed Image Retrieval (ZS-CIR) involves diverse tasks with a\nbroad range of visual content manipulation intent across domain, scene, object,\nand attribute. The key challenge for ZS-CIR tasks is to modify a reference\nimage according to manipulation text to accurately retrieve a target image,\nespecially when the reference image is missing essential target content. In\nthis paper, we propose a novel prediction-based mapping network, named\nPrediCIR, to adaptively predict the missing target visual content in reference\nimages in the latent space before mapping for accurate ZS-CIR. Specifically, a\nworld view generation module first constructs a source view by omitting certain\nvisual content of a target view, coupled with an action that includes the\nmanipulation intent derived from existing image-caption pairs. Then, a target\ncontent prediction module trains a world model as a predictor to adaptively\npredict the missing visual information guided by user intention in manipulating\ntext at the latent space. The two modules map an image with the predicted\nrelevant information to a pseudo-word token without extra supervision. Our\nmodel shows strong generalization ability on six ZS-CIR tasks. It obtains\nconsistent and significant performance boosts ranging from 1.73% to 4.45% over\nthe best methods and achieves new state-of-the-art results on ZS-CIR. Our code\nis available at https://github.com/Pter61/predicir.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yuanmin Tang",
      "Jing Yu",
      "Keke Gai",
      "Jiamin Zhuang",
      "Gang Xiong",
      "Gaopeng Gou",
      "Qi Wu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17107v1",
    "title": "Exploring Few-Shot Object Detection on Blood Smear Images: A Case Study of Leukocytes and Schistocytes",
    "abstract": "The detection of blood disorders often hinges upon the quantification of\nspecific blood cell types. Variations in cell counts may indicate the presence\nof pathological conditions. Thus, the significance of developing precise\nautomatic systems for blood cell enumeration is underscored. The investigation\nfocuses on a novel approach termed DE-ViT. This methodology is employed in a\nFew-Shot paradigm, wherein training relies on a limited number of images. Two\ndistinct datasets are utilised for experimental purposes: the Raabin-WBC\ndataset for Leukocyte detection and a local dataset for Schistocyte\nidentification. In addition to the DE-ViT model, two baseline models, Faster\nR-CNN 50 and Faster R-CNN X 101, are employed, with their outcomes being\ncompared against those of the proposed model. While DE-ViT has demonstrated\nstate-of-the-art performance on the COCO and LVIS datasets, both baseline\nmodels surpassed its performance on the Raabin-WBC dataset. Moreover, only\nFaster R-CNN X 101 yielded satisfactory results on the SC-IDB. The observed\ndisparities in performance may possibly be attributed to domain shift\nphenomena.",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Davide Antonio Mura",
      "Michela Pinna",
      "Lorenzo Putzu",
      "Andrea Loddo",
      "Alessandra Perniciano",
      "Olga Mulas",
      "Cecilia Di Ruberto"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17106v1",
    "title": "GAA-TSO: Geometry-Aware Assisted Depth Completion for Transparent and Specular Objects",
    "abstract": "Transparent and specular objects are frequently encountered in daily life,\nfactories, and laboratories. However, due to the unique optical properties, the\ndepth information on these objects is usually incomplete and inaccurate, which\nposes significant challenges for downstream robotics tasks. Therefore, it is\ncrucial to accurately restore the depth information of transparent and specular\nobjects. Previous depth completion methods for these objects usually use RGB\ninformation as an additional channel of the depth image to perform depth\nprediction. Due to the poor-texture characteristics of transparent and specular\nobjects, these methods that rely heavily on color information tend to generate\nstructure-less depth predictions. Moreover, these 2D methods cannot effectively\nexplore the 3D structure hidden in the depth channel, resulting in depth\nambiguity. To this end, we propose a geometry-aware assisted depth completion\nmethod for transparent and specular objects, which focuses on exploring the 3D\nstructural cues of the scene. Specifically, besides extracting 2D features from\nRGB-D input, we back-project the input depth to a point cloud and build the 3D\nbranch to extract hierarchical scene-level 3D structural features. To exploit\n3D geometric information, we design several gated cross-modal fusion modules to\neffectively propagate multi-level 3D geometric features to the image branch. In\naddition, we propose an adaptive correlation aggregation strategy to\nappropriately assign 3D features to the corresponding 2D features. Extensive\nexperiments on ClearGrasp, OOD, TransCG, and STD datasets show that our method\noutperforms other state-of-the-art methods. We further demonstrate that our\nmethod significantly enhances the performance of downstream robotic grasping\ntasks.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Yizhe Liu",
      "Tong Jia",
      "Da Cai",
      "Hao Wang",
      "Dongyue Chen"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17105v1",
    "title": "A Comparative Analysis of Image Descriptors for Histopathological Classification of Gastric Cancer",
    "abstract": "Gastric cancer ranks as the fifth most common and fourth most lethal cancer\nglobally, with a dismal 5-year survival rate of approximately 20%. Despite\nextensive research on its pathobiology, the prognostic predictability remains\ninadequate, compounded by pathologists' high workload and potential diagnostic\nerrors. Thus, automated, accurate histopathological diagnosis tools are\ncrucial. This study employs Machine Learning and Deep Learning techniques to\nclassify histopathological images into healthy and cancerous categories. Using\nhandcrafted and deep features with shallow learning classifiers on the\nGasHisSDB dataset, we offer a comparative analysis and insights into the most\nrobust and high-performing combinations of features and classifiers for\ndistinguishing between normal and abnormal histopathological images without\nfine-tuning strategies. With the RF classifier, our approach can reach F1 of\n93.4%, demonstrating its validity.",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Marco Usai",
      "Andrea Loddo",
      "Alessandra Perniciano",
      "Maurizio Atzori",
      "Cecilia Di Ruberto"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17101v1",
    "title": "Large Language Model Compression via the Nested Activation-Aware Decomposition",
    "abstract": "In this paper, we tackle the critical challenge of compressing large language\nmodels (LLMs) to facilitate their practical deployment and broader adoption. We\nintroduce a novel post-training compression paradigm that focuses on low-rank\ndecomposition of LLM weights. Our analysis identifies two main challenges in\nthis task: the variability in LLM activation distributions and handling unseen\nactivations from different datasets and models.\n  To address these challenges, we propose a nested activation-aware framework\n(NSVD) for LLMs, a training-free approach designed to enhance the accuracy of\nlow-rank decompositions by managing activation outliers through transforming\nthe weight matrix based on activation distribution and the original weight\nmatrix. This method allows for the absorption of outliers into the transformed\nweight matrix, improving decomposition accuracy. Our comprehensive evaluation\nacross eight datasets and six models from three distinct LLM families\ndemonstrates the superiority of NSVD over current state-of-the-art methods,\nespecially at medium to large compression ratios or in multilingual and\nmultitask settings.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Jun Lu",
      "Tianyi Xu",
      "Bill Ding",
      "David Li",
      "Yu Kang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17097v1",
    "title": "R2LDM: An Efficient 4D Radar Super-Resolution Framework Leveraging Diffusion Model",
    "abstract": "We introduce R2LDM, an innovative approach for generating dense and accurate\n4D radar point clouds, guided by corresponding LiDAR point clouds. Instead of\nutilizing range images or bird's eye view (BEV) images, we represent both LiDAR\nand 4D radar point clouds using voxel features, which more effectively capture\n3D shape information. Subsequently, we propose the Latent Voxel Diffusion Model\n(LVDM), which performs the diffusion process in the latent space. Additionally,\na novel Latent Point Cloud Reconstruction (LPCR) module is utilized to\nreconstruct point clouds from high-dimensional latent voxel features. As a\nresult, R2LDM effectively generates LiDAR-like point clouds from paired raw\nradar data. We evaluate our approach on two different datasets, and the\nexperimental results demonstrate that our model achieves 6- to 10-fold\ndensification of radar point clouds, outperforming state-of-the-art baselines\nin 4D radar point cloud super-resolution. Furthermore, the enhanced radar point\nclouds generated by our method significantly improve downstream tasks,\nachieving up to 31.7% improvement in point cloud registration recall rate and\n24.9% improvement in object detection accuracy.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Boyuan Zheng",
      "Shouyi Lu",
      "Renbo Huang",
      "Minqing Huang",
      "Fan Lu",
      "Wei Tian",
      "Guirong Zhuo",
      "Lu Xiong"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17096v1",
    "title": "Multi-modal Multi-platform Person Re-Identification: Benchmark and Method",
    "abstract": "Conventional person re-identification (ReID) research is often limited to\nsingle-modality sensor data from static cameras, which fails to address the\ncomplexities of real-world scenarios where multi-modal signals are increasingly\nprevalent. For instance, consider an urban ReID system integrating stationary\nRGB cameras, nighttime infrared sensors, and UAVs equipped with dynamic\ntracking capabilities. Such systems face significant challenges due to\nvariations in camera perspectives, lighting conditions, and sensor modalities,\nhindering effective person ReID. To address these challenges, we introduce the\nMP-ReID benchmark, a novel dataset designed specifically for multi-modality and\nmulti-platform ReID. This benchmark uniquely compiles data from 1,930\nidentities across diverse modalities, including RGB, infrared, and thermal\nimaging, captured by both UAVs and ground-based cameras in indoor and outdoor\nenvironments. Building on this benchmark, we introduce Uni-Prompt ReID, a\nframework with specific-designed prompts, tailored for cross-modality and\ncross-platform scenarios. Our method consistently outperforms state-of-the-art\napproaches, establishing a robust foundation for future research in complex and\ndynamic ReID environments. Our dataset are available\nat:https://mp-reid.github.io/.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Ruiyang Ha",
      "Songyi Jiang",
      "Bin Li",
      "Bikang Pan",
      "Yihang Zhu",
      "Junjie Zhang",
      "Xiatian Zhu",
      "Shaogang Gong",
      "Jingya Wang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17095v1",
    "title": "FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields",
    "abstract": "Recent 3D face editing methods using masks have produced high-quality edited\nimages by leveraging Neural Radiance Fields (NeRF). Despite their impressive\nperformance, existing methods often provide limited user control due to the use\nof pre-trained segmentation masks. To utilize masks with a desired layout, an\nextensive training dataset is required, which is challenging to gather. We\npresent FFaceNeRF, a NeRF-based face editing technique that can overcome the\nchallenge of limited user control due to the use of fixed mask layouts. Our\nmethod employs a geometry adapter with feature injection, allowing for\neffective manipulation of geometry attributes. Additionally, we adopt latent\nmixing for tri-plane augmentation, which enables training with a few samples.\nThis facilitates rapid model adaptation to desired mask layouts, crucial for\napplications in fields like personalized medical imaging or creative face\nediting. Our comparative evaluations demonstrate that FFaceNeRF surpasses\nexisting mask based face editing methods in terms of flexibility, control, and\ngenerated image quality, paving the way for future advancements in customized\nand high-fidelity 3D face editing. The code is available on the\n{\\href{https://kwanyun.github.io/FFaceNeRF_page/}{project-page}}.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "68T45, 68U05",
      "I.3.3; I.3.8"
    ],
    "authors": [
      "Kwan Yun",
      "Chaelin Kim",
      "Hangyeul Shin",
      "Junyong Noh"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17093v1",
    "title": "ColabSfM: Collaborative Structure-from-Motion by Point Cloud Registration",
    "abstract": "Structure-from-Motion (SfM) is the task of estimating 3D structure and camera\nposes from images. We define Collaborative SfM (ColabSfM) as sharing\ndistributed SfM reconstructions. Sharing maps requires estimating a joint\nreference frame, which is typically referred to as registration. However, there\nis a lack of scalable methods and training datasets for registering SfM\nreconstructions. In this paper, we tackle this challenge by proposing the\nscalable task of point cloud registration for SfM reconstructions. We find that\ncurrent registration methods cannot register SfM point clouds when trained on\nexisting datasets. To this end, we propose a SfM registration dataset\ngeneration pipeline, leveraging partial reconstructions from synthetically\ngenerated camera trajectories for each scene. Finally, we propose a simple but\nimpactful neural refiner on top of the SotA registration method RoITr that\nyields significant improvements, which we call RefineRoITr. Our extensive\nexperimental evaluation shows that our proposed pipeline and model enables\nColabSfM. Code is available at https://github.com/EricssonResearch/ColabSfM",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Johan Edstedt",
      "André Mateus",
      "Alberto Jaenal"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17089v1",
    "title": "Does a Rising Tide Lift All Boats? Bias Mitigation for AI-based CMR Segmentation",
    "abstract": "Artificial intelligence (AI) is increasingly being used for medical imaging\ntasks. However, there can be biases in the resulting models, particularly when\nthey were trained using imbalanced training datasets. One such example has been\nthe strong race bias effect in cardiac magnetic resonance (CMR) image\nsegmentation models. Although this phenomenon has been reported in a number of\npublications, little is known about the effectiveness of bias mitigation\nalgorithms in this domain. We aim to investigate the impact of common bias\nmitigation methods to address bias between Black and White subjects in AI-based\nCMR segmentation models. Specifically, we use oversampling, importance\nreweighing and Group DRO as well as combinations of these techniques to\nmitigate the race bias. Furthermore, motivated by recent findings on the root\ncauses of AI-based CMR segmentation bias, we evaluate the same methods using\nmodels trained and evaluated on cropped CMR images. We find that bias can be\nmitigated using oversampling, significantly improving performance for the\nunderrepresented Black subjects whilst not significantly reducing the majority\nWhite subjects' performance. Group DRO also improves performance for Black\nsubjects but not significantly, while reweighing decreases performance for\nBlack subjects. Using a combination of oversampling and Group DRO also improves\nperformance for Black subjects but not significantly. Using cropped images\nincreases performance for both races and reduces the bias, whilst adding\noversampling as a bias mitigation technique with cropped images reduces the\nbias further.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Tiarna Lee",
      "Esther Puyol-Antón",
      "Bram Ruijsink",
      "Miaojing Shi",
      "Andrew P. King"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17085v1",
    "title": "Deterministic AI Agent Personality Expression through Standard Psychological Diagnostics",
    "abstract": "Artificial intelligence (AI) systems powered by large language models have\nbecome increasingly prevalent in modern society, enabling a wide range of\napplications through natural language interaction. As AI agents proliferate in\nour daily lives, their generic and uniform expressiveness presents a\nsignificant limitation to their appeal and adoption. Personality expression\nrepresents a key prerequisite for creating more human-like and distinctive AI\nsystems. We show that AI models can express deterministic and consistent\npersonalities when instructed using established psychological frameworks, with\nvarying degrees of accuracy depending on model capabilities. We find that more\nadvanced models like GPT-4o and o1 demonstrate the highest accuracy in\nexpressing specified personalities across both Big Five and Myers-Briggs\nassessments, and further analysis suggests that personality expression emerges\nfrom a combination of intelligence and reasoning capabilities. Our results\nreveal that personality expression operates through holistic reasoning rather\nthan question-by-question optimization, with response-scale metrics showing\nhigher variance than test-scale metrics. Furthermore, we find that model\nfine-tuning affects communication style independently of personality expression\naccuracy. These findings establish a foundation for creating AI agents with\ndiverse and consistent personalities, which could significantly enhance\nhuman-AI interaction across applications from education to healthcare, while\nadditionally enabling a broader range of more unique AI agents. The ability to\nquantitatively assess and implement personality expression in AI systems opens\nnew avenues for research into more relatable, trustworthy, and ethically\ndesigned AI.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "authors": [
      "J. M. Diederik Kruijssen",
      "Nicholas Emmons"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17080v1",
    "title": "Seeing What Matters: Empowering CLIP with Patch Generation-to-Selection",
    "abstract": "The CLIP model has demonstrated significant advancements in aligning visual\nand language modalities through large-scale pre-training on image-text pairs,\nenabling strong zero-shot classification and retrieval capabilities on various\ndomains. However, CLIP's training remains computationally intensive, with high\ndemands on both data processing and memory. To address these challenges, recent\nmasking strategies have emerged, focusing on the selective removal of image\npatches to improve training efficiency. Although effective, these methods often\ncompromise key semantic information, resulting in suboptimal alignment between\nvisual features and text descriptions. In this work, we present a concise yet\neffective approach called Patch Generation-to-Selection to enhance CLIP's\ntraining efficiency while preserving critical semantic content. Our method\nintroduces a gradual masking process in which a small set of candidate patches\nis first pre-selected as potential mask regions. Then, we apply Sobel edge\ndetection across the entire image to generate an edge mask that prioritizes the\nretention of the primary object areas. Finally, similarity scores between the\ncandidate mask patches and their neighboring patches are computed, with optimal\ntransport normalization refining the selection process to ensure a balanced\nsimilarity matrix. Our approach, CLIP-PGS, sets new state-of-the-art results in\nzero-shot classification and retrieval tasks, achieving superior performance in\nrobustness evaluation and language compositionality benchmarks.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Gensheng Pei",
      "Tao Chen",
      "Yujia Wang",
      "Xinhao Cai",
      "Xiangbo Shu",
      "Tianfei Zhou",
      "Yazhou Yao"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17076v1",
    "title": "Halton Scheduler For Masked Generative Image Transformer",
    "abstract": "Masked Generative Image Transformers (MaskGIT) have emerged as a scalable and\nefficient image generation framework, able to deliver high-quality visuals with\nlow inference costs. However, MaskGIT's token unmasking scheduler, an essential\ncomponent of the framework, has not received the attention it deserves. We\nanalyze the sampling objective in MaskGIT, based on the mutual information\nbetween tokens, and elucidate its shortcomings. We then propose a new sampling\nstrategy based on our Halton scheduler instead of the original Confidence\nscheduler. More precisely, our method selects the token's position according to\na quasi-random, low-discrepancy Halton sequence. Intuitively, that method\nspreads the tokens spatially, progressively covering the image uniformly at\neach step. Our analysis shows that it allows reducing non-recoverable sampling\nerrors, leading to simpler hyper-parameters tuning and better quality images.\nOur scheduler does not require retraining or noise injection and may serve as a\nsimple drop-in replacement for the original sampling strategy. Evaluation of\nboth class-to-image synthesis on ImageNet and text-to-image generation on the\nCOCO dataset demonstrates that the Halton scheduler outperforms the Confidence\nscheduler quantitatively by reducing the FID and qualitatively by generating\nmore diverse and more detailed images. Our code is at\nhttps://github.com/valeoai/Halton-MaskGIT.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Victor Besnier",
      "Mickael Chen",
      "David Hurych",
      "Eduardo Valle",
      "Matthieu Cord"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17074v1",
    "title": "Zero-Shot Styled Text Image Generation, but Make It Autoregressive",
    "abstract": "Styled Handwritten Text Generation (HTG) has recently received attention from\nthe computer vision and document analysis communities, which have developed\nseveral solutions, either GAN- or diffusion-based, that achieved promising\nresults. Nonetheless, these strategies fail to generalize to novel styles and\nhave technical constraints, particularly in terms of maximum output length and\ntraining efficiency. To overcome these limitations, in this work, we propose a\nnovel framework for text image generation, dubbed Emuru. Our approach leverages\na powerful text image representation model (a variational autoencoder) combined\nwith an autoregressive Transformer. Our approach enables the generation of\nstyled text images conditioned on textual content and style examples, such as\nspecific fonts or handwriting styles. We train our model solely on a diverse,\nsynthetic dataset of English text rendered in over 100,000 typewritten and\ncalligraphy fonts, which gives it the capability to reproduce unseen styles\n(both fonts and users' handwriting) in zero-shot. To the best of our knowledge,\nEmuru is the first autoregressive model for HTG, and the first designed\nspecifically for generalization to novel styles. Moreover, our model generates\nimages without background artifacts, which are easier to use for downstream\napplications. Extensive evaluation on both typewritten and handwritten,\nany-length text image generation scenarios demonstrates the effectiveness of\nour approach.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Vittorio Pippi",
      "Fabio Quattrini",
      "Silvia Cascianelli",
      "Alessio Tonioni",
      "Rita Cucchiara"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17073v1",
    "title": "A Study into Investigating Temporal Robustness of LLMs",
    "abstract": "Large Language Models (LLMs) encapsulate a surprising amount of factual world\nknowledge. However, their performance on temporal questions and historical\nknowledge is limited because they often cannot understand temporal scope and\norientation or neglect the temporal aspect altogether. In this study, we aim to\nmeasure precisely how robust LLMs are for question answering based on their\nability to process temporal information and perform tasks requiring temporal\nreasoning and temporal factual knowledge. Specifically, we design eight\ntime-sensitive robustness tests for factual information to check the\nsensitivity of six popular LLMs in the zero-shot setting. Overall, we find LLMs\nlacking temporal robustness, especially to temporal reformulations and the use\nof different granularities of temporal references. We show how a selection of\nthese eight tests can be used automatically to judge a model's temporal\nrobustness for user questions on the fly. Finally, we apply the findings of\nthis study to improve the temporal QA performance by up to 55 percent.",
    "categories": [
      "cs.CL",
      "cs.IR",
      "68T50",
      "I.2.7"
    ],
    "authors": [
      "Jonas Wallat",
      "Abdelrahman Abdallah",
      "Adam Jatowt",
      "Avishek Anand"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17072v1",
    "title": "Multi-Span Optical Power Spectrum Evolution Modeling using ML-based Multi-Decoder Attention Framework",
    "abstract": "We implement a ML-based attention framework with component-specific decoders,\nimproving optical power spectrum prediction in multi-span networks. By reducing\nthe need for in-depth training on each component, the framework can be scaled\nto multi-span topologies with minimal data collection, making it suitable for\nbrown-field scenarios.",
    "categories": [
      "cs.LG",
      "cs.NI"
    ],
    "authors": [
      "Agastya Raj",
      "Zehao Wang",
      "Frank Slyne",
      "Tingjun Chen",
      "Dan Kilper",
      "Marco Ruffini"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17071v1",
    "title": "Superpowering Open-Vocabulary Object Detectors for X-ray Vision",
    "abstract": "Open-vocabulary object detection (OvOD) is set to revolutionize security\nscreening by enabling systems to recognize any item in X-ray scans. However,\ndeveloping effective OvOD models for X-ray imaging presents unique challenges\ndue to data scarcity and the modality gap that prevents direct adoption of\nRGB-based solutions. To overcome these limitations, we propose RAXO, a\ntraining-free framework that repurposes off-the-shelf RGB OvOD detectors for\nrobust X-ray detection. RAXO builds high-quality X-ray class descriptors using\na dual-source retrieval strategy. It gathers relevant RGB images from the web\nand enriches them via a novel X-ray material transfer mechanism, eliminating\nthe need for labeled databases. These visual descriptors replace text-based\nclassification in OvOD, leveraging intra-modal feature distances for robust\ndetection. Extensive experiments demonstrate that RAXO consistently improves\nOvOD performance, providing an average mAP increase of up to 17.0 points over\nbase detectors. To further support research in this emerging field, we also\nintroduce DET-COMPASS, a new benchmark featuring bounding box annotations for\nover 300 object categories, enabling large-scale evaluation of OvOD in X-ray.\nCode and dataset available at: https://github.com/PAGF188/RAXO.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Pablo Garcia-Fernandez",
      "Lorenzo Vaquero",
      "Mingxuan Liu",
      "Feng Xue",
      "Daniel Cores",
      "Nicu Sebe",
      "Manuel Mucientes",
      "Elisa Ricci"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17070v1",
    "title": "A Thorough Assessment of the Non-IID Data Impact in Federated Learning",
    "abstract": "Federated learning (FL) allows collaborative machine learning (ML) model\ntraining among decentralized clients' information, ensuring data privacy. The\ndecentralized nature of FL deals with non-independent and identically\ndistributed (non-IID) data. This open problem has notable consequences, such as\ndecreased model performance and more significant convergence times. Despite its\nimportance, experimental studies systematically addressing all types of data\nheterogeneity (a.k.a. non-IIDness) remain scarce. We aim to fill this gap by\nassessing and quantifying the non-IID effect through a thorough empirical\nanalysis. We use the Hellinger Distance (HD) to measure differences in\ndistribution among clients. Our study benchmarks four state-of-the-art\nstrategies for handling non-IID data, including label, feature, quantity, and\nspatiotemporal skewness, under realistic and controlled conditions. This is the\nfirst comprehensive analysis of the spatiotemporal skew effect in FL. Our\nfindings highlight the significant impact of label and spatiotemporal skew\nnon-IID types on FL model performance, with notable performance drops occurring\nat specific HD thresholds. Additionally, the FL performance is heavily affected\nmainly when the non-IIDness is extreme. Thus, we provide recommendations for FL\nresearch to tackle data heterogeneity effectively. Our work represents the most\nextensive examination of non-IIDness in FL, offering a robust foundation for\nfuture research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Daniel M. Jimenez-Gutierrez",
      "Mehrdad Hassanzadeh",
      "Aris Anagnostopoulos",
      "Ioannis Chatzigiannakis",
      "Andrea Vitaletti"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17069v1",
    "title": "PVChat: Personalized Video Chat with One-Shot Learning",
    "abstract": "Video large language models (ViLLMs) excel in general video understanding,\ne.g., recognizing activities like talking and eating, but struggle with\nidentity-aware comprehension, such as \"Wilson is receiving chemotherapy\" or\n\"Tom is discussing with Sarah\", limiting their applicability in smart\nhealthcare and smart home environments. To address this limitation, we propose\na one-shot learning framework PVChat, the first personalized ViLLM that enables\nsubject-aware question answering (QA) from a single video for each subject. Our\napproach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically\naugmented video-QA dataset, leveraging a progressive image-to-video learning\nstrategy. Specifically, we introduce an automated augmentation pipeline that\nsynthesizes identity-preserving positive samples and retrieves hard negatives\nfrom existing video corpora, generating a diverse training dataset with four QA\ntypes: existence, appearance, action, and location inquiries. To enhance\nsubject-specific learning, we propose a ReLU Routing MoH attention mechanism,\nalongside two novel objectives: (1) Smooth Proximity Regularization for\nprogressive learning through exponential distance scaling and (2) Head\nActivation Enhancement for balanced attention routing. Finally, we adopt a\ntwo-stage training strategy, transitioning from image pre-training to video\nfine-tuning, enabling a gradual learning process from static attributes to\ndynamic representations. We evaluate PVChat on diverse datasets covering\nmedical scenarios, TV series, anime, and real-world footage, demonstrating its\nsuperiority in personalized feature understanding after learning from a single\nvideo, compared to state-of-the-art ViLLMs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Yufei Shi",
      "Weilong Yan",
      "Gang Xu",
      "Yumeng Li",
      "Yuchen Li",
      "Zhenxi Li",
      "Fei Richard Yu",
      "Ming Li",
      "Si Yong Yeo"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17061v1",
    "title": "Replay4NCL: An Efficient Memory Replay-based Methodology for Neuromorphic Continual Learning in Embedded AI Systems",
    "abstract": "Neuromorphic Continual Learning (NCL) paradigm leverages Spiking Neural\nNetworks (SNNs) to enable continual learning (CL) capabilities for AI systems\nto adapt to dynamically changing environments. Currently, the state-of-the-art\nemploy a memory replay-based method to maintain the old knowledge. However,\nthis technique relies on long timesteps and compression-decompression steps,\nthereby incurring significant latency and energy overheads, which are not\nsuitable for tightly-constrained embedded AI systems (e.g., mobile\nagents/robotics). To address this, we propose Replay4NCL, a novel efficient\nmemory replay-based methodology for enabling NCL in embedded AI systems.\nSpecifically, Replay4NCL compresses the latent data (old knowledge), then\nreplays them during the NCL training phase with small timesteps, to minimize\nthe processing latency and energy consumption. To compensate the information\nloss from reduced spikes, we adjust the neuron threshold potential and learning\nrate settings. Experimental results on the class-incremental scenario with the\nSpiking Heidelberg Digits (SHD) dataset show that Replay4NCL can preserve old\nknowledge with Top-1 accuracy of 90.43% compared to 86.22% from the\nstate-of-the-art, while effectively learning new tasks, achieving 4.88x latency\nspeed-up, 20% latent memory saving, and 36.43% energy saving. These results\nhighlight the potential of our Replay4NCL methodology to further advances NCL\ncapabilities for embedded AI systems.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Mishal Fatima Minhas",
      "Rachmad Vidya Wicaksana Putra",
      "Falah Awwad",
      "Osman Hasan",
      "Muhammad Shafique"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17059v1",
    "title": "DIDiffGes: Decoupled Semi-Implicit Diffusion Models for Real-time Gesture Generation from Speech",
    "abstract": "Diffusion models have demonstrated remarkable synthesis quality and diversity\nin generating co-speech gestures. However, the computationally intensive\nsampling steps associated with diffusion models hinder their practicality in\nreal-world applications. Hence, we present DIDiffGes, for a Decoupled\nSemi-Implicit Diffusion model-based framework, that can synthesize\nhigh-quality, expressive gestures from speech using only a few sampling steps.\nOur approach leverages Generative Adversarial Networks (GANs) to enable\nlarge-step sampling for diffusion model. We decouple gesture data into body and\nhands distributions and further decompose them into marginal and conditional\ndistributions. GANs model the marginal distribution implicitly, while L2\nreconstruction loss learns the conditional distributions exciplictly. This\nstrategy enhances GAN training stability and ensures expressiveness of\ngenerated full-body gestures. Our framework also learns to denoise root noise\nconditioned on local body representation, guaranteeing stability and realism.\nDIDiffGes can generate gestures from speech with just 10 sampling steps,\nwithout compromising quality and expressiveness, reducing the number of\nsampling steps by a factor of 100 compared to existing methods. Our user study\nreveals that our method outperforms state-of-the-art approaches in human\nlikeness, appropriateness, and style correctness. Project is\nhttps://cyk990422.github.io/DIDiffGes.",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.SD"
    ],
    "authors": [
      "Yongkang Cheng",
      "Shaoli Huang",
      "Xuelin Chen",
      "Jifeng Ning",
      "Mingming Gong"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17057v1",
    "title": "Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks",
    "abstract": "Accurate segmentation of ultrasound (US) images of the cervical muscles is\ncrucial for precision healthcare. The demand for automatic computer-assisted\nmethods is high. However, the scarcity of labeled data hinders the development\nof these methods. Advanced semi-supervised learning approaches have displayed\npromise in overcoming this challenge by utilizing labeled and unlabeled data.\nThis study introduces a novel semi-supervised learning (SSL) framework that\nintegrates dual neural networks. This SSL framework utilizes both networks to\ngenerate pseudo-labels and cross-supervise each other at the pixel level.\nAdditionally, a self-supervised contrastive learning strategy is introduced,\nwhich employs a pair of deep representations to enhance feature learning\ncapabilities, particularly on unlabeled data. Our framework demonstrates\ncompetitive performance in cervical segmentation tasks. Our codes are publicly\navailable on https://github.com/13204942/SSL\\_Cervical\\_Segmentation.",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Fangyijie Wang",
      "Kathleen M. Curran",
      "Guénolé Silvestre"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17055v1",
    "title": "Data-Driven Optimization of EV Charging Station Placement Using Causal Discovery",
    "abstract": "This paper addresses the critical challenge of optimizing electric vehicle\ncharging station placement through a novel data-driven methodology employing\ncausal discovery techniques. While traditional approaches prioritize economic\nfactors or power grid constraints, they often neglect empirical charging\npatterns that ultimately determine station utilization. We analyze extensive\ncharging data from Palo Alto and Boulder (337,344 events across 100 stations)\nto uncover latent relationships between station characteristics and\nutilization. Applying structural learning algorithms (NOTEARS and DAGMA) to\nthis data reveals that charging demand is primarily determined by three\nfactors: proximity to amenities, EV registration density, and adjacency to\nhigh-traffic routes. These findings, consistent across multiple algorithms and\nurban contexts, challenge conventional infrastructure distribution strategies.\nWe develop an optimization framework that translates these insights into\nactionable placement recommendations, identifying locations likely to\nexperience high utilization based on the discovered dependency structures. The\nresulting site selection model prioritizes strategic clustering in high-amenity\nareas with substantial EV populations rather than uniform spatial distribution.\nOur approach contributes a framework that integrates empirical charging\nbehavior into infrastructure planning, potentially enhancing both station\nutilization and user convenience. By focusing on data-driven insights instead\nof theoretical distribution models, we provide a more effective strategy for\nexpanding charging networks that can adjust to various stages of EV market\ndevelopment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Julius Stephan Junker",
      "Rong Hu",
      "Ziyue Li",
      "Wolfgang Ketter"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17050v1",
    "title": "Scoring, Remember, and Reference: Catching Camouflaged Objects in Videos",
    "abstract": "Video Camouflaged Object Detection (VCOD) aims to segment objects whose\nappearances closely resemble their surroundings, posing a challenging and\nemerging task. Existing vision models often struggle in such scenarios due to\nthe indistinguishable appearance of camouflaged objects and the insufficient\nexploitation of dynamic information in videos. To address these challenges, we\npropose an end-to-end VCOD framework inspired by human memory-recognition,\nwhich leverages historical video information by integrating memory reference\nframes for camouflaged sequence processing. Specifically, we design a\ndual-purpose decoder that simultaneously generates predicted masks and scores,\nenabling reference frame selection based on scores while introducing auxiliary\nsupervision to enhance feature extraction.Furthermore, this study introduces a\nnovel reference-guided multilevel asymmetric attention mechanism, effectively\nintegrating long-term reference information with short-term motion cues for\ncomprehensive feature extraction. By combining these modules, we develop the\nScoring, Remember, and Reference (SRR) framework, which efficiently extracts\ninformation to locate targets and employs memory guidance to improve subsequent\nprocessing. With its optimized module design and effective utilization of video\ndata, our model achieves significant performance improvements, surpassing\nexisting approaches by 10% on benchmark datasets while requiring fewer\nparameters (54M) and only a single pass through the video. The code will be\nmade publicly available.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yuang Feng",
      "Shuyong Gao",
      "Fuzhen Yan",
      "Yicheng Song",
      "Lingyi Hong",
      "Junjie Hu",
      "Wenqiang Zhang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17046v1",
    "title": "HAPI: A Model for Learning Robot Facial Expressions from Human Preferences",
    "abstract": "Automatic robotic facial expression generation is crucial for human-robot\ninteraction, as handcrafted methods based on fixed joint configurations often\nyield rigid and unnatural behaviors. Although recent automated techniques\nreduce the need for manual tuning, they tend to fall short by not adequately\nbridging the gap between human preferences and model predictions-resulting in a\ndeficiency of nuanced and realistic expressions due to limited degrees of\nfreedom and insufficient perceptual integration. In this work, we propose a\nnovel learning-to-rank framework that leverages human feedback to address this\ndiscrepancy and enhanced the expressiveness of robotic faces. Specifically, we\nconduct pairwise comparison annotations to collect human preference data and\ndevelop the Human Affective Pairwise Impressions (HAPI) model, a Siamese\nRankNet-based approach that refines expression evaluation. Results obtained via\nBayesian Optimization and online expression survey on a 35-DOF android platform\ndemonstrate that our approach produces significantly more realistic and\nsocially resonant expressions of Anger, Happiness, and Surprise than those\ngenerated by baseline and expert-designed methods. This confirms that our\nframework effectively bridges the gap between human preferences and model\npredictions while robustly aligning robotic expression generation with human\naffective responses.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Dongsheng Yang",
      "Qianying Liu",
      "Wataru Sato",
      "Takashi Minato",
      "Chaoran Liu",
      "Shin'ya Nishida"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17044v1",
    "title": "ExCap3D: Expressive 3D Scene Understanding via Object Captioning with Varying Detail",
    "abstract": "Generating text descriptions of objects in 3D indoor scenes is an important\nbuilding block of embodied understanding. Existing methods do this by\ndescribing objects at a single level of detail, which often does not capture\nfine-grained details such as varying textures, materials, and shapes of the\nparts of objects. We propose the task of expressive 3D captioning: given an\ninput 3D scene, describe objects at multiple levels of detail: a high-level\nobject description, and a low-level description of the properties of its parts.\nTo produce such captions, we present ExCap3D, an expressive 3D captioning model\nwhich takes as input a 3D scan, and for each detected object in the scan,\ngenerates a fine-grained collective description of the parts of the object,\nalong with an object-level description conditioned on the part-level\ndescription. We design ExCap3D to encourage semantic consistency between the\ngenerated text descriptions, as well as textual similarity in the latent space,\nto further increase the quality of the generated captions. To enable this task,\nwe generated the ExCap3D Dataset by leveraging a visual-language model (VLM)\nfor multi-view captioning. The ExCap3D Dataset contains captions on the\nScanNet++ dataset with varying levels of detail, comprising 190k text\ndescriptions of 34k 3D objects in 947 indoor scenes. Our experiments show that\nthe object- and part-level of detail captions generated by ExCap3D are of\nhigher quality than those produced by state-of-the-art methods, with a Cider\nscore improvement of 17% and 124% for object- and part-level details\nrespectively. Our code, dataset and models will be made publicly available.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Chandan Yeshwanth",
      "David Rozenberszki",
      "Angela Dai"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17039v1",
    "title": "Summarization Metrics for Spanish and Basque: Do Automatic Scores and LLM-Judges Correlate with Humans?",
    "abstract": "Studies on evaluation metrics and LLM-as-a-Judge models for automatic text\nsummarization have largely been focused on English, limiting our understanding\nof their effectiveness in other languages. Through our new dataset BASSE\n(BAsque and Spanish Summarization Evaluation), we address this situation by\ncollecting human judgments on 2,040 abstractive summaries in Basque and\nSpanish, generated either manually or by five LLMs with four different prompts.\nFor each summary, annotators evaluated five criteria on a 5-point Likert scale:\ncoherence, consistency, fluency, relevance, and 5W1H. We use these data to\nreevaluate traditional automatic metrics used for evaluating summaries, as well\nas several LLM-as-a-Judge models that show strong performance on this task in\nEnglish. Our results show that currently proprietary judge LLMs have the\nhighest correlation with human judgments, followed by criteria-specific\nautomatic metrics, while open-sourced judge LLMs perform poorly. We release\nBASSE and our code publicly, along with the first large-scale Basque\nsummarization dataset containing 22,525 news articles with their subheads.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Jeremy Barnes",
      "Naiara Perez",
      "Alba Bonet-Jover",
      "Begoña Altuna"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17037v1",
    "title": "Unitless Unrestricted Markov-Consistent SCM Generation: Better Benchmark Datasets for Causal Discovery",
    "abstract": "Causal discovery aims to extract qualitative causal knowledge in the form of\ncausal graphs from data. Because causal ground truth is rarely known in the\nreal world, simulated data plays a vital role in evaluating the performance of\nthe various causal discovery algorithms proposed in the literature. But recent\nwork highlighted certain artifacts of commonly used data generation techniques\nfor a standard class of structural causal models (SCM) that may be nonphysical,\nincluding var- and R2-sortability, where the variables' variance and\ncoefficients of determination (R2) after regressing on all other variables,\nrespectively, increase along the causal order. Some causal methods exploit such\nartifacts, leading to unrealistic expectations for their performance on\nreal-world data. Some modifications have been proposed to remove these\nartifacts; notably, the internally-standardized structural causal model (iSCM)\navoids varsortability and largely alleviates R2-sortability on sparse causal\ngraphs, but exhibits a reversed R2-sortability pattern for denser graphs not\nfeatured in their work. We analyze which sortability patterns we expect to see\nin real data, and propose a method for drawing coefficients that we argue more\neffectively samples the space of SCMs. Finally, we propose a novel extension of\nour SCM generation method to the time series setting.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Rebecca J. Herman",
      "Jonas Wahl",
      "Urmi Ninad",
      "Jakob Runge"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17034v1",
    "title": "An Attentive Representative Sample Selection Strategy Combined with Balanced Batch Training for Skin Lesion Segmentation",
    "abstract": "An often overlooked problem in medical image segmentation research is the\neffective selection of training subsets to annotate from a complete set of\nunlabelled data. Many studies select their training sets at random, which may\nlead to suboptimal model performance, especially in the minimal supervision\nsetting where each training image has a profound effect on performance\noutcomes. This work aims to address this issue. We use prototypical contrasting\nlearning and clustering to extract representative and diverse samples for\nannotation. We improve upon prior works with a bespoke cluster-based image\nselection process. Additionally, we introduce the concept of unsupervised\nbalanced batch dataloading to medical image segmentation, which aims to improve\nmodel learning with minimally annotated data. We evaluated our method on a\npublic skin lesion dataset (ISIC 2018) and compared it to another\nstate-of-the-art data sampling method. Our method achieved superior performance\nin a low annotation budget scenario.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Stephen Lloyd-Brown",
      "Susan Francis",
      "Caroline Hoad",
      "Penny Gowland",
      "Karen Mullinger",
      "Andrew French",
      "Xin Chen"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17032v1",
    "title": "TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented Reality via 3D Gaussian Splatting",
    "abstract": "Realistic 3D full-body talking avatars hold great potential in AR, with\napplications ranging from e-commerce live streaming to holographic\ncommunication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike\navatar creation, existing methods struggle with fine-grained control of facial\nexpressions and body movements in full-body talking tasks. Additionally, they\noften lack sufficient details and cannot run in real-time on mobile devices. We\npresent TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking\navatar driven by various signals. Our approach starts by creating a\npersonalized clothed human parametric template that binds Gaussians to\nrepresent appearances. We then pre-train a StyleUnet-based network to handle\ncomplex pose-dependent non-rigid deformation, which can capture high-frequency\nappearance details but is too resource-intensive for mobile devices. To\novercome this, we \"bake\" the non-rigid deformations into a lightweight\nMLP-based network using a distillation technique and develop blend shapes to\ncompensate for details. Extensive experiments show that TaoAvatar achieves\nstate-of-the-art rendering quality while running in real-time across various\ndevices, maintaining 90 FPS on high-definition stereo devices such as the Apple\nVision Pro.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jianchuan Chen",
      "Jingchuan Hu",
      "Gaige Wang",
      "Zhonghua Jiang",
      "Tiansong Zhou",
      "Zhiwen Chen",
      "Chengfei Lv"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17030v1",
    "title": "Exploring the Efficacy of Partial Denoising Using Bit Plane Slicing for Enhanced Fracture Identification: A Comparative Study of Deep Learning-Based Approaches and Handcrafted Feature Extraction Techniques",
    "abstract": "Computer vision has transformed medical diagnosis, treatment, and research\nthrough advanced image processing and machine learning techniques. Fracture\nclassification, a critical area in healthcare, has greatly benefited from these\nadvancements, yet accurate detection is challenged by complex patterns and\nimage noise. Bit plane slicing enhances medical images by reducing noise\ninterference and extracting informative features. This research explores\npartial denoising techniques to provide practical solutions for improved\nfracture analysis, ultimately enhancing patient care. The study explores deep\nlearning model DenseNet and handcrafted feature extraction. Decision Tree and\nRandom Forest, were employed to train and evaluate distinct image\nrepresentations. These include the original image, the concatenation of the\nfour bit planes from the LSB as well as MSB, the fully denoised image, and an\nimage consisting of 6 bit planes from MSB and 2 denoised bit planes from LSB.\nThe purpose of forming these diverse image representations is to analyze SNR as\nwell as classification accuracy and identify the bit planes that contain the\nmost informative features. Moreover, the study delves into the significance of\npartial denoising techniques in preserving crucial features, leading to\nimprovements in classification results. Notably, this study shows that\nemploying the Random Forest classifier, the partially denoised image\nrepresentation exhibited a testing accuracy of 95.61% surpassing the\nperformance of other image representations. The outcomes of this research\nprovide valuable insights into the development of efficient preprocessing,\nfeature extraction and classification approaches for fracture identification.\nBy enhancing diagnostic accuracy, these advancements hold the potential to\npositively impact patient care and overall medical outcomes.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Snigdha Paul",
      "Sambit Mallick",
      "Anindya Sen"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17029v1",
    "title": "AnimatePainter: A Self-Supervised Rendering Framework for Reconstructing Painting Process",
    "abstract": "Humans can intuitively decompose an image into a sequence of strokes to\ncreate a painting, yet existing methods for generating drawing processes are\nlimited to specific data types and often rely on expensive human-annotated\ndatasets. We propose a novel self-supervised framework for generating drawing\nprocesses from any type of image, treating the task as a video generation\nproblem. Our approach reverses the drawing process by progressively removing\nstrokes from a reference image, simulating a human-like creation sequence.\nCrucially, our method does not require costly datasets of real human drawing\nprocesses; instead, we leverage depth estimation and stroke rendering to\nconstruct a self-supervised dataset. We model human drawings as \"refinement\"\nand \"layering\" processes and introduce depth fusion layers to enable video\ngeneration models to learn and replicate human drawing behavior. Extensive\nexperiments validate the effectiveness of our approach, demonstrating its\nability to generate realistic drawings without the need for real drawing\nprocess data.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Junjie Hu",
      "Shuyong Gao",
      "Qianyu Guo",
      "Yan Wang",
      "Qishan Wang",
      "Yuang Feng",
      "Wenqiang Zhang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17027v1",
    "title": "RAW-Adapter: Adapting Pre-trained Visual Model to Camera RAW Images and A Benchmark",
    "abstract": "In the computer vision community, the preference for pre-training visual\nmodels has largely shifted toward sRGB images due to their ease of acquisition\nand compact storage. However, camera RAW images preserve abundant physical\ndetails across diverse real-world scenarios. Despite this, most existing visual\nperception methods that utilize RAW data directly integrate image signal\nprocessing (ISP) stages with subsequent network modules, often overlooking\npotential synergies at the model level. Building on recent advances in\nadapter-based methodologies in both NLP and computer vision, we propose\nRAW-Adapter, a novel framework that incorporates learnable ISP modules as\ninput-level adapters to adjust RAW inputs. At the same time, it employs\nmodel-level adapters to seamlessly bridge ISP processing with high-level\ndownstream architectures. Moreover, RAW-Adapter serves as a general framework\napplicable to various computer vision frameworks.\n  Furthermore, we introduce RAW-Bench, which incorporates 17 types of RAW-based\ncommon corruptions, including lightness degradations, weather effects,\nblurriness, camera imaging degradations, and variations in camera color\nresponse. Using this benchmark, we systematically compare the performance of\nRAW-Adapter with state-of-the-art (SOTA) ISP methods and other RAW-based\nhigh-level vision algorithms. Additionally, we propose a RAW-based data\naugmentation strategy to further enhance RAW-Adapter's performance and improve\nits out-of-domain (OOD) generalization ability. Extensive experiments\nsubstantiate the effectiveness and efficiency of RAW-Adapter, highlighting its\nrobust performance across diverse scenarios.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Ziteng Cui",
      "Jianfei Yang",
      "Tatsuya Harada"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17025v1",
    "title": "A Guide to Bayesian Networks Software Packages for Structure and Parameter Learning -- 2025 Edition",
    "abstract": "A representation of the cause-effect mechanism is needed to enable artificial\nintelligence to represent how the world works. Bayesian Networks (BNs) have\nproven to be an effective and versatile tool for this task. BNs require\nconstructing a structure of dependencies among variables and learning the\nparameters that govern these relationships. These tasks, referred to as\nstructural learning and parameter learning, are actively investigated by the\nresearch community, with several algorithms proposed and no single method\nhaving established itself as standard. A wide range of software, tools, and\npackages have been developed for BNs analysis and made available to academic\nresearchers and industry practitioners. As a consequence of having no\none-size-fits-all solution, moving the first practical steps and getting\noriented into this field is proving to be challenging to outsiders and\nbeginners. In this paper, we review the most relevant tools and software for\nBNs structural and parameter learning to date, providing our subjective\nrecommendations directed to an audience of beginners. In addition, we provide\nan extensive easy-to-consult overview table summarizing all software packages\nand their main features. By improving the reader understanding of which\navailable software might best suit their needs, we improve accessibility to the\nfield and make it easier for beginners to take their first step into it.",
    "categories": [
      "cs.AI",
      "I.2"
    ],
    "authors": [
      "Joverlyn Gaudillo",
      "Nicole Astrologo",
      "Fabio Stella",
      "Enzo Acerbi",
      "Francesco Canonaco"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17024v1",
    "title": "A Tale of Two Classes: Adapting Supervised Contrastive Learning to Binary Imbalanced Datasets",
    "abstract": "Supervised contrastive learning (SupCon) has proven to be a powerful\nalternative to the standard cross-entropy loss for classification of\nmulti-class balanced datasets. However, it struggles to learn well-conditioned\nrepresentations of datasets with long-tailed class distributions. This problem\nis potentially exacerbated for binary imbalanced distributions, which are\ncommonly encountered during many real-world problems such as medical diagnosis.\nIn experiments on seven binary datasets of natural and medical images, we show\nthat the performance of SupCon decreases with increasing class imbalance. To\nsubstantiate these findings, we introduce two novel metrics that evaluate the\nquality of the learned representation space. By measuring the class\ndistribution in local neighborhoods, we are able to uncover structural\ndeficiencies of the representation space that classical metrics cannot detect.\nInformed by these insights, we propose two new supervised contrastive learning\nstrategies tailored to binary imbalanced datasets that improve the structure of\nthe representation space and increase downstream classification accuracy over\nstandard SupCon by up to 35%. We make our code available.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "David Mildenberger",
      "Paul Hager",
      "Daniel Rueckert",
      "Martin J Menten"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17020v1",
    "title": "Benign Overfitting with Quantum Kernels",
    "abstract": "Quantum kernels quantify similarity between data points by measuring the\ninner product between quantum states, computed through quantum circuit\nmeasurements. By embedding data into quantum systems, quantum kernel feature\nmaps, that may be classically intractable to compute, could efficiently exploit\nhigh-dimensional Hilbert spaces to capture complex patterns. However, designing\neffective quantum feature maps remains a major challenge. Many quantum kernels,\nsuch as the fidelity kernel, suffer from exponential concentration, leading to\nnear-identity kernel matrices that fail to capture meaningful data correlations\nand lead to overfitting and poor generalization. In this paper, we propose a\nnovel strategy for constructing quantum kernels that achieve good\ngeneralization performance, drawing inspiration from benign overfitting in\nclassical machine learning. Our approach introduces the concept of local-global\nquantum kernels, which combine two complementary components: a local quantum\nkernel based on measurements of small subsystems and a global quantum kernel\nderived from full-system measurements. Through numerical experiments, we\ndemonstrate that local-global quantum kernels exhibit benign overfitting,\nsupporting the effectiveness of our approach in enhancing quantum kernel\nmethods.",
    "categories": [
      "quant-ph",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Joachim Tomasi",
      "Sandrine Anthoine",
      "Hachem Kadri"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17018v1",
    "title": "Symbolic Audio Classification via Modal Decision Tree Learning",
    "abstract": "The range of potential applications of acoustic analysis is wide.\nClassification of sounds, in particular, is a typical machine learning task\nthat received a lot of attention in recent years. The most common approaches to\nsound classification are sub-symbolic, typically based on neural networks, and\nresult in black-box models with high performances but very low transparency. In\nthis work, we consider several audio tasks, namely, age and gender recognition,\nemotion classification, and respiratory disease diagnosis, and we approach them\nwith a symbolic technique, that is, (modal) decision tree learning. We prove\nthat such tasks can be solved using the same symbolic pipeline, that allows to\nextract simple rules with very high accuracy and low complexity. In principle,\nall such tasks could be associated to an autonomous conversation system, which\ncould be useful in different contexts, such as an automatic reservation agent\nfor an hospital or a clinic.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS",
      "68T05",
      "I.2.6"
    ],
    "authors": [
      "Enrico Marzano",
      "Giovanni Pagliarini",
      "Riccardo Pasini",
      "Guido Sciavicco",
      "Ionel Eduard Stan"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17017v1",
    "title": "Specifying What You Know or Not for Multi-Label Class-Incremental Learning",
    "abstract": "Existing class incremental learning is mainly designed for single-label\nclassification task, which is ill-equipped for multi-label scenarios due to the\ninherent contradiction of learning objectives for samples with incomplete\nlabels. We argue that the main challenge to overcome this contradiction in\nmulti-label class-incremental learning (MLCIL) lies in the model's inability to\nclearly distinguish between known and unknown knowledge. This ambiguity hinders\nthe model's ability to retain historical knowledge, master current classes, and\nprepare for future learning simultaneously. In this paper, we target at\nspecifying what is known or not to accommodate Historical, Current, and\nProspective knowledge for MLCIL and propose a novel framework termed as HCP.\nSpecifically, (i) we clarify the known classes by dynamic feature purification\nand recall enhancement with distribution prior, enhancing the precision and\nretention of known information. (ii) We design prospective knowledge mining to\nprobe the unknown, preparing the model for future learning. Extensive\nexperiments validate that our method effectively alleviates catastrophic\nforgetting in MLCIL, surpassing the previous state-of-the-art by 3.3% on\naverage accuracy for MS-COCO B0-C10 setting without replay buffers.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Aoting Zhang",
      "Dongbao Yang",
      "Chang Liu",
      "Xiaopeng Hong",
      "Yu Zhou"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17015v1",
    "title": "Do regularization methods for shortcut mitigation work as intended?",
    "abstract": "Mitigating shortcuts, where models exploit spurious correlations in training\ndata, remains a significant challenge for improving generalization.\nRegularization methods have been proposed to address this issue by enhancing\nmodel generalizability. However, we demonstrate that these methods can\nsometimes overregularize, inadvertently suppressing causal features along with\nspurious ones. In this work, we analyze the theoretical mechanisms by which\nregularization mitigates shortcuts and explore the limits of its effectiveness.\nAdditionally, we identify the conditions under which regularization can\nsuccessfully eliminate shortcuts without compromising causal features. Through\nexperiments on synthetic and real-world datasets, our comprehensive analysis\nprovides valuable insights into the strengths and limitations of regularization\ntechniques for addressing shortcuts, offering guidance for developing more\nrobust models.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Haoyang Hong",
      "Ioanna Papanikolaou",
      "Sonali Parbhoo"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17013v1",
    "title": "Developing Critical Thinking in Second Language Learners: Exploring Generative AI like ChatGPT as a Tool for Argumentative Essay Writing",
    "abstract": "This study employs the Paul-Elder Critical Thinking Model and Tan's\nargumentative writing framework to create a structured methodology. This\nmethodology, ChatGPT Guideline for Critical Argumentative Writing (CGCAW)\nframework, integrates the models with ChatGPT's capabilities to guide L2\nlearners in utilizing ChatGPT to enhance their critical thinking skills. A\nquantitative experiment was conducted with 10 participants from a state\nuniversity, divided into experimental and control groups. The experimental\ngroup utilized the CGCAW framework, while the control group used ChatGPT\nwithout specific guidelines. Participants wrote an argumentative essay within a\n40-minute timeframe, and essays were evaluated by three assessors: ChatGPT,\nGrammarly, and a course instructor. Results indicated that the experimental\ngroup showed improvements in clarity, logical coherence, and use of evidence,\ndemonstrating ChatGPT's potential to enhance specific aspects of argumentative\nwriting. However, the control group performed better in overall language\nmechanics and articulation of main arguments, indicating areas where the CGCAW\nframework could be further refined. This study highlights the need for further\nresearch to optimize the use of AI tools like ChatGPT in L2 learning\nenvironments to enhance critical thinking and writing skills.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "I.2.7; K.3.1"
    ],
    "authors": [
      "Simon Suh",
      "Jihyuk Bang",
      "Ji Woo Han"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17004v1",
    "title": "Text2Model: Generating dynamic chemical reactor models using large language models (LLMs)",
    "abstract": "As large language models have shown remarkable capabilities in conversing via\nnatural language, the question arises as to how LLMs could potentially assist\nchemical engineers in research and industry with domain-specific tasks. We\ngenerate dynamic chemical reactor models in Modelica code format from textual\ndescriptions as user input. We fine-tune Llama 3.1 8B Instruct on synthetically\ngenerated Modelica code for different reactor scenarios. We compare the\nperformance of our fine-tuned model to the baseline Llama 3.1 8B Instruct model\nand GPT4o. We manually assess the models' predictions regarding the syntactic\nand semantic accuracy of the generated dynamic models. We find that\nconsiderable improvements are achieved by the fine-tuned model with respect to\nboth the semantic and the syntactic accuracy of the Modelica models. However,\nthe fine-tuned model lacks a satisfactory ability to generalize to unseen\nscenarios compared to GPT4o.",
    "categories": [
      "cs.PL",
      "cs.CL"
    ],
    "authors": [
      "Sophia Rupprecht",
      "Yassine Hounat",
      "Monisha Kumar",
      "Giacomo Lastrucci",
      "Artur M. Schweidtmann"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17003v1",
    "title": "A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir transition to real-world applications reveals a critical limitation: the\ninability to adapt to individual preferences while maintaining alignment with\nuniversal human values. Current alignment techniques adopt a one-size-fits-all\napproach that fails to accommodate users' diverse backgrounds and needs. This\npaper presents the first comprehensive survey of personalized alignment-a\nparadigm that enables LLMs to adapt their behavior within ethical boundaries\nbased on individual preferences. We propose a unified framework comprising\npreference memory management, personalized generation, and feedback-based\nalignment, systematically analyzing implementation approaches and evaluating\ntheir effectiveness across various scenarios. By examining current techniques,\npotential risks, and future challenges, this survey provides a structured\nfoundation for developing more adaptable and ethically-aligned LLMs.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Jian Guan",
      "Junfei Wu",
      "Jia-Nan Li",
      "Chuanqi Cheng",
      "Wei Wu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.17002v1",
    "title": "Targetless 6DoF Calibration of LiDAR and 2D Scanning Radar Based on Cylindrical Occupancy",
    "abstract": "Owing to the capability for reliable and all-weather long-range sensing, the\nfusion of LiDAR and Radar has been widely applied to autonomous vehicles for\nrobust perception. In practical operation, well manually calibrated extrinsic\nparameters, which are crucial for the fusion of multi-modal sensors, may drift\ndue to the vibration. To address this issue, we present a novel targetless\ncalibration approach, termed LiRaCo, for the extrinsic 6DoF calibration of\nLiDAR and Radar sensors. Although both types of sensors can obtain geometric\ninformation, bridging the geometric correspondences between multi-modal data\nwithout any clues of explicit artificial markers is nontrivial, mainly due to\nthe low vertical resolution of scanning Radar. To achieve the targetless\ncalibration, LiRaCo leverages a spatial occupancy consistency between LiDAR\npoint clouds and Radar scans in a common cylindrical representation,\nconsidering the increasing data sparsity with distance for both sensors.\nSpecifically, LiRaCo expands the valid Radar scanned pixels into 3D occupancy\ngrids to constrain LiDAR point clouds based on spatial consistency.\nConsequently, a cost function involving extrinsic calibration parameters is\nformulated based on the spatial overlap of 3D grids and LiDAR points. Extrinsic\nparameters are finally estimated by optimizing the cost function. Comprehensive\nquantitative and qualitative experiments on two real outdoor datasets with\ndifferent LiDAR sensors demonstrate the feasibility and accuracy of the\nproposed method. The source code will be publicly available.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Weimin Wang",
      "Yu Du",
      "Ting Yang",
      "Yu Liu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16997v1",
    "title": "Steady Progress Beats Stagnation: Mutual Aid of Foundation and Conventional Models in Mixed Domain Semi-Supervised Medical Image Segmentation",
    "abstract": "Large pretrained visual foundation models exhibit impressive general\ncapabilities. However, the extensive prior knowledge inherent in these models\ncan sometimes be a double-edged sword when adapting them to downstream tasks in\nspecific domains. In the context of semi-supervised medical image segmentation\nwith domain shift, foundation models like MedSAM tend to make overconfident\npredictions, some of which are incorrect. The error accumulation hinders the\neffective utilization of unlabeled data and limits further improvements. In\nthis paper, we introduce a Synergistic training framework for Foundation and\nConventional models (SynFoC) to address the issue. We observe that a\nconventional model trained from scratch has the ability to correct the\nhigh-confidence mispredictions of the foundation model, while the foundation\nmodel can supervise it with high-quality pseudo-labels in the early training\nstages. Furthermore, to enhance the collaborative training effectiveness of\nboth models and promote reliable convergence towards optimization, the\nconsensus-divergence consistency regularization is proposed. We demonstrate the\nsuperiority of our method across four public multi-domain datasets. In\nparticular, our method improves the Dice score by 10.31\\% on the Prostate\ndataset. Our code is available at https://github.com/MQinghe/SynFoC .",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Qinghe Ma",
      "Jian Zhang",
      "Zekun Li",
      "Lei Qi",
      "Qian Yu",
      "Yinghuan Shi"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16991v1",
    "title": "TRACE: Time SeRies PArameter EffiCient FinE-tuning",
    "abstract": "We propose an efficient fine-tuning method for time series foundation models,\ntermed TRACE: Time Series Parameter Efficient Fine-tuning. While pretrained\ntime series foundation models are gaining popularity, they face the following\nchallenges: (1) Unlike natural language tasks, time series data vary in\nfrequency, channel numbers, historical/prediction lengths. For long-term\nforecasting tasks in particular, tailored fine-tuning can significantly enhance\nperformance.(2) Existing parameter-efficient tuning methods like LoRA remain\napplicable but require adaptation to temporal characteristics.\n  To address these challenges, our TRACE framework introduces two key\ninnovations: (1) Gated DSIC (Gated Dynamic Simulation Importance Calculation),\nan unbiased LoRA module importance selection mechanism that ensures conditional\nparameter consistency before and after masking. Experiments demonstrate that\nGated DSIC outperforms common fine-tuning. (2) Reconstructed prediction heads\nfor long-term forecasting tasks, which achieve comparable or superior\nperformance to linear probing heads while drastically reducing parameter\ncounts.\n  Extensive experiments on long-/short-term forecasting and anomaly detection\ntasks across diverse datasets, coupled with ablation studies, validate the\neffectiveness of our method.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Yuze Li",
      "Wei Zhu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16988v1",
    "title": "High Accuracy Pulmonary Vessel Segmentation for Contrast and Non-contrast CT Images and Its Clinical Evaluation",
    "abstract": "Accurate segmentation of pulmonary vessels plays a very critical role in\ndiagnosing and assessing various lung diseases. In clinical practice, diagnosis\nis typically carried out using CTPA images. However, there is a lack of\nhigh-precision pulmonary vessel segmentation algorithms for CTPA, and pulmonary\nvessel segmentation for NCCT poses an even greater challenge. In this study, we\npropose a 3D image segmentation algorithm for automated pulmonary vessel\nsegmentation from both contrast and non-contrast CT images. In the network, we\ndesigned a Vessel Lumen Structure Optimization Module (VLSOM), which extracts\nthe centerline of vessels and adjusts the weights based on the positional\ninformation and adds a Cl-Dice-Loss to supervise the stability of the vessels\nstructure. In addition, we designed a method for generating vessel GT from CTPA\nto NCCT for training models that support both CTPA and NCCT. In this work, we\nused 427 sets of high-precision annotated CT data from multiple vendors and\ncountries. Finally, our experimental model achieved Cl-Recall, Cl-DICE and\nRecall values of 0.879, 0.909, 0.934 (CTPA) and 0.928, 0.936, 0.955 (NCCT)\nrespectively. This shows that our model has achieved good performance in both\naccuracy and completeness of pulmonary vessel segmentation. In clinical visual\nevaluation, our model also had good segmentation performance on various disease\ntypes and can assist doctors in medical diagnosis, verifying the great\npotential of this method in clinical application.",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Ying Ming",
      "Shaoze Luo",
      "Longfei Zhao",
      "Qiqi Xu",
      "Wei Song"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16983v1",
    "title": "Enabling Versatile Controls for Video Diffusion Models",
    "abstract": "Despite substantial progress in text-to-video generation, achieving precise\nand flexible control over fine-grained spatiotemporal attributes remains a\nsignificant unresolved challenge in video generation research. To address these\nlimitations, we introduce VCtrl (also termed PP-VCtrl), a novel framework\ndesigned to enable fine-grained control over pre-trained video diffusion models\nin a unified manner. VCtrl integrates diverse user-specified control\nsignals-such as Canny edges, segmentation masks, and human keypoints-into\npretrained video diffusion models via a generalizable conditional module\ncapable of uniformly encoding multiple types of auxiliary signals without\nmodifying the underlying generator. Additionally, we design a unified control\nsignal encoding pipeline and a sparse residual connection mechanism to\nefficiently incorporate control representations. Comprehensive experiments and\nhuman evaluations demonstrate that VCtrl effectively enhances controllability\nand generation quality. The source code and pre-trained models are publicly\navailable and implemented using the PaddlePaddle framework at\nhttp://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Xu Zhang",
      "Hao Zhou",
      "Haoming Qin",
      "Xiaobin Lu",
      "Jiaxing Yan",
      "Guanzhong Wang",
      "Zeyu Chen",
      "Yi Liu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16980v1",
    "title": "Token Dynamics: Towards Efficient and Dynamic Video Token Representation for Video Large Language Models",
    "abstract": "Token-based video representation has emerged as a promising approach for\nenabling large language models to interpret video content. However, existing\ntoken reduction techniques, such as token pruning and token merging, often\ndisrupt essential spatial-temporal positional embeddings, failing to adequately\nbalance computational efficiency with fewer tokens. Consequently, these methods\nresult in relatively lengthy token sequences, limiting their applicability in\nscenarios requiring extreme token compression, such as video large language\nmodels. In this paper, we introduce the novel task of extreme short token\nreduction, aiming to represent extensive video sequences with a minimal number\nof tokens. To address this challenge, we propose Token Dynamics, a new video\nrepresentation framework that dynamically reduces token count while preserving\nspatial-temporal coherence. Specifically, we disentangle video representations\nby separating visual embeddings from grid-level motion information, structuring\nthem into: 1. a concise token base, created by clustering tokens that describe\nobject-level content; 2. a token dynamics map, capturing detailed\nspatial-temporal motion patterns across grids. Furthermore, we introduce a\ncross-dynamics attention mechanism that integrates motion features into the\ntoken base without increasing token length, thereby maintaining compactness and\nspatial-temporal integrity. The experiments demonstrate a reduction of token\ncount to merely 0.07% of the original tokens, with only a minor performance\ndrop of 1.13%. Additionally, we propose two novel subtasks within extreme token\nreduction (fixed-length and adaptive-length compression), both effectively\nrepresenting long token sequences for video-language tasks. Our method offers\nsignificantly lower theoretical complexity, fewer tokens, and enhanced\nthroughput, thus providing an efficient solution for video LLMs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Haichao Zhang",
      "Zhuowei Li",
      "Dimitris Metaxas",
      "Yun Fu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16979v1",
    "title": "Instant Gaussian Stream: Fast and Generalizable Streaming of Dynamic Scene Reconstruction via Gaussian Splatting",
    "abstract": "Building Free-Viewpoint Videos in a streaming manner offers the advantage of\nrapid responsiveness compared to offline training methods, greatly enhancing\nuser experience. However, current streaming approaches face challenges of high\nper-frame reconstruction time (10s+) and error accumulation, limiting their\nbroader application. In this paper, we propose Instant Gaussian Stream (IGS), a\nfast and generalizable streaming framework, to address these issues. First, we\nintroduce a generalized Anchor-driven Gaussian Motion Network, which projects\nmulti-view 2D motion features into 3D space, using anchor points to drive the\nmotion of all Gaussians. This generalized Network generates the motion of\nGaussians for each target frame in the time required for a single inference.\nSecond, we propose a Key-frame-guided Streaming Strategy that refines each key\nframe, enabling accurate reconstruction of temporally complex scenes while\nmitigating error accumulation. We conducted extensive in-domain and\ncross-domain evaluations, demonstrating that our approach can achieve streaming\nwith a average per-frame reconstruction time of 2s+, alongside a enhancement in\nview synthesis quality.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jinbo Yan",
      "Rui Peng",
      "Zhiyan Wang",
      "Luyang Tang",
      "Jiayu Yang",
      "Jie Liang",
      "Jiahao Wu",
      "Ronggang Wang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16978v1",
    "title": "Real-Time Diffusion Policies for Games: Enhancing Consistency Policies with Q-Ensembles",
    "abstract": "Diffusion models have shown impressive performance in capturing complex and\nmulti-modal action distributions for game agents, but their slow inference\nspeed prevents practical deployment in real-time game environments. While\nconsistency models offer a promising approach for one-step generation, they\noften suffer from training instability and performance degradation when applied\nto policy learning. In this paper, we present CPQE (Consistency Policy with\nQ-Ensembles), which combines consistency models with Q-ensembles to address\nthese challenges.CPQE leverages uncertainty estimation through Q-ensembles to\nprovide more reliable value function approximations, resulting in better\ntraining stability and improved performance compared to classic double\nQ-network methods. Our extensive experiments across multiple game scenarios\ndemonstrate that CPQE achieves inference speeds of up to 60 Hz -- a significant\nimprovement over state-of-the-art diffusion policies that operate at only 20 Hz\n-- while maintaining comparable performance to multi-step diffusion approaches.\nCPQE consistently outperforms state-of-the-art consistency model approaches,\nshowing both higher rewards and enhanced training stability throughout the\nlearning process. These results indicate that CPQE offers a practical solution\nfor deploying diffusion-based policies in games and other real-time\napplications where both multi-modal behavior modeling and rapid inference are\ncritical requirements.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Ruoqi Zhang",
      "Ziwei Luo",
      "Jens Sjölund",
      "Per Mattsson",
      "Linus Gisslén",
      "Alessandro Sestini"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16976v1",
    "title": "GeoT: Geometry-guided Instance-dependent Transition Matrix for Semi-supervised Tooth Point Cloud Segmentation",
    "abstract": "Achieving meticulous segmentation of tooth point clouds from intra-oral scans\nstands as an indispensable prerequisite for various orthodontic applications.\nGiven the labor-intensive nature of dental annotation, a significant amount of\ndata remains unlabeled, driving increasing interest in semi-supervised\napproaches. One primary challenge of existing semi-supervised medical\nsegmentation methods lies in noisy pseudo labels generated for unlabeled data.\nTo address this challenge, we propose GeoT, the first framework that employs\ninstance-dependent transition matrix (IDTM) to explicitly model noise in pseudo\nlabels for semi-supervised dental segmentation. Specifically, to handle the\nextensive solution space of IDTM arising from tens of thousands of dental\npoints, we introduce tooth geometric priors through two key components:\npoint-level geometric regularization (PLGR) to enhance consistency between\npoint adjacency relationships in 3D and IDTM spaces, and class-level geometric\nsmoothing (CLGS) to leverage the fixed spatial distribution of tooth categories\nfor optimal IDTM estimation. Extensive experiments performed on the public\nTeeth3DS dataset and private dataset demonstrate that our method can make full\nutilization of unlabeled data to facilitate segmentation, achieving performance\ncomparable to fully supervised methods with only $20\\%$ of the labeled data.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Weihao Yu",
      "Xiaoqing Guo",
      "Chenxin Li",
      "Yifan Liu",
      "Yixuan Yuan"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16975v1",
    "title": "EasyRobust: A Comprehensive and Easy-to-use Toolkit for Robust and Generalized Vision",
    "abstract": "Deep neural networks (DNNs) has shown great promise in computer vision tasks.\nHowever, machine vision achieved by DNNs cannot be as robust as human\nperception. Adversarial attacks and data distribution shifts have been known as\ntwo major scenarios which degrade machine performance and obstacle the wide\ndeployment of machines \"in the wild\". In order to break these obstructions and\nfacilitate the research of model robustness, we develop EasyRobust, a\ncomprehensive and easy-to-use toolkit for training, evaluation and analysis of\nrobust vision models. EasyRobust targets at two types of robustness: 1)\nAdversarial robustness enables the model to defense against malicious inputs\ncrafted by worst-case perturbations, also known as adversarial examples; 2)\nNon-adversarial robustness enhances the model performance on natural test\nimages with corruptions or distribution shifts. Thorough benchmarks on image\nclassification enable EasyRobust to provide an accurate robustness evaluation\non vision models. We wish our EasyRobust can help for training\npractically-robust models and promote academic and industrial progress in\nclosing the gap between human and machine vision. Codes and models of\nEasyRobust have been open-sourced in https://github.com/alibaba/easyrobust.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Xiaofeng Mao",
      "Yuefeng Chen",
      "Rong Zhang",
      "Hui Xue",
      "Zhao Li",
      "Hang Su"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16974v1",
    "title": "Assessing Consistency and Reproducibility in the Outputs of Large Language Models: Evidence Across Diverse Finance and Accounting Tasks",
    "abstract": "This study provides the first comprehensive assessment of consistency and\nreproducibility in Large Language Model (LLM) outputs in finance and accounting\nresearch. We evaluate how consistently LLMs produce outputs given identical\ninputs through extensive experimentation with 50 independent runs across five\ncommon tasks: classification, sentiment analysis, summarization, text\ngeneration, and prediction. Using three OpenAI models (GPT-3.5-turbo,\nGPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse\nfinancial source texts and data, covering MD&As, FOMC statements, finance news\narticles, earnings call transcripts, and financial statements. Our findings\nreveal substantial but task-dependent consistency, with binary classification\nand sentiment analysis achieving near-perfect reproducibility, while complex\ntasks show greater variability. More advanced models do not consistently\ndemonstrate better consistency and reproducibility, with task-specific patterns\nemerging. LLMs significantly outperform expert human annotators in consistency\nand maintain high agreement even where human experts significantly disagree. We\nfurther find that simple aggregation strategies across 3-5 runs dramatically\nimprove consistency. Simulation analysis reveals that despite measurable\ninconsistency in LLM outputs, downstream statistical inferences remain\nremarkably robust. These findings address concerns about what we term\n\"G-hacking,\" the selective reporting of favorable outcomes from multiple\nGenerative AI runs, by demonstrating that such risks are relatively low for\nfinance and accounting tasks.",
    "categories": [
      "q-fin.GN",
      "cs.AI",
      "cs.CE",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Julian Junyan Wang",
      "Victor Xiaoqi Wang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16973v1",
    "title": "ARFlow: Human Action-Reaction Flow Matching with Physical Guidance",
    "abstract": "Human action-reaction synthesis, a fundamental challenge in modeling causal\nhuman interactions, plays a critical role in applications ranging from virtual\nreality to social robotics. While diffusion-based models have demonstrated\npromising performance, they exhibit two key limitations for interaction\nsynthesis: reliance on complex noise-to-reaction generators with intricate\nconditional mechanisms, and frequent physical violations in generated motions.\nTo address these issues, we propose Action-Reaction Flow Matching (ARFlow), a\nnovel framework that establishes direct action-to-reaction mappings,\neliminating the need for complex conditional mechanisms. Our approach\nintroduces two key innovations: an x1-prediction method that directly outputs\nhuman motions instead of velocity fields, enabling explicit constraint\nenforcement; and a training-free, gradient-based physical guidance mechanism\nthat effectively prevents body penetration artifacts during sampling. Extensive\nexperiments on NTU120 and Chi3D datasets demonstrate that ARFlow not only\noutperforms existing methods in terms of Fr\\'echet Inception Distance and\nmotion diversity but also significantly reduces body collisions, as measured by\nour new Intersection Volume and Intersection Frequency metrics.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Wentao Jiang",
      "Jingya Wang",
      "Haotao Lu",
      "Kaiyang Ji",
      "Baoxiong Jia",
      "Siyuan Huang",
      "Ye Shi"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16970v1",
    "title": "Distilling Monocular Foundation Model for Fine-grained Depth Completion",
    "abstract": "Depth completion involves predicting dense depth maps from sparse LiDAR\ninputs. However, sparse depth annotations from sensors limit the availability\nof dense supervision, which is necessary for learning detailed geometric\nfeatures. In this paper, we propose a two-stage knowledge distillation\nframework that leverages powerful monocular foundation models to provide dense\nsupervision for depth completion. In the first stage, we introduce a\npre-training strategy that generates diverse training data from natural images,\nwhich distills geometric knowledge to depth completion. Specifically, we\nsimulate LiDAR scans by utilizing monocular depth and mesh reconstruction,\nthereby creating training data without requiring ground-truth depth. Besides,\nmonocular depth estimation suffers from inherent scale ambiguity in real-world\nsettings. To address this, in the second stage, we employ a scale- and\nshift-invariant loss (SSI Loss) to learn real-world scales when fine-tuning on\nreal-world datasets. Our two-stage distillation framework enables depth\ncompletion models to harness the strengths of monocular foundation models.\nExperimental results demonstrate that models trained with our two-stage\ndistillation framework achieve state-of-the-art performance, ranking\n\\textbf{first place} on the KITTI benchmark. Code is available at\nhttps://github.com/Sharpiless/DMD3C",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yingping Liang",
      "Yutao Hu",
      "Wenqi Shao",
      "Ying Fu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16965v1",
    "title": "When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only Training For Human-Centered Decision Making",
    "abstract": "Embodied decision-making is fundamental for AI agents operating in real-world\nenvironments. While Visual Language Models (VLMs) have advanced this\ncapability, they still struggle with complex decisions, particularly in\nhuman-centered situations that require deep reasoning about human needs and\nvalues. In this study, we systematically evaluate open-sourced VLMs on\nmultimodal human-centered decision-making tasks. We find that LLMs receiving\nonly textual descriptions unexpectedly outperform their VLM counterparts of\nsimilar scale that process actual images, suggesting that visual alignment may\nhinder VLM abilities. To address this challenge, we propose a novel text-only\ntraining approach with synthesized textual data. This method strengthens VLMs'\nlanguage components and transfers the learned abilities to multimodal\ninference, eliminating the need for expensive image-text paired data.\nFurthermore, we show that VLMs can achieve substantial performance gains\nthrough self-improvement, using training data generated by their LLM\ncounterparts rather than relying on larger teacher models like GPT-4. Our\nfindings establish a more efficient and scalable approach to enhancing VLMs'\nhuman-centered decision-making capabilities, opening new avenues for optimizing\nVLMs through self-improvement mechanisms.",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "authors": [
      "Zhe Hu",
      "Jing Li",
      "Yu Yin"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16964v1",
    "title": "DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from In-the-Wild Drone Imagery",
    "abstract": "Drones have become essential tools for reconstructing wild scenes due to\ntheir outstanding maneuverability. Recent advances in radiance field methods\nhave achieved remarkable rendering quality, providing a new avenue for 3D\nreconstruction from drone imagery. However, dynamic distractors in wild\nenvironments challenge the static scene assumption in radiance fields, while\nlimited view constraints hinder the accurate capture of underlying scene\ngeometry. To address these challenges, we introduce DroneSplat, a novel\nframework designed for robust 3D reconstruction from in-the-wild drone imagery.\nOur method adaptively adjusts masking thresholds by integrating local-global\nsegmentation heuristics with statistical approaches, enabling precise\nidentification and elimination of dynamic distractors in static scenes. We\nenhance 3D Gaussian Splatting with multi-view stereo predictions and a\nvoxel-guided optimization strategy, supporting high-quality rendering under\nlimited view constraints. For comprehensive evaluation, we provide a\ndrone-captured 3D reconstruction dataset encompassing both dynamic and static\nscenes. Extensive experiments demonstrate that DroneSplat outperforms both 3DGS\nand NeRF baselines in handling in-the-wild drone imagery.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jiadong Tang",
      "Yu Gao",
      "Dianyi Yang",
      "Liqi Yan",
      "Yufeng Yue",
      "Yi Yang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16963v1",
    "title": "Center-guided Classifier for Semantic Segmentation of Remote Sensing Images",
    "abstract": "Compared with natural images, remote sensing images (RSIs) have the unique\ncharacteristic. i.e., larger intraclass variance, which makes semantic\nsegmentation for remote sensing images more challenging. Moreover, existing\nsemantic segmentation models for remote sensing images usually employ a vanilla\nsoftmax classifier, which has three drawbacks: (1) non-direct supervision for\nthe pixel representations during training; (2) inadequate modeling ability of\nparametric softmax classifiers under large intraclass variance; and (3) opaque\nprocess of classification decision. In this paper, we propose a novel\nclassifier (called CenterSeg) customized for RSI semantic segmentation, which\nsolves the abovementioned problems with multiple prototypes, direct supervision\nunder Grassmann manifold, and interpretability strategy. Specifically, for each\nclass, our CenterSeg obtains local class centers by aggregating corresponding\npixel features based on ground-truth masks, and generates multiple prototypes\nthrough hard attention assignment and momentum updating. In addition, we\nintroduce the Grassmann manifold and constrain the joint embedding space of\npixel features and prototypes based on two additional regularization terms.\nEspecially, during the inference, CenterSeg can further provide\ninterpretability to the model by restricting the prototype as a sample of the\ntraining set. Experimental results on three remote sensing segmentation\ndatasets validate the effectiveness of the model. Besides the superior\nperformance, CenterSeg has the advantages of simplicity, lightweight,\ncompatibility, and interpretability. Code is available at\nhttps://github.com/xwmaxwma/rssegmentation.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Wei Zhang",
      "Mengting Ma",
      "Yizhen Jiang",
      "Rongrong Lian",
      "Zhenkai Wu",
      "Kangning Cui",
      "Xiaowen Ma"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16957v1",
    "title": "Uncertainty-Driven Modeling of Microporosity and Permeability in Clastic Reservoirs Using Random Forest",
    "abstract": "Predicting microporosity and permeability in clastic reservoirs is a\nchallenge in reservoir quality assessment, especially in formations where\ndirect measurements are difficult or expensive. These reservoir properties are\nfundamental in determining a reservoir's capacity for fluid storage and\ntransmission, yet conventional methods for evaluating them, such as Mercury\nInjection Capillary Pressure (MICP) and Scanning Electron Microscopy (SEM), are\nresource-intensive. The aim of this study is to develop a cost-effective\nmachine learning model to predict complex reservoir properties using readily\navailable field data and basic laboratory analyses. A Random Forest classifier\nwas employed, utilizing key geological parameters such as porosity, grain size\ndistribution, and spectral gamma-ray (SGR) measurements. An uncertainty\nanalysis was applied to account for natural variability, expanding the dataset,\nand enhancing the model's robustness. The model achieved a high level of\naccuracy in predicting microporosity (93%) and permeability levels (88%). By\nusing easily obtainable data, this model reduces the reliance on expensive\nlaboratory methods, making it a valuable tool for early-stage exploration,\nespecially in remote or offshore environments. The integration of machine\nlearning with uncertainty analysis provides a reliable and cost-effective\napproach for evaluating key reservoir properties in siliciclastic formations.\nThis model offers a practical solution to improve reservoir quality\nassessments, enabling more informed decision-making and optimizing exploration\nefforts.",
    "categories": [
      "physics.geo-ph",
      "cs.LG"
    ],
    "authors": [
      "Muhammad Risha",
      "Mohamed Elsaadany",
      "Paul Liu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16956v1",
    "title": "From Faces to Voices: Learning Hierarchical Representations for High-quality Video-to-Speech",
    "abstract": "The objective of this study is to generate high-quality speech from silent\ntalking face videos, a task also known as video-to-speech synthesis. A\nsignificant challenge in video-to-speech synthesis lies in the substantial\nmodality gap between silent video and multi-faceted speech. In this paper, we\npropose a novel video-to-speech system that effectively bridges this modality\ngap, significantly enhancing the quality of synthesized speech. This is\nachieved by learning of hierarchical representations from video to speech.\nSpecifically, we gradually transform silent video into acoustic feature spaces\nthrough three sequential stages -- content, timbre, and prosody modeling. In\neach stage, we align visual factors -- lip movements, face identity, and facial\nexpressions -- with corresponding acoustic counterparts to ensure the seamless\ntransformation. Additionally, to generate realistic and coherent speech from\nthe visual representations, we employ a flow matching model that estimates\ndirect trajectories from a simple prior distribution to the target speech\ndistribution. Extensive experiments demonstrate that our method achieves\nexceptional generation quality comparable to real utterances, outperforming\nexisting methods by a significant margin.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CV",
      "cs.SD"
    ],
    "authors": [
      "Ji-Hoon Kim",
      "Jeongsoo Choi",
      "Jaehun Kim",
      "Chaeyoung Jung",
      "Joon Son Chung"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16953v1",
    "title": "Neural-Guided Equation Discovery",
    "abstract": "Deep learning approaches are becoming increasingly attractive for equation\ndiscovery. We show the advantages and disadvantages of using neural-guided\nequation discovery by giving an overview of recent papers and the results of\nexperiments using our modular equation discovery system MGMT\n($\\textbf{M}$ulti-Task $\\textbf{G}$rammar-Guided $\\textbf{M}$onte-Carlo\n$\\textbf{T}$ree Search for Equation Discovery). The system uses neural-guided\nMonte-Carlo Tree Search (MCTS) and supports both supervised and reinforcement\nlearning, with a search space defined by a context-free grammar. We summarize\nseven desirable properties of equation discovery systems, emphasizing the\nimportance of embedding tabular data sets for such learning approaches. Using\nthe modular structure of MGMT, we compare seven architectures (among them,\nRNNs, CNNs, and Transformers) for embedding tabular datasets on the auxiliary\ntask of contrastive learning for tabular data sets on an equation discovery\ntask. For almost all combinations of modules, supervised learning outperforms\nreinforcement learning. Moreover, our experiments indicate an advantage of\nusing grammar rules as action space instead of tokens. Two adaptations of MCTS\n-- risk-seeking MCTS and AmEx-MCTS -- can improve equation discovery with that\nkind of search.",
    "categories": [
      "cs.AI",
      "I.2.6; I.1.1; G.3"
    ],
    "authors": [
      "Jannis Brugger",
      "Mattia Cerrato",
      "David Richter",
      "Cedric Derstroff",
      "Daniel Maninger",
      "Mira Mezini",
      "Stefan Kramer"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16948v1",
    "title": "MagicColor: Multi-Instance Sketch Colorization",
    "abstract": "We present \\textit{MagicColor}, a diffusion-based framework for\nmulti-instance sketch colorization. The production of multi-instance 2D line\nart colorization adheres to an industry-standard workflow, which consists of\nthree crucial stages: the design of line art characters, the coloring of\nindividual objects, and the refinement process. The artists are required to\nrepeat the process of coloring each instance one by one, which is inaccurate\nand inefficient. Meanwhile, current generative methods fail to solve this task\ndue to the challenge of multi-instance pair data collection. To tackle these\nchallenges, we incorporate three technical designs to ensure precise character\ndetail transcription and achieve multi-instance sketch colorization in a single\nforward. Specifically, we first propose the self-play training strategy to\nsolve the lack of training data. Then we introduce an instance guider to feed\nthe color of the instance. To achieve accurate color matching, we present\nfine-grained color matching with edge loss to enhance visual quality. Equipped\nwith the proposed modules, MagicColor enables automatically transforming\nsketches into vividly-colored images with accurate consistency and\nmulti-instance control. Experiments on our collected datasets show that our\nmodel outperforms existing methods regarding chromatic precision. Specifically,\nour model critically automates the colorization process with zero manual\nadjustments, so novice users can produce stylistically consistent artwork by\nproviding reference instances and the original line art. Our code and\nadditional details are available at https://yinhan-zhang.github.io/color",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yinhan Zhang",
      "Yue Ma",
      "Bingyuan Wang",
      "Qifeng Chen",
      "Zeyu Wang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16945v1",
    "title": "PE-CLIP: A Parameter-Efficient Fine-Tuning of Vision Language Models for Dynamic Facial Expression Recognition",
    "abstract": "Vision-Language Models (VLMs) like CLIP offer promising solutions for Dynamic\nFacial Expression Recognition (DFER) but face challenges such as inefficient\nfull fine-tuning, high complexity, and poor alignment between textual and\nvisual representations. Additionally, existing methods struggle with\nineffective temporal modeling. To address these issues, we propose PE-CLIP, a\nparameter-efficient fine-tuning (PEFT) framework that adapts CLIP for DFER\nwhile significantly reducing trainable parameters while maintaining high\naccuracy. PE-CLIP introduces two specialized adapters: a Temporal Dynamic\nAdapter (TDA) and a Shared Adapter (ShA). The TDA is a GRU-based module with\ndynamic scaling that captures sequential dependencies while emphasizing\ninformative temporal features and suppressing irrelevant variations. The ShA is\na lightweight adapter that refines representations within both textual and\nvisual encoders, ensuring consistency and efficiency. Additionally, we\nintegrate Multi-modal Prompt Learning (MaPLe), introducing learnable prompts\nfor visual and action unit-based textual inputs, enhancing semantic alignment\nbetween modalities and enabling efficient CLIP adaptation for dynamic tasks. We\nevaluate PE-CLIP on two benchmark datasets, DFEW and FERV39K, achieving\ncompetitive performance compared to state-of-the-art methods while requiring\nfewer trainable parameters. By balancing efficiency and accuracy, PE-CLIP sets\na new benchmark in resource-efficient DFER. The source code of the proposed\nPE-CLIP will be publicly available at https://github.com/Ibtissam-SAADI/PE-CLIP .",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Ibtissam Saadi",
      "Abdenour Hadid",
      "Douglas W. Cunningham",
      "Abdelmalik Taleb-Ahmed",
      "Yassin El Hillali"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16944v1",
    "title": "HyperLoRA: Parameter-Efficient Adaptive Generation for Portrait Synthesis",
    "abstract": "Personalized portrait synthesis, essential in domains like social\nentertainment, has recently made significant progress. Person-wise fine-tuning\nbased methods, such as LoRA and DreamBooth, can produce photorealistic outputs\nbut need training on individual samples, consuming time and resources and\nposing an unstable risk. Adapter based techniques such as IP-Adapter freeze the\nfoundational model parameters and employ a plug-in architecture to enable\nzero-shot inference, but they often exhibit a lack of naturalness and\nauthenticity, which are not to be overlooked in portrait synthesis tasks. In\nthis paper, we introduce a parameter-efficient adaptive generation method,\nnamely HyperLoRA, that uses an adaptive plug-in network to generate LoRA\nweights, merging the superior performance of LoRA with the zero-shot capability\nof adapter scheme. Through our carefully designed network structure and\ntraining strategy, we achieve zero-shot personalized portrait generation\n(supporting both single and multiple image inputs) with high photorealism,\nfidelity, and editability.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Mengtian Li",
      "Jinshu Chen",
      "Wanquan Feng",
      "Bingchuan Li",
      "Fei Dai",
      "Songtao Zhao",
      "Qian He"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16943v1",
    "title": "Model-free front-to-end training of a large high performance laser neural network",
    "abstract": "Artificial neural networks (ANNs), have become ubiquitous and revolutionized\nmany applications ranging from computer vision to medical diagnoses. However,\nthey offer a fundamentally connectionist and distributed approach to computing,\nin stark contrast to classical computers that use the von Neumann architecture.\nThis distinction has sparked renewed interest in developing unconventional\nhardware to support more efficient implementations of ANNs, rather than merely\nemulating them on traditional systems. Photonics stands out as a particularly\npromising platform, providing scalability, high speed, energy efficiency, and\nthe ability for parallel information processing. However, fully realized\nautonomous optical neural networks (ONNs) with in-situ learning capabilities\nare still rare. In this work, we demonstrate a fully autonomous and parallel\nONN using a multimode vertical cavity surface emitting laser (VCSEL) using\noff-the-shelf components. Our ONN is highly efficient and is scalable both in\nnetwork size and inference bandwidth towards the GHz range. High performance\nhardware-compatible optimization algorithms are necessary in order to minimize\nreliance on external von Neumann computers to fully exploit the potential of\nONNs. As such we present and extensively study several algorithms which are\nbroadly compatible with a wide range of systems. We then apply these algorithms\nto optimize our ONN, and benchmark them using the MNIST dataset. We show that\nour ONN can achieve high accuracy and convergence efficiency, even under\nlimited hardware resources. Crucially, we compare these different algorithms in\nterms of scaling and optimization efficiency in term of convergence time which\nis crucial when working with limited external resources. Our work provides some\nguidance for the design of future ONNs as well as a simple and flexible way to\ntrain them.",
    "categories": [
      "cs.LG",
      "cs.ET"
    ],
    "authors": [
      "Anas Skalli",
      "Satoshi Sunada",
      "Mirko Goldmann",
      "Marcin Gebski",
      "Stephan Reitzenstein",
      "James A. Lott",
      "Tomasz Czyszanowski",
      "Daniel Brunner"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16942v1",
    "title": "Re-HOLD: Video Hand Object Interaction Reenactment via adaptive Layout-instructed Diffusion Model",
    "abstract": "Current digital human studies focusing on lip-syncing and body movement are\nno longer sufficient to meet the growing industrial demand, while human video\ngeneration techniques that support interacting with real-world environments\n(e.g., objects) have not been well investigated. Despite human hand synthesis\nalready being an intricate problem, generating objects in contact with hands\nand their interactions presents an even more challenging task, especially when\nthe objects exhibit obvious variations in size and shape. To cope with these\nissues, we present a novel video Reenactment framework focusing on Human-Object\nInteraction (HOI) via an adaptive Layout-instructed Diffusion model (Re-HOLD).\nOur key insight is to employ specialized layout representation for hands and\nobjects, respectively. Such representations enable effective disentanglement of\nhand modeling and object adaptation to diverse motion sequences. To further\nimprove the generation quality of HOI, we have designed an interactive textural\nenhancement module for both hands and objects by introducing two independent\nmemory banks. We also propose a layout-adjusting strategy for the cross-object\nreenactment scenario to adaptively adjust unreasonable layouts caused by\ndiverse object sizes during inference. Comprehensive qualitative and\nquantitative evaluations demonstrate that our proposed framework significantly\noutperforms existing methods. Project page: https://fyycs.github.io/Re-HOLD.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yingying Fan",
      "Quanwei Yang",
      "Kaisiyuan Wang",
      "Hang Zhou",
      "Yingying Li",
      "Haocheng Feng",
      "Yu Wu",
      "Jingdong Wang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16941v1",
    "title": "Sparse Additive Contextual Bandits: A Nonparametric Approach for Online Decision-making with High-dimensional Covariates",
    "abstract": "Personalized services are central to today's digital landscape, where online\ndecision-making is commonly formulated as contextual bandit problems. Two key\nchallenges emerge in modern applications: high-dimensional covariates and the\nneed for nonparametric models to capture complex reward-covariate\nrelationships. We address these challenges by developing a contextual bandit\nalgorithm based on sparse additive reward models in reproducing kernel Hilbert\nspaces. We establish statistical properties of the doubly penalized method\napplied to random regions, introducing novel analyses under bandit feedback.\nOur algorithm achieves sublinear cumulative regret over the time horizon $T$\nwhile scaling logarithmically with covariate dimensionality $d$. Notably, we\nprovide the first regret upper bound with logarithmic growth in $d$ for\nnonparametric contextual bandits with high-dimensional covariates. We also\nestablish a lower bound, with the gap to the upper bound vanishing as\nsmoothness increases. Extensive numerical experiments demonstrate our\nalgorithm's superior performance in high-dimensional settings compared to\nexisting approaches.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "authors": [
      "Wenjia Wang",
      "Qingwen Zhang",
      "Xiaowei Zhang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16939v1",
    "title": "On-Sensor Convolutional Neural Networks with Early-Exits",
    "abstract": "Tiny Machine Learning (TinyML) is a novel research field aiming at\nintegrating Machine Learning (ML) within embedded devices with limited memory,\ncomputation, and energy. Recently, a new branch of TinyML has emerged, focusing\non integrating ML directly into the sensors to further reduce the power\nconsumption of embedded devices. Interestingly, despite their state-of-the-art\nperformance in many tasks, none of the current solutions in the literature aims\nto optimize the implementation of Convolutional Neural Networks (CNNs)\noperating directly into sensors. In this paper, we introduce for the first time\nin the literature the optimized design and implementation of Depth-First CNNs\noperating on the Intelligent Sensor Processing Unit (ISPU) within an Inertial\nMeasurement Unit (IMU) by STMicroelectronics. Our approach partitions the CNN\nbetween the ISPU and the microcontroller (MCU) and employs an Early-Exit\nmechanism to stop the computations on the IMU when enough confidence about the\nresults is achieved, hence significantly reducing power consumption. When using\na NUCLEO-F411RE board, this solution achieved an average current consumption of\n4.8 mA, marking an 11% reduction compared to the regular inference pipeline on\nthe MCU, while having equal accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Hazem Hesham Yousef Shalby",
      "Arianna De Vecchi",
      "Alice Scandelli",
      "Pietro Bartoli",
      "Diana Trojaniello",
      "Manuel Roveri",
      "Federica Villa"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16938v1",
    "title": "Interpretable Machine Learning for Oral Lesion Diagnosis through Prototypical Instances Identification",
    "abstract": "Decision-making processes in healthcare can be highly complex and\nchallenging. Machine Learning tools offer significant potential to assist in\nthese processes. However, many current methodologies rely on complex models\nthat are not easily interpretable by experts. This underscores the need to\ndevelop interpretable models that can provide meaningful support in clinical\ndecision-making. When approaching such tasks, humans typically compare the\nsituation at hand to a few key examples and representative cases imprinted in\ntheir memory. Using an approach which selects such exemplary cases and grounds\nits predictions on them could contribute to obtaining high-performing\ninterpretable solutions to such problems. To this end, we evaluate PivotTree,\nan interpretable prototype selection model, on an oral lesion detection\nproblem, specifically trying to detect the presence of neoplastic, aphthous and\ntraumatic ulcerated lesions from oral cavity images. We demonstrate the\nefficacy of using such method in terms of performance and offer a qualitative\nand quantitative comparison between exemplary cases and ground-truth prototypes\nselected by experts.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Alessio Cascione",
      "Mattia Setzu",
      "Federico A. Galatolo",
      "Mario G. C. A. Cimino",
      "Riccardo Guidotti"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16932v1",
    "title": "Rude Humans and Vengeful Robots: Examining Human Perceptions of Robot Retaliatory Intentions in Professional Settings",
    "abstract": "Humans and robots are increasingly working in personal and professional\nsettings. In workplace settings, humans and robots may work together as\ncolleagues, potentially leading to social expectations, or violation thereof.\nExtant research has primarily sought to understand social interactions and\nexpectations in personal rather than professional settings, and none of these\nstudies have examined negative outcomes arising from violations of social\nexpectations. This paper reports the results of a 2x3 online experiment that\nused a unique first-person perspective video to immerse participants in a\ncollaborative workplace setting. The results are nuanced and reveal that while\nrobots are expected to act in accordance with social expectations despite human\nbehavior, there are benefits for robots perceived as being the bigger person in\nthe face of human rudeness. Theoretical and practical implications are provided\nwhich discuss the import of these findings for the design of social robots.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Kate Letheren",
      "Nicole Robinson"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16930v1",
    "title": "Vision-Language Gradient Descent-driven All-in-One Deep Unfolding Networks",
    "abstract": "Dynamic image degradations, including noise, blur and lighting\ninconsistencies, pose significant challenges in image restoration, often due to\nsensor limitations or adverse environmental conditions. Existing Deep Unfolding\nNetworks (DUNs) offer stable restoration performance but require manual\nselection of degradation matrices for each degradation type, limiting their\nadaptability across diverse scenarios. To address this issue, we propose the\nVision-Language-guided Unfolding Network (VLU-Net), a unified DUN framework for\nhandling multiple degradation types simultaneously. VLU-Net leverages a\nVision-Language Model (VLM) refined on degraded image-text pairs to align image\nfeatures with degradation descriptions, selecting the appropriate transform for\ntarget degradation. By integrating an automatic VLM-based gradient estimation\nstrategy into the Proximal Gradient Descent (PGD) algorithm, VLU-Net\neffectively tackles complex multi-degradation restoration tasks while\nmaintaining interpretability. Furthermore, we design a hierarchical feature\nunfolding structure to enhance VLU-Net framework, efficiently synthesizing\ndegradation patterns across various levels. VLU-Net is the first all-in-one DUN\nframework and outperforms current leading one-by-one and all-in-one end-to-end\nmethods by 3.74 dB on the SOTS dehazing dataset and 1.70 dB on the Rain100L\nderaining dataset.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Haijin Zeng",
      "Xiangming Wang",
      "Yongyong Chen",
      "Jingyong Su",
      "Jie Liu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16929v1",
    "title": "TEMPO: Temporal Preference Optimization of Video LLMs via Difficulty Scheduling and Pre-SFT Alignment",
    "abstract": "Video Large Language Models (Video LLMs) have achieved significant success by\nleveraging a two-stage paradigm: pretraining on large-scale video-text data for\nvision-language alignment, followed by supervised fine-tuning (SFT) for\ntask-specific capabilities. However, existing approaches struggle with temporal\nreasoning due to weak temporal correspondence in the data and reliance on the\nnext-token prediction paradigm during training. To address these limitations,\nwe propose TEMPO (TEMporal Preference Optimization), a systematic framework\nthat enhances Video LLMs' temporal reasoning capabilities through Direct\nPreference Optimization (DPO). To facilitate this, we introduce an automated\npreference data generation pipeline that systematically constructs preference\npairs by selecting videos that are rich in temporal information, designing\nvideo-specific perturbation strategies, and finally evaluating model responses\non clean and perturbed video inputs. Our temporal alignment features two key\ninnovations: curriculum learning which that progressively increases\nperturbation difficulty to improve model robustness and adaptability; and\n``Pre-SFT Alignment'', applying preference optimization before instruction\ntuning to prioritize fine-grained temporal comprehension. Extensive experiments\ndemonstrate that our approach consistently improves Video LLM performance\nacross multiple benchmarks with a relatively small set of self-generated DPO\ndata. We further analyze the transferability of DPO data across architectures\nand the role of difficulty scheduling in optimization. Our findings highlight\nour TEMPO as a scalable and efficient complement to SFT-based methods, paving\nthe way for developing reliable Video LLMs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Shicheng Li",
      "Lei Li",
      "Kun Ouyang",
      "Shuhuai Ren",
      "Yuanxin Liu",
      "Yuanxing Zhang",
      "Fuzheng Zhang",
      "Lingpeng Kong",
      "Qi Liu",
      "Xu Sun"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16928v1",
    "title": "MerGen: Micro-electrode recording synthesis using a generative data-driven approach",
    "abstract": "The analysis of electrophysiological data is crucial for certain surgical\nprocedures such as deep brain stimulation, which has been adopted for the\ntreatment of a variety of neurological disorders. During the procedure,\nauditory analysis of these signals helps the clinical team to infer the\nneuroanatomical location of the stimulation electrode and thus optimize\nclinical outcomes. This task is complex, and requires an expert who in turn\nrequires significant training. In this paper, we propose a generative neural\nnetwork, called MerGen, capable of simulating de novo electrophysiological\nrecordings, with a view to providing a realistic learning tool for clinicians\ntrainees for identifying these signals. We demonstrate that the generated\nsignals are perceptually indistinguishable from real signals by experts in the\nfield, and that it is even possible to condition the generation efficiently to\nprovide a didactic simulator adapted to a particular surgical scenario. The\nefficacy of this conditioning is demonstrated, comparing it to intra-observer\nand inter-observer variability amongst experts. We also demonstrate the use of\nthis network for data augmentation for automatic signal classification which\ncan play a role in decision-making support in the operating theatre.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Thibault Martin",
      "Paul Sauleau",
      "Claire Haegelen",
      "Pierre Jannin",
      "John S. H. Baxter"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16924v1",
    "title": "Optimized Minimal 3D Gaussian Splatting",
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for\nreal-time, high-performance rendering, enabling a wide range of applications.\nHowever, representing 3D scenes with numerous explicit Gaussian primitives\nimposes significant storage and memory overhead. Recent studies have shown that\nhigh-quality rendering can be achieved with a substantially reduced number of\nGaussians when represented with high-precision attributes. Nevertheless,\nexisting 3DGS compression methods still rely on a relatively large number of\nGaussians, focusing primarily on attribute compression. This is because a\nsmaller set of Gaussians becomes increasingly sensitive to lossy attribute\ncompression, leading to severe quality degradation. Since the number of\nGaussians is directly tied to computational costs, it is essential to reduce\nthe number of Gaussians effectively rather than only optimizing storage. In\nthis paper, we propose Optimized Minimal Gaussians representation (OMG), which\nsignificantly reduces storage while using a minimal number of primitives.\nFirst, we determine the distinct Gaussian from the near ones, minimizing\nredundancy without sacrificing quality. Second, we propose a compact and\nprecise attribute representation that efficiently captures both continuity and\nirregularity among primitives. Additionally, we propose a sub-vector\nquantization technique for improved irregularity representation, maintaining\nfast training with a negligible codebook size. Extensive experiments\ndemonstrate that OMG reduces storage requirements by nearly 50% compared to the\nprevious state-of-the-art and enables 600+ FPS rendering while maintaining high\nrendering quality. Our source code is available at\nhttps://maincold2.github.io/omg/.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Joo Chan Lee",
      "Jong Hwan Ko",
      "Eunbyung Park"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16922v1",
    "title": "RustEvo^2: An Evolving Benchmark for API Evolution in LLM-based Rust Code Generation",
    "abstract": "Large Language Models (LLMs) have become pivotal tools for automating code\ngeneration in software development. However, these models face significant\nchallenges in producing version-aware code for rapidly evolving languages like\nRust, where frequent Application Programming Interfaces (API) changes across\nversions lead to compatibility issues and correctness errors. Existing\nbenchmarks lack systematic evaluation of how models navigate API transitions,\nrelying on labor-intensive manual curation and offering limited\nversion-specific insights. To address this gap, we present RustEvo, a novel\nframework for constructing dynamic benchmarks that evaluate the ability of LLMs\nto adapt to evolving Rust APIs. RustEvo automates dataset creation by\nsynthesizing 588 API changes (380 from Rust standard libraries, 208 from 15\nthird-party crates) into programming tasks mirroring real-world challenges.\nThese tasks cover four API evolution categories: Stabilizations, Signature\nChanges, Behavioral Changes, and Deprecations, reflecting their actual\ndistribution in the Rust ecosystem.\n  Experiments on state-of-the-art (SOTA) LLMs reveal significant performance\nvariations: models achieve a 65.8% average success rate on stabilized APIs but\nonly 38.0% on behavioral changes, highlighting difficulties in detecting\nsemantic shifts without signature alterations. Knowledge cutoff dates strongly\ninfluence performance, with models scoring 56.1% on before-cutoff APIs versus\n32.5% on after-cutoff tasks. Retrieval-Augmented Generation (RAG) mitigates\nthis gap, improving success rates by 13.5% on average for APIs released after\nmodel training. Our findings underscore the necessity of our evolution-aware\nbenchmarks to advance the adaptability of LLMs in fast-paced software\necosystems. The framework and the benchmarks are publicly released at\nhttps://github.com/SYSUSELab/RustEvo.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Linxi Liang",
      "Jing Gong",
      "Mingwei Liu",
      "Chong Wang",
      "Guangsheng Ou",
      "Yanlin Wang",
      "Xin Peng",
      "Zibin Zheng"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16921v1",
    "title": "When Preferences Diverge: Aligning Diffusion Models with Minority-Aware Adaptive DPO",
    "abstract": "In recent years, the field of image generation has witnessed significant\nadvancements, particularly in fine-tuning methods that align models with\nuniversal human preferences. This paper explores the critical role of\npreference data in the training process of diffusion models, particularly in\nthe context of Diffusion-DPO and its subsequent adaptations. We investigate the\ncomplexities surrounding universal human preferences in image generation,\nhighlighting the subjective nature of these preferences and the challenges\nposed by minority samples in preference datasets. Through pilot experiments, we\ndemonstrate the existence of minority samples and their detrimental effects on\nmodel performance. We propose Adaptive-DPO -- a novel approach that\nincorporates a minority-instance-aware metric into the DPO objective. This\nmetric, which includes intra-annotator confidence and inter-annotator\nstability, distinguishes between majority and minority samples. We introduce an\nAdaptive-DPO loss function which improves the DPO loss in two ways: enhancing\nthe model's learning of majority labels while mitigating the negative impact of\nminority samples. Our experiments demonstrate that this method effectively\nhandles both synthetic minority data and real-world preference data, paving the\nway for more effective training methodologies in image generation tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Lingfan Zhang",
      "Chen Liu",
      "Chengming Xu",
      "Kai Hu",
      "Donghao Luo",
      "Chengjie Wang",
      "Yanwei Fu",
      "Yuan Yao"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16917v1",
    "title": "Malliavin-Bismut Score-based Diffusion Models",
    "abstract": "We introduce a new framework that employs Malliavin calculus to derive\nexplicit expressions for the score function -- i.e., the gradient of the\nlog-density -- associated with solutions to stochastic differential equations\n(SDEs). Our approach integrates classical integration-by-parts techniques with\nmodern tools, such as Bismut's formula and Malliavin calculus, to address\nlinear and nonlinear SDEs. In doing so, we establish a rigorous connection\nbetween the Malliavin derivative, its adjoint (the Malliavin divergence or the\nSkorokhod integral), Bismut's formula, and diffusion generative models, thus\nproviding a systematic method for computing $\\nabla \\log p_t(x)$. For the\nlinear case, we present a detailed study proving that our formula is equivalent\nto the actual score function derived from the solution of the Fokker--Planck\nequation for linear SDEs. Additionally, we derive a closed-form expression for\n$\\nabla \\log p_t(x)$ for nonlinear SDEs with state-independent diffusion\ncoefficients. These advancements provide fresh theoretical insights into the\nsmoothness and structure of probability densities and practical implications\nfor score-based generative modelling, including the design and analysis of new\ndiffusion models. Moreover, our findings promote the adoption of the robust\nMalliavin calculus framework in machine learning research. These results\ndirectly apply to various pure and applied mathematics fields, such as\ngenerative modelling, the study of SDEs driven by fractional Brownian motion,\nand the Fokker--Planck equations associated with nonlinear SDEs.",
    "categories": [
      "cs.LG",
      "math.PR"
    ],
    "authors": [
      "Ehsan Mirafzali",
      "Utkarsh Gupta",
      "Patrick Wyrod",
      "Frank Proske",
      "Daniele Venturi",
      "Razvan Marinescu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16916v1",
    "title": "Temporal Action Detection Model Compression by Progressive Block Drop",
    "abstract": "Temporal action detection (TAD) aims to identify and localize action\ninstances in untrimmed videos, which is essential for various video\nunderstanding tasks. However, recent improvements in model performance, driven\nby larger feature extractors and datasets, have led to increased computational\ndemands. This presents a challenge for applications like autonomous driving and\nrobotics, which rely on limited computational resources. While existing channel\npruning methods can compress these models, reducing the number of channels\noften hinders the parallelization efficiency of GPU, due to the inefficient\nmultiplication between small matrices. Instead of pruning channels, we propose\na Progressive Block Drop method that reduces model depth while retaining layer\nwidth. In this way, we still use large matrices for computation but reduce the\nnumber of multiplications. Our approach iteratively removes redundant blocks in\ntwo steps: first, we drop blocks with minimal impact on model performance; and\nsecond, we employ a parameter-efficient cross-depth alignment technique,\nfine-tuning the pruned model to restore model accuracy. Our method achieves a\n25% reduction in computational overhead on two TAD benchmarks (THUMOS14 and\nActivityNet-1.3) to achieve lossless compression. More critically, we\nempirically show that our method is orthogonal to channel pruning methods and\ncan be combined with it to yield further efficiency gains.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Xiaoyong Chen",
      "Yong Guo",
      "Jiaming Liang",
      "Sitong Zhuang",
      "Runhao Zeng",
      "Xiping Hu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16914v1",
    "title": "A New Segment Routing method with Swap Node Selection Strategy Based on Deep Reinforcement Learning for Software Defined Network",
    "abstract": "The existing segment routing (SR) methods need to determine the routing first\nand then use path segmentation approaches to select swap nodes to form a\nsegment routing path (SRP). They require re-segmentation of the path when the\nrouting changes. Furthermore, they do not consider the flow table issuance\ntime, which cannot maximize the speed of issuance flow table. To address these\nissues, this paper establishes an optimization model that can simultaneously\nform routing strategies and path segmentation strategies for selecting the\nappropriate swap nodes to reduce flow table issuance time. It also designs an\nintelligent segment routing algorithm based on deep reinforcement learning\n(DRL-SR) to solve the proposed model. First, a traffic matrix is designed as\nthe state space for the deep reinforcement learning agent; this matrix includes\nmultiple QoS performance indicators, flow table issuance time overhead and SR\nlabel stack depth. Second, the action selection strategy and corresponding\nreward function are designed, where the agent selects the next node considering\nthe routing; in addition, the action selection strategy whether the newly added\nnode is selected as the swap node and the corresponding reward function are\ndesigned considering the time cost factor for the controller to issue the flow\ntable to the swap node. Finally, a series of experiments and their results show\nthat, compared with the existing methods, the designed segmented route\noptimization model and the intelligent solution algorithm (DRL-SR) can reduce\nthe time overhead required to complete the segmented route establishment task\nwhile optimizing performance metrics such as throughput, delays and packet\nlosses.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Miao Ye",
      "Jihao Zheng",
      "Qiuxiang Jiang",
      "Yuan Huang",
      "Ziheng Wang",
      "Yong Wang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16910v1",
    "title": "Salient Object Detection in Traffic Scene through the TSOD10K Dataset",
    "abstract": "Traffic Salient Object Detection (TSOD) aims to segment the objects critical\nto driving safety by combining semantic (e.g., collision risks) and visual\nsaliency. Unlike SOD in natural scene images (NSI-SOD), which prioritizes\nvisually distinctive regions, TSOD emphasizes the objects that demand immediate\ndriver attention due to their semantic impact, even with low visual contrast.\nThis dual criterion, i.e., bridging perception and contextual risk, re-defines\nsaliency for autonomous and assisted driving systems. To address the lack of\ntask-specific benchmarks, we collect the first large-scale TSOD dataset with\npixel-wise saliency annotations, named TSOD10K. TSOD10K covers the diverse\nobject categories in various real-world traffic scenes under various\nchallenging weather/illumination variations (e.g., fog, snowstorms,\nlow-contrast, and low-light). Methodologically, we propose a Mamba-based TSOD\nmodel, termed Tramba. Considering the challenge of distinguishing inconspicuous\nvisual information from complex traffic backgrounds, Tramba introduces a novel\nDual-Frequency Visual State Space module equipped with shifted window\npartitioning and dilated scanning to enhance the perception of fine details and\nglobal structure by hierarchically decomposing high/low-frequency components.\nTo emphasize critical regions in traffic scenes, we propose a traffic-oriented\nHelix 2D-Selective-Scan (Helix-SS2D) mechanism that injects driving attention\npriors while effectively capturing global multi-direction spatial dependencies.\nWe establish a comprehensive benchmark by evaluating Tramba and 22 existing\nNSI-SOD models on TSOD10K, demonstrating Tramba's superiority. Our research\nestablishes the first foundation for safety-aware saliency analysis in\nintelligent transportation systems.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yu Qiu",
      "Yuhang Sun",
      "Jie Mei",
      "Lin Xiao",
      "Jing Xu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16905v1",
    "title": "MAPS: A Multi-Agent Framework Based on Big Seven Personality and Socratic Guidance for Multimodal Scientific Problem Solving",
    "abstract": "Multimodal scientific problems (MSPs) involve complex issues that require the\nintegration of multiple modalities, such as text and diagrams, presenting a\nsignificant challenge in artificial intelligence. While progress has been made\nin addressing traditional scientific problems, MSPs still face two primary\nissues: the challenge of multi-modal comprehensive reasoning in scientific\nproblem-solving and the lack of reflective and rethinking capabilities. To\naddress these issues, we introduce a Multi-Agent framework based on the Big\nSeven Personality and Socratic guidance (MAPS). This framework employs seven\ndistinct agents that leverage feedback mechanisms and the Socratic method to\nguide the resolution of MSPs. To tackle the first issue, we propose a\nprogressive four-agent solving strategy, where each agent focuses on a specific\nstage of the problem-solving process. For the second issue, we introduce a\nCritic agent, inspired by Socratic questioning, which prompts critical thinking\nand stimulates autonomous learning. We conduct extensive experiments on the\nEMMA, Olympiad, and MathVista datasets, achieving promising results that\noutperform the current SOTA model by 15.84% across all tasks. Meanwhile, the\nadditional analytical experiments also verify the model's progress as well as\ngeneralization ability.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Jian Zhang",
      "Zhiyuan Wang",
      "Zhangqi Wang",
      "Xinyu Zhang",
      "Fangzhi Xu",
      "Qika Lin",
      "Rui Mao",
      "Erik Cambria",
      "Jun Liu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16904v1",
    "title": "Deep Learning for Human Locomotion Analysis in Lower-Limb Exoskeletons: A Comparative Study",
    "abstract": "Wearable robotics for lower-limb assistance have become a pivotal area of\nresearch, aiming to enhance mobility for individuals with physical impairments\nor augment the performance of able-bodied users. Accurate and adaptive control\nsystems are essential to ensure seamless interaction between the wearer and the\nrobotic device, particularly when navigating diverse and dynamic terrains.\nDespite the recent advances in neural networks for time series analysis, no\nattempts have been directed towards the classification of ground conditions,\ncategorized into five classes and subsequently determining the ramp's slope and\nstair's height. In this respect, this paper presents an experimental comparison\nbetween eight deep neural network backbones to predict high-level locomotion\nparameters across diverse terrains.\n  All the models are trained on the publicly available CAMARGO 2021 dataset.\nIMU-only data equally or outperformed IMU+EMG inputs, promoting a\ncost-effective and efficient design. Indeeds, using three IMU sensors, the LSTM\nachieved high terrain classification accuracy (0.94 +- 0.04) and precise ramp\nslope (1.95 +- 0.58{\\deg}) and the CNN-LSTM a stair height (15.65 +- 7.40 mm)\nestimations. As a further contribution, SHAP analysis justified sensor\nreduction without performance loss, ensuring a lightweight setup. The system\noperates with ~2 ms inference time, supporting real-time applications. The code\nis code available at\nhttps://github.com/cosbidev/Human-Locomotion-Identification.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "F.2.2, I.2.7"
    ],
    "authors": [
      "Omar Coser",
      "Christian Tamantini",
      "Matteo Tortora",
      "Leonardo Furia",
      "Rosa Sicilia",
      "Loredana Zollo",
      "Paolo Soda"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16901v1",
    "title": "TeMP-TraG: Edge-based Temporal Message Passing in Transaction Graphs",
    "abstract": "Transaction graphs, which represent financial and trade transactions between\nentities such as bank accounts and companies, can reveal patterns indicative of\nfinancial crimes like money laundering and fraud. However, effective detection\nof such cases requires node and edge classification methods capable of\naddressing the unique challenges of transaction graphs, including rich edge\nfeatures, multigraph structures and temporal dynamics. To tackle these\nchallenges, we propose TeMP-TraG, a novel graph neural network mechanism that\nincorporates temporal dynamics into message passing. TeMP-TraG prioritises more\nrecent transactions when aggregating node messages, enabling better detection\nof time-sensitive patterns. We demonstrate that TeMP-TraG improves four\nstate-of-the-art graph neural networks by 6.19% on average. Our results\nhighlight TeMP-TraG as an advancement in leveraging transaction graphs to\ncombat financial crime.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Steve Gounoue",
      "Ashutosh Sao",
      "Simon Gottschalk"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16893v1",
    "title": "Improving the End-to-End Efficiency of Offline Inference for Multi-LLM Applications Based on Sampling and Simulation",
    "abstract": "As large language models (LLMs) have shown great success in many tasks, they\nare used in various applications. While a lot of works have focused on the\nefficiency of single-LLM application (e.g., offloading, request scheduling,\nparallelism strategy selection), multi-LLM applications receive less attention,\nparticularly in offline inference scenarios. In this work, we aim to improve\nthe offline end-to-end inference efficiency of multi-LLM applications in the\nsingle-node multi-GPU environment. The problem involves two key decisions: (1)\ndetermining which LLMs to run concurrently each time (we may not run all the\nmodels at the same time), and (2) selecting a parallelism strategy to use for\neach LLM. This problem is NP-hard. Naive solutions may not work well because\nthe running time for a model to complete a set of requests depends on the\nrequest workload and the selected parallelism strategy, and they lack an\naccurate model of the running time. As the LLM output lengths are unknown\nbefore running, to estimate the model running time, we propose a\nsampling-then-simulation method which first estimates the output lengths by\nsampling from an empirical cumulative function we obtained from a large dataset\nin advance, and then simulates the LLM inference process accordingly. Based on\nthe simulation, we estimate the per-iteration latencys to get the total\nlatency. A greedy method is proposed to optimize the scheduling of the LLMs in\nthe application across the GPUs. We then propose a framework SamuLLM which\ncontains two phases: planning, which calls the greedy method for an application\nand running, which runs the application and dynamically adjust the model\nscheduling based on the runtime information. Experiments on 3 applications and\na mixed application show that SamuLLM can achieve 1.0-2.4$\\times$ end-to-end\nspeedups compared to the competitors.",
    "categories": [
      "cs.DC",
      "cs.LG"
    ],
    "authors": [
      "Jingzhi Fang",
      "Yanyan Shen",
      "Yue Wang",
      "Lei Chen"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16883v1",
    "title": "Assessing the Reliability and Validity of GPT-4 in Annotating Emotion Appraisal Ratings",
    "abstract": "Appraisal theories suggest that emotions arise from subjective evaluations of\nevents, referred to as appraisals. The taxonomy of appraisals is quite diverse,\nand they are usually given ratings on a Likert scale to be annotated in an\nexperiencer-annotator or reader-annotator paradigm. This paper studies GPT-4 as\na reader-annotator of 21 specific appraisal ratings in different prompt\nsettings, aiming to evaluate and improve its performance compared to human\nannotators. We found that GPT-4 is an effective reader-annotator that performs\nclose to or even slightly better than human annotators, and its results can be\nsignificantly improved by using a majority voting of five completions. GPT-4\nalso effectively predicts appraisal ratings and emotion labels using a single\nprompt, but adding instruction complexity results in poorer performance. We\nalso found that longer event descriptions lead to more accurate annotations for\nboth model and human annotator ratings. This work contributes to the growing\nusage of LLMs in psychology and the strategies for improving GPT-4 performance\nin annotating appraisals.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Deniss Ruder",
      "Andero Uusberg",
      "Kairit Sirts"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16875v1",
    "title": "Federated Cross-Domain Click-Through Rate Prediction With Large Language Model Augmentation",
    "abstract": "Accurately predicting click-through rates (CTR) under stringent privacy\nconstraints poses profound challenges, particularly when user-item interactions\nare sparse and fragmented across domains. Conventional cross-domain CTR (CCTR)\nmethods frequently assume homogeneous feature spaces and rely on centralized\ndata sharing, neglecting complex inter-domain discrepancies and the subtle\ntrade-offs imposed by privacy-preserving protocols. Here, we present Federated\nCross-Domain CTR Prediction with Large Language Model Augmentation\n(FedCCTR-LM), a federated framework engineered to address these limitations by\nsynchronizing data augmentation, representation disentanglement, and adaptive\nprivacy protection. Our approach integrates three core innovations. First, the\nPrivacy-Preserving Augmentation Network (PrivAugNet) employs large language\nmodels to enrich user and item representations and expand interaction\nsequences, mitigating data sparsity and feature incompleteness. Second, the\nIndependent Domain-Specific Transformer with Contrastive Learning (IDST-CL)\nmodule disentangles domain-specific and shared user preferences, employing\nintra-domain representation alignment (IDRA) and crossdomain representation\ndisentanglement (CDRD) to refine the learned embeddings and enhance knowledge\ntransfer across domains. Finally, the Adaptive Local Differential Privacy\n(AdaLDP) mechanism dynamically calibrates noise injection to achieve an optimal\nbalance between rigorous privacy guarantees and predictive accuracy. Empirical\nevaluations on four real-world datasets demonstrate that FedCCTR-LM\nsubstantially outperforms existing baselines, offering robust,\nprivacy-preserving, and generalizable cross-domain CTR prediction in\nheterogeneous, federated environments.",
    "categories": [
      "cs.IR",
      "cs.CL",
      "cs.DC"
    ],
    "authors": [
      "Jiangcheng Qin",
      "Xueyuan Zhang",
      "Baisong Liu",
      "Jiangbo Qian",
      "Yangyang Wang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16874v1",
    "title": "MARS: A Multi-Agent Framework Incorporating Socratic Guidance for Automated Prompt Optimization",
    "abstract": "The basic question-answering format of large language models involves\ninputting a prompt and receiving a response, and the quality of the prompt\ndirectly impacts the effectiveness of the response. Automated Prompt\nOptimization (APO) aims to break free from the cognitive biases of manually\ndesigned prompts and explores a broader design space for prompts. However,\nexisting APO methods suffer from limited flexibility of fixed templates and\ninefficient search in prompt spaces as key issues. To this end, we propose a\nMulti-Agent framework Incorporating Socratic guidance (MARS), which utilizes\nmulti-agent fusion technology for automatic planning, with gradual continuous\noptimization and evaluation. Specifically, MARS comprises seven agents, each\nwith distinct functionalities, which autonomously use the Planner to devise an\noptimization path that ensures flexibility. Additionally, it employs a\nTeacher-Critic-Student Socratic dialogue pattern to iteratively optimize the\nprompts while conducting effective search. We conduct extensive experiments on\nvarious datasets to validate the effectiveness of our method, and perform\nadditional analytical experiments to assess the model's advancement as well as\nthe interpretability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Jian Zhang",
      "Zhangqi Wang",
      "Haiping Zhu",
      "Jun Liu",
      "Qika Lin",
      "Erik Cambria"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16873v1",
    "title": "Classifier-guided CLIP Distillation for Unsupervised Multi-label Classification",
    "abstract": "Multi-label classification is crucial for comprehensive image understanding,\nyet acquiring accurate annotations is challenging and costly. To address this,\na recent study suggests exploiting unsupervised multi-label classification\nleveraging CLIP, a powerful vision-language model. Despite CLIP's proficiency,\nit suffers from view-dependent predictions and inherent bias, limiting its\neffectiveness. We propose a novel method that addresses these issues by\nleveraging multiple views near target objects, guided by Class Activation\nMapping (CAM) of the classifier, and debiasing pseudo-labels derived from CLIP\npredictions. Our Classifier-guided CLIP Distillation (CCD) enables selecting\nmultiple local views without extra labels and debiasing predictions to enhance\nclassification performance. Experimental results validate our method's\nsuperiority over existing techniques across diverse datasets. The code is\navailable at https://github.com/k0u-id/CCD.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Dongseob Kim",
      "Hyunjung Shim"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16872v1",
    "title": "Lie Detector: Unified Backdoor Detection via Cross-Examination Framework",
    "abstract": "Institutions with limited data and computing resources often outsource model\ntraining to third-party providers in a semi-honest setting, assuming adherence\nto prescribed training protocols with pre-defined learning paradigm (e.g.,\nsupervised or semi-supervised learning). However, this practice can introduce\nsevere security risks, as adversaries may poison the training data to embed\nbackdoors into the resulting model. Existing detection approaches predominantly\nrely on statistical analyses, which often fail to maintain universally accurate\ndetection accuracy across different learning paradigms. To address this\nchallenge, we propose a unified backdoor detection framework in the semi-honest\nsetting that exploits cross-examination of model inconsistencies between two\nindependent service providers. Specifically, we integrate central kernel\nalignment to enable robust feature similarity measurements across different\nmodel architectures and learning paradigms, thereby facilitating precise\nrecovery and identification of backdoor triggers. We further introduce backdoor\nfine-tuning sensitivity analysis to distinguish backdoor triggers from\nadversarial perturbations, substantially reducing false positives. Extensive\nexperiments demonstrate that our method achieves superior detection\nperformance, improving accuracy by 5.4%, 1.6%, and 11.9% over SoTA baselines\nacross supervised, semi-supervised, and autoregressive learning tasks,\nrespectively. Notably, it is the first to effectively detect backdoors in\nmultimodal large language models, further highlighting its broad applicability\nand advancing secure deep learning.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Xuan Wang",
      "Siyuan Liang",
      "Dongping Liao",
      "Han Fang",
      "Aishan Liu",
      "Xiaochun Cao",
      "Yu-liang Lu",
      "Ee-Chien Chang",
      "Xitong Gao"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16870v1",
    "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
    "abstract": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "68T50",
      "I.2.7"
    ],
    "authors": [
      "Anshumann",
      "Mohd Abbas Zaidi",
      "Akhil Kedia",
      "Jinwoo Ahn",
      "Taehwak Kwon",
      "Kangwook Lee",
      "Haejun Lee",
      "Joohyung Lee"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16868v1",
    "title": "Joint Extraction Matters: Prompt-Based Visual Question Answering for Multi-Field Document Information Extraction",
    "abstract": "Visual question answering (VQA) has emerged as a flexible approach for\nextracting specific pieces of information from document images. However,\nexisting work typically queries each field in isolation, overlooking potential\ndependencies across multiple items. This paper investigates the merits of\nextracting multiple fields jointly versus separately. Through experiments on\nmultiple large vision language models and datasets, we show that jointly\nextracting fields often improves accuracy, especially when the fields share\nstrong numeric or contextual dependencies. We further analyze how performance\nscales with the number of requested items and use a regression based metric to\nquantify inter field relationships. Our results suggest that multi field\nprompts can mitigate confusion arising from similar surface forms and related\nnumeric values, providing practical methods for designing robust VQA systems in\ndocument information extraction tasks.",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "authors": [
      "Mengsay Loem",
      "Taiju Hosaka"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16867v1",
    "title": "ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question Generation and Answering",
    "abstract": "Precisely evaluating semantic alignment between text prompts and generated\nvideos remains a challenge in Text-to-Video (T2V) Generation. Existing\ntext-to-video alignment metrics like CLIPScore only generate coarse-grained\nscores without fine-grained alignment details, failing to align with human\npreference. To address this limitation, we propose ETVA, a novel Evaluation\nmethod of Text-to-Video Alignment via fine-grained question generation and\nanswering. First, a multi-agent system parses prompts into semantic scene\ngraphs to generate atomic questions. Then we design a knowledge-augmented\nmulti-stage reasoning framework for question answering, where an auxiliary LLM\nfirst retrieves relevant common-sense knowledge (e.g., physical laws), and then\nvideo LLM answers the generated questions through a multi-stage reasoning\nmechanism. Extensive experiments demonstrate that ETVA achieves a Spearman's\ncorrelation coefficient of 58.47, showing a much higher correlation with human\njudgment than existing metrics which attain only 31.0. We also construct a\ncomprehensive benchmark specifically designed for text-to-video alignment\nevaluation, featuring 2k diverse prompts and 12k atomic questions spanning 10\ncategories. Through a systematic evaluation of 15 existing text-to-video\nmodels, we identify their key capabilities and limitations, paving the way for\nnext-generation T2V generation.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Kaisi Guan",
      "Zhengfeng Lai",
      "Yuchong Sun",
      "Peng Zhang",
      "Wei Liu",
      "Kieran Liu",
      "Meng Cao",
      "Ruihua Song"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16865v1",
    "title": "Nonparametric Factor Analysis and Beyond",
    "abstract": "Nearly all identifiability results in unsupervised representation learning\ninspired by, e.g., independent component analysis, factor analysis, and causal\nrepresentation learning, rely on assumptions of additive independent noise or\nnoiseless regimes. In contrast, we study the more general case where noise can\ntake arbitrary forms, depend on latent variables, and be non-invertibly\nentangled within a nonlinear function. We propose a general framework for\nidentifying latent variables in the nonparametric noisy settings. We first show\nthat, under suitable conditions, the generative model is identifiable up to\ncertain submanifold indeterminacies even in the presence of non-negligible\nnoise. Furthermore, under the structural or distributional variability\nconditions, we prove that latent variables of the general nonlinear models are\nidentifiable up to trivial indeterminacies. Based on the proposed theoretical\nframework, we have also developed corresponding estimation methods and\nvalidated them in various synthetic and real-world settings. Interestingly, our\nestimate of the true GDP growth from alternative measurements suggests more\ninsightful information on the economies than official reports. We expect our\nframework to provide new insight into how both researchers and practitioners\ndeal with latent variables in real-world scenarios.",
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "authors": [
      "Yujia Zheng",
      "Yang Liu",
      "Jiaxiong Yao",
      "Yingyao Hu",
      "Kun Zhang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16862v1",
    "title": "City2Scene: Improving Acoustic Scene Classification with City Features",
    "abstract": "Acoustic scene recordings are often collected from a diverse range of cities.\nMost existing acoustic scene classification (ASC) approaches focus on\nidentifying common acoustic scene patterns across cities to enhance\ngeneralization. In contrast, we hypothesize that city-specific environmental\nand cultural differences in acoustic features are beneficial for the ASC task.\nIn this paper, we introduce City2Scene, a novel framework that leverages city\nfeatures to improve ASC. City2Scene transfers the city-specific knowledge from\ncity classification models to a scene classification model using knowledge\ndistillation. We evaluated City2Scene on the DCASE Challenge Task 1 datasets,\nwhere each audio clip is annotated with both scene and city labels.\nExperimental results demonstrate that city features provide valuable\ninformation for classifying scenes. By distilling the city-specific knowledge,\nCity2Scene effectively improves accuracy for various state-of-the-art ASC\nbackbone models, including both CNNs and Transformers.",
    "categories": [
      "cs.SD",
      "cs.CV",
      "eess.AS"
    ],
    "authors": [
      "Yiqiang Cai",
      "Yizhou Tan",
      "Peihong Zhang",
      "Yuxuan Liu",
      "Shengchen Li",
      "Xi Shao",
      "Mark D. Plumbley"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16861v1",
    "title": "In-House Evaluation Is Not Enough: Towards Robust Third-Party Flaw Disclosure for General-Purpose AI",
    "abstract": "The widespread deployment of general-purpose AI (GPAI) systems introduces\nsignificant new risks. Yet the infrastructure, practices, and norms for\nreporting flaws in GPAI systems remain seriously underdeveloped, lagging far\nbehind more established fields like software security. Based on a collaboration\nbetween experts from the fields of software security, machine learning, law,\nsocial science, and policy, we identify key gaps in the evaluation and\nreporting of flaws in GPAI systems. We call for three interventions to advance\nsystem safety. First, we propose using standardized AI flaw reports and rules\nof engagement for researchers in order to ease the process of submitting,\nreproducing, and triaging flaws in GPAI systems. Second, we propose GPAI system\nproviders adopt broadly-scoped flaw disclosure programs, borrowing from bug\nbounties, with legal safe harbors to protect researchers. Third, we advocate\nfor the development of improved infrastructure to coordinate distribution of\nflaw reports across the many stakeholders who may be impacted. These\ninterventions are increasingly urgent, as evidenced by the prevalence of\njailbreaks and other flaws that can transfer across different providers' GPAI\nsystems. By promoting robust reporting and coordination in the AI ecosystem,\nthese proposals could significantly improve the safety, security, and\naccountability of GPAI systems.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Shayne Longpre",
      "Kevin Klyman",
      "Ruth E. Appel",
      "Sayash Kapoor",
      "Rishi Bommasani",
      "Michelle Sahar",
      "Sean McGregor",
      "Avijit Ghosh",
      "Borhane Blili-Hamelin",
      "Nathan Butters",
      "Alondra Nelson",
      "Amit Elazari",
      "Andrew Sellars",
      "Casey John Ellis",
      "Dane Sherrets",
      "Dawn Song",
      "Harley Geiger",
      "Ilona Cohen",
      "Lauren McIlvenny",
      "Madhulika Srikumar",
      "Mark M. Jaycox",
      "Markus Anderljung",
      "Nadine Farid Johnson",
      "Nicholas Carlini",
      "Nicolas Miailhe",
      "Nik Marda",
      "Peter Henderson",
      "Rebecca S. Portnoff",
      "Rebecca Weiss",
      "Victoria Westerhoff",
      "Yacine Jernite",
      "Rumman Chowdhury",
      "Percy Liang",
      "Arvind Narayanan"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16860v1",
    "title": "PRIOT: Pruning-Based Integer-Only Transfer Learning for Embedded Systems",
    "abstract": "On-device transfer learning is crucial for adapting a common backbone model\nto the unique environment of each edge device. Tiny microcontrollers, such as\nthe Raspberry Pi Pico, are key targets for on-device learning but often lack\nfloating-point units, necessitating integer-only training. Dynamic computation\nof quantization scale factors, which is adopted in former studies, incurs high\ncomputational costs. Therefore, this study focuses on integer-only training\nwith static scale factors, which is challenging with existing training methods.\nWe propose a new training method named PRIOT, which optimizes the network by\npruning selected edges rather than updating weights, allowing effective\ntraining with static scale factors. The pruning pattern is determined by the\nedge-popup algorithm, which trains a parameter named score assigned to each\nedge instead of the original parameters and prunes the edges with low scores\nbefore inference. Additionally, we introduce a memory-efficient variant,\nPRIOT-S, which only assigns scores to a small fraction of edges. We implement\nPRIOT and PRIOT-S on the Raspberry Pi Pico and evaluate their accuracy and\ncomputational costs using a tiny CNN model on the rotated MNIST dataset and the\nVGG11 model on the rotated CIFAR-10 dataset. Our results demonstrate that PRIOT\nimproves accuracy by 8.08 to 33.75 percentage points over existing methods,\nwhile PRIOT-S reduces memory footprint with minimal accuracy loss.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Honoka Anada",
      "Sefutsu Ryu",
      "Masayuki Usui",
      "Tatsuya Kaneko",
      "Shinya Takamaeda-Yamazaki"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16858v1",
    "title": "MTBench: A Multimodal Time Series Benchmark for Temporal Reasoning and Question Answering",
    "abstract": "Understanding the relationship between textual news and time-series evolution\nis a critical yet under-explored challenge in applied data science. While\nmultimodal learning has gained traction, existing multimodal time-series\ndatasets fall short in evaluating cross-modal reasoning and complex question\nanswering, which are essential for capturing complex interactions between\nnarrative information and temporal patterns. To bridge this gap, we introduce\nMultimodal Time Series Benchmark (MTBench), a large-scale benchmark designed to\nevaluate large language models (LLMs) on time series and text understanding\nacross financial and weather domains. MTbench comprises paired time series and\ntextual data, including financial news with corresponding stock price movements\nand weather reports aligned with historical temperature records. Unlike\nexisting benchmarks that focus on isolated modalities, MTbench provides a\ncomprehensive testbed for models to jointly reason over structured numerical\ntrends and unstructured textual narratives. The richness of MTbench enables\nformulation of diverse tasks that require a deep understanding of both text and\ntime-series data, including time-series forecasting, semantic and technical\ntrend analysis, and news-driven question answering (QA). These tasks target the\nmodel's ability to capture temporal dependencies, extract key insights from\ntextual context, and integrate cross-modal information. We evaluate\nstate-of-the-art LLMs on MTbench, analyzing their effectiveness in modeling the\ncomplex relationships between news narratives and temporal patterns. Our\nfindings reveal significant challenges in current models, including\ndifficulties in capturing long-term dependencies, interpreting causality in\nfinancial and weather trends, and effectively fusing multimodal information.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Jialin Chen",
      "Aosong Feng",
      "Ziyu Zhao",
      "Juan Garza",
      "Gaukhar Nurbek",
      "Cheng Qin",
      "Ali Maatouk",
      "Leandros Tassiulas",
      "Yifeng Gao",
      "Rex Ying"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16856v1",
    "title": "MMCR: Benchmarking Cross-Source Reasoning in Scientific Papers",
    "abstract": "Fully comprehending scientific papers by machines reflects a high level of\nArtificial General Intelligence, requiring the ability to reason across\nfragmented and heterogeneous sources of information, presenting a complex and\npractically significant challenge. While Vision-Language Models (VLMs) have\nmade remarkable strides in various tasks, particularly those involving\nreasoning with evidence source from single image or text page, their ability to\nuse cross-source information for reasoning remains an open problem. This work\npresents MMCR, a high-difficulty benchmark designed to evaluate VLMs' capacity\nfor reasoning with cross-source information from scientific papers. The\nbenchmark comprises 276 high-quality questions, meticulously annotated by\nhumans across 7 subjects and 10 task types. Experiments with 18 VLMs\ndemonstrate that cross-source reasoning presents a substantial challenge for\nexisting models. Notably, even the top-performing model, GPT-4o, achieved only\n48.55% overall accuracy, with only 20% accuracy in multi-table comprehension\ntasks, while the second-best model, Qwen2.5-VL-72B, reached 39.86% overall\naccuracy. Furthermore, we investigated the impact of the Chain-of-Thought (CoT)\ntechnique on cross-source reasoning and observed a detrimental effect on small\nmodels, whereas larger models demonstrated substantially enhanced performance.\nThese results highlight the pressing need to develop VLMs capable of\neffectively utilizing cross-source information for reasoning.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Yang Tian",
      "Zheng Lu",
      "Mingqi Gao",
      "Zheng Liu",
      "Bo Zhao"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16855v1",
    "title": "Stack Transformer Based Spatial-Temporal Attention Model for Dynamic Multi-Culture Sign Language Recognition",
    "abstract": "Hand gesture-based Sign Language Recognition (SLR) serves as a crucial\ncommunication bridge between deaf and non-deaf individuals. Existing SLR\nsystems perform well for their cultural SL but may struggle with multi-cultural\nsign languages (McSL). To address these challenges, this paper proposes a Stack\nSpatial-Temporal Transformer Network that leverages multi-head attention\nmechanisms to capture both spatial and temporal dependencies with hierarchical\nfeatures using the Stack Transfer concept. In the proceed, firstly, we applied\na fully connected layer to make a embedding vector which has high expressive\npower from the original dataset, then fed them a stack newly proposed\ntransformer to achieve hierarchical features with short-range and long-range\ndependency. The network architecture is composed of several stages that process\nspatial and temporal relationships sequentially, ensuring effective feature\nextraction. After making the fully connected layer, the embedding vector is\nprocessed by the Spatial Multi-Head Attention Transformer, which captures\nspatial dependencies between joints. In the next stage, the Temporal Multi-Head\nAttention Transformer captures long-range temporal dependencies, and again, the\nfeatures are concatenated with the output using another skip connection. The\nprocessed features are then passed to the Feed-Forward Network (FFN), which\nrefines the feature representations further. After the FFN, additional skip\nconnections are applied to combine the output with earlier layers, followed by\na final normalization layer to produce the final output feature tensor. This\nprocess is repeated for 10 transformer blocks. The extensive experiment shows\nthat the JSL, KSL and ASL datasets achieved good performance accuracy. Our\napproach demonstrates improved performance in McSL, and it will be consider as\na novel work in this domain.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Koki Hirooka",
      "Abu Saleh Musa Miah",
      "Tatsuya Murakami",
      "Yuto Akiba",
      "Yong Seok Hwang",
      "Jungpil Shin"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16854v1",
    "title": "Generative Compositor for Few-Shot Visual Information Extraction",
    "abstract": "Visual Information Extraction (VIE), aiming at extracting structured\ninformation from visually rich document images, plays a pivotal role in\ndocument processing. Considering various layouts, semantic scopes, and\nlanguages, VIE encompasses an extensive range of types, potentially numbering\nin the thousands. However, many of these types suffer from a lack of training\ndata, which poses significant challenges. In this paper, we propose a novel\ngenerative model, named Generative Compositor, to address the challenge of\nfew-shot VIE. The Generative Compositor is a hybrid pointer-generator network\nthat emulates the operations of a compositor by retrieving words from the\nsource text and assembling them based on the provided prompts. Furthermore,\nthree pre-training strategies are employed to enhance the model's perception of\nspatial context information. Besides, a prompt-aware resampler is specially\ndesigned to enable efficient matching by leveraging the entity-semantic prior\ncontained in prompts. The introduction of the prompt-based retrieval mechanism\nand the pre-training strategies enable the model to acquire more effective\nspatial and semantic clues with limited training samples. Experiments\ndemonstrate that the proposed method achieves highly competitive results in the\nfull-sample training, while notably outperforms the baseline in the 1-shot,\n5-shot, and 10-shot settings.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Zhibo Yang",
      "Wei Hua",
      "Sibo Song",
      "Cong Yao",
      "Yingying Zhu",
      "Wenqing Cheng",
      "Xiang Bai"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16853v1",
    "title": "Imagine to Hear: Auditory Knowledge Generation can be an Effective Assistant for Language Models",
    "abstract": "Language models pretrained on text-only corpora often struggle with tasks\nthat require auditory commonsense knowledge. Previous work addresses this\nproblem by augmenting the language model to retrieve knowledge from external\naudio databases. This approach has several limitations, such as the potential\nlack of relevant audio in databases and the high costs associated with\nconstructing and querying the databases. To address these issues, we propose\nImagine to Hear, a novel approach that dynamically generates auditory knowledge\nusing generative models. Our framework detects multiple audio-related textual\nspans from the given prompt and generates corresponding auditory knowledge. We\ndevelop several mechanisms to efficiently process multiple auditory knowledge,\nincluding a CLAP-based rejection sampler and a language-audio fusion module.\nOur experiments show that our method achieves state-of-the-art performance on\nAuditoryBench without relying on external databases, highlighting the\neffectiveness of our generation-based approach.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "authors": [
      "Suho Yoo",
      "Hyunjong Ok",
      "Jaeho Lee"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16852v1",
    "title": "Casual Inference via Style Bias Deconfounding for Domain Generalization",
    "abstract": "Deep neural networks (DNNs) often struggle with out-of-distribution data,\nlimiting their reliability in diverse realworld applications. To address this\nissue, domain generalization methods have been developed to learn\ndomain-invariant features from single or multiple training domains, enabling\ngeneralization to unseen testing domains. However, existing approaches usually\noverlook the impact of style frequency within the training set. This oversight\npredisposes models to capture spurious visual correlations caused by style\nconfounding factors, rather than learning truly causal representations, thereby\nundermining inference reliability. In this work, we introduce Style\nDeconfounding Causal Learning (SDCL), a novel causal inference-based framework\ndesigned to explicitly address style as a confounding factor. Our approaches\nbegins with constructing a structural causal model (SCM) tailored to the domain\ngeneralization problem and applies a backdoor adjustment strategy to account\nfor style influence. Building on this foundation, we design a style-guided\nexpert module (SGEM) to adaptively clusters style distributions during\ntraining, capturing the global confounding style. Additionally, a back-door\ncausal learning module (BDCL) performs causal interventions during feature\nextraction, ensuring fair integration of global confounding styles into sample\npredictions, effectively reducing style bias. The SDCL framework is highly\nversatile and can be seamlessly integrated with state-of-the-art data\naugmentation techniques. Extensive experiments across diverse natural and\nmedical image recognition tasks validate its efficacy, demonstrating superior\nperformance in both multi-domain and the more challenging single-domain\ngeneralization scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Jiaxi Li",
      "Di Lin",
      "Hao Chen",
      "Hongying Liu",
      "Liang Wan",
      "Wei Feng"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16851v1",
    "title": "Towards LLM Guardrails via Sparse Representation Steering",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in\nnatural language generation tasks, yet their uncontrolled outputs pose\nsignificant ethical and safety risks. Recently, representation engineering\nmethods have shown promising results in steering model behavior by modifying\nthe rich semantic information encoded in activation vectors. However, due to\nthe difficulty of precisely disentangling semantic directions within\nhigh-dimensional representation space, existing approaches suffer from three\nmajor limitations: lack of fine-grained control, quality degradation of\ngenerated content, and poor interpretability. To address these challenges, we\npropose a sparse encoding-based representation engineering method, named SRE,\nwhich decomposes polysemantic activations into a structured, monosemantic\nfeature space. By leveraging sparse autoencoding, our approach isolates and\nadjusts only task-specific sparse feature dimensions, enabling precise and\ninterpretable steering of model behavior while preserving content quality. We\nvalidate our method on three critical domains, i.e., safety, fairness, and\ntruthfulness using the open-source LLM Gemma-2-2B-it. Experimental results show\nthat SRE achieves superior controllability while maintaining the overall\nquality of generated content (i.e., controllability and quality), demonstrating\nits effectiveness as a fine-grained and interpretable activation steering\nframework.",
    "categories": [
      "cs.CR",
      "cs.CL"
    ],
    "authors": [
      "Zeqing He",
      "Zhibo Wang",
      "Huiyu Xu",
      "Kui Ren"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16850v1",
    "title": "Physics-Informed Neural Network Surrogate Models for River Stage Prediction",
    "abstract": "This work investigates the feasibility of using Physics-Informed Neural\nNetworks (PINNs) as surrogate models for river stage prediction, aiming to\nreduce computational cost while maintaining predictive accuracy. Our primary\ncontribution demonstrates that PINNs can successfully approximate HEC-RAS\nnumerical solutions when trained on a single river, achieving strong predictive\naccuracy with generally low relative errors, though some river segments exhibit\nhigher deviations.\n  By integrating the governing Saint-Venant equations into the learning\nprocess, the proposed PINN-based surrogate model enforces physical consistency\nand significantly improves computational efficiency compared to HEC-RAS. We\nevaluate the model's performance in terms of accuracy and computational speed,\ndemonstrating that it closely approximates HEC-RAS predictions while enabling\nreal-time inference.\n  These results highlight the potential of PINNs as effective surrogate models\nfor single-river hydrodynamics, offering a promising alternative for\ncomputationally efficient river stage forecasting. Future work will explore\ntechniques to enhance PINN training stability and robustness across a more\ngeneralized multi-river model.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Maximilian Zoch",
      "Edward Holmberg",
      "Pujan Pokhrel",
      "Ken Pathak",
      "Steven Sloan",
      "Kendall Niles",
      "Jay Ratcliff",
      "Maik Flanagin",
      "Elias Ioup",
      "Christian Guetl",
      "Mahdi Abdelguerfi"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16848v1",
    "title": "HSM: Hierarchical Scene Motifs for Multi-Scale Indoor Scene Generation",
    "abstract": "Despite advances in indoor 3D scene layout generation, synthesizing scenes\nwith dense object arrangements remains challenging. Existing methods primarily\nfocus on large furniture while neglecting smaller objects, resulting in\nunrealistically empty scenes. Those that place small objects typically do not\nhonor arrangement specifications, resulting in largely random placement not\nfollowing the text description. We present HSM, a hierarchical framework for\nindoor scene generation with dense object arrangements across spatial scales.\nIndoor scenes are inherently hierarchical, with surfaces supporting objects at\ndifferent scales, from large furniture on floors to smaller objects on tables\nand shelves. HSM embraces this hierarchy and exploits recurring cross-scale\nspatial patterns to generate complex and realistic indoor scenes in a unified\nmanner. Our experiments show that HSM outperforms existing methods by\ngenerating scenes that are more realistic and better conform to user input\nacross room types and spatial configurations.",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Hou In Derek Pun",
      "Hou In Ivan Tam",
      "Austin T. Wang",
      "Xiaoliang Huo",
      "Angel X. Chang",
      "Manolis Savva"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16846v1",
    "title": "An Accelerated Bregman Algorithm for ReLU-based Symmetric Matrix Decomposition",
    "abstract": "Symmetric matrix decomposition is an active research area in machine\nlearning. This paper focuses on exploiting the low-rank structure of\nnon-negative and sparse symmetric matrices via the rectified linear unit (ReLU)\nactivation function. We propose the ReLU-based nonlinear symmetric matrix\ndecomposition (ReLU-NSMD) model, introduce an accelerated alternating partial\nBregman (AAPB) method for its solution, and present the algorithm's convergence\nresults. Our algorithm leverages the Bregman proximal gradient framework to\novercome the challenge of estimating the global $L$-smooth constant in the\nclassic proximal gradient algorithm. Numerical experiments on synthetic and\nreal datasets validate the effectiveness of our model and algorithm.",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Qingsong Wang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16843v1",
    "title": "LoRASculpt: Sculpting LoRA for Harmonizing General and Specialized Knowledge in Multimodal Large Language Models",
    "abstract": "While Multimodal Large Language Models (MLLMs) excel at generalizing across\nmodalities and tasks, effectively adapting them to specific downstream tasks\nwhile simultaneously retaining both general and specialized knowledge remains\nchallenging. Although Low-Rank Adaptation (LoRA) is widely used to efficiently\nacquire specialized knowledge in MLLMs, it introduces substantial harmful\nredundancy during visual instruction tuning, which exacerbates the forgetting\nof general knowledge and degrades downstream task performance. To address this\nissue, we propose LoRASculpt to eliminate harmful redundant parameters, thereby\nharmonizing general and specialized knowledge. Specifically, under theoretical\nguarantees, we introduce sparse updates into LoRA to discard redundant\nparameters effectively. Furthermore, we propose a Conflict Mitigation\nRegularizer to refine the update trajectory of LoRA, mitigating knowledge\nconflicts with the pretrained weights. Extensive experimental results\ndemonstrate that even at very high degree of sparsity ($\\le$ 5%), our method\nsimultaneously enhances generalization and downstream task performance. This\nconfirms that our approach effectively mitigates the catastrophic forgetting\nissue and further promotes knowledge harmonization in MLLMs.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jian Liang",
      "Wenke Huang",
      "Guancheng Wan",
      "Qu Yang",
      "Mang Ye"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16842v1",
    "title": "Downstream Analysis of Foundational Medical Vision Models for Disease Progression",
    "abstract": "Medical vision foundational models are used for a wide variety of tasks,\nincluding medical image segmentation and registration. This work evaluates the\nability of these models to predict disease progression using a simple linear\nprobe. We hypothesize that intermediate layer features of segmentation models\ncapture structural information, while those of registration models encode\nknowledge of change over time. Beyond demonstrating that these features are\nuseful for disease progression prediction, we also show that registration model\nfeatures do not require spatially aligned input images. However, for\nsegmentation models, spatial alignment is essential for optimal performance.\nOur findings highlight the importance of spatial alignment and the utility of\nfoundation model features for image registration.",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Basar Demir",
      "Soumitri Chattopadhyay",
      "Thomas Hastings Greer",
      "Boqi Chen",
      "Marc Niethammer"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16841v1",
    "title": "Preferential Multi-Objective Bayesian Optimization for Drug Discovery",
    "abstract": "Despite decades of advancements in automated ligand screening, large-scale\ndrug discovery remains resource-intensive and requires post-processing hit\nselection, a step where chemists manually select a few promising molecules\nbased on their chemical intuition. This creates a major bottleneck in the\nvirtual screening process for drug discovery, demanding experts to repeatedly\nbalance complex trade-offs among drug properties across a vast pool of\ncandidates. To improve the efficiency and reliability of this process, we\npropose a novel human-centered framework named CheapVS that allows chemists to\nguide the ligand selection process by providing preferences regarding the\ntrade-offs between drug properties via pairwise comparison. Our framework\ncombines preferential multi-objective Bayesian optimization with a docking\nmodel for measuring binding affinity to capture human chemical intuition for\nimproving hit identification. Specifically, on a library of 100K chemical\ncandidates targeting EGFR and DRD2, CheapVS outperforms state-of-the-art\nscreening methods in identifying drugs within a limited computational budget.\nNotably, our method can recover up to 16/37 EGFR and 37/58 DRD2 known drugs\nwhile screening only 6% of the library, showcasing its potential to\nsignificantly advance drug discovery.",
    "categories": [
      "cs.LG",
      "cs.HC",
      "q-bio.BM"
    ],
    "authors": [
      "Tai Dang",
      "Long-Hung Pham",
      "Sang T. Truong",
      "Ari Glenn",
      "Wendy Nguyen",
      "Edward A. Pham",
      "Jeffrey S. Glenn",
      "Sanmi Koyejo",
      "Thang Luong"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16836v1",
    "title": "A Flexible Fairness Framework with Surrogate Loss Reweighting for Addressing Sociodemographic Disparities",
    "abstract": "This paper presents a new algorithmic fairness framework called\n$\\boldsymbol{\\alpha}$-$\\boldsymbol{\\beta}$ Fair Machine Learning\n($\\boldsymbol{\\alpha}$-$\\boldsymbol{\\beta}$ FML), designed to optimize fairness\nlevels across sociodemographic attributes. Our framework employs a new family\nof surrogate loss functions, paired with loss reweighting techniques, allowing\nprecise control over fairness-accuracy trade-offs through tunable\nhyperparameters $\\boldsymbol{\\alpha}$ and $\\boldsymbol{\\beta}$. To efficiently\nsolve the learning objective, we propose Parallel Stochastic Gradient Descent\nwith Surrogate Loss (P-SGD-S) and establish convergence guarantees for both\nconvex and nonconvex loss functions. Experimental results demonstrate that our\nframework improves overall accuracy while reducing fairness violations,\noffering a smooth trade-off between standard empirical risk minimization and\nstrict minimax fairness. Results across multiple datasets confirm its\nadaptability, ensuring fairness improvements without excessive performance\ndegradation.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Wen Xu",
      "Elham Dolatabadi"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16835v1",
    "title": "Safe and Reliable Diffusion Models via Subspace Projection",
    "abstract": "Large-scale text-to-image (T2I) diffusion models have revolutionized image\ngeneration, enabling the synthesis of highly detailed visuals from textual\ndescriptions. However, these models may inadvertently generate inappropriate\ncontent, such as copyrighted works or offensive images. While existing methods\nattempt to eliminate specific unwanted concepts, they often fail to ensure\ncomplete removal, allowing the concept to reappear in subtle forms. For\ninstance, a model may successfully avoid generating images in Van Gogh's style\nwhen explicitly prompted with 'Van Gogh', yet still reproduce his signature\nartwork when given the prompt 'Starry Night'. In this paper, we propose SAFER,\na novel and efficient approach for thoroughly removing target concepts from\ndiffusion models. At a high level, SAFER is inspired by the observed\nlow-dimensional structure of the text embedding space. The method first\nidentifies a concept-specific subspace $S_c$ associated with the target concept\nc. It then projects the prompt embeddings onto the complementary subspace of\n$S_c$, effectively erasing the concept from the generated images. Since\nconcepts can be abstract and difficult to fully capture using natural language\nalone, we employ textual inversion to learn an optimized embedding of the\ntarget concept from a reference image. This enables more precise subspace\nestimation and enhances removal performance. Furthermore, we introduce a\nsubspace expansion strategy to ensure comprehensive and robust concept erasure.\nExtensive experiments demonstrate that SAFER consistently and effectively\nerases unwanted concepts from diffusion models while preserving generation\nquality.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Huiqiang Chen",
      "Tianqing Zhu",
      "Linlin Wang",
      "Xin Yu",
      "Longxiang Gao",
      "Wanlei Zhou"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16833v1",
    "title": "The Deployment of End-to-End Audio Language Models Should Take into Account the Principle of Least Privilege",
    "abstract": "We are at a turning point for language models that accept audio input. The\nlatest end-to-end audio language models (Audio LMs) process speech directly\ninstead of relying on a separate transcription step. This shift preserves\ndetailed information, such as intonation or the presence of multiple speakers,\nthat would otherwise be lost in transcription. However, it also introduces new\nsafety risks, including the potential misuse of speaker identity cues and other\nsensitive vocal attributes, which could have legal implications. In this\nposition paper, we urge a closer examination of how these models are built and\ndeployed. We argue that the principle of least privilege should guide decisions\non whether to deploy cascaded or end-to-end models. Specifically, evaluations\nshould assess (1) whether end-to-end modeling is necessary for a given\napplication; and (2), the appropriate scope of information access. Finally, We\nhighlight related gaps in current audio LM benchmarks and identify key open\nresearch questions, both technical and policy-related, that must be addressed\nto enable the responsible deployment of end-to-end Audio LMs.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "authors": [
      "Luxi He",
      "Xiangyu Qi",
      "Michel Liao",
      "Inyoung Cheong",
      "Prateek Mittal",
      "Danqi Chen",
      "Peter Henderson"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16832v1",
    "title": "Joint Self-Supervised Video Alignment and Action Segmentation",
    "abstract": "We introduce a novel approach for simultaneous self-supervised video\nalignment and action segmentation based on a unified optimal transport\nframework. In particular, we first tackle self-supervised video alignment by\ndeveloping a fused Gromov-Wasserstein optimal transport formulation with a\nstructural prior, which trains efficiently on GPUs and needs only a few\niterations for solving the optimal transport problem. Our single-task method\nachieves the state-of-the-art performance on multiple video alignment\nbenchmarks and outperforms VAVA, which relies on a traditional Kantorovich\noptimal transport formulation with an optimality prior. Furthermore, we extend\nour approach by proposing a unified optimal transport framework for joint\nself-supervised video alignment and action segmentation, which requires\ntraining and storing a single model and saves both time and memory consumption\nas compared to two different single-task models. Extensive evaluations on\nseveral video alignment and action segmentation datasets demonstrate that our\nmulti-task method achieves comparable video alignment yet superior action\nsegmentation results over previous methods in video alignment and action\nsegmentation respectively. Finally, to the best of our knowledge, this is the\nfirst work to unify video alignment and action segmentation into a single\nmodel.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Ali Shah Ali",
      "Syed Ahmed Mahmood",
      "Mubin Saeed",
      "Andrey Konin",
      "M. Zeeshan Zia",
      "Quoc-Huy Tran"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16826v1",
    "title": "When Tom Eats Kimchi: Evaluating Cultural Bias of Multimodal Large Language Models in Cultural Mixture Contexts",
    "abstract": "In a highly globalized world, it is important for multi-modal large language\nmodels (MLLMs) to recognize and respond correctly to mixed-cultural inputs. For\nexample, a model should correctly identify kimchi (Korean food) in an image\nboth when an Asian woman is eating it, as well as an African man is eating it.\nHowever, current MLLMs show an over-reliance on the visual features of the\nperson, leading to misclassification of the entities. To examine the robustness\nof MLLMs to different ethnicity, we introduce MixCuBe, a cross-cultural bias\nbenchmark, and study elements from five countries and four ethnicities. Our\nfindings reveal that MLLMs achieve both higher accuracy and lower sensitivity\nto such perturbation for high-resource cultures, but not for low-resource\ncultures. GPT-4o, the best-performing model overall, shows up to 58% difference\nin accuracy between the original and perturbed cultural settings in\nlow-resource cultures. Our dataset is publicly available at:\nhttps://huggingface.co/datasets/kyawyethu/MixCuBe.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Jun Seong Kim",
      "Kyaw Ye Thu",
      "Javad Ismayilzada",
      "Junyeong Park",
      "Eunsu Kim",
      "Huzama Ahmad",
      "Na Min An",
      "James Thorne",
      "Alice Oh"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16825v1",
    "title": "SGFormer: Satellite-Ground Fusion for 3D Semantic Scene Completion",
    "abstract": "Recently, camera-based solutions have been extensively explored for scene\nsemantic completion (SSC). Despite their success in visible areas, existing\nmethods struggle to capture complete scene semantics due to frequent visual\nocclusions. To address this limitation, this paper presents the first\nsatellite-ground cooperative SSC framework, i.e., SGFormer, exploring the\npotential of satellite-ground image pairs in the SSC task. Specifically, we\npropose a dual-branch architecture that encodes orthogonal satellite and ground\nviews in parallel, unifying them into a common domain. Additionally, we design\na ground-view guidance strategy that corrects satellite image biases during\nfeature encoding, addressing misalignment between satellite and ground views.\nMoreover, we develop an adaptive weighting strategy that balances contributions\nfrom satellite and ground views. Experiments demonstrate that SGFormer\noutperforms the state of the art on SemanticKITTI and SSCBench-KITTI-360\ndatasets. Our code is available on https://github.com/gxytcrc/SGFormer.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Xiyue Guo",
      "Jiarui Hu",
      "Junjie Hu",
      "Hujun Bao",
      "Guofeng Zhang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16822v1",
    "title": "RigGS: Rigging of 3D Gaussians for Modeling Articulated Objects in Videos",
    "abstract": "This paper considers the problem of modeling articulated objects captured in\n2D videos to enable novel view synthesis, while also being easily editable,\ndrivable, and re-posable. To tackle this challenging problem, we propose RigGS,\na new paradigm that leverages 3D Gaussian representation and skeleton-based\nmotion representation to model dynamic objects without utilizing additional\ntemplate priors. Specifically, we first propose skeleton-aware node-controlled\ndeformation, which deforms a canonical 3D Gaussian representation over time to\ninitialize the modeling process, producing candidate skeleton nodes that are\nfurther simplified into a sparse 3D skeleton according to their motion and\nsemantic information. Subsequently, based on the resulting skeleton, we design\nlearnable skin deformations and pose-dependent detailed deformations, thereby\neasily deforming the 3D Gaussian representation to generate new actions and\nrender further high-quality images from novel views. Extensive experiments\ndemonstrate that our method can generate realistic new actions easily for\nobjects and achieve high-quality rendering.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yuxin Yao",
      "Zhi Deng",
      "Junhui Hou"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16818v1",
    "title": "Depth-Aided Color Image Inpainting in Quaternion Domain",
    "abstract": "In this paper, we propose a depth-aided color image inpainting method in the\nquaternion domain, called depth-aided low-rank quaternion matrix completion\n(D-LRQMC). In conventional quaternion-based inpainting techniques, the color\nimage is expressed as a quaternion matrix by using the three imaginary parts as\nthe color channels, whereas the real part is set to zero and has no\ninformation. Our approach incorporates depth information as the real part of\nthe quaternion representations, leveraging the correlation between color and\ndepth to improve the result of inpainting. In the proposed method, we first\nrestore the observed image with the conventional LRQMC and estimate the depth\nof the restored result. We then incorporate the estimated depth into the real\npart of the observed image and perform LRQMC again. Simulation results\ndemonstrate that the proposed D-LRQMC can improve restoration accuracy and\nvisual quality for various images compared to the conventional LRQMC. These\nresults suggest the effectiveness of the depth information for color image\nprocessing in quaternion domain.",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Shunki Tatsumi",
      "Ryo Hayakawa",
      "Youji Iiguni"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16816v1",
    "title": "ST-Prompt Guided Histological Hypergraph Learning for Spatial Gene Expression Prediction",
    "abstract": "Spatial Transcriptomics (ST) reveals the spatial distribution of gene\nexpression in tissues, offering critical insights into biological processes and\ndisease mechanisms. However, predicting ST from H\\&E-stained histology images\nis challenging due to the heterogeneous relationship between histomorphology\nand gene expression, which arises from substantial variability across different\npatients and tissue sections. A more practical and valuable approach is to\nutilize ST data from a few local regions to predict the spatial transcriptomic\nlandscape across the remaining regions in H&E slides. In response, we propose\nPHG2ST, an ST-prompt guided histological hypergraph learning framework, which\nleverages sparse ST signals as prompts to guide histological hypergraph\nlearning for global spatial gene expression prediction. Our framework fuses\nhistological hypergraph representations at multiple scales through a masked\nST-prompt encoding mechanism, improving robustness and generalizability.\nBenchmark evaluations on two public ST datasets demonstrate that PHG2ST\noutperforms the existing state-of-the-art methods and closely aligns with the\nground truth. These results underscore the potential of leveraging sparse local\nST data for scalable and cost-effective spatial gene expression mapping in\nreal-world biomedical applications.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yi Niu",
      "Jiashuai Liu",
      "Yingkang Zhan",
      "Jiangbo Shi",
      "Di Zhang",
      "Ines Machado",
      "Mireia Crispin-Ortuzar",
      "Chen Li",
      "Zeyu Gao"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16814v1",
    "title": "When Debate Fails: Bias Reinforcement in Large Language Models",
    "abstract": "Large Language Models $($LLMs$)$ solve complex problems using training-free\nmethods like prompt engineering and in-context learning, yet ensuring reasoning\ncorrectness remains challenging. While self-correction methods such as\nself-consistency and self-refinement aim to improve reliability, they often\nreinforce biases due to the lack of effective feedback mechanisms. Multi-Agent\nDebate $($MAD$)$ has emerged as an alternative, but we identify two key\nlimitations: bias reinforcement, where debate amplifies model biases instead of\ncorrecting them, and lack of perspective diversity, as all agents share the\nsame model and reasoning patterns, limiting true debate effectiveness. To\nsystematically evaluate these issues, we introduce $\\textit{MetaNIM Arena}$, a\nbenchmark designed to assess LLMs in adversarial strategic decision-making,\nwhere dynamic interactions influence optimal decisions. To overcome MAD's\nlimitations, we propose $\\textbf{DReaMAD}$ $($$\\textbf{D}$iverse\n$\\textbf{Rea}$soning via $\\textbf{M}$ulti-$\\textbf{A}$gent $\\textbf{D}$ebate\nwith Refined Prompt$)$, a novel framework that $(1)$ refines LLM's strategic\nprior knowledge to improve reasoning quality and $(2)$ promotes diverse\nviewpoints within a single model by systematically modifying prompts, reducing\nbias. Empirical results show that $\\textbf{DReaMAD}$ significantly improves\ndecision accuracy, reasoning diversity, and bias mitigation across multiple\nstrategic tasks, establishing it as a more effective approach for LLM-based\ndecision-making.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Jihwan Oh",
      "Minchan Jeong",
      "Jongwoo Ko",
      "Se-Young Yun"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16811v1",
    "title": "Seg2Box: 3D Object Detection by Point-Wise Semantics Supervision",
    "abstract": "LiDAR-based 3D object detection and semantic segmentation are critical tasks\nin 3D scene understanding. Traditional detection and segmentation methods\nsupervise their models through bounding box labels and semantic mask labels.\nHowever, these two independent labels inherently contain significant\nredundancy. This paper aims to eliminate the redundancy by supervising 3D\nobject detection using only semantic labels. However, the challenge arises due\nto the incomplete geometry structure and boundary ambiguity of point-cloud\ninstances, leading to inaccurate pseudo labels and poor detection results. To\naddress these challenges, we propose a novel method, named Seg2Box. We first\nintroduce a Multi-Frame Multi-Scale Clustering (MFMS-C) module, which leverages\nthe spatio-temporal consistency of point clouds to generate accurate box-level\npseudo-labels. Additionally, the Semantic?Guiding Iterative-Mining\nSelf-Training (SGIM-ST) module is proposed to enhance the performance by\nprogressively refining the pseudo-labels and mining the instances without\ngenerating pseudo-labels. Experiments on the Waymo Open Dataset and nuScenes\nDataset show that our method significantly outperforms other competitive\nmethods by 23.7\\% and 10.3\\% in mAP, respectively. The results demonstrate the\ngreat label-efficient potential and advancement of our method.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Maoji Zheng",
      "Ziyu Xu",
      "Qiming Xia",
      "Hai Wu",
      "Chenglu Wen",
      "Cheng Wang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16809v1",
    "title": "Online Selective Conformal Prediction: Errors and Solutions",
    "abstract": "In online selective conformal inference, data arrives sequentially, and\nprediction intervals are constructed only when an online selection rule is met.\nSince online selections may break the exchangeability between the selected test\ndatum and the rest of the data, one must correct for this by suitably selecting\nthe calibration data. In this paper, we evaluate existing calibration selection\nstrategies and pinpoint some fundamental errors in the associated claims that\nguarantee selection-conditional coverage and control of the false coverage rate\n(FCR). To address these shortcomings, we propose novel calibration selection\nstrategies that provably preserve the exchangeability of the calibration data\nand the selected test datum. Consequently, we demonstrate that online selective\nconformal inference with these strategies guarantees both selection-conditional\ncoverage and FCR control. Our theoretical findings are supported by\nexperimental evidence examining tradeoffs between valid methods.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Yusuf Sale",
      "Aaditya Ramdas"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16806v1",
    "title": "DyWA: Dynamics-adaptive World Action Model for Generalizable Non-prehensile Manipulation",
    "abstract": "Nonprehensile manipulation is crucial for handling objects that are too thin,\nlarge, or otherwise ungraspable in unstructured environments. While\nconventional planning-based approaches struggle with complex contact modeling,\nlearning-based methods have recently emerged as a promising alternative.\nHowever, existing learning-based approaches face two major limitations: they\nheavily rely on multi-view cameras and precise pose tracking, and they fail to\ngeneralize across varying physical conditions, such as changes in object mass\nand table friction. To address these challenges, we propose the\nDynamics-Adaptive World Action Model (DyWA), a novel framework that enhances\naction learning by jointly predicting future states while adapting to dynamics\nvariations based on historical trajectories. By unifying the modeling of\ngeometry, state, physics, and robot actions, DyWA enables more robust policy\nlearning under partial observability. Compared to baselines, our method\nimproves the success rate by 31.5% using only single-view point cloud\nobservations in the simulation. Furthermore, DyWA achieves an average success\nrate of 68% in real-world experiments, demonstrating its ability to generalize\nacross diverse object geometries, adapt to varying table friction, and\nrobustness in challenging scenarios such as half-filled water bottles and\nslippery surfaces.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Jiangran Lyu",
      "Ziming Li",
      "Xuesong Shi",
      "Chaoyi Xu",
      "Yizhou Wang",
      "He Wang"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16803v1",
    "title": "BEAC: Imitating Complex Exploration and Task-oriented Behaviors for Invisible Object Nonprehensile Manipulation",
    "abstract": "Applying imitation learning (IL) is challenging to nonprehensile manipulation\ntasks of invisible objects with partial observations, such as excavating buried\nrocks. The demonstrator must make such complex action decisions as exploring to\nfind the object and task-oriented actions to complete the task while estimating\nits hidden state, perhaps causing inconsistent action demonstration and high\ncognitive load problems. For these problems, work in human cognitive science\nsuggests that promoting the use of pre-designed, simple exploration rules for\nthe demonstrator may alleviate the problems of action inconsistency and high\ncognitive load. Therefore, when performing imitation learning from\ndemonstrations using such exploration rules, it is important to accurately\nimitate not only the demonstrator's task-oriented behavior but also his/her\nmode-switching behavior (exploratory or task-oriented behavior) under partial\nobservation. Based on the above considerations, this paper proposes a novel\nimitation learning framework called Belief Exploration-Action Cloning (BEAC),\nwhich has a switching policy structure between a pre-designed exploration\npolicy and a task-oriented action policy trained on the estimated belief states\nbased on past history. In simulation and real robot experiments, we confirmed\nthat our proposed method achieved the best task performance, higher mode and\naction prediction accuracies, while reducing the cognitive load in the\ndemonstration indicated by a user study.",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Hirotaka Tahara",
      "Takamitsu Matsubara"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16801v1",
    "title": "Auto-Regressive Diffusion for Generating 3D Human-Object Interactions",
    "abstract": "Text-driven Human-Object Interaction (Text-to-HOI) generation is an emerging\nfield with applications in animation, video games, virtual reality, and\nrobotics. A key challenge in HOI generation is maintaining interaction\nconsistency in long sequences. Existing Text-to-Motion-based approaches, such\nas discrete motion tokenization, cannot be directly applied to HOI generation\ndue to limited data in this domain and the complexity of the modality. To\naddress the problem of interaction consistency in long sequences, we propose an\nautoregressive diffusion model (ARDHOI) that predicts the next continuous\ntoken. Specifically, we introduce a Contrastive Variational Autoencoder (cVAE)\nto learn a physically plausible space of continuous HOI tokens, thereby\nensuring that generated human-object motions are realistic and natural. For\ngenerating sequences autoregressively, we develop a Mamba-based context encoder\nto capture and maintain consistent sequential actions. Additionally, we\nimplement an MLP-based denoiser to generate the subsequent token conditioned on\nthe encoded context. Our model has been evaluated on the OMOMO and BEHAVE\ndatasets, where it outperforms existing state-of-the-art methods in terms of\nboth performance and inference speed. This makes ARDHOI a robust and efficient\nsolution for text-driven HOI tasks",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Zichen Geng",
      "Zeeshan Hayder",
      "Wei Liu",
      "Ajmal Saeed Mian"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16799v1",
    "title": "Causally Aligned Curriculum Learning",
    "abstract": "A pervasive challenge in Reinforcement Learning (RL) is the \"curse of\ndimensionality\" which is the exponential growth in the state-action space when\noptimizing a high-dimensional target task. The framework of curriculum learning\ntrains the agent in a curriculum composed of a sequence of related and more\nmanageable source tasks. The expectation is that when some optimal decision\nrules are shared across source tasks and the target task, the agent could more\nquickly pick up the necessary skills to behave optimally in the environment,\nthus accelerating the learning process. However, this critical assumption of\ninvariant optimal decision rules does not necessarily hold in many practical\napplications, specifically when the underlying environment contains unobserved\nconfounders. This paper studies the problem of curriculum RL through causal\nlenses. We derive a sufficient graphical condition characterizing causally\naligned source tasks, i.e., the invariance of optimal decision rules holds. We\nfurther develop an efficient algorithm to generate a causally aligned\ncurriculum, provided with qualitative causal knowledge of the target task.\nFinally, we validate our proposed methodology through experiments in discrete\nand continuous confounded tasks with pixel observations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Mingxuan Li",
      "Junzhe Zhang",
      "Elias Bareinboim"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16797v1",
    "title": "A Learnability Analysis on Neuro-Symbolic Learning",
    "abstract": "This paper analyzes the learnability of neuro-symbolic (NeSy) tasks within\nhybrid systems. We show that the learnability of NeSy tasks can be\ncharacterized by their derived constraint satisfaction problems (DCSPs).\nSpecifically, a task is learnable if the corresponding DCSP has a unique\nsolution; otherwise, it is unlearnable. For learnable tasks, we establish error\nbounds by exploiting the clustering property of the hypothesis space.\nAdditionally, we analyze the asymptotic error for general NeSy tasks, showing\nthat the expected error scales with the disagreement among solutions. Our\nresults offer a principled approach to determining learnability and provide\ninsights into the design of new algorithms.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Hao-Yuan He",
      "Ming Li"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16795v1",
    "title": "DCEdit: Dual-Level Controlled Image Editing via Precisely Localized Semantics",
    "abstract": "This paper presents a novel approach to improving text-guided image editing\nusing diffusion-based models. Text-guided image editing task poses key\nchallenge of precisly locate and edit the target semantic, and previous methods\nfall shorts in this aspect. Our method introduces a Precise Semantic\nLocalization strategy that leverages visual and textual self-attention to\nenhance the cross-attention map, which can serve as a regional cues to improve\nediting performance. Then we propose a Dual-Level Control mechanism for\nincorporating regional cues at both feature and latent levels, offering\nfine-grained control for more precise edits. To fully compare our methods with\nother DiT-based approaches, we construct the RW-800 benchmark, featuring high\nresolution images, long descriptive texts, real-world images, and a new text\nediting task. Experimental results on the popular PIE-Bench and RW-800\nbenchmarks demonstrate the superior performance of our approach in preserving\nbackground and providing accurate edits.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yihan Hu",
      "Jianing Peng",
      "Yiheng Lin",
      "Ting Liu",
      "Xiaochao Qu",
      "Luoqi Liu",
      "Yao Zhao",
      "Yunchao Wei"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16793v1",
    "title": "Restoring Forgotten Knowledge in Non-Exemplar Class Incremental Learning through Test-Time Semantic Evolution",
    "abstract": "Continual learning aims to accumulate knowledge over a data stream while\nmitigating catastrophic forgetting. In Non-exemplar Class Incremental Learning\n(NECIL), forgetting arises during incremental optimization because old classes\nare inaccessible, hindering the retention of prior knowledge. To solve this,\nprevious methods struggle in achieving the stability-plasticity balance in the\ntraining stages. However, we note that the testing stage is rarely considered\namong them, but is promising to be a solution to forgetting. Therefore, we\npropose RoSE, which is a simple yet effective method that\n\\textbf{R}est\\textbf{o}res forgotten knowledge through test-time\n\\textbf{S}emantic \\textbf{E}volution. Specifically designed for minimizing\nforgetting, RoSE is a test-time semantic drift compensation framework that\nenables more accurate drift estimation in a self-supervised manner. Moreover,\nto avoid incomplete optimization during online testing, we derive an analytical\nsolution as an alternative to gradient descent. We evaluate RoSE on CIFAR-100,\nTinyImageNet, and ImageNet100 datasets, under both cold-start and warm-start\nsettings. Our method consistently outperforms most state-of-the-art (SOTA)\nmethods across various scenarios, validating the potential and feasibility of\ntest-time evolution in NECIL.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Haori Lu",
      "Xusheng Cao",
      "Linlan Huang",
      "Enguang Wang",
      "Fei Yang",
      "Xialei Liu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16791v1",
    "title": "\"The Diagram is like Guardrails\": Structuring GenAI-assisted Hypotheses Exploration with an Interactive Shared Representation",
    "abstract": "Data analysis encompasses a spectrum of tasks, from high-level conceptual\nreasoning to lower-level execution. While AI-powered tools increasingly support\nexecution tasks, there remains a need for intelligent assistance in conceptual\ntasks. This paper investigates the design of an ordered node-link tree\ninterface augmented with AI-generated information hints and visualizations, as\na potential shared representation for hypothesis exploration. Through a design\nprobe (n=22), participants generated diagrams averaging 21.82 hypotheses. Our\nfindings showed that the node-link diagram acts as \"guardrails\" for hypothesis\nexploration, facilitating structured workflows, providing comprehensive\noverviews, and enabling efficient backtracking. The AI-generated information\nhints, particularly visualizations, aided users in transforming abstract ideas\ninto data-backed concepts while reducing cognitive load. We further discuss how\nnode-link diagrams can support both parallel exploration and iterative\nrefinement in hypothesis formulation, potentially enhancing the breadth and\ndepth of human-AI collaborative data analysis.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "authors": [
      "Zijian Ding",
      "Michelle Brachman",
      "Joel Chan",
      "Werner Geyer"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16789v1",
    "title": "Conversational User-AI Intervention: A Study on Prompt Rewriting for Improved LLM Response Generation",
    "abstract": "Human-LLM conversations are increasingly becoming more pervasive in peoples'\nprofessional and personal lives, yet many users still struggle to elicit\nhelpful responses from LLM Chatbots. One of the reasons for this issue is\nusers' lack of understanding in crafting effective prompts that accurately\nconvey their information needs. Meanwhile, the existence of real-world\nconversational datasets on the one hand, and the text understanding faculties\nof LLMs on the other, present a unique opportunity to study this problem, and\nits potential solutions at scale. Thus, in this paper we present the first\nLLM-centric study of real human-AI chatbot conversations, focused on\ninvestigating aspects in which user queries fall short of expressing\ninformation needs, and the potential of using LLMs to rewrite suboptimal user\nprompts. Our findings demonstrate that rephrasing ineffective prompts can\nelicit better responses from a conversational system, while preserving the\nuser's original intent. Notably, the performance of rewrites improves in longer\nconversations, where contextual inferences about user needs can be made more\naccurately. Additionally, we observe that LLMs often need to -- and inherently\ndo -- make \\emph{plausible} assumptions about a user's intentions and goals\nwhen interpreting prompts. Our findings largely hold true across conversational\ndomains, user intents, and LLMs of varying sizes and families, indicating the\npromise of using prompt rewriting as a solution for better human-AI\ninteractions.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Rupak Sarkar",
      "Bahareh Sarrafzadeh",
      "Nirupama Chandrasekaran",
      "Nagu Rangan",
      "Philip Resnik",
      "Longqi Yang",
      "Sujay Kumar Jauhar"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16788v1",
    "title": "Does Chain-of-Thought Reasoning Help Mobile GUI Agent? An Empirical Study",
    "abstract": "Reasoning capabilities have significantly improved the performance of\nvision-language models (VLMs) in domains such as mathematical problem-solving,\ncoding, and visual question-answering. However, their impact on real-world\napplications remains unclear. This paper presents the first empirical study on\nthe effectiveness of reasoning-enabled VLMs in mobile GUI agents, a domain that\nrequires interpreting complex screen layouts, understanding user instructions,\nand executing multi-turn interactions. We evaluate two pairs of commercial\nmodels--Gemini 2.0 Flash and Claude 3.7 Sonnet--comparing their base and\nreasoning-enhanced versions across two static benchmarks (ScreenSpot and\nAndroidControl) and one interactive environment (AndroidWorld). We surprisingly\nfind the Claude 3.7 Sonnet reasoning model achieves state-of-the-art\nperformance on AndroidWorld. However, reasoning VLMs generally offer marginal\nimprovements over non-reasoning models on static benchmarks and even degrade\nperformance in some agent setups. Notably, reasoning and non-reasoning VLMs\nfail on different sets of tasks, suggesting that reasoning does have an impact,\nbut its benefits and drawbacks counterbalance each other. We attribute these\ninconsistencies to the limitations of benchmarks and VLMs. Based on the\nfindings, we provide insights for further enhancing mobile GUI agents in terms\nof benchmarks, VLMs, and their adaptability in dynamically invoking reasoning\nVLMs. The experimental data are publicly available at\nhttps://github.com/LlamaTouch/VLM-Reasoning-Traces.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Li Zhang",
      "Longxi Gao",
      "Mengwei Xu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16782v1",
    "title": "Learning Part Knowledge to Facilitate Category Understanding for Fine-Grained Generalized Category Discovery",
    "abstract": "Generalized Category Discovery (GCD) aims to classify unlabeled data\ncontaining both seen and novel categories. Although existing methods perform\nwell on generic datasets, they struggle in fine-grained scenarios. We attribute\nthis difficulty to their reliance on contrastive learning over global image\nfeatures to automatically capture discriminative cues, which fails to capture\nthe subtle local differences essential for distinguishing fine-grained\ncategories. Therefore, in this paper, we propose incorporating part knowledge\nto address fine-grained GCD, which introduces two key challenges: the absence\nof annotations for novel classes complicates the extraction of the part\nfeatures, and global contrastive learning prioritizes holistic feature\ninvariance, inadvertently suppressing discriminative local part patterns. To\naddress these challenges, we propose PartGCD, including 1) Adaptive Part\nDecomposition, which automatically extracts class-specific semantic parts via\nGaussian Mixture Models, and 2) Part Discrepancy Regularization, enforcing\nexplicit separation between part features to amplify fine-grained local part\ndistinctions.\n  Experiments demonstrate state-of-the-art performance across multiple\nfine-grained benchmarks while maintaining competitiveness on generic datasets,\nvalidating the effectiveness and robustness of our approach.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Enguang Wang",
      "Zhimao Peng",
      "Zhengyuan Xie",
      "Haori Lu",
      "Fei Yang",
      "Xialei Liu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16780v1",
    "title": "A-IDE : Agent-Integrated Denoising Experts",
    "abstract": "Recent advances in deep-learning based denoising methods have improved\nLow-Dose CT image quality. However, due to distinct HU distributions and\ndiverse anatomical characteristics, a single model often struggles to\ngeneralize across multiple anatomies. To address this limitation, we introduce\n\\textbf{Agent-Integrated Denoising Experts (A-IDE)} framework, which integrates\nthree anatomical region-specialized RED-CNN models under the management of\ndecision-making LLM agent. The agent analyzes semantic cues from BiomedCLIP to\ndynamically route incoming LDCT scans to the most appropriate expert model. We\nhighlight three major advantages of our approach. A-IDE excels in\nheterogeneous, data-scarce environments. The framework automatically prevents\noverfitting by distributing tasks among multiple experts. Finally, our\nLLM-driven agentic pipeline eliminates the need for manual interventions.\nExperimental evaluations on the Mayo-2016 dataset confirm that A-IDE achieves\nsuperior performance in RMSE, PSNR, and SSIM compared to a single unified\ndenoiser.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Uihyun Cho",
      "Namhun Kim"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16779v1",
    "title": "Chain-of-Tools: Utilizing Massive Unseen Tools in the CoT Reasoning of Frozen Language Models",
    "abstract": "Tool learning can further broaden the usage scenarios of large language\nmodels (LLMs). However most of the existing methods either need to finetune\nthat the model can only use tools seen in the training data, or add tool\ndemonstrations into the prompt with lower efficiency. In this paper, we present\na new Tool Learning method Chain-of-Tools. It makes full use of the powerful\nsemantic representation capability of frozen LLMs to finish tool calling in CoT\nreasoning with a huge and flexible tool pool which may contain unseen tools.\nEspecially, to validate the effectiveness of our approach in the massive unseen\ntool scenario, we construct a new dataset SimpleToolQuestions. We conduct\nexperiments on two numerical reasoning benchmarks (GSM8K-XL and FuncQA) and two\nknowledge-based question answering benchmarks (KAMEL and SimpleToolQuestions).\nExperimental results show that our approach performs better than the baseline.\nWe also identify dimensions of the model output that are critical in tool\nselection, enhancing the model interpretability. Our code and data are\navailable at: https://github.com/fairyshine/Chain-of-Tools .",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Mengsong Wu",
      "Tong Zhu",
      "Han Han",
      "Xiang Zhang",
      "Wenbiao Shao",
      "Wenliang Chen"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16777v1",
    "title": "Physics-Informed Deep B-Spline Networks for Dynamical Systems",
    "abstract": "Physics-informed machine learning provides an approach to combining data and\ngoverning physics laws for solving complex partial differential equations\n(PDEs). However, efficiently solving PDEs with varying parameters and changing\ninitial conditions and boundary conditions (ICBCs) with theoretical guarantees\nremains an open challenge. We propose a hybrid framework that uses a neural\nnetwork to learn B-spline control points to approximate solutions to PDEs with\nvarying system and ICBC parameters. The proposed network can be trained\nefficiently as one can directly specify ICBCs without imposing losses,\ncalculate physics-informed loss functions through analytical formulas, and\nrequires only learning the weights of B-spline functions as opposed to both\nweights and basis as in traditional neural operator learning methods. We\nprovide theoretical guarantees that the proposed B-spline networks serve as\nuniversal approximators for the set of solutions of PDEs with varying ICBCs\nunder mild conditions and establish bounds on the generalization errors in\nphysics-informed learning. We also demonstrate in experiments that the proposed\nB-spline network can solve problems with discontinuous ICBCs and outperforms\nexisting methods, and is able to learn solutions of 3D dynamics with diverse\ninitial conditions.",
    "categories": [
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Zhuoyuan Wang",
      "Raffaele Romagnoli",
      "Jasmine Ratchford",
      "Yorie Nakahira"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16776v1",
    "title": "OpenCity3D: What do Vision-Language Models know about Urban Environments?",
    "abstract": "Vision-language models (VLMs) show great promise for 3D scene understanding\nbut are mainly applied to indoor spaces or autonomous driving, focusing on\nlow-level tasks like segmentation. This work expands their use to urban-scale\nenvironments by leveraging 3D reconstructions from multi-view aerial imagery.\nWe propose OpenCity3D, an approach that addresses high-level tasks, such as\npopulation density estimation, building age classification, property price\nprediction, crime rate assessment, and noise pollution evaluation. Our findings\nhighlight OpenCity3D's impressive zero-shot and few-shot capabilities,\nshowcasing adaptability to new contexts. This research establishes a new\nparadigm for language-driven urban analytics, enabling applications in\nplanning, policy, and environmental monitoring. See our project page:\nopencity3d.github.io",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Valentin Bieri",
      "Marco Zamboni",
      "Nicolas S. Blumer",
      "Qingxuan Chen",
      "Francis Engelmann"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16775v1",
    "title": "Region Masking to Accelerate Video Processing on Neuromorphic Hardware",
    "abstract": "The rapidly growing demand for on-chip edge intelligence on\nresource-constrained devices has motivated approaches to reduce energy and\nlatency of deep learning models. Spiking neural networks (SNNs) have gained\nparticular interest due to their promise to reduce energy consumption using\nevent-based processing. We assert that while sigma-delta encoding in SNNs can\ntake advantage of the temporal redundancy across video frames, they still\ninvolve a significant amount of redundant computations due to processing\ninsignificant events. In this paper, we propose a region masking strategy that\nidentifies regions of interest at the input of the SNN, thereby eliminating\ncomputation and data movement for events arising from unimportant regions. Our\napproach demonstrates that masking regions at the input not only significantly\nreduces the overall spiking activity of the network, but also provides\nsignificant improvement in throughput and latency. We apply region masking\nduring video object detection on Loihi 2, demonstrating that masking\napproximately 60% of input regions can reduce energy-delay product by 1.65x\nover a baseline sigma-delta network, with a degradation in mAP@0.5 by 1.09%.",
    "categories": [
      "cs.CV",
      "cs.NE",
      "eess.IV"
    ],
    "authors": [
      "Sreetama Sarkar",
      "Sumit Bam Shrestha",
      "Yue Che",
      "Leobardo Campos-Macias",
      "Gourav Datta",
      "Peter A. Beerel"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16771v1",
    "title": "On Explaining (Large) Language Models For Code Using Global Code-Based Explanations",
    "abstract": "In recent years, Language Models for Code (LLM4Code) have significantly\nchanged the landscape of software engineering (SE) on downstream tasks, such as\ncode generation, by making software development more efficient. Therefore, a\ngrowing interest has emerged in further evaluating these Language Models to\nhomogenize the quality assessment of generated code. As the current evaluation\nprocess can significantly overreact on accuracy-based metrics, practitioners\noften seek methods to interpret LLM4Code outputs beyond canonical benchmarks.\nWhile the majority of research reports on code generation effectiveness in\nterms of expected ground truth, scant attention has been paid to LLMs'\nexplanations. In essence, the decision-making process to generate code is hard\nto interpret. To bridge this evaluation gap, we introduce code rationales\n(Code$Q$), a technique with rigorous mathematical underpinning, to identify\nsubsets of tokens that can explain individual code predictions. We conducted a\nthorough Exploratory Analysis to demonstrate the method's applicability and a\nUser Study to understand the usability of code-based explanations. Our\nevaluation demonstrates that Code$Q$ is a powerful interpretability method to\nexplain how (less) meaningful input concepts (i.e., natural language particle\n`at') highly impact output generation. Moreover, participants of this study\nhighlighted Code$Q$'s ability to show a causal relationship between the input\nand output of the model with readable and informative explanations on code\ncompletion and test generation tasks. Additionally, Code$Q$ also helps to\nuncover model rationale, facilitating comparison with a human rationale to\npromote a fair level of trust and distrust in the model.",
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "authors": [
      "David N. Palacio",
      "Dipin Khati",
      "Daniel Rodriguez-Cardenas",
      "Alejandro Velasco",
      "Denys Poshyvanyk"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16768v1",
    "title": "Dynamic Attention Mechanism in Spatiotemporal Memory Networks for Object Tracking",
    "abstract": "Mainstream visual object tracking frameworks predominantly rely on template\nmatching paradigms. Their performance heavily depends on the quality of\ntemplate features, which becomes increasingly challenging to maintain in\ncomplex scenarios involving target deformation, occlusion, and background\nclutter. While existing spatiotemporal memory-based trackers emphasize memory\ncapacity expansion, they lack effective mechanisms for dynamic feature\nselection and adaptive fusion. To address this gap, we propose a Dynamic\nAttention Mechanism in Spatiotemporal Memory Network (DASTM) with two key\ninnovations: 1) A differentiable dynamic attention mechanism that adaptively\nadjusts channel-spatial attention weights by analyzing spatiotemporal\ncorrelations between the templates and memory features; 2) A lightweight gating\nnetwork that autonomously allocates computational resources based on target\nmotion states, prioritizing high-discriminability features in challenging\nscenarios. Extensive evaluations on OTB-2015, VOT 2018, LaSOT, and GOT-10K\nbenchmarks demonstrate our DASTM's superiority, achieving state-of-the-art\nperformance in success rate, robustness, and real-time efficiency, thereby\noffering a novel solution for real-time tracking in complex environments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Meng Zhou",
      "Jiadong Xie",
      "Mingsheng Xu"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16760v1",
    "title": "Rethinking the Role of Spatial Mixing",
    "abstract": "Until quite recently, the backbone of nearly every state-of-the-art computer\nvision model has been the 2D convolution. At its core, a 2D convolution\nsimultaneously mixes information across both the spatial and channel dimensions\nof a representation. Many recent computer vision architectures consist of\nsequences of isotropic blocks that disentangle the spatial and channel-mixing\ncomponents. This separation of the operations allows us to more closely\njuxtapose the effects of spatial and channel mixing in deep learning. In this\npaper, we take an initial step towards garnering a deeper understanding of the\nroles of these mixing operations. Through our experiments and analysis, we\ndiscover that on both classical (ResNet) and cutting-edge (ConvMixer) models,\nwe can reach nearly the same level of classification performance by and leaving\nthe spatial mixers at their random initializations. Furthermore, we show that\nmodels with random, fixed spatial mixing are naturally more robust to\nadversarial perturbations. Lastly, we show that this phenomenon extends past\nthe classification regime, as such models can also decode pixel-shuffled\nimages.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "George Cazenavette",
      "Joel Julin",
      "Simon Lucey"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16759v1",
    "title": "elaTCSF: A Temporal Contrast Sensitivity Function for Flicker Detection and Modeling Variable Refresh Rate Flicker",
    "abstract": "The perception of flicker has been a prominent concern in illumination and\nelectronic display fields for over a century. Traditional approaches often rely\non Critical Flicker Frequency (CFF), primarily suited for high-contrast\n(full-on, full-off) flicker. To tackle varying contrast flicker, the\nInternational Committee for Display Metrology (ICDM) introduced a Temporal\nContrast Sensitivity Function TCSF$_{IDMS}$ within the Information Display\nMeasurements Standard (IDMS). Nevertheless, this standard overlooks crucial\nparameters: luminance, eccentricity, and area. Existing models incorporating\nthese parameters are inadequate for flicker detection, especially at low\nspatial frequencies. To address these limitations, we extend the TCSF$_{IDMS}$\nand combine it with a new spatial probability summation model to incorporate\nthe effects of luminance, eccentricity, and area (elaTCSF). We train the\nelaTCSF on various flicker detection datasets and establish the first variable\nrefresh rate flicker detection dataset for further verification. Additionally,\nwe contribute to resolving a longstanding debate on whether the flicker is more\nvisible in peripheral vision. We demonstrate how elaTCSF can be used to predict\nflicker due to low-persistence in VR headsets, identify flicker-free VRR\noperational ranges, and determine flicker sensitivity in lighting design.",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Yancheng Cai",
      "Ali Bozorgian",
      "Maliha Ashraf",
      "Robert Wanat",
      "Rafał K. Mantiuk"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16755v1",
    "title": "Fast online node labeling with graph subsampling",
    "abstract": "Large data applications rely on storing data in massive, sparse graphs with\nmillions to trillions of nodes. Graph-based methods, such as node prediction,\naim for computational efficiency regardless of graph size. Techniques like\nlocalized approximate personalized page rank (APPR) solve sparse linear systems\nwith complexity independent of graph size, but is in terms of the maximum node\ndegree, which can be much larger in practice than the average node degree for\nreal-world large graphs. In this paper, we consider an \\emph{online subsampled\nAPPR method}, where messages are intentionally dropped at random. We use tools\nfrom graph sparsifiers and matrix linear algebra to give approximation bounds\non the graph's spectral properties ($O(1/\\epsilon^2)$ edges), and node\nclassification performance (added $O(n\\epsilon)$ overhead).",
    "categories": [
      "cs.DS",
      "cs.LG"
    ],
    "authors": [
      "Yushen Huang",
      "Ertai Luo",
      "Reza Babenezhad",
      "Yifan Sun"
    ],
    "published_date": "2025-03-21",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16753v1",
    "title": "EarlyStopping: Implicit Regularization for Iterative Learning Procedures in Python",
    "abstract": "Iterative learning procedures are ubiquitous in machine learning and modern\nstatistics.\n  Regularision is typically required to prevent inflating the expected loss of\na procedure in\n  later iterations via the propagation of noise inherent in the data.\n  Significant emphasis has been placed on achieving this regularisation\nimplicitly by stopping\n  procedures early.\n  The EarlyStopping-package provides a toolbox of (in-sample) sequential early\nstopping rules for\n  several well-known iterative estimation procedures, such as truncated SVD,\nLandweber (gradient\n  descent), conjugate gradient descent, L2-boosting and regression trees.\n  One of the central features of the package is that the algorithms allow the\nspecification of the\n  true data-generating process and keep track of relevant theoretical\nquantities.\n  In this paper, we detail the principles governing the implementation of the\nEarlyStopping-package and provide\n  a survey of recent foundational advances in the theoretical literature.\n  We demonstrate how to use the EarlyStopping-package to explore core features\nof implicit regularisation\n  and replicate results from the literature.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "cs.MS"
    ],
    "authors": [
      "Eric Ziebell",
      "Ratmir Miftachov",
      "Bernhard Stankewitz",
      "Laura Hucker"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16747v1",
    "title": "SAGE: Semantic-Driven Adaptive Gaussian Splatting in Extended Reality",
    "abstract": "3D Gaussian Splatting (3DGS) has significantly improved the efficiency and\nrealism of three-dimensional scene visualization in several applications,\nranging from robotics to eXtended Reality (XR). This work presents SAGE\n(Semantic-Driven Adaptive Gaussian Splatting in Extended Reality), a novel\nframework designed to enhance the user experience by dynamically adapting the\nLevel of Detail (LOD) of different 3DGS objects identified via a semantic\nsegmentation. Experimental results demonstrate how SAGE effectively reduces\nmemory and computational overhead while keeping a desired target visual\nquality, thus providing a powerful optimization for interactive XR\napplications.",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.MM"
    ],
    "authors": [
      "Chiara Schiavo",
      "Elena Camuffo",
      "Leonardo Badia",
      "Simone Milani"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16746v1",
    "title": "Ordered Topological Deep Learning: a Network Modeling Case Study",
    "abstract": "Computer networks are the foundation of modern digital infrastructure,\nfacilitating global communication and data exchange. As demand for reliable\nhigh-bandwidth connectivity grows, advanced network modeling techniques become\nincreasingly essential to optimize performance and predict network behavior.\nTraditional modeling methods, such as packet-level simulators and queueing\ntheory, have notable limitations --either being computationally expensive or\nrelying on restrictive assumptions that reduce accuracy. In this context, the\ndeep learning-based RouteNet family of models has recently redefined network\nmodeling by showing an unprecedented cost-performance trade-off. In this work,\nwe revisit RouteNet's sophisticated design and uncover its hidden connection to\nTopological Deep Learning (TDL), an emerging field that models higher-order\ninteractions beyond standard graph-based methods. We demonstrate that, although\noriginally formulated as a heterogeneous Graph Neural Network, RouteNet serves\nas the first instantiation of a new form of TDL. More specifically, this paper\npresents OrdGCCN, a novel TDL framework that introduces the notion of ordered\nneighbors in arbitrary discrete topological spaces, and shows that RouteNet's\narchitecture can be naturally described as an ordered topological neural\nnetwork. To the best of our knowledge, this marks the first successful\nreal-world application of state-of-the-art TDL principles --which we confirm\nthrough extensive testbed experiments--, laying the foundation for the next\ngeneration of ordered TDL-driven applications.",
    "categories": [
      "cs.LG",
      "cs.NI"
    ],
    "authors": [
      "Guillermo Bernárdez",
      "Miquel Ferriol-Galmés",
      "Carlos Güemes-Palau",
      "Mathilde Papillon",
      "Pere Barlet-Ros",
      "Albert Cabellos-Aparicio",
      "Nina Miolane"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16745v1",
    "title": "SPACER: A Parallel Dataset of Speech Production And Comprehension of Error Repairs",
    "abstract": "Speech errors are a natural part of communication, yet they rarely lead to\ncomplete communicative failure because both speakers and comprehenders can\ndetect and correct errors. Although prior research has examined error\nmonitoring and correction in production and comprehension separately,\nintegrated investigation of both systems has been impeded by the scarcity of\nparallel data. In this study, we present SPACER, a parallel dataset that\ncaptures how naturalistic speech errors are corrected by both speakers and\ncomprehenders. We focus on single-word substitution errors extracted from the\nSwitchboard corpus, accompanied by speaker's self-repairs and comprehenders'\nresponses from an offline text-editing experiment. Our exploratory analysis\nsuggests asymmetries in error correction strategies: speakers are more likely\nto repair errors that introduce greater semantic and phonemic deviations,\nwhereas comprehenders tend to correct errors that are phonemically similar to\nmore plausible alternatives or do not fit into prior contexts. Our dataset\nenables future research on integrated approaches toward studying language\nproduction and comprehension.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Shiva Upadhye",
      "Jiaxuan Li",
      "Richard Futrell"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16743v1",
    "title": "SuperARC: A Test for General and Super Intelligence Based on First Principles of Recursion Theory and Algorithmic Probability",
    "abstract": "We introduce an open-ended test grounded in algorithmic probability that can\navoid benchmark contamination in the quantitative evaluation of frontier models\nin the context of their Artificial General Intelligence (AGI) and\nSuperintelligence (ASI) claims. Unlike other tests, this test does not rely on\nstatistical compression methods (such as GZIP or LZW), which are more closely\nrelated to Shannon entropy than to Kolmogorov complexity. The test challenges\naspects related to features of intelligence of fundamental nature such as\nsynthesis and model creation in the context of inverse problems (generating new\nknowledge from observation). We argue that metrics based on model abstraction\nand optimal Bayesian inference for planning can provide a robust framework for\ntesting intelligence, including natural intelligence (human and animal), narrow\nAI, AGI, and ASI. Our results show no clear evidence of LLM convergence towards\na defined level of intelligence, particularly AGI or ASI. We found that LLM\nmodel versions tend to be fragile and incremental, as new versions may perform\nworse than older ones, with progress largely driven by the size of training\ndata. The results were compared with a hybrid neurosymbolic approach that\ntheoretically guarantees model convergence from optimal inference based on the\nprinciples of algorithmic probability and Kolmogorov complexity. The method\noutperforms LLMs in a proof-of-concept on short binary sequences. Our findings\nconfirm suspicions regarding the fundamental limitations of LLMs, exposing them\nas systems optimised for the perception of mastery over human language.\nProgress among different LLM versions from the same developers was found to be\ninconsistent and limited, particularly in the absence of a solid symbolic\ncounterpart.",
    "categories": [
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "authors": [
      "Alberto Hernández-Espinosa",
      "Luan Ozelim",
      "Felipe S. Abrahão",
      "Hector Zenil"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16742v1",
    "title": "Digitally Prototype Your Eye Tracker: Simulating Hardware Performance using 3D Synthetic Data",
    "abstract": "Eye tracking (ET) is a key enabler for Augmented and Virtual Reality (AR/VR).\nPrototyping new ET hardware requires assessing the impact of hardware choices\non eye tracking performance. This task is compounded by the high cost of\nobtaining data from sufficiently many variations of real hardware, especially\nfor machine learning, which requires large training datasets. We propose a\nmethod for end-to-end evaluation of how hardware changes impact machine\nlearning-based ET performance using only synthetic data. We utilize a dataset\nof real 3D eyes, reconstructed from light dome data using neural radiance\nfields (NeRF), to synthesize captured eyes from novel viewpoints and camera\nparameters. Using this framework, we demonstrate that we can predict the\nrelative performance across various hardware configurations, accounting for\nvariations in sensor noise, illumination brightness, and optical blur. We also\ncompare our simulator with the publicly available eye tracking dataset from the\nProject Aria glasses, demonstrating a strong correlation with real-world\nperformance. Finally, we present a first-of-its-kind analysis in which we vary\nET camera positions, evaluating ET performance ranging from on-axis direct\nviews of the eye to peripheral views on the frame. Such an analysis would have\npreviously required manufacturing physical devices to capture evaluation data.\nIn short, our method enables faster prototyping of ET hardware.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Esther Y. H. Lin",
      "Yimin Ding",
      "Jogendra Kundu",
      "Yatong An",
      "Mohamed T. El-Haddad",
      "Alexander Fix"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16737v1",
    "title": "Optimal Nonlinear Online Learning under Sequential Price Competition via s-Concavity",
    "abstract": "We consider price competition among multiple sellers over a selling horizon\nof $T$ periods. In each period, sellers simultaneously offer their prices and\nsubsequently observe their respective demand that is unobservable to\ncompetitors. The demand function for each seller depends on all sellers' prices\nthrough a private, unknown, and nonlinear relationship. To address this\nchallenge, we propose a semi-parametric least-squares estimation of the\nnonlinear mean function, which does not require sellers to communicate demand\ninformation. We show that when all sellers employ our policy, their prices\nconverge at a rate of $O(T^{-1/7})$ to the Nash equilibrium prices that sellers\nwould reach if they were fully informed. Each seller incurs a regret of\n$O(T^{5/7})$ relative to a dynamic benchmark policy. A theoretical contribution\nof our work is proving the existence of equilibrium under shape-constrained\ndemand functions via the concept of $s$-concavity and establishing regret\nbounds of our proposed policy. Technically, we also establish new concentration\nresults for the least squares estimator under shape constraints. Our findings\noffer significant insights into dynamic competition-aware pricing and\ncontribute to the broader study of non-parametric learning in strategic\ndecision-making.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.PR",
      "math.ST",
      "stat.TH"
    ],
    "authors": [
      "Daniele Bracale",
      "Moulinath Banerjee",
      "Cong Shi",
      "Yuekai Sun"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16734v1",
    "title": "Towards Agentic Recommender Systems in the Era of Multimodal Large Language Models",
    "abstract": "Recent breakthroughs in Large Language Models (LLMs) have led to the\nemergence of agentic AI systems that extend beyond the capabilities of\nstandalone models. By empowering LLMs to perceive external environments,\nintegrate multimodal information, and interact with various tools, these\nagentic systems exhibit greater autonomy and adaptability across complex tasks.\nThis evolution brings new opportunities to recommender systems (RS): LLM-based\nAgentic RS (LLM-ARS) can offer more interactive, context-aware, and proactive\nrecommendations, potentially reshaping the user experience and broadening the\napplication scope of RS. Despite promising early results, fundamental\nchallenges remain, including how to effectively incorporate external knowledge,\nbalance autonomy with controllability, and evaluate performance in dynamic,\nmultimodal settings. In this perspective paper, we first present a systematic\nanalysis of LLM-ARS: (1) clarifying core concepts and architectures; (2)\nhighlighting how agentic capabilities -- such as planning, memory, and\nmultimodal reasoning -- can enhance recommendation quality; and (3) outlining\nkey research questions in areas such as safety, efficiency, and lifelong\npersonalization. We also discuss open problems and future directions, arguing\nthat LLM-ARS will drive the next wave of RS innovation. Ultimately, we foresee\na paradigm shift toward intelligent, autonomous, and collaborative\nrecommendation experiences that more closely align with users' evolving needs\nand complex decision-making processes.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "authors": [
      "Chengkai Huang",
      "Junda Wu",
      "Yu Xia",
      "Zixu Yu",
      "Ruhan Wang",
      "Tong Yu",
      "Ruiyi Zhang",
      "Ryan A. Rossi",
      "Branislav Kveton",
      "Dongruo Zhou",
      "Julian McAuley",
      "Lina Yao"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16731v1",
    "title": "Design and Implementation of an FPGA-Based Tiled Matrix Multiplication Accelerator for Transformer Self-Attention on the Xilinx KV260 SoM",
    "abstract": "Transformer-based LLMs spend most of their compute in large matrix\nmultiplications for attention and feed-forward layers. Recognizing that the Q,\nK, and V linear projections within the Multi-Head Self-Attention (MHA) module\nrepresent a critical computational bottleneck, we strategically focused our\nefforts on accelerating these operations. We present a tiled matrix\nmultiplication accelerator optimized for such workloads on a Xilinx KV260\non-board FPGA. Key innovations include persistent on-chip storage for one\nmatrix operand, two-level tiling for data reuse, and a systolic-like unrolled\ncompute engine. Implemented via high-level synthesis (HLS) and integrated with\nDistilBERT for Q, K, V projections, our accelerator achieves significant\nspeedup and energy efficiency gains over CPU baselines. Standalone GEMM\nbenchmarks show up to a 7x speedup over an ARM CPU (PyTorch) and ~200x over\nnaive numpy, with a throughput of up to 3.1 GFLOPs on 768x3072 matrices.\nAlthough the overall end-to-end DistilBERT acceleration is more modest, our\nresults validate the potential of FPGA-based acceleration for critical\ncomponents of Transformer models.",
    "categories": [
      "cs.AR",
      "cs.CL",
      "cs.LG",
      "B.7.1; C.1.4"
    ],
    "authors": [
      "Zhaoqin \"Richie\" Li",
      "Sicheng Chen"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16728v1",
    "title": "Natural Language Generation",
    "abstract": "This article provides a brief overview of the field of Natural Language\nGeneration. The term Natural Language Generation (NLG), in its broadest\ndefinition, refers to the study of systems that verbalize some form of\ninformation through natural language. That information could be stored in a\nlarge database or knowledge graph (in data-to-text applications), but NLG\nresearchers may also study summarisation (text-to-text) or image captioning\n(image-to-text), for example. As a subfield of Natural Language Processing, NLG\nis closely related to other sub-disciplines such as Machine Translation (MT)\nand Dialog Systems. Some NLG researchers exclude MT from their definition of\nthe field, since there is no content selection involved where the system has to\ndetermine what to say. Conversely, dialog systems do not typically fall under\nthe header of Natural Language Generation since NLG is just one component of\ndialog systems (the others being Natural Language Understanding and Dialog\nManagement). However, with the rise of Large Language Models (LLMs), different\nsubfields of Natural Language Processing have converged on similar\nmethodologies for the production of natural language and the evaluation of\nautomatically generated text.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Emiel van Miltenburg",
      "Chenghua Lin"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16726v1",
    "title": "EDiT: Efficient Diffusion Transformers with Linear Compressed Attention",
    "abstract": "Diffusion Transformers (DiTs) have emerged as a leading architecture for\ntext-to-image synthesis, producing high-quality and photorealistic images.\nHowever, the quadratic scaling properties of the attention in DiTs hinder image\ngeneration with higher resolution or on devices with limited resources. This\nwork introduces an efficient diffusion transformer (EDiT) to alleviate these\nefficiency bottlenecks in conventional DiTs and Multimodal DiTs (MM-DiTs).\nFirst, we present a novel linear compressed attention method that uses a\nmulti-layer convolutional network to modulate queries with local information\nwhile keys and values are spatially aggregated. Second, we formulate a hybrid\nattention scheme for multi-modal inputs that combines linear attention for\nimage-to-image interactions and standard scaled dot-product attention for\ninteractions involving prompts. Merging these two approaches leads to an\nexpressive, linear-time Multimodal Efficient Diffusion Transformer (MM-EDiT).\nWe demonstrate the effectiveness of the EDiT and MM-EDiT architectures by\nintegrating them into PixArt-Sigma(conventional DiT) and Stable Diffusion\n3.5-Medium (MM-DiT), achieving up to 2.2x speedup with comparable image quality\nafter distillation.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Philipp Becker",
      "Abhinav Mehrotra",
      "Ruchika Chavhan",
      "Malcolm Chadwick",
      "Luca Morreale",
      "Mehdi Noroozi",
      "Alberto Gil Ramos",
      "Sourav Bhattacharya"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16724v1",
    "title": "Towards Automated Semantic Interpretability in Reinforcement Learning via Vision-Language Models",
    "abstract": "Semantic Interpretability in Reinforcement Learning (RL) enables\ntransparency, accountability, and safer deployment by making the agent's\ndecisions understandable and verifiable. Achieving this, however, requires a\nfeature space composed of human-understandable concepts, which traditionally\nrely on human specification and fail to generalize to unseen environments. In\nthis work, we introduce Semantically Interpretable Reinforcement Learning with\nVision-Language Models Empowered Automation (SILVA), an automated framework\nthat leverages pre-trained vision-language models (VLM) for semantic feature\nextraction and interpretable tree-based models for policy optimization. SILVA\nfirst queries a VLM to identify relevant semantic features for an unseen\nenvironment, then extracts these features from the environment. Finally, it\ntrains an Interpretable Control Tree via RL, mapping the extracted features to\nactions in a transparent and interpretable manner. To address the computational\ninefficiency of extracting features directly with VLMs, we develop a feature\nextraction pipeline that generates a dataset for training a lightweight\nconvolutional network, which is subsequently used during RL. By leveraging VLMs\nto automate tree-based RL, SILVA removes the reliance on human annotation\npreviously required by interpretable models while also overcoming the inability\nof VLMs alone to generate valid robot policies, enabling semantically\ninterpretable reinforcement learning without human-in-the-loop.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Zhaoxin Li",
      "Zhang Xi-Jia",
      "Batuhan Altundas",
      "Letian Chen",
      "Rohan Paleja",
      "Matthew Gombolay"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16718v1",
    "title": "CAARMA: Class Augmentation with Adversarial Mixup Regularization",
    "abstract": "Speaker verification is a typical zero-shot learning task, where inference of\nunseen classes is performed by comparing embeddings of test instances to known\nexamples. The models performing inference must hence naturally generate\nembeddings that cluster same-class instances compactly, while maintaining\nseparation across classes. In order to learn to do so, they are typically\ntrained on a large number of classes (speakers), often using specialized\nlosses. However real-world speaker datasets often lack the class diversity\nneeded to effectively learn this in a generalizable manner. We introduce\nCAARMA, a class augmentation framework that addresses this problem by\ngenerating synthetic classes through data mixing in the embedding space,\nexpanding the number of training classes. To ensure the authenticity of the\nsynthetic classes we adopt a novel adversarial refinement mechanism that\nminimizes categorical distinctions between synthetic and real classes. We\nevaluate CAARMA on multiple speaker verification tasks, as well as other\nrepresentative zero-shot comparison-based speech analysis tasks and obtain\nconsistent improvements: our framework demonstrates a significant improvement\nof 8\\% over all baseline models. Code for CAARMA will be released.",
    "categories": [
      "cs.SD",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Massa Baali",
      "Xiang Li",
      "Hao Chen",
      "Rita Singh",
      "Bhiksha Raj"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16711v1",
    "title": "Depth Matters: Multimodal RGB-D Perception for Robust Autonomous Agents",
    "abstract": "Autonomous agents that rely purely on perception to make real-time control\ndecisions require efficient and robust architectures. In this work, we\ndemonstrate that augmenting RGB input with depth information significantly\nenhances our agents' ability to predict steering commands compared to using RGB\nalone. We benchmark lightweight recurrent controllers that leverage the fused\nRGB-D features for sequential decision-making. To train our models, we collect\nhigh-quality data using a small-scale autonomous car controlled by an expert\ndriver via a physical steering wheel, capturing varying levels of steering\ndifficulty. Our models, trained under diverse configurations, were successfully\ndeployed on real hardware. Specifically, our findings reveal that the early\nfusion of depth data results in a highly robust controller, which remains\neffective even with frame drops and increased noise levels, without\ncompromising the network's focus on the task.",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Mihaela-Larisa Clement",
      "Mónika Farsang",
      "Felix Resch",
      "Radu Grosu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16710v1",
    "title": "4D Gaussian Splatting SLAM",
    "abstract": "Simultaneously localizing camera poses and constructing Gaussian radiance\nfields in dynamic scenes establish a crucial bridge between 2D images and the\n4D real world. Instead of removing dynamic objects as distractors and\nreconstructing only static environments, this paper proposes an efficient\narchitecture that incrementally tracks camera poses and establishes the 4D\nGaussian radiance fields in unknown scenarios by using a sequence of RGB-D\nimages. First, by generating motion masks, we obtain static and dynamic priors\nfor each pixel. To eliminate the influence of static scenes and improve the\nefficiency on learning the motion of dynamic objects, we classify the Gaussian\nprimitives into static and dynamic Gaussian sets, while the sparse control\npoints along with an MLP is utilized to model the transformation fields of the\ndynamic Gaussians. To more accurately learn the motion of dynamic Gaussians, a\nnovel 2D optical flow map reconstruction algorithm is designed to render\noptical flows of dynamic objects between neighbor images, which are further\nused to supervise the 4D Gaussian radiance fields along with traditional\nphotometric and geometric constraints. In experiments, qualitative and\nquantitative evaluation results show that the proposed method achieves robust\ntracking and high-quality view synthesis performance in real-world\nenvironments.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yanyan Li",
      "Youxu Fang",
      "Zunjie Zhu",
      "Kunyi Li",
      "Yong Ding",
      "Federico Tombari"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16709v1",
    "title": "QuartDepth: Post-Training Quantization for Real-Time Depth Estimation on the Edge",
    "abstract": "Monocular Depth Estimation (MDE) has emerged as a pivotal task in computer\nvision, supporting numerous real-world applications. However, deploying\naccurate depth estimation models on resource-limited edge devices, especially\nApplication-Specific Integrated Circuits (ASICs), is challenging due to the\nhigh computational and memory demands. Recent advancements in foundational\ndepth estimation deliver impressive results but further amplify the difficulty\nof deployment on ASICs. To address this, we propose QuartDepth which adopts\npost-training quantization to quantize MDE models with hardware accelerations\nfor ASICs. Our approach involves quantizing both weights and activations to\n4-bit precision, reducing the model size and computation cost. To mitigate the\nperformance degradation, we introduce activation polishing and compensation\nalgorithm applied before and after activation quantization, as well as a weight\nreconstruction method for minimizing errors in weight quantization.\nFurthermore, we design a flexible and programmable hardware accelerator by\nsupporting kernel fusion and customized instruction programmability, enhancing\nthroughput and efficiency. Experimental results demonstrate that our framework\nachieves competitive accuracy while enabling fast inference and higher energy\nefficiency on ASICs, bridging the gap between high-performance depth estimation\nand practical edge-device applicability. Code:\nhttps://github.com/shawnricecake/quart-depth",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Xuan Shen",
      "Weize Ma",
      "Jing Liu",
      "Changdi Yang",
      "Rui Ding",
      "Quanyi Wang",
      "Henghui Ding",
      "Wei Niu",
      "Yanzhi Wang",
      "Pu Zhao",
      "Jun Lin",
      "Jiuxiang Gu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16708v1",
    "title": "NeuroSep-CP-LCB: A Deep Learning-based Contextual Multi-armed Bandit Algorithm with Uncertainty Quantification for Early Sepsis Prediction",
    "abstract": "In critical care settings, timely and accurate predictions can significantly\nimpact patient outcomes, especially for conditions like sepsis, where early\nintervention is crucial. We aim to model patient-specific reward functions in a\ncontextual multi-armed bandit setting. The goal is to leverage patient-specific\nclinical features to optimize decision-making under uncertainty. This paper\nproposes NeuroSep-CP-LCB, a novel integration of neural networks with\ncontextual bandits and conformal prediction tailored for early sepsis\ndetection. Unlike the algorithm pool selection problem in the previous paper,\nwhere the primary focus was identifying the most suitable pre-trained model for\nprediction tasks, this work directly models the reward function using a neural\nnetwork, allowing for personalized and adaptive decision-making. Combining the\nrepresentational power of neural networks with the robustness of conformal\nprediction intervals, this framework explicitly accounts for uncertainty in\noffline data distributions and provides actionable confidence bounds on\npredictions.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Anni Zhou",
      "Raheem Beyah",
      "Rishikesan Kamaleswaran"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16707v1",
    "title": "Cross-Modal and Uncertainty-Aware Agglomeration for Open-Vocabulary 3D Scene Understanding",
    "abstract": "The lack of a large-scale 3D-text corpus has led recent works to distill\nopen-vocabulary knowledge from vision-language models (VLMs). owever, these\nmethods typically rely on a single VLM to align the feature spaces of 3D models\nwithin a common language space, which limits the potential of 3D models to\nleverage the diverse spatial and semantic capabilities encapsulated in various\nfoundation models. In this paper, we propose Cross-modal and Uncertainty-aware\nAgglomeration for Open-vocabulary 3D Scene Understanding dubbed CUA-O3D, the\nfirst model to integrate multiple foundation models-such as CLIP, DINOv2, and\nStable Diffusion-into 3D scene understanding. We further introduce a\ndeterministic uncertainty estimation to adaptively distill and harmonize the\nheterogeneous 2D feature embeddings from these models. Our method addresses two\nkey challenges: (1) incorporating semantic priors from VLMs alongside the\ngeometric knowledge of spatially-aware vision foundation models, and (2) using\na novel deterministic uncertainty estimation to capture model-specific\nuncertainties across diverse semantic and geometric sensitivities, helping to\nreconcile heterogeneous representations during training. Extensive experiments\non ScanNetV2 and Matterport3D demonstrate that our method not only advances\nopen-vocabulary segmentation but also achieves robust cross-domain alignment\nand competitive spatial perception capabilities. The code will be available at\n\\href{https://github.com/TyroneLi/CUA_O3D}{CUA_O3D}.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jinlong Li",
      "Cristiano Saltori",
      "Fabio Poiesi",
      "Nicu Sebe"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16700v1",
    "title": "Deep Q-Learning with Gradient Target Tracking",
    "abstract": "This paper introduces Q-learning with gradient target tracking, a novel\nreinforcement learning framework that provides a learned continuous target\nupdate mechanism as an alternative to the conventional hard update paradigm. In\nthe standard deep Q-network (DQN), the target network is a copy of the online\nnetwork's weights, held fixed for a number of iterations before being\nperiodically replaced via a hard update. While this stabilizes training by\nproviding consistent targets, it introduces a new challenge: the hard update\nperiod must be carefully tuned to achieve optimal performance. To address this\nissue, we propose two gradient-based target update methods: DQN with asymmetric\ngradient target tracking (AGT2-DQN) and DQN with symmetric gradient target\ntracking (SGT2-DQN). These methods replace the conventional hard target updates\nwith continuous and structured updates using gradient descent, which\neffectively eliminates the need for manual tuning. We provide a theoretical\nanalysis proving the convergence of these methods in tabular settings.\nAdditionally, empirical evaluations demonstrate their advantages over standard\nDQN baselines, which suggest that gradient-based target updates can serve as an\neffective alternative to conventional target update mechanisms in Q-learning.",
    "categories": [
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Donghwan Lee",
      "Bum Geun Park",
      "Taeho Lee"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16696v1",
    "title": "Universal approximation property of neural stochastic differential equations",
    "abstract": "We identify various classes of neural networks that are able to approximate\ncontinuous functions locally uniformly subject to fixed global linear growth\nconstraints. For such neural networks the associated neural stochastic\ndifferential equations can approximate general stochastic differential\nequations, both of It\\^o diffusion type, arbitrarily well. Moreover,\nquantitative error estimates are derived for stochastic differential equations\nwith sufficiently regular coefficients.",
    "categories": [
      "math.PR",
      "cs.LG",
      "math.FA",
      "q-fin.MF",
      "stat.ML",
      "41A29, 60H10, 68T07, 91G80"
    ],
    "authors": [
      "Anna P. Kwossek",
      "David J. Prömel",
      "Josef Teichmann"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16693v1",
    "title": "ATOM: A Framework of Detecting Query-Based Model Extraction Attacks for Graph Neural Networks",
    "abstract": "Graph Neural Networks (GNNs) have gained traction in Graph-based Machine\nLearning as a Service (GMLaaS) platforms, yet they remain vulnerable to\ngraph-based model extraction attacks (MEAs), where adversaries reconstruct\nsurrogate models by querying the victim model. Existing defense mechanisms,\nsuch as watermarking and fingerprinting, suffer from poor real-time\nperformance, susceptibility to evasion, or reliance on post-attack\nverification, making them inadequate for handling the dynamic characteristics\nof graph-based MEA variants. To address these limitations, we propose ATOM, a\nnovel real-time MEA detection framework tailored for GNNs. ATOM integrates\nsequential modeling and reinforcement learning to dynamically detect evolving\nattack patterns, while leveraging $k$-core embedding to capture the structural\nproperties, enhancing detection precision. Furthermore, we provide theoretical\nanalysis to characterize query behaviors and optimize detection strategies.\nExtensive experiments on multiple real-world datasets demonstrate that ATOM\noutperforms existing approaches in detection performance, maintaining stable\nacross different time steps, thereby offering a more effective defense\nmechanism for GMLaaS environments.",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "authors": [
      "Zhan Cheng",
      "Bolin Shen",
      "Tianming Sha",
      "Yuan Gao",
      "Shibo Li",
      "Yushun Dong"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16692v1",
    "title": "Limits of trust in medical AI",
    "abstract": "Artificial intelligence (AI) is expected to revolutionize the practice of\nmedicine. Recent advancements in the field of deep learning have demonstrated\nsuccess in a variety of clinical tasks: detecting diabetic retinopathy from\nimages, predicting hospital readmissions, aiding in the discovery of new drugs,\netc. AI's progress in medicine, however, has led to concerns regarding the\npotential effects of this technology upon relationships of trust in clinical\npractice. In this paper, I will argue that there is merit to these concerns,\nsince AI systems can be relied upon, and are capable of reliability, but cannot\nbe trusted, and are not capable of trustworthiness. Insofar as patients are\nrequired to rely upon AI systems for their medical decision-making, there is\npotential for this to produce a deficit of trust in relationships in clinical\npractice.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "J.3; K.4"
    ],
    "authors": [
      "Joshua Hatherley"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16690v1",
    "title": "Making the unmodulated pyramid wavefront sensor smart II. First on-sky demonstration of extreme adaptive optics with deep learning",
    "abstract": "Pyramid wavefront sensors (PWFSs) are the preferred choice for current and\nfuture extreme adaptive optics (XAO) systems. Almost all instruments use the\nPWFS in its modulated form to mitigate its limited linearity range. However,\nthis modulation comes at the cost of a reduction in sensitivity, a blindness to\npetal-piston modes, and a limit to the sensor's ability to operate at high\nspeeds. Therefore, there is strong interest to use the PWFS without modulation,\nwhich can be enabled with nonlinear reconstructors. Here, we present the first\non-sky demonstration of XAO with an unmodulated PWFS using a nonlinear\nreconstructor based on convolutional neural networks. We discuss the real-time\nimplementation on the Magellan Adaptive Optics eXtreme (MagAO-X) instrument\nusing the optimized TensorRT framework and show that inference is fast enough\nto run the control loop at >2 kHz frequencies. Our on-sky results demonstrate a\nsuccessful closed-loop operation using a model calibrated with internal source\ndata that delivers stable and robust correction under varying conditions.\nPerformance analysis reveals that our smart PWFS achieves nearly the same\nStrehl ratio as the highly optimized modulated PWFS under favorable conditions\non bright stars. Notably, we observe an improvement in performance on a fainter\nstar under the influence of strong winds. These findings confirm the\nfeasibility of using the PWFS in its unmodulated form and highlight its\npotential for next-generation instruments. Future efforts will focus on\nachieving even higher control loop frequencies (>3 kHz), optimizing the\ncalibration procedures, and testing its performance on fainter stars, where\nmore gain is expected for the unmodulated PWFS compared to its modulated\ncounterpart.",
    "categories": [
      "astro-ph.IM",
      "astro-ph.EP",
      "cs.LG",
      "physics.optics"
    ],
    "authors": [
      "R. Landman",
      "S. Y. Haffert",
      "J. D. Long",
      "J. R. Males",
      "L. M. Close",
      "W. B. Foster",
      "K. Van Gorkom",
      "O. Guyon",
      "A. D. Hedglen",
      "P. T. Johnson",
      "M. Y. Kautz",
      "J. K. Kueny",
      "J. Li",
      "J. Liberman",
      "J. Lumbres",
      "E. A. McEwen",
      "A. McLeod",
      "L. Schatz",
      "E. Tonucci",
      "K. Twitchell"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16689v1",
    "title": "WaveFM: A High-Fidelity and Efficient Vocoder Based on Flow Matching",
    "abstract": "Flow matching offers a robust and stable approach to training diffusion\nmodels. However, directly applying flow matching to neural vocoders can result\nin subpar audio quality. In this work, we present WaveFM, a reparameterized\nflow matching model for mel-spectrogram conditioned speech synthesis, designed\nto enhance both sample quality and generation speed for diffusion vocoders.\nSince mel-spectrograms represent the energy distribution of waveforms, WaveFM\nadopts a mel-conditioned prior distribution instead of a standard Gaussian\nprior to minimize unnecessary transportation costs during synthesis. Moreover,\nwhile most diffusion vocoders rely on a single loss function, we argue that\nincorporating auxiliary losses, including a refined multi-resolution STFT loss,\ncan further improve audio quality. To speed up inference without degrading\nsample quality significantly, we introduce a tailored consistency distillation\nmethod for WaveFM. Experiment results demonstrate that our model achieves\nsuperior performance in both quality and efficiency compared to previous\ndiffusion vocoders, while enabling waveform generation in a single inference\nstep.",
    "categories": [
      "cs.SD",
      "cs.CL"
    ],
    "authors": [
      "Tianze Luo",
      "Xingchen Miao",
      "Wenbo Duan"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16683v1",
    "title": "GAIR: Improving Multimodal Geo-Foundation Model with Geo-Aligned Implicit Representations",
    "abstract": "Advancements in vision and language foundation models have inspired the\ndevelopment of geo-foundation models (GeoFMs), enhancing performance across\ndiverse geospatial tasks. However, many existing GeoFMs primarily focus on\noverhead remote sensing (RS) data while neglecting other data modalities such\nas ground-level imagery. A key challenge in multimodal GeoFM development is to\nexplicitly model geospatial relationships across modalities, which enables\ngeneralizability across tasks, spatial scales, and temporal contexts. To\naddress these limitations, we propose GAIR, a novel multimodal GeoFM\narchitecture integrating overhead RS data, street view (SV) imagery, and their\ngeolocation metadata. We utilize three factorized neural encoders to project an\nSV image, its geolocation, and an RS image into the embedding space. The SV\nimage needs to be located within the RS image's spatial footprint but does not\nneed to be at its geographic center. In order to geographically align the SV\nimage and RS image, we propose a novel implicit neural representations (INR)\nmodule that learns a continuous RS image representation and looks up the RS\nembedding at the SV image's geolocation. Next, these geographically aligned SV\nembedding, RS embedding, and location embedding are trained with contrastive\nlearning objectives from unlabeled data. We evaluate GAIR across 10 geospatial\ntasks spanning RS image-based, SV image-based, and location embedding-based\nbenchmarks. Experimental results demonstrate that GAIR outperforms\nstate-of-the-art GeoFMs and other strong baselines, highlighting its\neffectiveness in learning generalizable and transferable geospatial\nrepresentations.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.4.10"
    ],
    "authors": [
      "Zeping Liu",
      "Fan Zhang",
      "Junfeng Jiao",
      "Ni Lao",
      "Gengchen Mai"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16681v1",
    "title": "GauRast: Enhancing GPU Triangle Rasterizers to Accelerate 3D Gaussian Splatting",
    "abstract": "3D intelligence leverages rich 3D features and stands as a promising frontier\nin AI, with 3D rendering fundamental to many downstream applications. 3D\nGaussian Splatting (3DGS), an emerging high-quality 3D rendering method,\nrequires significant computation, making real-time execution on existing\nGPU-equipped edge devices infeasible. Previous efforts to accelerate 3DGS rely\non dedicated accelerators that require substantial integration overhead and\nhardware costs. This work proposes an acceleration strategy that leverages the\nsimilarities between the 3DGS pipeline and the highly optimized conventional\ngraphics pipeline in modern GPUs. Instead of developing a dedicated\naccelerator, we enhance existing GPU rasterizer hardware to efficiently support\n3DGS operations. Our results demonstrate a 23$\\times$ increase in processing\nspeed and a 24$\\times$ reduction in energy consumption, with improvements\nyielding 6$\\times$ faster end-to-end runtime for the original 3DGS algorithm\nand 4$\\times$ for the latest efficiency-improved pipeline, achieving 24 FPS and\n46 FPS respectively. These enhancements incur only a minimal area overhead of\n0.2\\% relative to the entire SoC chip area, underscoring the practicality and\nefficiency of our approach for enabling 3DGS rendering on resource-constrained\nplatforms.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.AR"
    ],
    "authors": [
      "Sixu Li",
      "Ben Keller",
      "Yingyan Celine Lin",
      "Brucek Khailany"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16679v1",
    "title": "Echoes of Power: Investigating Geopolitical Bias in US and China Large Language Models",
    "abstract": "Large Language Models (LLMs) have emerged as powerful tools for generating\nhuman-like text, transforming human-machine interactions. However, their\nwidespread adoption has raised concerns about their potential to influence\npublic opinion and shape political narratives. In this work, we investigate the\ngeopolitical biases in US and Chinese LLMs, focusing on how these models\nrespond to questions related to geopolitics and international relations. We\ncollected responses from ChatGPT and DeepSeek to a set of geopolitical\nquestions and evaluated their outputs through both qualitative and quantitative\nanalyses. Our findings show notable biases in both models, reflecting distinct\nideological perspectives and cultural influences. However, despite these\nbiases, for a set of questions, the models' responses are more aligned than\nexpected, indicating that they can address sensitive topics without necessarily\npresenting directly opposing viewpoints. This study highlights the potential of\nLLMs to shape public discourse and underscores the importance of critically\nassessing AI-generated content, particularly in politically sensitive contexts.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "authors": [
      "Andre G. C. Pacheco",
      "Athus Cavalini",
      "Giovanni Comarela"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16678v1",
    "title": "QCPINN: Quantum Classical Physics-Informed Neural Networks for Solving PDEs",
    "abstract": "Hybrid quantum-classical neural network methods represent an emerging\napproach to solving computational challenges by leveraging advantages from both\nparadigms. As physics-informed neural networks (PINNs) have successfully\napplied to solve partial differential equations (PDEs) by incorporating\nphysical constraints into neural architectures, this work investigates whether\nquantum-classical physics-informed neural networks (QCPINNs) can efficiently\nsolve PDEs with reduced parameter counts compared to classical approaches. We\nevaluate two quantum circuit paradigms: continuous-variable (CV) and\nqubit-based discrete-variable (DV) across multiple circuit ansatze (Alternate,\nCascade, Cross mesh, and Layered). Benchmarking across five challenging PDEs\n(Helmholtz, Cavity, Wave, Klein-Gordon, and Convection-Diffusion equations)\ndemonstrates that our hybrid approaches achieve comparable accuracy to\nclassical PINNs while requiring up to 89% fewer trainable parameters. DV-based\nimplementations, particularly those with angle encoding and cascade circuit\nconfigurations, exhibit better stability and convergence properties across all\nproblem types. For the Convection-Diffusion equation, our angle-cascade QCPINN\nachieves parameter efficiency and a 37% reduction in relative L2 error compared\nto classical counterparts. Our findings highlight the potential of\nquantum-enhanced architectures for physics-informed learning, establishing\nparameter efficiency as a quantifiable quantum advantage while providing a\nfoundation for future quantum-classical hybrid systems solving complex physical\nmodels.",
    "categories": [
      "quant-ph",
      "cs.LG"
    ],
    "authors": [
      "Afrah Farea",
      "Saiful Khan",
      "Mustafa Serdar Celebi"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16674v1",
    "title": "Through the LLM Looking Glass: A Socratic Self-Assessment of Donkeys, Elephants, and Markets",
    "abstract": "While detecting and avoiding bias in LLM-generated text is becoming\nincreasingly important, media bias often remains subtle and subjective, making\nit particularly difficult to identify and mitigate. In this study, we assess\nmedia bias in LLM-generated content and LLMs' ability to detect subtle\nideological bias. We conduct this evaluation using two datasets, PoliGen and\nEconoLex, covering political and economic discourse, respectively. We evaluate\neight widely used LLMs by prompting them to generate articles and analyze their\nideological preferences via self-assessment. By using self-assessment, the\nstudy aims to directly measure the models' biases rather than relying on\nexternal interpretations, thereby minimizing subjective judgments about media\nbias. Our results reveal a consistent preference of Democratic over Republican\npositions across all models. Conversely, in economic topics, biases vary among\nWestern LLMs, while those developed in China lean more strongly toward\nsocialism.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Molly Kennedy",
      "Ayyoob Imani",
      "Timo Spinde",
      "Hinrich Schütze"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16673v1",
    "title": "Subgradient Method for System Identification with Non-Smooth Objectives",
    "abstract": "This paper investigates a subgradient-based algorithm to solve the system\nidentification problem for linear time-invariant systems with non-smooth\nobjectives. This is essential for robust system identification in\nsafety-critical applications. While existing work provides theoretical exact\nrecovery guarantees using optimization solvers, the design of fast learning\nalgorithms with convergence guarantees for practical use remains unexplored. We\nanalyze the subgradient method in this setting where the optimization problems\nto be solved change over time as new measurements are taken, and we establish\nlinear convergence results for both the best and Polyak step sizes after a\nburn-in period. Additionally, we characterize the asymptotic convergence of the\nbest average sub-optimality gap under diminishing and constant step sizes.\nFinally, we compare the time complexity of standard solvers with the\nsubgradient algorithm and support our findings with experimental results. This\nis the first work to analyze subgradient algorithms for system identification\nwith non-smooth objectives.",
    "categories": [
      "math.OC",
      "cs.CC",
      "cs.LG",
      "cs.SY",
      "eess.SY",
      "62, 90, 93"
    ],
    "authors": [
      "Baturalp Yalcin",
      "Javad Lavaei"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16672v1",
    "title": "Accelerating Transformer Inference and Training with 2:4 Activation Sparsity",
    "abstract": "In this paper, we demonstrate how to leverage 2:4 sparsity, a popular\nhardware-accelerated GPU sparsity pattern, to activations to accelerate large\nlanguage model training and inference. Crucially we exploit the intrinsic\nsparsity found in Squared-ReLU activations to provide this acceleration with no\naccuracy loss. Our approach achieves up to 1.3x faster Feed Forward Network\n(FFNs) in both the forwards and backwards pass. This work highlights the\npotential for sparsity to play a key role in accelerating large language model\ntraining and inference.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2"
    ],
    "authors": [
      "Daniel Haziza",
      "Timothy Chou",
      "Dhruv Choudhary",
      "Luca Wehrstedt",
      "Francisco Massa",
      "Jiecao Yu",
      "Geonhwa Jeong",
      "Supriya Rao",
      "Patrick Labatut",
      "Jesse Cai"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16669v1",
    "title": "Aligning Text-to-Music Evaluation with Human Preferences",
    "abstract": "Despite significant recent advances in generative acoustic text-to-music\n(TTM) modeling, robust evaluation of these models lags behind, relying in\nparticular on the popular Fr\\'echet Audio Distance (FAD). In this work, we\nrigorously study the design space of reference-based divergence metrics for\nevaluating TTM models through (1) designing four synthetic meta-evaluations to\nmeasure sensitivity to particular musical desiderata, and (2) collecting and\nevaluating on MusicPrefs, the first open-source dataset of human preferences\nfor TTM systems. We find that not only is the standard FAD setup inconsistent\non both synthetic and human preference data, but that nearly all existing\nmetrics fail to effectively capture desiderata, and are only weakly correlated\nwith human perception. We propose a new metric, the MAUVE Audio Divergence\n(MAD), computed on representations from a self-supervised audio embedding\nmodel. We find that this metric effectively captures diverse musical desiderata\n(average rank correlation 0.84 for MAD vs. 0.49 for FAD and also correlates\nmore strongly with MusicPrefs (0.62 vs. 0.14).",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "authors": [
      "Yichen Huang",
      "Zachary Novack",
      "Koichi Saito",
      "Jiatong Shi",
      "Shinji Watanabe",
      "Yuki Mitsufuji",
      "John Thickstun",
      "Chris Donahue"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16668v1",
    "title": "Code Evolution Graphs: Understanding Large Language Model Driven Design of Algorithms",
    "abstract": "Large Language Models (LLMs) have demonstrated great promise in generating\ncode, especially when used inside an evolutionary computation framework to\niteratively optimize the generated algorithms. However, in some cases they fail\nto generate competitive algorithms or the code optimization stalls, and we are\nleft with no recourse because of a lack of understanding of the generation\nprocess and generated codes. We present a novel approach to mitigate this\nproblem by enabling users to analyze the generated codes inside the\nevolutionary process and how they evolve over repeated prompting of the LLM. We\nshow results for three benchmark problem classes and demonstrate novel\ninsights. In particular, LLMs tend to generate more complex code with repeated\nprompting, but additional complexity can hurt algorithmic performance in some\ncases. Different LLMs have different coding ``styles'' and generated code tends\nto be dissimilar to other LLMs. These two findings suggest that using different\nLLMs inside the code evolution frameworks might produce higher performing code\nthan using only one LLM.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "authors": [
      "Niki van Stein",
      "Anna V. Kononova",
      "Lars Kotthoff",
      "Thomas Bäck"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16667v1",
    "title": "A preliminary data fusion study to assess the feasibility of Foundation Process-Property Models in Laser Powder Bed Fusion",
    "abstract": "Foundation models are at the forefront of an increasing number of critical\napplications. In regards to technologies such as additive manufacturing (AM),\nthese models have the potential to dramatically accelerate process optimization\nand, in turn, design of next generation materials. A major challenge that\nimpedes the construction of foundation process-property models is data\nscarcity. To understand the impact of this challenge, and since foundation\nmodels rely on data fusion, in this work we conduct controlled experiments\nwhere we focus on the transferability of information across different material\nsystems and properties. More specifically, we generate experimental datasets\nfrom 17-4 PH and 316L stainless steels (SSs) in Laser Powder Bed Fusion (LPBF)\nwhere we measure the effect of five process parameters on porosity and\nhardness. We then leverage Gaussian processes (GPs) for process-property\nmodeling in various configurations to test if knowledge about one material\nsystem or property can be leveraged to build more accurate machine learning\nmodels for other material systems or properties. Through extensive\ncross-validation studies and probing the GPs' interpretable hyperparameters, we\nstudy the intricate relation among data size and dimensionality, complexity of\nthe process-property relations, noise, and characteristics of machine learning\nmodels. Our findings highlight the need for structured learning approaches that\nincorporate domain knowledge in building foundation process-property models\nrather than relying on uninformed data fusion in data-limited applications.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Oriol Vendrell-Gallart",
      "Nima Negarandeh",
      "Zahra Zanjani Foumani",
      "Mahsa Amiri",
      "Lorenzo Valdevit",
      "Ramin Bostanabad"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16666v1",
    "title": "Efficient Training of Neural Fractional-Order Differential Equation via Adjoint Backpropagation",
    "abstract": "Fractional-order differential equations (FDEs) enhance traditional\ndifferential equations by extending the order of differential operators from\nintegers to real numbers, offering greater flexibility in modeling complex\ndynamical systems with nonlocal characteristics. Recent progress at the\nintersection of FDEs and deep learning has catalyzed a new wave of innovative\nmodels, demonstrating the potential to address challenges such as graph\nrepresentation learning. However, training neural FDEs has primarily relied on\ndirect differentiation through forward-pass operations in FDE numerical\nsolvers, leading to increased memory usage and computational complexity,\nparticularly in large-scale applications. To address these challenges, we\npropose a scalable adjoint backpropagation method for training neural FDEs by\nsolving an augmented FDE backward in time, which substantially reduces memory\nrequirements. This approach provides a practical neural FDE toolbox and holds\nconsiderable promise for diverse applications. We demonstrate the effectiveness\nof our method in several tasks, achieving performance comparable to baseline\nmodels while significantly reducing computational overhead.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Qiyu Kang",
      "Xuhao Li",
      "Kai Zhao",
      "Wenjun Cui",
      "Yanan Zhao",
      "Weihua Deng",
      "Wee Peng Tay"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16664v1",
    "title": "TextBite: A Historical Czech Document Dataset for Logical Page Segmentation",
    "abstract": "Logical page segmentation is an important step in document analysis, enabling\nbetter semantic representations, information retrieval, and text understanding.\nPrevious approaches define logical segmentation either through text or\ngeometric objects, relying on OCR or precise geometry. To avoid the need for\nOCR, we define the task purely as segmentation in the image domain.\nFurthermore, to ensure the evaluation remains unaffected by geometrical\nvariations that do not impact text segmentation, we propose to use only\nforeground text pixels in the evaluation metric and disregard all background\npixels. To support research in logical document segmentation, we introduce\nTextBite, a dataset of historical Czech documents spanning the 18th to 20th\ncenturies, featuring diverse layouts from newspapers, dictionaries, and\nhandwritten records. The dataset comprises 8,449 page images with 78,863\nannotated segments of logically and thematically coherent text. We propose a\nset of baseline methods combining text region detection and relation\nprediction. The dataset, baselines and evaluation framework can be accessed at\nhttps://github.com/DCGM/textbite-dataset.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Martin Kostelník",
      "Karel Beneš",
      "Michal Hradiš"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16661v1",
    "title": "ContextGNN goes to Elliot: Towards Benchmarking Relational Deep Learning for Static Link Prediction (aka Personalized Item Recommendation)",
    "abstract": "Relational deep learning (RDL) settles among the most exciting advances in\nmachine learning for relational databases, leveraging the representational\npower of message passing graph neural networks (GNNs) to derive useful\nknowledge and run predicting tasks on tables connected through\nprimary-to-foreign key links. The RDL paradigm has been successfully applied to\nrecommendation lately, through its most recent representative deep learning\narchitecture namely, ContextGNN. While acknowledging ContextGNN's improved\nperformance on real-world recommendation datasets and tasks, preliminary tests\nfor the more traditional static link prediction task (aka personalized item\nrecommendation) on the popular Amazon Book dataset have demonstrated how\nContextGNN has still room for improvement compared to other state-of-the-art\nGNN-based recommender systems. To this end, with this paper, we integrate\nContextGNN within Elliot, a popular framework for reproducibility and\nbenchmarking analyses, counting around 50 state-of-the-art recommendation\nmodels from the literature to date. On such basis, we run preliminary\nexperiments on three standard recommendation datasets and against six\nstate-of-the-art GNN-based recommender systems, confirming similar trends to\nthose observed by the authors in their original paper. The code is publicly\navailable on GitHub:\nhttps://github.com/danielemalitesta/Rel-DeepLearning-RecSys.",
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "authors": [
      "Alejandro Ariza-Casabona",
      "Nikos Kanakaris",
      "Daniele Malitesta"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16660v1",
    "title": "When Less is Enough: Adaptive Token Reduction for Efficient Image Representation",
    "abstract": "Vision encoders typically generate a large number of visual tokens, providing\ninformation-rich representations but significantly increasing computational\ndemands. This raises the question of whether all generated tokens are equally\nvaluable or if some of them can be discarded to reduce computational costs\nwithout compromising quality. In this paper, we introduce a new method for\ndetermining feature utility based on the idea that less valuable features can\nbe reconstructed from more valuable ones. We implement this concept by\nintegrating an autoencoder with a Gumbel-Softmax selection mechanism, that\nallows identifying and retaining only the most informative visual tokens. To\nvalidate our approach, we compared the performance of the LLaVA-NeXT model,\nusing features selected by our method with randomly selected features. We found\nthat on OCR-based tasks, more than 50% of the visual context can be removed\nwith minimal performance loss, whereas randomly discarding the same proportion\nof features significantly affects the model capabilities. Furthermore, in\ngeneral-domain tasks, even randomly retaining only 30% of tokens achieves\nperformance comparable to using the full set of visual tokens. Our results\nhighlight a promising direction towards adaptive and efficient multimodal\npruning that facilitates scalable and low-overhead inference without\ncompromising performance.",
    "categories": [
      "cs.CV",
      "68T10, 68T30, 68T45",
      "I.2.10"
    ],
    "authors": [
      "Eduard Allakhverdov",
      "Elizaveta Goncharova",
      "Andrey Kuznetsov"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16659v1",
    "title": "Advances in Protein Representation Learning: Methods, Applications, and Future Directions",
    "abstract": "Proteins are complex biomolecules that play a central role in various\nbiological processes, making them critical targets for breakthroughs in\nmolecular biology, medical research, and drug discovery. Deciphering their\nintricate, hierarchical structures, and diverse functions is essential for\nadvancing our understanding of life at the molecular level. Protein\nRepresentation Learning (PRL) has emerged as a transformative approach,\nenabling the extraction of meaningful computational representations from\nprotein data to address these challenges. In this paper, we provide a\ncomprehensive review of PRL research, categorizing methodologies into five key\nareas: feature-based, sequence-based, structure-based, multimodal, and\ncomplex-based approaches. To support researchers in this rapidly evolving\nfield, we introduce widely used databases for protein sequences, structures,\nand functions, which serve as essential resources for model development and\nevaluation. We also explore the diverse applications of these approaches in\nmultiple domains, demonstrating their broad impact. Finally, we discuss\npressing technical challenges and outline future directions to advance PRL,\noffering insights to inspire continued innovation in this foundational field.",
    "categories": [
      "cs.LG",
      "q-bio.BM"
    ],
    "authors": [
      "Viet Thanh Duy Nguyen",
      "Truong-Son Hy"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16655v1",
    "title": "Accelerating Antibiotic Discovery with Large Language Models and Knowledge Graphs",
    "abstract": "The discovery of novel antibiotics is critical to address the growing\nantimicrobial resistance (AMR). However, pharmaceutical industries face high\ncosts (over $1 billion), long timelines, and a high failure rate, worsened by\nthe rediscovery of known compounds. We propose an LLM-based pipeline that acts\nas an alarm system, detecting prior evidence of antibiotic activity to prevent\ncostly rediscoveries. The system integrates organism and chemical literature\ninto a Knowledge Graph (KG), ensuring taxonomic resolution, synonym handling,\nand multi-level evidence classification. We tested the pipeline on a private\nlist of 73 potential antibiotic-producing organisms, disclosing 12 negative\nhits for evaluation. The results highlight the effectiveness of the pipeline\nfor evidence reviewing, reducing false negatives, and accelerating\ndecision-making. The KG for negative hits and the user interface for\ninteractive exploration will be made publicly available.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Maxime Delmas",
      "Magdalena Wysocka",
      "Danilo Gusicuma",
      "André Freitas"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16653v1",
    "title": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh Generation",
    "abstract": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Hanxiao Wang",
      "Biao Zhang",
      "Weize Quan",
      "Dong-Ming Yan",
      "Peter Wonka"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16644v1",
    "title": "To impute or not to impute: How machine learning modelers treat missing data",
    "abstract": "Missing data is prevalent in tabular machine learning (ML) models, and\ndifferent missing data treatment methods can significantly affect ML model\ntraining results. However, little is known about how ML researchers and\nengineers choose missing data treatment methods and what factors affect their\nchoices. To this end, we conducted a survey of 70 ML researchers and engineers.\nOur results revealed that most participants were not making informed decisions\nregarding missing data treatment, which could significantly affect the validity\nof the ML models trained by these researchers. We advocate for better education\non missing data, more standardized missing data reporting, and better missing\ndata analysis tools.",
    "categories": [
      "cs.LG",
      "cs.HC"
    ],
    "authors": [
      "Wanyi Chen",
      "Mary Cummings"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16639v1",
    "title": "Whenever, Wherever: Towards Orchestrating Crowd Simulations with Spatio-Temporal Spawn Dynamics",
    "abstract": "Realistic crowd simulations are essential for immersive virtual environments,\nrelying on both individual behaviors (microscopic dynamics) and overall crowd\npatterns (macroscopic characteristics). While recent data-driven methods like\ndeep reinforcement learning improve microscopic realism, they often overlook\ncritical macroscopic features such as crowd density and flow, which are\ngoverned by spatio-temporal spawn dynamics, namely, when and where agents enter\na scene. Traditional methods, like random spawn rates, stochastic processes, or\nfixed schedules, are not guaranteed to capture the underlying complexity or\nlack diversity and realism. To address this issue, we propose a novel approach\ncalled nTPP-GMM that models spatio-temporal spawn dynamics using Neural\nTemporal Point Processes (nTPPs) that are coupled with a spawn-conditional\nGaussian Mixture Model (GMM) for agent spawn and goal positions. We evaluate\nour approach by orchestrating crowd simulations of three diverse real-world\ndatasets with nTPP-GMM. Our experiments demonstrate the orchestration with\nnTPP-GMM leads to realistic simulations that reflect real-world crowd scenarios\nand allow crowd analysis.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Thomas Kreutz",
      "Max Mühlhäuser",
      "Alejandro Sanchez Guinea"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16635v1",
    "title": "Fed-NDIF: A Noise-Embedded Federated Diffusion Model For Low-Count Whole-Body PET Denoising",
    "abstract": "Low-count positron emission tomography (LCPET) imaging can reduce patients'\nexposure to radiation but often suffers from increased image noise and reduced\nlesion detectability, necessitating effective denoising techniques. Diffusion\nmodels have shown promise in LCPET denoising for recovering degraded image\nquality. However, training such models requires large and diverse datasets,\nwhich are challenging to obtain in the medical domain. To address data scarcity\nand privacy concerns, we combine diffusion models with federated learning -- a\ndecentralized training approach where models are trained individually at\ndifferent sites, and their parameters are aggregated on a central server over\nmultiple iterations. The variation in scanner types and image noise levels\nwithin and across institutions poses additional challenges for federated\nlearning in LCPET denoising. In this study, we propose a novel noise-embedded\nfederated learning diffusion model (Fed-NDIF) to address these challenges,\nleveraging a multicenter dataset and varying count levels. Our approach\nincorporates liver normalized standard deviation (NSTD) noise embedding into a\n2.5D diffusion model and utilizes the Federated Averaging (FedAvg) algorithm to\naggregate locally trained models into a global model, which is subsequently\nfine-tuned on local datasets to optimize performance and obtain personalized\nmodels. Extensive validation on datasets from the University of Bern, Ruijin\nHospital in Shanghai, and Yale-New Haven Hospital demonstrates the superior\nperformance of our method in enhancing image quality and improving lesion\nquantification. The Fed-NDIF model shows significant improvements in PSNR,\nSSIM, and NMSE of the entire 3D volume, as well as enhanced lesion\ndetectability and quantification, compared to local diffusion models and\nfederated UNet-based models.",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Yinchi Zhou",
      "Huidong Xie",
      "Menghua Xia",
      "Qiong Liu",
      "Bo Zhou",
      "Tianqi Chen",
      "Jun Hou",
      "Liang Guo",
      "Xinyuan Zheng",
      "Hanzhong Wang",
      "Biao Li",
      "Axel Rominger",
      "Kuangyu Shi",
      "Nicha C. Dvorneka",
      "Chi Liu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16630v1",
    "title": "TriTex: Learning Texture from a Single Mesh via Triplane Semantic Features",
    "abstract": "As 3D content creation continues to grow, transferring semantic textures\nbetween 3D meshes remains a significant challenge in computer graphics. While\nrecent methods leverage text-to-image diffusion models for texturing, they\noften struggle to preserve the appearance of the source texture during texture\ntransfer. We present \\ourmethod, a novel approach that learns a volumetric\ntexture field from a single textured mesh by mapping semantic features to\nsurface colors. Using an efficient triplane-based architecture, our method\nenables semantic-aware texture transfer to a novel target mesh. Despite\ntraining on just one example, it generalizes effectively to diverse shapes\nwithin the same category. Extensive evaluation on our newly created benchmark\ndataset shows that \\ourmethod{} achieves superior texture transfer quality and\nfast inference times compared to existing methods. Our approach advances\nsingle-example texture transfer, providing a practical solution for maintaining\nvisual coherence across related 3D models in applications like game development\nand simulation.",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Dana Cohen-Bar",
      "Daniel Cohen-Or",
      "Gal Chechik",
      "Yoni Kasten"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16629v1",
    "title": "Utilizing Reinforcement Learning for Bottom-Up part-wise Reconstruction of 2D Wire-Frame Projections",
    "abstract": "This work concerns itself with the task of reconstructing all edges of an\narbitrary 3D wire-frame model projected to an image plane. We explore a\nbottom-up part-wise procedure undertaken by an RL agent to segment and\nreconstruct these 2D multipart objects. The environment's state is represented\nas a four-colour image, where different colours correspond to background, a\ntarget edge, a reconstruction line, and the overlap of both. At each step, the\nagent can transform the reconstruction line within a four-dimensional action\nspace or terminate the episode using a specific termination action. To\ninvestigate the impact of reward function formulations, we tested episodic and\nincremental rewards, as well as combined approaches. Empirical results\ndemonstrated that the latter yielded the most effective training performance.\nTo further enhance efficiency and stability, we introduce curriculum learning\nstrategies. First, an action-based curriculum was implemented, where the agent\nwas initially restricted to a reduced action space, being able to only perform\nthree of the five possible actions, before progressing to the full action\nspace. Second, we test a task-based curriculum, where the agent first solves a\nsimplified version of the problem before being presented with the full, more\ncomplex task. This second approach produced promising results, as the agent not\nonly successfully transitioned from learning the simplified task to mastering\nthe full task, but in doing so gained significant performance. This study\ndemonstrates the potential of an iterative RL wire-frame reconstruction in two\ndimensions. By combining optimized reward function formulations with curriculum\nlearning strategies, we achieved significant improvements in training success.\nThe proposed methodology provides an effective framework for solving similar\ntasks and represents a promising direction for future research in the field.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Julian Ziegler",
      "Patrick Frenzel",
      "Mirco Fuchs"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16628v1",
    "title": "MobilePlantViT: A Mobile-friendly Hybrid ViT for Generalized Plant Disease Image Classification",
    "abstract": "Plant diseases significantly threaten global food security by reducing crop\nyields and undermining agricultural sustainability. AI-driven automated\nclassification has emerged as a promising solution, with deep learning models\ndemonstrating impressive performance in plant disease identification. However,\ndeploying these models on mobile and edge devices remains challenging due to\nhigh computational demands and resource constraints, highlighting the need for\nlightweight, accurate solutions for accessible smart agriculture systems. To\naddress this, we propose MobilePlantViT, a novel hybrid Vision Transformer\n(ViT) architecture designed for generalized plant disease classification, which\noptimizes resource efficiency while maintaining high performance. Extensive\nexperiments across diverse plant disease datasets of varying scales show our\nmodel's effectiveness and strong generalizability, achieving test accuracies\nranging from 80% to over 99%. Notably, with only 0.69 million parameters, our\narchitecture outperforms the smallest versions of MobileViTv1 and MobileViTv2,\ndespite their higher parameter counts. These results underscore the potential\nof our approach for real-world, AI-powered automated plant disease\nclassification in sustainable and resource-efficient smart agriculture systems.\nAll codes will be available in the GitHub repository:\nhttps://github.com/moshiurtonmoy/MobilePlantViT",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Moshiur Rahman Tonmoy",
      "Md. Mithun Hossain",
      "Nilanjan Dey",
      "M. F. Mridha"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16622v1",
    "title": "Leveraging Large Language Models for Explainable Activity Recognition in Smart Homes: A Critical Evaluation",
    "abstract": "Explainable Artificial Intelligence (XAI) aims to uncover the inner reasoning\nof machine learning models. In IoT systems, XAI improves the transparency of\nmodels processing sensor data from multiple heterogeneous devices, ensuring\nend-users understand and trust their outputs. Among the many applications, XAI\nhas also been applied to sensor-based Activities of Daily Living (ADLs)\nrecognition in smart homes. Existing approaches highlight which sensor events\nare most important for each predicted activity, using simple rules to convert\nthese events into natural language explanations for non-expert users. However,\nthese methods produce rigid explanations lacking natural language flexibility\nand are not scalable. With the recent rise of Large Language Models (LLMs), it\nis worth exploring whether they can enhance explanation generation, considering\ntheir proven knowledge of human activities. This paper investigates potential\napproaches to combine XAI and LLMs for sensor-based ADL recognition. We\nevaluate if LLMs can be used: a) as explainable zero-shot ADL recognition\nmodels, avoiding costly labeled data collection, and b) to automate the\ngeneration of explanations for existing data-driven XAI approaches when\ntraining data is available and the goal is higher recognition rates. Our\ncritical evaluation provides insights into the benefits and challenges of using\nLLMs for explainable ADL recognition.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Michele Fiori",
      "Gabriele Civitarese",
      "Priyankar Choudhary",
      "Claudio Bettini"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16616v1",
    "title": "Progressive Test Time Energy Adaptation for Medical Image Segmentation",
    "abstract": "We propose a model-agnostic, progressive test-time energy adaptation approach\nfor medical image segmentation. Maintaining model performance across diverse\nmedical datasets is challenging, as distribution shifts arise from inconsistent\nimaging protocols and patient variations. Unlike domain adaptation methods that\nrequire multiple passes through target data - impractical in clinical settings\n- our approach adapts pretrained models progressively as they process test\ndata. Our method leverages a shape energy model trained on source data, which\nassigns an energy score at the patch level to segmentation maps: low energy\nrepresents in-distribution (accurate) shapes, while high energy signals\nout-of-distribution (erroneous) predictions. By minimizing this energy score at\ntest time, we refine the segmentation model to align with the target\ndistribution. To validate the effectiveness and adaptability, we evaluated our\nframework on eight public MRI (bSSFP, T1- and T2-weighted) and X-ray datasets\nspanning cardiac, spinal cord, and lung segmentation. We consistently\noutperform baselines both quantitatively and qualitatively.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Xiaoran Zhang",
      "Byung-Woo Hong",
      "Hyoungseob Park",
      "Daniel H. Pak",
      "Anne-Marie Rickmann",
      "Lawrence H. Staib",
      "James S. Duncan",
      "Alex Wong"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16614v1",
    "title": "Classification of User Reports for Detection of Faulty Computer Components using NLP Models: A Case Study",
    "abstract": "Computer manufacturers typically offer platforms for users to report faults.\nHowever, there remains a significant gap in these platforms' ability to\neffectively utilize textual reports, which impedes users from describing their\nissues in their own words. In this context, Natural Language Processing (NLP)\noffers a promising solution, by enabling the analysis of user-generated text.\nThis paper presents an innovative approach that employs NLP models to classify\nuser reports for detecting faulty computer components, such as CPU, memory,\nmotherboard, video card, and more. In this work, we build a dataset of 341 user\nreports obtained from many sources. Additionally, through extensive\nexperimental evaluation, our approach achieved an accuracy of 79% with our\ndataset.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Maria de Lourdes M. Silva",
      "André L. C. Mendonça",
      "Eduardo R. D. Neto",
      "Iago C. Chaves",
      "Felipe T. Brito",
      "Victor A. E. Farias",
      "Javam C. Machado"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16613v1",
    "title": "Informative Path Planning to Explore and Map Unknown Planetary Surfaces with Gaussian Processes",
    "abstract": "Many environments, such as unvisited planetary surfaces and oceanic regions,\nremain unexplored due to a lack of prior knowledge. Autonomous vehicles must\nsample upon arrival, process data, and either transmit findings to a\nteleoperator or decide where to explore next. Teleoperation is suboptimal, as\nhuman intuition lacks mathematical guarantees for optimality. This study\nevaluates an informative path planning algorithm for mapping a scalar variable\ndistribution while minimizing travel distance and ensuring model convergence.\nWe compare traditional open loop coverage methods (e.g., Boustrophedon, Spiral)\nwith information-theoretic approaches using Gaussian processes, which update\nmodels iteratively with confidence metrics. The algorithm's performance is\ntested on three surfaces, a parabola, Townsend function, and lunar crater\nhydration map, to assess noise, convexity, and function behavior. Results\ndemonstrate that information-driven methods significantly outperform naive\nexploration in reducing model error and travel distance while improving\nconvergence potential.",
    "categories": [
      "cs.RO",
      "cs.IR",
      "cs.LG"
    ],
    "authors": [
      "Ashten Akemoto",
      "Frances Zhu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16611v1",
    "title": "A Recipe for Generating 3D Worlds From a Single Image",
    "abstract": "We introduce a recipe for generating immersive 3D worlds from a single image\nby framing the task as an in-context learning problem for 2D inpainting models.\nThis approach requires minimal training and uses existing generative models.\nOur process involves two steps: generating coherent panoramas using a\npre-trained diffusion model and lifting these into 3D with a metric depth\nestimator. We then fill unobserved regions by conditioning the inpainting model\non rendered point clouds, requiring minimal fine-tuning. Tested on both\nsynthetic and real images, our method produces high-quality 3D environments\nsuitable for VR display. By explicitly modeling the 3D structure of the\ngenerated environment from the start, our approach consistently outperforms\nstate-of-the-art, video synthesis-based methods along multiple quantitative\nimage quality metrics. Project Page: https://katjaschwarz.github.io/worlds/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Katja Schwarz",
      "Denys Rozumnyi",
      "Samuel Rota Bulò",
      "Lorenzo Porzi",
      "Peter Kontschieder"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16429v1",
    "title": "Sonata: Self-Supervised Learning of Reliable Point Representations",
    "abstract": "In this paper, we question whether we have a reliable self-supervised point\ncloud model that can be used for diverse 3D tasks via simple linear probing,\neven with limited data and minimal computation. We find that existing 3D\nself-supervised learning approaches fall short when evaluated on representation\nquality through linear probing. We hypothesize that this is due to what we term\nthe \"geometric shortcut\", which causes representations to collapse to low-level\nspatial features. This challenge is unique to 3D and arises from the sparse\nnature of point cloud data. We address it through two key strategies: obscuring\nspatial information and enhancing the reliance on input features, ultimately\ncomposing a Sonata of 140k point clouds through self-distillation. Sonata is\nsimple and intuitive, yet its learned representations are strong and reliable:\nzero-shot visualizations demonstrate semantic grouping, alongside strong\nspatial reasoning through nearest-neighbor relationships. Sonata demonstrates\nexceptional parameter and data efficiency, tripling linear probing accuracy\n(from 21.8% to 72.5%) on ScanNet and nearly doubling performance with only 1%\nof the data compared to previous approaches. Full fine-tuning further advances\nSOTA across both 3D indoor and outdoor perception tasks.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Xiaoyang Wu",
      "Daniel DeTone",
      "Duncan Frost",
      "Tianwei Shen",
      "Chris Xie",
      "Nan Yang",
      "Jakob Engel",
      "Richard Newcombe",
      "Hengshuang Zhao",
      "Julian Straub"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16430v1",
    "title": "Bridging Continuous and Discrete Tokens for Autoregressive Visual Generation",
    "abstract": "Autoregressive visual generation models typically rely on tokenizers to\ncompress images into tokens that can be predicted sequentially. A fundamental\ndilemma exists in token representation: discrete tokens enable straightforward\nmodeling with standard cross-entropy loss, but suffer from information loss and\ntokenizer training instability; continuous tokens better preserve visual\ndetails, but require complex distribution modeling, complicating the generation\npipeline. In this paper, we propose TokenBridge, which bridges this gap by\nmaintaining the strong representation capacity of continuous tokens while\npreserving the modeling simplicity of discrete tokens. To achieve this, we\ndecouple discretization from the tokenizer training process through\npost-training quantization that directly obtains discrete tokens from\ncontinuous representations. Specifically, we introduce a dimension-wise\nquantization strategy that independently discretizes each feature dimension,\npaired with a lightweight autoregressive prediction mechanism that efficiently\nmodel the resulting large token space. Extensive experiments show that our\napproach achieves reconstruction and generation quality on par with continuous\nmethods while using standard categorical prediction. This work demonstrates\nthat bridging discrete and continuous paradigms can effectively harness the\nstrengths of both approaches, providing a promising direction for high-quality\nvisual generation with simple autoregressive modeling. Project page:\nhttps://yuqingwang1029.github.io/TokenBridge.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yuqing Wang",
      "Zhijie Lin",
      "Yao Teng",
      "Yuanzhi Zhu",
      "Shuhuai Ren",
      "Jiashi Feng",
      "Xihui Liu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16428v1",
    "title": "XAttention: Block Sparse Attention with Antidiagonal Scoring",
    "abstract": "Long-Context Transformer Models (LCTMs) are vital for real-world applications\nbut suffer high computational costs due to attention's quadratic complexity.\nBlock-sparse attention mitigates this by focusing computation on critical\nregions, yet existing methods struggle with balancing accuracy and efficiency\ndue to costly block importance measurements. In this paper, we introduce\nXAttention, a plug-and-play framework that dramatically accelerates\nlong-context inference in Transformers models using sparse attention.\nXAttention's key innovation is the insight that the sum of antidiagonal values\n(i.e., from the lower-left to upper-right) in the attention matrix provides a\npowerful proxy for block importance. This allows for precise identification and\npruning of non-essential blocks, resulting in high sparsity and dramatically\naccelerated inference. Across comprehensive evaluations on demanding\nlong-context benchmarks-including RULER and LongBench for language, VideoMME\nfor video understanding, and VBench for video generation. XAttention achieves\naccuracy comparable to full attention while delivering substantial\ncomputational gains. We demonstrate up to 13.5x acceleration in attention\ncomputation. These results underscore XAttention's ability to unlock the\npractical potential of block sparse attention, paving the way for scalable and\nefficient deployment of LCTMs in real-world applications. Code is available at\nhttps://github.com/mit-han-lab/x-attention.",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "authors": [
      "Ruyi Xu",
      "Guangxuan Xiao",
      "Haofeng Huang",
      "Junxian Guo",
      "Song Han"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16426v1",
    "title": "DynamicVis: An Efficient and General Visual Foundation Model for Remote Sensing Image Understanding",
    "abstract": "The advancement of remote sensing technology has improved the spatial\nresolution of satellite imagery, facilitating more detailed visual\nrepresentations for diverse interpretations. However, existing methods exhibit\nlimited generalization capabilities across varied applications. While some\ncontemporary foundation models demonstrate potential, they are hindered by\ninsufficient cross-task adaptability and primarily process low-resolution\nimagery of restricted sizes, thus failing to fully exploit high-resolution data\nor leverage comprehensive large-scene semantics. Crucially, remote sensing\nimagery differs fundamentally from natural images, as key foreground targets\n(eg., maritime objects, artificial structures) often occupy minimal spatial\nproportions (~1%) and exhibit sparse distributions. Efficiently modeling\ncross-task generalizable knowledge from lengthy 2D tokens (~100,000) poses a\nsignificant challenge yet remains critical for remote sensing image\nunderstanding. Motivated by the selective attention mechanisms inherent to the\nhuman visual system, we propose DynamicVis, a dynamic visual perception\nfoundation model for remote sensing imagery. The framework integrates a novel\ndynamic region perception backbone based on the selective state space model,\nwhich strategically balances localized detail extraction with global contextual\nintegration, enabling computationally efficient encoding of large-scale data\nwhile maintaining architectural scalability. To enhance cross-task knowledge\ntransferring, we introduce a multi-instance learning paradigm utilizing\nmeta-embedding representations, trained on million-scale region-level\nannotations. Evaluations across nine downstream tasks demonstrate the model's\nversatility. DynamicVis achieves multi-level feature modeling with exceptional\nefficiency, processing (2048x2048) pixels with 97 ms latency (6% of ViT's) and\n833 MB GPU memory (3% of ViT's).",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Keyan Chen",
      "Chenyang Liu",
      "Bowen Chen",
      "Wenyuan Li",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16425v1",
    "title": "Tokenize Image as a Set",
    "abstract": "This paper proposes a fundamentally new paradigm for image generation through\nset-based tokenization and distribution modeling. Unlike conventional methods\nthat serialize images into fixed-position latent codes with a uniform\ncompression ratio, we introduce an unordered token set representation to\ndynamically allocate coding capacity based on regional semantic complexity.\nThis TokenSet enhances global context aggregation and improves robustness\nagainst local perturbations. To address the critical challenge of modeling\ndiscrete sets, we devise a dual transformation mechanism that bijectively\nconverts sets into fixed-length integer sequences with summation constraints.\nFurther, we propose Fixed-Sum Discrete Diffusion--the first framework to\nsimultaneously handle discrete values, fixed sequence length, and summation\ninvariance--enabling effective set distribution modeling. Experiments\ndemonstrate our method's superiority in semantic-aware representation and\ngeneration quality. Our innovations, spanning novel representation and modeling\nstrategies, advance visual generation beyond traditional sequential token\nparadigms. Our code and models are publicly available at\nhttps://github.com/Gengzigang/TokenSet.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Zigang Geng",
      "Mengde Xu",
      "Han Hu",
      "Shuyang Gu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16424v1",
    "title": "Bézier Splatting for Fast and Differentiable Vector Graphics",
    "abstract": "Differentiable vector graphics (VGs) are widely used in image vectorization\nand vector synthesis, while existing representations are costly to optimize and\nstruggle to achieve high-quality rendering results for high-resolution images.\nThis work introduces a new differentiable VG representation, dubbed B\\'ezier\nsplatting, that enables fast yet high-fidelity VG rasterization. B\\'ezier\nsplatting samples 2D Gaussians along B\\'ezier curves, which naturally provide\npositional gradients at object boundaries. Thanks to the efficient\nsplatting-based differentiable rasterizer, B\\'ezier splatting achieves over 20x\nand 150x faster per forward and backward rasterization step for open curves\ncompared to DiffVG. Additionally, we introduce an adaptive pruning and\ndensification strategy that dynamically adjusts the spatial distribution of\ncurves to escape local minima, further improving VG quality. Experimental\nresults show that B\\'ezier splatting significantly outperforms existing methods\nwith better visual fidelity and 10x faster optimization speed.",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Xi Liu",
      "Chaoyi Zhou",
      "Nanxuan Zhao",
      "Siyu Huang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16423v1",
    "title": "GAEA: A Geolocation Aware Conversational Model",
    "abstract": "Image geolocalization, in which, traditionally, an AI model predicts the\nprecise GPS coordinates of an image is a challenging task with many downstream\napplications. However, the user cannot utilize the model to further their\nknowledge other than the GPS coordinate; the model lacks an understanding of\nthe location and the conversational ability to communicate with the user. In\nrecent days, with tremendous progress of large multimodal models (LMMs)\nproprietary and open-source researchers have attempted to geolocalize images\nvia LMMs. However, the issues remain unaddressed; beyond general tasks, for\nmore specialized downstream tasks, one of which is geolocalization, LMMs\nstruggle. In this work, we propose to solve this problem by introducing a\nconversational model GAEA that can provide information regarding the location\nof an image, as required by a user. No large-scale dataset enabling the\ntraining of such a model exists. Thus we propose a comprehensive dataset GAEA\nwith 800K images and around 1.6M question answer pairs constructed by\nleveraging OpenStreetMap (OSM) attributes and geographical context clues. For\nquantitative evaluation, we propose a diverse benchmark comprising 4K\nimage-text pairs to evaluate conversational capabilities equipped with diverse\nquestion types. We consider 11 state-of-the-art open-source and proprietary\nLMMs and demonstrate that GAEA significantly outperforms the best open-source\nmodel, LLaVA-OneVision by 25.69% and the best proprietary model, GPT-4o by\n8.28%. Our dataset, model and codes are available",
    "categories": [
      "cs.CV",
      "cs.LG",
      "I.4; I.2.7; I.5"
    ],
    "authors": [
      "Ron Campos",
      "Ashmal Vayani",
      "Parth Parag Kulkarni",
      "Rohit Gupta",
      "Aritra Dutta",
      "Mubarak Shah"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16422v1",
    "title": "1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering",
    "abstract": "4D Gaussian Splatting (4DGS) has recently gained considerable attention as a\nmethod for reconstructing dynamic scenes. Despite achieving superior quality,\n4DGS typically requires substantial storage and suffers from slow rendering\nspeed. In this work, we delve into these issues and identify two key sources of\ntemporal redundancy. (Q1) \\textbf{Short-Lifespan Gaussians}: 4DGS uses a large\nportion of Gaussians with short temporal span to represent scene dynamics,\nleading to an excessive number of Gaussians. (Q2) \\textbf{Inactive Gaussians}:\nWhen rendering, only a small subset of Gaussians contributes to each frame.\nDespite this, all Gaussians are processed during rasterization, resulting in\nredundant computation overhead. To address these redundancies, we present\n\\textbf{4DGS-1K}, which runs at over 1000 FPS on modern GPUs. For Q1, we\nintroduce the Spatial-Temporal Variation Score, a new pruning criterion that\neffectively removes short-lifespan Gaussians while encouraging 4DGS to capture\nscene dynamics using Gaussians with longer temporal spans. For Q2, we store a\nmask for active Gaussians across consecutive frames, significantly reducing\nredundant computations in rendering. Compared to vanilla 4DGS, our method\nachieves a $41\\times$ reduction in storage and $9\\times$ faster rasterization\nspeed on complex dynamic scenes, while maintaining comparable visual quality.\nPlease see our project page at https://4DGS-1K.github.io.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yuheng Yuan",
      "Qiuhong Shen",
      "Xingyi Yang",
      "Xinchao Wang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16421v1",
    "title": "MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance",
    "abstract": "Recent advances in video generation have led to remarkable improvements in\nvisual quality and temporal coherence. Upon this, trajectory-controllable video\ngeneration has emerged to enable precise object motion control through\nexplicitly defined spatial paths. However, existing methods struggle with\ncomplex object movements and multi-object motion control, resulting in\nimprecise trajectory adherence, poor object consistency, and compromised visual\nquality. Furthermore, these methods only support trajectory control in a single\nformat, limiting their applicability in diverse scenarios. Additionally, there\nis no publicly available dataset or benchmark specifically tailored for\ntrajectory-controllable video generation, hindering robust training and\nsystematic evaluation. To address these challenges, we introduce MagicMotion, a\nnovel image-to-video generation framework that enables trajectory control\nthrough three levels of conditions from dense to sparse: masks, bounding boxes,\nand sparse boxes. Given an input image and trajectories, MagicMotion seamlessly\nanimates objects along defined trajectories while maintaining object\nconsistency and visual quality. Furthermore, we present MagicData, a\nlarge-scale trajectory-controlled video dataset, along with an automated\npipeline for annotation and filtering. We also introduce MagicBench, a\ncomprehensive benchmark that assesses both video quality and trajectory control\naccuracy across different numbers of objects. Extensive experiments demonstrate\nthat MagicMotion outperforms previous methods across various metrics. Our\nproject page are publicly available at\nhttps://quanhaol.github.io/magicmotion-site.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "authors": [
      "Quanhao Li",
      "Zhen Xing",
      "Rui Wang",
      "Hui Zhang",
      "Qi Dai",
      "Zuxuan Wu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16420v1",
    "title": "SynCity: Training-Free Generation of 3D Worlds",
    "abstract": "We address the challenge of generating 3D worlds from textual descriptions.\nWe propose SynCity, a training- and optimization-free approach, which leverages\nthe geometric precision of pre-trained 3D generative models and the artistic\nversatility of 2D image generators to create large, high-quality 3D spaces.\nWhile most 3D generative models are object-centric and cannot generate\nlarge-scale worlds, we show how 3D and 2D generators can be combined to\ngenerate ever-expanding scenes. Through a tile-based approach, we allow\nfine-grained control over the layout and the appearance of scenes. The world is\ngenerated tile-by-tile, and each new tile is generated within its world-context\nand then fused with the scene. SynCity generates compelling and immersive\nscenes that are rich in detail and diversity.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Paul Engstler",
      "Aleksandar Shtedritski",
      "Iro Laina",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16419v1",
    "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Yang Sui",
      "Yu-Neng Chuang",
      "Guanchu Wang",
      "Jiamu Zhang",
      "Tianyi Zhang",
      "Jiayi Yuan",
      "Hongyi Liu",
      "Andrew Wen",
      "Shaochen",
      "Zhong",
      "Hanjie Chen",
      "Xia Hu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16418v1",
    "title": "InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity",
    "abstract": "Achieving flexible and high-fidelity identity-preserved image generation\nremains formidable, particularly with advanced Diffusion Transformers (DiTs)\nlike FLUX. We introduce InfiniteYou (InfU), one of the earliest robust\nframeworks leveraging DiTs for this task. InfU addresses significant issues of\nexisting methods, such as insufficient identity similarity, poor text-image\nalignment, and low generation quality and aesthetics. Central to InfU is\nInfuseNet, a component that injects identity features into the DiT base model\nvia residual connections, enhancing identity similarity while maintaining\ngeneration capabilities. A multi-stage training strategy, including pretraining\nand supervised fine-tuning (SFT) with synthetic single-person-multiple-sample\n(SPMS) data, further improves text-image alignment, ameliorates image quality,\nand alleviates face copy-pasting. Extensive experiments demonstrate that InfU\nachieves state-of-the-art performance, surpassing existing baselines. In\naddition, the plug-and-play design of InfU ensures compatibility with various\nexisting methods, offering a valuable contribution to the broader community.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Liming Jiang",
      "Qing Yan",
      "Yumin Jia",
      "Zichuan Liu",
      "Hao Kang",
      "Xin Lu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16416v1",
    "title": "Survey on Evaluation of LLM-based Agents",
    "abstract": "The emergence of LLM-based agents represents a paradigm shift in AI, enabling\nautonomous systems to plan, reason, use tools, and maintain memory while\ninteracting with dynamic environments. This paper provides the first\ncomprehensive survey of evaluation methodologies for these increasingly capable\nagents. We systematically analyze evaluation benchmarks and frameworks across\nfour critical dimensions: (1) fundamental agent capabilities, including\nplanning, tool use, self-reflection, and memory; (2) application-specific\nbenchmarks for web, software engineering, scientific, and conversational\nagents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating\nagents. Our analysis reveals emerging trends, including a shift toward more\nrealistic, challenging evaluations with continuously updated benchmarks. We\nalso identify critical gaps that future research must address-particularly in\nassessing cost-efficiency, safety, and robustness, and in developing\nfine-grained, and scalable evaluation methods. This survey maps the rapidly\nevolving landscape of agent evaluation, reveals the emerging trends in the\nfield, identifies current limitations, and proposes directions for future\nresearch.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Asaf Yehudai",
      "Lilach Eden",
      "Alan Li",
      "Guy Uziel",
      "Yilun Zhao",
      "Roy Bar-Haim",
      "Arman Cohan",
      "Michal Shmueli-Scheuer"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16412v1",
    "title": "DreamTexture: Shape from Virtual Texture with Analysis by Augmentation",
    "abstract": "DreamFusion established a new paradigm for unsupervised 3D reconstruction\nfrom virtual views by combining advances in generative models and\ndifferentiable rendering. However, the underlying multi-view rendering, along\nwith supervision from large-scale generative models, is computationally\nexpensive and under-constrained. We propose DreamTexture, a novel\nShape-from-Virtual-Texture approach that leverages monocular depth cues to\nreconstruct 3D objects. Our method textures an input image by aligning a\nvirtual texture with the real depth cues in the input, exploiting the inherent\nunderstanding of monocular geometry encoded in modern diffusion models. We then\nreconstruct depth from the virtual texture deformation with a new conformal map\noptimization, which alleviates memory-intensive volumetric representations. Our\nexperiments reveal that generative models possess an understanding of monocular\nshape cues, which can be extracted by augmenting and aligning texture cues -- a\nnovel monocular reconstruction paradigm that we call Analysis by Augmentation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Ananta R. Bhattarai",
      "Xingzhe He",
      "Alla Sheffer",
      "Helge Rhodin"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16413v1",
    "title": "M3: 3D-Spatial MultiModal Memory",
    "abstract": "We present 3D Spatial MultiModal Memory (M3), a multimodal memory system\ndesigned to retain information about medium-sized static scenes through video\nsources for visual perception. By integrating 3D Gaussian Splatting techniques\nwith foundation models, M3 builds a multimodal memory capable of rendering\nfeature representations across granularities, encompassing a wide range of\nknowledge. In our exploration, we identify two key challenges in previous works\non feature splatting: (1) computational constraints in storing high-dimensional\nfeatures for each Gaussian primitive, and (2) misalignment or information loss\nbetween distilled features and foundation model features. To address these\nchallenges, we propose M3 with key components of principal scene components and\nGaussian memory attention, enabling efficient training and inference. To\nvalidate M3, we conduct comprehensive quantitative evaluations of feature\nsimilarity and downstream tasks, as well as qualitative visualizations to\nhighlight the pixel trace of Gaussian memory attention. Our approach\nencompasses a diverse range of foundation models, including vision-language\nmodels (VLMs), perception models, and large multimodal and language models\n(LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy\nM3's feature field in indoor scenes on a quadruped robot. Notably, we claim\nthat M3 is the first work to address the core compression challenges in 3D\nfeature distillation.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Xueyan Zou",
      "Yuchen Song",
      "Ri-Zhao Qiu",
      "Xuanbin Peng",
      "Jianglong Ye",
      "Sifei Liu",
      "Xiaolong Wang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16408v1",
    "title": "RoboFactory: Exploring Embodied Agent Collaboration with Compositional Constraints",
    "abstract": "Designing effective embodied multi-agent systems is critical for solving\ncomplex real-world tasks across domains. Due to the complexity of multi-agent\nembodied systems, existing methods fail to automatically generate safe and\nefficient training data for such systems. To this end, we propose the concept\nof compositional constraints for embodied multi-agent systems, addressing the\nchallenges arising from collaboration among embodied agents. We design various\ninterfaces tailored to different types of constraints, enabling seamless\ninteraction with the physical world. Leveraging compositional constraints and\nspecifically designed interfaces, we develop an automated data collection\nframework for embodied multi-agent systems and introduce the first benchmark\nfor embodied multi-agent manipulation, RoboFactory. Based on RoboFactory\nbenchmark, we adapt and evaluate the method of imitation learning and analyzed\nits performance in different difficulty agent tasks. Furthermore, we explore\nthe architectures and training strategies for multi-agent imitation learning,\naiming to build safe and efficient embodied multi-agent systems.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Yiran Qin",
      "Li Kang",
      "Xiufeng Song",
      "Zhenfei Yin",
      "Xiaohong Liu",
      "Xihui Liu",
      "Ruimao Zhang",
      "Lei Bai"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16594v1",
    "title": "Transformer-based Wireless Symbol Detection Over Fading Channels",
    "abstract": "Pre-trained Transformers, through in-context learning (ICL), have\ndemonstrated exceptional capabilities to adapt to new tasks using example\nprompts without model update. Transformer-based wireless receivers, where\nprompts consist of the pilot data in the form of transmitted and received\nsignal pairs, have shown high detection accuracy when pilot data are abundant.\nHowever, pilot information is often costly and limited in practice. In this\nwork, we propose the DEcision Feedback INcontExt Detection (DEFINED) solution\nas a new wireless receiver design, which bypasses channel estimation and\ndirectly performs symbol detection using the (sometimes extremely) limited\npilot data. The key innovation in DEFINED is the proposed decision feedback\nmechanism in ICL, where we sequentially incorporate the detected symbols into\nthe prompts as pseudo-labels to improve the detection for subsequent symbols.\nFurthermore, we proposed another detection method where we combine ICL with\nSemi-Supervised Learning (SSL) to extract information from both labeled and\nunlabeled data during inference, thus avoiding the errors propagated during the\ndecision feedback process of the original DEFINED. Extensive experiments across\na broad range of wireless communication settings demonstrate that a small\nTransformer trained with DEFINED or IC-SSL achieves significant performance\nimprovements over conventional methods, in some cases only needing a single\npilot pair to achieve similar performance of the latter with more than 4 pilot\npairs.",
    "categories": [
      "cs.IT",
      "cs.LG",
      "eess.SP",
      "math.IT",
      "stat.ML"
    ],
    "authors": [
      "Li Fan",
      "Jing Yang",
      "Cong Shen"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16406v1",
    "title": "VerbDiff: Text-Only Diffusion Models with Enhanced Interaction Awareness",
    "abstract": "Recent large-scale text-to-image diffusion models generate photorealistic\nimages but often struggle to accurately depict interactions between humans and\nobjects due to their limited ability to differentiate various interaction\nwords. In this work, we propose VerbDiff to address the challenge of capturing\nnuanced interactions within text-to-image diffusion models. VerbDiff is a novel\ntext-to-image generation model that weakens the bias between interaction words\nand objects, enhancing the understanding of interactions. Specifically, we\ndisentangle various interaction words from frequency-based anchor words and\nleverage localized interaction regions from generated images to help the model\nbetter capture semantics in distinctive words without extra conditions. Our\napproach enables the model to accurately understand the intended interaction\nbetween humans and objects, producing high-quality images with accurate\ninteractions aligned with specified verbs. Extensive experiments on the\nHICO-DET dataset demonstrate the effectiveness of our method compared to\nprevious approaches.",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.MM"
    ],
    "authors": [
      "SeungJu Cha",
      "Kwanyoung Lee",
      "Ye-Chan Kim",
      "Hyunwoo Oh",
      "Dong-Jin Kim"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16402v1",
    "title": "The Emperor's New Clothes in Benchmarking? A Rigorous Examination of Mitigation Strategies for LLM Benchmark Data Contamination",
    "abstract": "Benchmark Data Contamination (BDC)-the inclusion of benchmark testing samples\nin the training set-has raised increasing concerns in Large Language Model\n(LLM) evaluation, leading to falsely inflated performance estimates and\nundermining evaluation reliability. To address this, researchers have proposed\nvarious mitigation strategies to update existing benchmarks, including\nmodifying original questions or generating new ones based on them. However, a\nrigorous examination of the effectiveness of these mitigation strategies\nremains lacking. In this paper, we design a systematic and controlled pipeline\nalong with two novel metrics-fidelity and contamination resistance-to provide a\nfine-grained and comprehensive assessment of existing BDC mitigation\nstrategies. Previous assessment methods, such as accuracy drop and accuracy\nmatching, focus solely on aggregate accuracy, often leading to incomplete or\nmisleading conclusions. Our metrics address this limitation by emphasizing\nquestion-level evaluation result matching. Extensive experiments with 10 LLMs,\n5 benchmarks, 20 BDC mitigation strategies, and 2 contamination scenarios\nreveal that no existing strategy significantly improves resistance over the\nvanilla case (i.e., no benchmark update) across all benchmarks, and none\neffectively balances fidelity and contamination resistance. These findings\nunderscore the urgent need for designing more effective BDC mitigation\nstrategies. Our code repository is available at\nhttps://github.com/ASTRAL-Group/BDC_mitigation_assessment.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Yifan Sun",
      "Han Wang",
      "Dongbai Li",
      "Gang Wang",
      "Huan Zhang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16401v1",
    "title": "Exploring the Hidden Reasoning Process of Large Language Models by Misleading Them",
    "abstract": "Large language models (LLMs) and Vision language models (VLMs) have been able\nto perform various forms of reasoning tasks in a wide range of scenarios, but\nare they truly engaging in task abstraction and rule-based reasoning beyond\nmere memorization and pattern matching? To answer this question, we propose a\nnovel experimental approach, Misleading Fine-Tuning (MisFT), to examine whether\nLLMs/VLMs perform abstract reasoning by altering their original understanding\nof fundamental rules. In particular, by constructing a dataset with math\nexpressions that contradict correct operation principles, we fine-tune the\nmodel to learn those contradictory rules and assess its generalization ability\non different test domains. Through a series of experiments, we find that\ncurrent LLMs/VLMs are capable of effectively applying contradictory rules to\nsolve practical math word problems and math expressions represented by images,\nimplying the presence of an internal mechanism that abstracts before reasoning.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Guanyu Chen",
      "Peiyang Wang",
      "Tianren Zhang",
      "Feng Chen"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16400v1",
    "title": "ScalingNoise: Scaling Inference-Time Search for Generating Infinite Videos",
    "abstract": "Video diffusion models (VDMs) facilitate the generation of high-quality\nvideos, with current research predominantly concentrated on scaling efforts\nduring training through improvements in data quality, computational resources,\nand model complexity. However, inference-time scaling has received less\nattention, with most approaches restricting models to a single generation\nattempt. Recent studies have uncovered the existence of \"golden noises\" that\ncan enhance video quality during generation. Building on this, we find that\nguiding the scaling inference-time search of VDMs to identify better noise\ncandidates not only evaluates the quality of the frames generated in the\ncurrent step but also preserves the high-level object features by referencing\nthe anchor frame from previous multi-chunks, thereby delivering long-term\nvalue. Our analysis reveals that diffusion models inherently possess flexible\nadjustments of computation by varying denoising steps, and even a one-step\ndenoising approach, when guided by a reward signal, yields significant\nlong-term benefits. Based on the observation, we proposeScalingNoise, a\nplug-and-play inference-time search strategy that identifies golden initial\nnoises for the diffusion sampling process to improve global content consistency\nand visual diversity. Specifically, we perform one-step denoising to convert\ninitial noises into a clip and subsequently evaluate its long-term value,\nleveraging a reward model anchored by previously generated content. Moreover,\nto preserve diversity, we sample candidates from a tilted noise distribution\nthat up-weights promising noises. In this way, ScalingNoise significantly\nreduces noise-induced errors, ensuring more coherent and spatiotemporally\nconsistent video generation. Extensive experiments on benchmark datasets\ndemonstrate that the proposed ScalingNoise effectively improves long video\ngeneration.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Haolin Yang",
      "Feilong Tang",
      "Ming Hu",
      "Yulong Li",
      "Junjie Guo",
      "Yexin Liu",
      "Zelin Peng",
      "Junjun He",
      "Zongyuan Ge",
      "Imran Razzak"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16399v1",
    "title": "SA-Occ: Satellite-Assisted 3D Occupancy Prediction in Real World",
    "abstract": "Existing vision-based 3D occupancy prediction methods are inherently limited\nin accuracy due to their exclusive reliance on street-view imagery, neglecting\nthe potential benefits of incorporating satellite views. We propose SA-Occ, the\nfirst Satellite-Assisted 3D occupancy prediction model, which leverages GPS &\nIMU to integrate historical yet readily available satellite imagery into\nreal-time applications, effectively mitigating limitations of ego-vehicle\nperceptions, involving occlusions and degraded performance in distant regions.\nTo address the core challenges of cross-view perception, we propose: 1)\nDynamic-Decoupling Fusion, which resolves inconsistencies in dynamic regions\ncaused by the temporal asynchrony between satellite and street views; 2)\n3D-Proj Guidance, a module that enhances 3D feature extraction from inherently\n2D satellite imagery; and 3) Uniform Sampling Alignment, which aligns the\nsampling density between street and satellite views. Evaluated on\nOcc3D-nuScenes, SA-Occ achieves state-of-the-art performance, especially among\nsingle-frame methods, with a 39.05% mIoU (a 6.97% improvement), while incurring\nonly 6.93 ms of additional latency per frame. Our code and newly curated\ndataset are available at https://github.com/chenchen235/SA-Occ.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Chen Chen",
      "Zhirui Wang",
      "Taowei Sheng",
      "Yi Jiang",
      "Yundu Li",
      "Peirui Cheng",
      "Luning Zhang",
      "Kaiqiang Chen",
      "Yanfeng Hu",
      "Xue Yang",
      "Xian Sun"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16398v1",
    "title": "The global convergence time of stochastic gradient descent in non-convex landscapes: Sharp estimates via large deviations",
    "abstract": "In this paper, we examine the time it takes for stochastic gradient descent\n(SGD) to reach the global minimum of a general, non-convex loss function. We\napproach this question through the lens of randomly perturbed dynamical systems\nand large deviations theory, and we provide a tight characterization of the\nglobal convergence time of SGD via matching upper and lower bounds. These\nbounds are dominated by the most \"costly\" set of obstacles that the algorithm\nmay need to overcome to reach a global minimizer from a given initialization,\ncoupling in this way the global geometry of the underlying loss landscape with\nthe statistics of the noise entering the process. Finally, motivated by\napplications to the training of deep neural networks, we also provide a series\nof refinements and extensions of our analysis for loss functions with shallow\nlocal minima.",
    "categories": [
      "math.OC",
      "cs.LG",
      "Primary 90C15, 90C26, 60F10, secondary 90C30, 68Q32"
    ],
    "authors": [
      "Waïss Azizian",
      "Franck Iutzeler",
      "Jérôme Malick",
      "Panayotis Mertikopoulos"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16397v1",
    "title": "Scale-wise Distillation of Diffusion Models",
    "abstract": "We present SwD, a scale-wise distillation framework for diffusion models\n(DMs), which effectively employs next-scale prediction ideas for\ndiffusion-based few-step generators. In more detail, SwD is inspired by the\nrecent insights relating diffusion processes to the implicit spectral\nautoregression. We suppose that DMs can initiate generation at lower data\nresolutions and gradually upscale the samples at each denoising step without\nloss in performance while significantly reducing computational costs. SwD\nnaturally integrates this idea into existing diffusion distillation methods\nbased on distribution matching. Also, we enrich the family of distribution\nmatching approaches by introducing a novel patch loss enforcing finer-grained\nsimilarity to the target distribution. When applied to state-of-the-art\ntext-to-image diffusion models, SwD approaches the inference times of two full\nresolution steps and significantly outperforms the counterparts under the same\ncomputation budget, as evidenced by automated metrics and human preference\nstudies.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Nikita Starodubcev",
      "Denis Kuznedelev",
      "Artem Babenko",
      "Dmitry Baranchuk"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16396v2",
    "title": "SV4D 2.0: Enhancing Spatio-Temporal Consistency in Multi-View Video Diffusion for High-Quality 4D Generation",
    "abstract": "We present Stable Video 4D 2.0 (SV4D 2.0), a multi-view video diffusion model\nfor dynamic 3D asset generation. Compared to its predecessor SV4D, SV4D 2.0 is\nmore robust to occlusions and large motion, generalizes better to real-world\nvideos, and produces higher-quality outputs in terms of detail sharpness and\nspatio-temporal consistency. We achieve this by introducing key improvements in\nmultiple aspects: 1) network architecture: eliminating the dependency of\nreference multi-views and designing blending mechanism for 3D and frame\nattention, 2) data: enhancing quality and quantity of training data, 3)\ntraining strategy: adopting progressive 3D-4D training for better\ngeneralization, and 4) 4D optimization: handling 3D inconsistency and large\nmotion via 2-stage refinement and progressive frame sampling. Extensive\nexperiments demonstrate significant performance gain by SV4D 2.0 both visually\nand quantitatively, achieving better detail (-14\\% LPIPS) and 4D consistency\n(-44\\% FV4D) in novel-view video synthesis and 4D optimization (-12\\% LPIPS and\n-24\\% FV4D) compared to SV4D.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Chun-Han Yao",
      "Yiming Xie",
      "Vikram Voleti",
      "Huaizu Jiang",
      "Varun Jampani"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16395v1",
    "title": "Truthful Elicitation of Imprecise Forecasts",
    "abstract": "The quality of probabilistic forecasts is crucial for decision-making under\nuncertainty. While proper scoring rules incentivize truthful reporting of\nprecise forecasts, they fall short when forecasters face epistemic uncertainty\nabout their beliefs, limiting their use in safety-critical domains where\ndecision-makers (DMs) prioritize proper uncertainty management. To address\nthis, we propose a framework for scoring imprecise forecasts -- forecasts given\nas a set of beliefs. Despite existing impossibility results for deterministic\nscoring rules, we enable truthful elicitation by drawing connection to social\nchoice theory and introducing a two-way communication framework where DMs first\nshare their aggregation rules (e.g., averaging or min-max) used in downstream\ndecisions for resolving forecast ambiguity. This, in turn, helps forecasters\nresolve indecision during elicitation. We further show that truthful\nelicitation of imprecise forecasts is achievable using proper scoring rules\nrandomized over the aggregation procedure. Our approach allows DM to elicit and\nintegrate the forecaster's epistemic uncertainty into their decision-making\nprocess, thus improving credibility.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Anurag Singh",
      "Siu Lun Chau",
      "Krikamol Muandet"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16394v1",
    "title": "Do Visual Imaginations Improve Vision-and-Language Navigation Agents?",
    "abstract": "Vision-and-Language Navigation (VLN) agents are tasked with navigating an\nunseen environment using natural language instructions. In this work, we study\nif visual representations of sub-goals implied by the instructions can serve as\nnavigational cues and lead to increased navigation performance. To synthesize\nthese visual representations or imaginations, we leverage a text-to-image\ndiffusion model on landmark references contained in segmented instructions.\nThese imaginations are provided to VLN agents as an added modality to act as\nlandmark cues and an auxiliary loss is added to explicitly encourage relating\nthese with their corresponding referring expressions. Our findings reveal an\nincrease in success rate (SR) of around 1 point and up to 0.5 points in success\nscaled by inverse path length (SPL) across agents. These results suggest that\nthe proposed approach reinforces visual understanding compared to relying on\nlanguage instructions alone. Code and data for our work can be found at\nhttps://www.akhilperincherry.com/VLN-Imagine-website/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "authors": [
      "Akhil Perincherry",
      "Jacob Krantz",
      "Stefan Lee"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16392v1",
    "title": "Graph of Effort: Quantifying Risk of AI Usage for Vulnerability Assessment",
    "abstract": "With AI-based software becoming widely available, the risk of exploiting its\ncapabilities, such as high automation and complex pattern recognition, could\nsignificantly increase. An AI used offensively to attack non-AI assets is\nreferred to as offensive AI.\n  Current research explores how offensive AI can be utilized and how its usage\ncan be classified. Additionally, methods for threat modeling are being\ndeveloped for AI-based assets within organizations. However, there are gaps\nthat need to be addressed. Firstly, there is a need to quantify the factors\ncontributing to the AI threat. Secondly, there is a requirement to create\nthreat models that analyze the risk of being attacked by AI for vulnerability\nassessment across all assets of an organization. This is particularly crucial\nand challenging in cloud environments, where sophisticated infrastructure and\naccess control landscapes are prevalent. The ability to quantify and further\nanalyze the threat posed by offensive AI enables analysts to rank\nvulnerabilities and prioritize the implementation of proactive countermeasures.\n  To address these gaps, this paper introduces the Graph of Effort, an\nintuitive, flexible, and effective threat modeling method for analyzing the\neffort required to use offensive AI for vulnerability exploitation by an\nadversary. While the threat model is functional and provides valuable support,\nits design choices need further empirical validation in future work.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ],
    "authors": [
      "Anket Mehra",
      "Andreas Aßmuth",
      "Malte Prieß"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16591v1",
    "title": "UniK3D: Universal Camera Monocular 3D Estimation",
    "abstract": "Monocular 3D estimation is crucial for visual perception. However, current\nmethods fall short by relying on oversimplified assumptions, such as pinhole\ncamera models or rectified images. These limitations severely restrict their\ngeneral applicability, causing poor performance in real-world scenarios with\nfisheye or panoramic images and resulting in substantial context loss. To\naddress this, we present UniK3D, the first generalizable method for monocular\n3D estimation able to model any camera. Our method introduces a spherical 3D\nrepresentation which allows for better disentanglement of camera and scene\ngeometry and enables accurate metric 3D reconstruction for unconstrained camera\nmodels. Our camera component features a novel, model-independent representation\nof the pencil of rays, achieved through a learned superposition of spherical\nharmonics. We also introduce an angular loss, which, together with the camera\nmodule design, prevents the contraction of the 3D outputs for wide-view\ncameras. A comprehensive zero-shot evaluation on 13 diverse datasets\ndemonstrates the state-of-the-art performance of UniK3D across 3D, depth, and\ncamera metrics, with substantial gains in challenging large-field-of-view and\npanoramic settings, while maintaining top accuracy in conventional pinhole\nsmall-field-of-view domains. Code and models are available at\ngithub.com/lpiccinelli-eth/unik3d .",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Luigi Piccinelli",
      "Christos Sakaridis",
      "Mattia Segu",
      "Yung-Hsu Yang",
      "Siyuan Li",
      "Wim Abbeloos",
      "Luc Van Gool"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16389v1",
    "title": "Attentional Triple-Encoder Network in Spatiospectral Domains for Medical Image Segmentation",
    "abstract": "Retinal Optical Coherence Tomography (OCT) segmentation is essential for\ndiagnosing pathology. Traditional methods focus on either spatial or spectral\ndomains, overlooking their combined dependencies. We propose a triple-encoder\nnetwork that integrates CNNs for spatial features, Fast Fourier Convolution\n(FFC) for spectral features, and attention mechanisms to capture global\nrelationships across both domains. Attention fusion modules integrate\nconvolution and cross-attention to further enhance features. Our method\nachieves an average Dice score improvement from 0.855 to 0.864, outperforming\nprior work.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Kristin Qi",
      "Xinhan Di"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16385v1",
    "title": "Deconstructing Long Chain-of-Thought: A Structured Reasoning Optimization Framework for Long CoT Distillation",
    "abstract": "Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities through long chain-of-thought (CoT)\nreasoning. The R1 distillation scheme has emerged as a promising approach for\ntraining cost-effective models with enhanced reasoning abilities. However, the\nunderlying mechanisms driving its effectiveness remain unclear. This study\nexamines the universality of distillation data and identifies key components\nthat enable the efficient transfer of long-chain reasoning capabilities in LLM\ndistillation. Our findings reveal that the effectiveness of long CoT reasoning\ndistillation from teacher models like Qwen-QwQ degrades significantly on\nnonhomologous models, challenging the assumed universality of current\ndistillation methods. To gain deeper insights into the structure and patterns\nof long CoT reasoning, we propose DLCoT (Deconstructing Long Chain-of-Thought),\na distillation data enhancement framework. DLCoT consists of three key steps:\n(1) data segmentation to decompose complex long CoT structures, (2)\nsimplification by eliminating unsolvable and redundant solutions, and (3)\noptimization of intermediate error states. Our approach significantly improves\nmodel performance and token efficiency, facilitating the development of\nhigh-performance LLMs.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Yijia Luo",
      "Yulin Song",
      "Xingyao Zhang",
      "Jiaheng Liu",
      "Weixun Wang",
      "GengRu Chen",
      "Wenbo Su",
      "Bo Zheng"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16382v1",
    "title": "Sparse Nonparametric Contextual Bandits",
    "abstract": "This paper studies the problem of simultaneously learning relevant features\nand minimising regret in contextual bandit problems. We introduce and analyse a\nnew class of contextual bandit problems, called sparse nonparametric contextual\nbandits, in which the expected reward function lies in the linear span of a\nsmall unknown set of features that belongs to a known infinite set of candidate\nfeatures. We consider two notions of sparsity, for which the set of candidate\nfeatures is either countable or uncountable. Our contribution is two-fold.\nFirst, we provide lower bounds on the minimax regret, which show that\npolynomial dependence on the number of actions is generally unavoidable in this\nsetting. Second, we show that a variant of the Feel-Good Thompson Sampling\nalgorithm enjoys regret bounds that match our lower bounds up to logarithmic\nfactors of the horizon, and have logarithmic dependence on the effective number\nof candidate features. When we apply our results to kernelised and neural\ncontextual bandits, we find that sparsity always enables better regret bounds,\nas long as the horizon is large enough relative to the sparsity and the number\nof actions.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Hamish Flynn",
      "Julia Olkhovskaya",
      "Paul Rognon-Vael"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16378v1",
    "title": "Panoptic-CUDAL Technical Report: Rural Australia Point Cloud Dataset in Rainy Conditions",
    "abstract": "Existing autonomous driving datasets are predominantly oriented towards\nwell-structured urban settings and favorable weather conditions, leaving the\ncomplexities of rural environments and adverse weather conditions largely\nunaddressed. Although some datasets encompass variations in weather and\nlighting, bad weather scenarios do not appear often. Rainfall can significantly\nimpair sensor functionality, introducing noise and reflections in LiDAR and\ncamera data and reducing the system's capabilities for reliable environmental\nperception and safe navigation. We introduce the Panoptic-CUDAL dataset, a\nnovel dataset purpose-built for panoptic segmentation in rural areas subject to\nrain. By recording high-resolution LiDAR, camera, and pose data, Panoptic-CUDAL\noffers a diverse, information-rich dataset in a challenging scenario. We\npresent analysis of the recorded data and provide baseline results for panoptic\nand semantic segmentation methods on LiDAR point clouds. The dataset can be\nfound here:\nhttps://robotics.sydney.edu.au/our-research/intelligent-transportation-systems/",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Tzu-Yun Tseng",
      "Alexey Nekrasov",
      "Malcolm Burdorf",
      "Bastian Leibe",
      "Julie Stephany Berrio",
      "Mao Shan",
      "Stewart Worrall"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16376v1",
    "title": "LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial Images",
    "abstract": "The success of modern machine learning, particularly in facial translation\nnetworks, is highly dependent on the availability of high-quality, paired,\nlarge-scale datasets. However, acquiring sufficient data is often challenging\nand costly. Inspired by the recent success of diffusion models in high-quality\nimage synthesis and advancements in Large Language Models (LLMs), we propose a\nnovel framework called LLM-assisted Paired Image Generation (LaPIG). This\nframework enables the construction of comprehensive, high-quality paired\nvisible and thermal images using captions generated by LLMs. Our method\nencompasses three parts: visible image synthesis with ArcFace embedding,\nthermal image translation using Latent Diffusion Models (LDMs), and caption\ngeneration with LLMs. Our approach not only generates multi-view paired visible\nand thermal images to increase data diversity but also produces high-quality\npaired data while maintaining their identity information. We evaluate our\nmethod on public datasets by comparing it with existing methods, demonstrating\nthe superiority of LaPIG.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Leyang Wang",
      "Joice Lin"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16589v1",
    "title": "A Statistical Analysis for Per-Instance Evaluation of Stochastic Optimizers: How Many Repeats Are Enough?",
    "abstract": "A key trait of stochastic optimizers is that multiple runs of the same\noptimizer in attempting to solve the same problem can produce different\nresults. As a result, their performance is evaluated over several repeats, or\nruns, on the problem. However, the accuracy of the estimated performance\nmetrics depends on the number of runs and should be studied using statistical\ntools. We present a statistical analysis of the common metrics, and develop\nguidelines for experiment design to measure the optimizer's performance using\nthese metrics to a high level of confidence and accuracy. To this end, we first\ndiscuss the confidence interval of the metrics and how they are related to the\nnumber of runs of an experiment. We then derive a lower bound on the number of\nrepeats in order to guarantee achieving a given accuracy in the metrics. Using\nthis bound, we propose an algorithm to adaptively adjust the number of repeats\nneeded to ensure the accuracy of the evaluated metric. Our simulation results\ndemonstrate the utility of our analysis and how it allows us to conduct\nreliable benchmarking as well as hyperparameter tuning and prevent us from\ndrawing premature conclusions regarding the performance of stochastic\noptimizers.",
    "categories": [
      "cs.LG",
      "cs.ET",
      "math.ST",
      "stat.TH"
    ],
    "authors": [
      "Moslem Noori",
      "Elisabetta Valiante",
      "Thomas Van Vaerenbergh",
      "Masoud Mohseni",
      "Ignacio Rozada"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16375v1",
    "title": "NuiScene: Exploring Efficient Generation of Unbounded Outdoor Scenes",
    "abstract": "In this paper, we explore the task of generating expansive outdoor scenes,\nranging from castles to high-rises. Unlike indoor scene generation, which has\nbeen a primary focus of prior work, outdoor scene generation presents unique\nchallenges, including wide variations in scene heights and the need for a\nmethod capable of rapidly producing large landscapes. To address this, we\npropose an efficient approach that encodes scene chunks as uniform vector sets,\noffering better compression and performance than the spatially structured\nlatents used in prior methods. Furthermore, we train an explicit outpainting\nmodel for unbounded generation, which improves coherence compared to prior\nresampling-based inpainting schemes while also speeding up generation by\neliminating extra diffusion steps. To facilitate this task, we curate\nNuiScene43, a small but high-quality set of scenes, preprocessed for joint\ntraining. Notably, when trained on scenes of varying styles, our model can\nblend different environments, such as rural houses and city skyscrapers, within\nthe same scene, highlighting the potential of our curation process to leverage\nheterogeneous scenes for joint training.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Han-Hung Lee",
      "Qinghong Han",
      "Angel X. Chang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16371v1",
    "title": "Reinforcement Learning-based Heuristics to Guide Domain-Independent Dynamic Programming",
    "abstract": "Domain-Independent Dynamic Programming (DIDP) is a state-space search\nparadigm based on dynamic programming for combinatorial optimization. In its\ncurrent implementation, DIDP guides the search using user-defined dual bounds.\nReinforcement learning (RL) is increasingly being applied to combinatorial\noptimization problems and shares several key structures with DP, being\nrepresented by the Bellman equation and state-based transition systems. We\npropose using reinforcement learning to obtain a heuristic function to guide\nthe search in DIDP. We develop two RL-based guidance approaches: value-based\nguidance using Deep Q-Networks and policy-based guidance using Proximal Policy\nOptimization. Our experiments indicate that RL-based guidance significantly\noutperforms standard DIDP and problem-specific greedy heuristics with the same\nnumber of node expansions. Further, despite longer node evaluation times, RL\nguidance achieves better run-time performance than standard DIDP on three of\nfour benchmark domains.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Minori Narita",
      "Ryo Kuroiwa",
      "J. Christopher Beck"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16365v1",
    "title": "JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse",
    "abstract": "Recently, action-based decision-making in open-world environments has gained\nsignificant attention. Visual Language Action (VLA) models, pretrained on\nlarge-scale web datasets, have shown promise in decision-making tasks. However,\nprevious work has primarily focused on action post-training, often neglecting\nenhancements to the foundational model itself. In response, we introduce a\nnovel approach, Act from Visual Language Post-Training, which refines Visual\nLanguage Models (VLMs) through visual and linguistic guidance in a\nself-supervised manner. This enhancement improves the models' capabilities in\nworld knowledge, visual recognition, and spatial grounding in open-world\nenvironments. Following the above post-training paradigms, we obtain the first\nVLA models in Minecraft that can follow human instructions on over 1k different\natomic tasks, including crafting, smelting, cooking, mining, and killing. Our\nexperiments demonstrate that post-training on non-trajectory tasks leads to a\nsignificant 40% improvement over the best agent baseline on a diverse set of\natomic tasks. Furthermore, we demonstrate that our approach surpasses\ntraditional imitation learning-based policies in Minecraft, achieving\nstate-of-the-art performance. We have open-sourced the code, models, and\ndatasets to foster further research. The project page can be found in\nhttps://craftjarvis.github.io/JarvisVLA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Muyao Li",
      "Zihao Wang",
      "Kaichen He",
      "Xiaojian Ma",
      "Yitao Liang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16364v1",
    "title": "Neural Networks: According to the Principles of Grassmann Algebra",
    "abstract": "In this paper, we explore the algebra of quantum idempotents and the\nquantization of fermions which gives rise to a Hilbert space equal to the\nGrassmann algebra associated with the Lie algebra. Since idempotents carry\nrepresentations of the algebra under consideration, they form algebraic\nvarieties and smooth manifolds in the natural topology. In addition to the\nmotivation of linking up mathematical physics with machine learning, it is also\nshown that by using idempotents and invariant subspace of the corresponding\nalgebras, these representations encode and perhaps provide a probabilistic\ninterpretation of reasoning and relational paths in geometrical terms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Z. Zarezadeh",
      "N. Zarezadeh"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16363v1",
    "title": "Probabilistic Quantum SVM Training on Ising Machine",
    "abstract": "Quantum computing holds significant potential to accelerate machine learning\nalgorithms, especially in solving optimization problems like those encountered\nin Support Vector Machine (SVM) training. However, current QUBO-based Quantum\nSVM (QSVM) methods rely solely on binary optimal solutions, limiting their\nability to identify fuzzy boundaries in data. Additionally, the limited qubit\ncount in contemporary quantum devices constrains training on larger datasets.\nIn this paper, we propose a probabilistic quantum SVM training framework\nsuitable for Coherent Ising Machines (CIMs). By formulating the SVM training\nproblem as a QUBO model, we leverage CIMs' energy minimization capabilities and\nintroduce a Boltzmann distribution-based probabilistic approach to better\napproximate optimal SVM solutions, enhancing robustness. To address qubit\nlimitations, we employ batch processing and multi-batch ensemble strategies,\nenabling small-scale quantum devices to train SVMs on larger datasets and\nsupport multi-class classification tasks via a one-vs-one approach. Our method\nis validated through simulations and real-machine experiments on binary and\nmulti-class datasets. On the banknote binary classification dataset, our\nCIM-based QSVM, utilizing an energy-based probabilistic approach, achieved up\nto 20% higher accuracy compared to the original QSVM, while training up to\n$10^4$ times faster than simulated annealing methods. Compared with classical\nSVM, our approach either matched or reduced training time. On the IRIS\nthree-class dataset, our improved QSVM outperformed existing QSVM models in all\nkey metrics. As quantum technology advances, increased qubit counts are\nexpected to further enhance QSVM performance relative to classical SVM.",
    "categories": [
      "cs.LG",
      "quant-ph"
    ],
    "authors": [
      "Haoqi He",
      "Yan Xiao"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16361v1",
    "title": "Enhancing variational quantum algorithms by balancing training on classical and quantum hardware",
    "abstract": "Quantum computers offer a promising route to tackling problems that are\nclassically intractable such as in prime-factorization, solving large-scale\nlinear algebra and simulating complex quantum systems, but require\nfault-tolerant quantum hardware. On the other hand, variational quantum\nalgorithms (VQAs) have the potential to provide a near-term route to quantum\nutility or advantage, and is usually constructed by using parametrized quantum\ncircuits (PQCs) in combination with a classical optimizer for training.\nAlthough VQAs have been proposed for a multitude of tasks such as ground-state\nestimation, combinatorial optimization and unitary compilation, there remain\nmajor challenges in its trainability and resource costs on quantum hardware.\nHere we address these challenges by adopting Hardware Efficient and dynamical\nLIe algebra Supported Ansatz (HELIA), and propose two training schemes that\ncombine an existing g-sim method (that uses the underlying group structure of\nthe operators) and the Parameter-Shift Rule (PSR). Our improvement comes from\ndistributing the resources required for gradient estimation and training to\nboth classical and quantum hardware. We numerically test our proposal for\nground-state estimation using Variational Quantum Eigensolver (VQE) and\nclassification of quantum phases using quantum neural networks. Our methods\nshow better accuracy and success of trials, and also need fewer calls to the\nquantum hardware on an average than using only PSR (upto 60% reduction), that\nruns exclusively on quantum hardware. We also numerically demonstrate the\ncapability of HELIA in mitigating barren plateaus, paving the way for training\nlarge-scale quantum models.",
    "categories": [
      "quant-ph",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Rahul Bhowmick",
      "Harsh Wadhwa",
      "Avinash Singh",
      "Tania Sidana",
      "Quoc Hoan Tran",
      "Krishna Kumar Sabapathy"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16357v1",
    "title": "UniSync: A Unified Framework for Audio-Visual Synchronization",
    "abstract": "Precise audio-visual synchronization in speech videos is crucial for content\nquality and viewer comprehension. Existing methods have made significant\nstrides in addressing this challenge through rule-based approaches and\nend-to-end learning techniques. However, these methods often rely on limited\naudio-visual representations and suboptimal learning strategies, potentially\nconstraining their effectiveness in more complex scenarios. To address these\nlimitations, we present UniSync, a novel approach for evaluating audio-visual\nsynchronization using embedding similarities. UniSync offers broad\ncompatibility with various audio representations (e.g., Mel spectrograms,\nHuBERT) and visual representations (e.g., RGB images, face parsing maps, facial\nlandmarks, 3DMM), effectively handling their significant dimensional\ndifferences. We enhance the contrastive learning framework with a margin-based\nloss component and cross-speaker unsynchronized pairs, improving discriminative\ncapabilities. UniSync outperforms existing methods on standard datasets and\ndemonstrates versatility across diverse audio-visual representations. Its\nintegration into talking face generation frameworks enhances synchronization\nquality in both natural and AI-generated content.",
    "categories": [
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "authors": [
      "Tao Feng",
      "Yifan Xie",
      "Xun Guan",
      "Jiyuan Song",
      "Zhou Liu",
      "Fei Ma",
      "Fei Yu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16356v1",
    "title": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners",
    "abstract": "Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they struggle to generalize these updates to multi-hop\nreasoning tasks that depend on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we observe that current layer-localized KE approaches, such as MEMIT\nand WISE, which edit only single or a few model layers, struggle to effectively\nincorporate updated information into these reasoning pathways. To address this\nlimitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method\nthat enables more effective integration of updated knowledge in LLMs. CaKE\nleverages strategically curated data, guided by our circuits-based analysis,\nthat enforces the model to utilize the modified knowledge, stimulating the\nmodel to develop appropriate reasoning circuits for newly integrated knowledge.\nExperimental results show that CaKE enables more accurate and consistent use of\nupdated knowledge across related reasoning tasks, leading to an average of 20%\nimprovement in multi-hop reasoning accuracy on MQuAKE dataset compared to\nexisting KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.IR",
      "cs.LG"
    ],
    "authors": [
      "Yunzhi Yao",
      "Jizhan Fang",
      "Jia-Chen Gu",
      "Ningyu Zhang",
      "Shumin Deng",
      "Huajun Chen",
      "Nanyun Peng"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16351v1",
    "title": "Lyra: An Efficient and Expressive Subquadratic Architecture for Modeling Biological Sequences",
    "abstract": "Deep learning architectures such as convolutional neural networks and\nTransformers have revolutionized biological sequence modeling, with recent\nadvances driven by scaling up foundation and task-specific models. The\ncomputational resources and large datasets required, however, limit their\napplicability in biological contexts. We introduce Lyra, a subquadratic\narchitecture for sequence modeling, grounded in the biological framework of\nepistasis for understanding sequence-to-function relationships. Mathematically,\nwe demonstrate that state space models efficiently capture global epistatic\ninteractions and combine them with projected gated convolutions for modeling\nlocal relationships. We demonstrate that Lyra is performant across over 100\nwide-ranging biological tasks, achieving state-of-the-art (SOTA) performance in\nmany key areas, including protein fitness landscape prediction, biophysical\nproperty prediction (e.g. disordered protein region functions) peptide\nengineering applications (e.g. antibody binding, cell-penetrating peptide\nprediction), RNA structure analysis, RNA function prediction, and CRISPR guide\ndesign. It achieves this with orders-of-magnitude improvements in inference\nspeed and reduction in parameters (up to 120,000-fold in our tests) compared to\nrecent biology foundation models. Using Lyra, we were able to train and run\nevery task in this study on two or fewer GPUs in under two hours, democratizing\naccess to biological sequence modeling at SOTA performance, with potential\napplications to many fields.",
    "categories": [
      "cs.LG",
      "q-bio.GN"
    ],
    "authors": [
      "Krithik Ramesh",
      "Sameed M. Siddiqui",
      "Albert Gu",
      "Michael D. Mitzenmacher",
      "Pardis C. Sabeti"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16348v1",
    "title": "Palatable Conceptions of Disembodied Being: Terra Incognita in the Space of Possible Minds",
    "abstract": "Is it possible to articulate a conception of consciousness that is compatible\nwith the exotic characteristics of contemporary, disembodied AI systems, and\nthat can stand up to philosophical scrutiny? How would subjective time and\nselfhood show up for an entity that conformed to such a conception? Trying to\nanswer these questions, even metaphorically, stretches the language of\nconsciousness to breaking point. Ultimately, the attempt yields something like\nemptiness, in the Buddhist sense, and helps to undermine our dualistic\ninclinations towards subjectivity and selfhood.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Murray Shanahan"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16342v1",
    "title": "HiQ-Lip: The First Quantum-Classical Hierarchical Method for Global Lipschitz Constant Estimation of ReLU Networks",
    "abstract": "Estimating the global Lipschitz constant of neural networks is crucial for\nunderstanding and improving their robustness and generalization capabilities.\nHowever, precise calculations are NP-hard, and current semidefinite programming\n(SDP) methods face challenges such as high memory usage and slow processing\nspeeds. In this paper, we propose \\textbf{HiQ-Lip}, a hybrid quantum-classical\nhierarchical method that leverages Coherent Ising Machines (CIMs) to estimate\nthe global Lipschitz constant. We tackle the estimation by converting it into a\nQuadratic Unconstrained Binary Optimization (QUBO) problem and implement a\nmultilevel graph coarsening and refinement strategy to adapt to the constraints\nof contemporary quantum hardware. Our experimental evaluations on fully\nconnected neural networks demonstrate that HiQ-Lip not only provides estimates\ncomparable to state-of-the-art methods but also significantly accelerates the\ncomputation process. In specific tests involving two-layer neural networks with\n256 hidden neurons, HiQ-Lip doubles the solving speed and offers more accurate\nupper bounds than the existing best method, LiPopt. These findings highlight\nthe promising utility of small-scale quantum devices in advancing the\nestimation of neural network robustness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "quant-ph"
    ],
    "authors": [
      "Haoqi He",
      "Yan Xiao"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16340v1",
    "title": "Nonlinear action prediction models reveal multi-timescale locomotor control",
    "abstract": "Modeling movement in real-world tasks is a fundamental scientific goal.\nHowever, it is unclear whether existing models and their assumptions,\noverwhelmingly tested in laboratory-constrained settings, generalize to the\nreal world. For example, data-driven models of foot placement control -- a\ncrucial action for stable locomotion -- assume linear and single timescale\nmappings. We develop nonlinear foot placement prediction models, finding that\nneural network architectures with flexible input history-dependence like GRU\nand Transformer perform best across multiple contexts (walking and running,\ntreadmill and overground, varying terrains) and input modalities (multiple body\nstates, gaze), outperforming traditional models. These models reveal context-\nand modality-dependent timescales: there is more reliance on fast-timescale\npredictions in complex terrain, gaze predictions precede body state\npredictions, and full-body state predictions precede center-of-mass-relevant\npredictions. Thus, nonlinear action prediction models provide quantifiable\ninsights into real-world motor control and can be extended to other actions,\ncontexts, and populations.",
    "categories": [
      "cs.LG",
      "cs.RO"
    ],
    "authors": [
      "Wei-Chen Wang",
      "Antoine De Comite",
      "Monica Daley",
      "Alexandra Voloshina",
      "Nidhi Seethapathi"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16338v1",
    "title": "Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images",
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis\nperformance. While conventional methods require per-scene optimization, more\nrecently several feed-forward methods have been proposed to generate\npixel-aligned Gaussian representations with a learnable network, which are\ngeneralizable to different scenes. However, these methods simply combine\npixel-aligned Gaussians from multiple views as scene representations, thereby\nleading to artifacts and extra memory cost without fully capturing the\nrelations of Gaussians from different images. In this paper, we propose\nGaussian Graph Network (GGN) to generate efficient and generalizable Gaussian\nrepresentations. Specifically, we construct Gaussian Graphs to model the\nrelations of Gaussian groups from different views. To support message passing\nat Gaussian level, we reformulate the basic graph operations over Gaussian\nrepresentations, enabling each Gaussian to benefit from its connected Gaussian\ngroups with Gaussian feature fusion. Furthermore, we design a Gaussian pooling\nlayer to aggregate various Gaussian groups for efficient representations. We\nconduct experiments on the large-scale RealEstate10K and ACID datasets to\ndemonstrate the efficiency and generalization of our method. Compared to the\nstate-of-the-art methods, our model uses fewer Gaussians and achieves better\nimage quality with higher rendering speed.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Shengjun Zhang",
      "Xin Fei",
      "Fangfu Liu",
      "Haixu Song",
      "Yueqi Duan"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16337v1",
    "title": "Optimal Complexity in Byzantine-Robust Distributed Stochastic Optimization with Data Heterogeneity",
    "abstract": "In this paper, we establish tight lower bounds for Byzantine-robust\ndistributed first-order stochastic optimization methods in both strongly convex\nand non-convex stochastic optimization. We reveal that when the distributed\nnodes have heterogeneous data, the convergence error comprises two components:\na non-vanishing Byzantine error and a vanishing optimization error. We\nestablish the lower bounds on the Byzantine error and on the minimum number of\nqueries to a stochastic gradient oracle required to achieve an arbitrarily\nsmall optimization error. Nevertheless, we identify significant discrepancies\nbetween our established lower bounds and the existing upper bounds. To fill\nthis gap, we leverage the techniques of Nesterov's acceleration and variance\nreduction to develop novel Byzantine-robust distributed stochastic optimization\nmethods that provably match these lower bounds, up to logarithmic factors,\nimplying that our established lower bounds are tight.",
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "authors": [
      "Qiankun Shi",
      "Jie Peng",
      "Kun Yuan",
      "Xiao Wang",
      "Qing Ling"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16335v1",
    "title": "Enhancing Software Quality Assurance with an Adaptive Differential Evolution based Quantum Variational Autoencoder-Transformer Model",
    "abstract": "An AI-powered quality engineering platform uses artificial intelligence to\nboost software quality assessments through automated defect prediction and\noptimized performance alongside improved feature extraction. Existing models\nresult in difficulties addressing noisy data types together with imbalances,\npattern recognition complexities, ineffective feature extraction, and\ngeneralization weaknesses. To overcome those existing challenges in this\nresearch, we develop a new model Adaptive Differential Evolution based Quantum\nVariational Autoencoder-Transformer Model (ADE-QVAET), that combines a Quantum\nVariational Autoencoder-Transformer (QVAET) to obtain high-dimensional latent\nfeatures and maintain sequential dependencies together with contextual\nrelationships, resulting in superior defect prediction accuracy. Adaptive\nDifferential Evolution (ADE) Optimization utilizes an adaptive parameter tuning\nmethod that enhances model convergence and predictive performance. ADE-QVAET\nintegrates advanced AI techniques to create a robust solution for scalable and\naccurate software defect prediction that represents a top-level AI-driven\ntechnology for quality engineering applications. The proposed ADE-QVAET model\nattains high accuracy, precision, recall, and f1-score during the training\npercentage (TP) 90 of 98.08%, 92.45%, 94.67%, and 98.12%.",
    "categories": [
      "cs.AI",
      "cs.ET"
    ],
    "authors": [
      "Seshu Babu Barma",
      "Mohanakrishnan Hariharan",
      "Satish Arvapalli"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16334v1",
    "title": "LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates",
    "abstract": "Recent findings reveal that much of the knowledge in a Transformer-based\nLarge Language Model (LLM) is encoded in its feed-forward (FFN) layers, where\neach FNN layer can be interpreted as the summation of sub-updates, each\ncorresponding to a weighted column vector from the FFN's value parameter matrix\nthat often encodes human-interpretable concepts. In light of this, we\nhypothesize that model performance and behaviors can be further enhanced and\ncontrolled by modulating the contributions of these sub-updates based on their\nrelevance to the input or target output style, and propose LLMBRACES, a novel\nand efficient method that computes relevance scores associated with value\nvectors in FFN layers and leverages these scores to dynamically adjust the\ncontribution of sub-updates. By optimizing sub-update contributions, LLMBRACES\nrefines the prediction process, leading to more accurate and reliable outputs,\nmuch like a 'brace' providing support and stability. Moreover, LLMBRACES can be\nextended to support conditional control over generation characteristics, such\nas sentiment, thereby offering fine-grained steering of LLM outputs. Extensive\nexperiments on various LLMs-including Qwen2.5-1.5B, Llama2-7B, and\nLlama3-8B-demonstrate that LLMBRACES outperforms baseline approaches in both\nfine-tuning and zero-shot settings while requiring significantly fewer tunable\nparameters, up to 75% fewer compared to LoRA. Furthermore, LLMBRACES excels in\nsentiment-controlled generation and toxicity reduction, highlighting its\npotential for flexible, controlled text generation across applications.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Ying Shen",
      "Lifu Huang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16328v1",
    "title": "Knowledge-guided machine learning model with soil moisture for corn yield prediction under drought conditions",
    "abstract": "Remote sensing (RS) techniques, by enabling non-contact acquisition of\nextensive ground observations, have become a valuable tool for corn yield\nprediction. Traditional process-based (PB) models are limited by fixed input\nfeatures and struggle to incorporate large volumes of RS data. In contrast,\nmachine learning (ML) models are often criticized for being ``black boxes''\nwith limited interpretability. To address these limitations, we used\nKnowledge-Guided Machine Learning (KGML), which combined the strengths of both\napproaches and fully used RS data. However, previous KGML methods overlooked\nthe crucial role of soil moisture in plant growth. To bridge this gap, we\nproposed the Knowledge-Guided Machine Learning with Soil Moisture (KGML-SM)\nframework, using soil moisture as an intermediate variable to emphasize its key\nrole in plant development. Additionally, based on the prior knowledge that the\nmodel may overestimate under drought conditions, we designed a drought-aware\nloss function that penalizes predicted yield in drought-affected areas. Our\nexperiments showed that the KGML-SM model outperformed other ML models.\nFinally, we explored the relationships between drought, soil moisture, and corn\nyield prediction, assessing the importance of various features and analyzing\nhow soil moisture impacts corn yield predictions across different regions and\ntime periods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Xiaoyu Wang",
      "Yijia Xu",
      "Jingyi Huang",
      "Zhengwei Yang",
      "Zhou Zhang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16326v1",
    "title": "OmniGeo: Towards a Multimodal Large Language Models for Geospatial Artificial Intelligence",
    "abstract": "The rapid advancement of multimodal large language models (LLMs) has opened\nnew frontiers in artificial intelligence, enabling the integration of diverse\nlarge-scale data types such as text, images, and spatial information. In this\npaper, we explore the potential of multimodal LLMs (MLLM) for geospatial\nartificial intelligence (GeoAI), a field that leverages spatial data to address\nchallenges in domains including Geospatial Semantics, Health Geography, Urban\nGeography, Urban Perception, and Remote Sensing. We propose a MLLM (OmniGeo)\ntailored to geospatial applications, capable of processing and analyzing\nheterogeneous data sources, including satellite imagery, geospatial metadata,\nand textual descriptions. By combining the strengths of natural language\nunderstanding and spatial reasoning, our model enhances the ability of\ninstruction following and the accuracy of GeoAI systems. Results demonstrate\nthat our model outperforms task-specific models and existing LLMs on diverse\ngeospatial tasks, effectively addressing the multimodality nature while\nachieving competitive results on the zero-shot geospatial tasks. Our code will\nbe released after publication.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Long Yuan",
      "Fengran Mo",
      "Kaiyu Huang",
      "Wenjie Wang",
      "Wangyuxuan Zhai",
      "Xiaoyu Zhu",
      "You Li",
      "Jinan Xu",
      "Jian-Yun Nie"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16323v1",
    "title": "NeuralFoil: An Airfoil Aerodynamics Analysis Tool Using Physics-Informed Machine Learning",
    "abstract": "NeuralFoil is an open-source Python-based tool for rapid aerodynamics\nanalysis of airfoils, similar in purpose to XFoil. Speedups ranging from 8x to\n1,000x over XFoil are demonstrated, after controlling for equivalent accuracy.\nNeuralFoil computes both global and local quantities (lift, drag, velocity\ndistribution, etc.) over a broad input space, including: an 18-dimensional\nspace of airfoil shapes, possibly including control deflections; a 360 degree\nrange of angles of attack; Reynolds numbers from $10^2$ to $10^{10}$; subsonic\nflows up to the transonic drag rise; and with varying turbulence parameters.\nResults match those of XFoil closely: the mean relative error of drag is 0.37%\non simple cases, and remains as low as 2.0% on a test dataset with numerous\npost-stall and transitional cases. NeuralFoil facilitates gradient-based design\noptimization, due to its $C^\\infty$-continuous solutions,\nautomatic-differentiation-compatibility, and bounded computational cost without\nnon-convergence issues.\n  NeuralFoil is a hybrid of physics-informed machine learning techniques and\nanalytical models. Here, physics information includes symmetries that are\nstructurally embedded into the model architecture, feature engineering using\ndomain knowledge, and guaranteed extrapolation to known limit cases. This work\nalso introduces a new approach for surrogate model uncertainty quantification\nthat enables robust design optimization.\n  This work discusses the methodology and performance of NeuralFoil with\nseveral case studies, including a practical airfoil design optimization study\nincluding both aerodynamic and non-aerodynamic constraints. Here, NeuralFoil\noptimization is able to produce airfoils nearly identical in performance and\nshape to expert-designed airfoils within seconds; these\ncomputationally-optimized airfoils provide a useful starting point for further\nexpert refinement.",
    "categories": [
      "physics.flu-dyn",
      "cs.LG"
    ],
    "authors": [
      "Peter Sharpe",
      "R. John Hansman"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16322v1",
    "title": "Ultra-Resolution Adaptation with Ease",
    "abstract": "Text-to-image diffusion models have achieved remarkable progress in recent\nyears. However, training models for high-resolution image generation remains\nchallenging, particularly when training data and computational resources are\nlimited. In this paper, we explore this practical problem from two key\nperspectives: data and parameter efficiency, and propose a set of key\nguidelines for ultra-resolution adaptation termed \\emph{URAE}. For data\nefficiency, we theoretically and empirically demonstrate that synthetic data\ngenerated by some teacher models can significantly promote training\nconvergence. For parameter efficiency, we find that tuning minor components of\nthe weight matrices outperforms widely-used low-rank adapters when synthetic\ndata are unavailable, offering substantial performance gains while maintaining\nefficiency. Additionally, for models leveraging guidance distillation, such as\nFLUX, we show that disabling classifier-free guidance, \\textit{i.e.}, setting\nthe guidance scale to 1 during adaptation, is crucial for satisfactory\nperformance. Extensive experiments validate that URAE achieves comparable\n2K-generation performance to state-of-the-art closed-source models like FLUX1.1\n[Pro] Ultra with only 3K samples and 2K iterations, while setting new\nbenchmarks for 4K-resolution generation. Codes are available\n\\href{https://github.com/Huage001/URAE}{here}.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Ruonan Yu",
      "Songhua Liu",
      "Zhenxiong Tan",
      "Xinchao Wang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16318v1",
    "title": "Dynamic Point Maps: A Versatile Representation for Dynamic 3D Reconstruction",
    "abstract": "DUSt3R has recently shown that one can reduce many tasks in multi-view\ngeometry, including estimating camera intrinsics and extrinsics, reconstructing\nthe scene in 3D, and establishing image correspondences, to the prediction of a\npair of viewpoint-invariant point maps, i.e., pixel-aligned point clouds\ndefined in a common reference frame. This formulation is elegant and powerful,\nbut unable to tackle dynamic scenes. To address this challenge, we introduce\nthe concept of Dynamic Point Maps (DPM), extending standard point maps to\nsupport 4D tasks such as motion segmentation, scene flow estimation, 3D object\ntracking, and 2D correspondence. Our key intuition is that, when time is\nintroduced, there are several possible spatial and time references that can be\nused to define the point maps. We identify a minimal subset of such\ncombinations that can be regressed by a network to solve the sub tasks\nmentioned above. We train a DPM predictor on a mixture of synthetic and real\ndata and evaluate it across diverse benchmarks for video depth prediction,\ndynamic point cloud reconstruction, 3D scene flow and object pose tracking,\nachieving state-of-the-art performance. Code, models and additional results are\navailable at https://www.robots.ox.ac.uk/~vgg/research/dynamic-point-maps/.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Edgar Sucar",
      "Zihang Lai",
      "Eldar Insafutdinov",
      "Andrea Vedaldi"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16316v1",
    "title": "On the Cone Effect in the Learning Dynamics",
    "abstract": "Understanding the learning dynamics of neural networks is a central topic in\nthe deep learning community. In this paper, we take an empirical perspective to\nstudy the learning dynamics of neural networks in real-world settings.\nSpecifically, we investigate the evolution process of the empirical Neural\nTangent Kernel (eNTK) during training. Our key findings reveal a two-phase\nlearning process: i) in Phase I, the eNTK evolves significantly, signaling the\nrich regime, and ii) in Phase II, the eNTK keeps evolving but is constrained in\na narrow space, a phenomenon we term the cone effect. This two-phase framework\nbuilds on the hypothesis proposed by Fort et al. (2020), but we uniquely\nidentify the cone effect in Phase II, demonstrating its significant performance\nadvantages over fully linearized training.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Zhanpeng Zhou",
      "Yongyi Yang",
      "Jie Ren",
      "Mahito Sugiyama",
      "Junchi Yan"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16315v1",
    "title": "Active Learning For Repairable Hardware Systems With Partial Coverage",
    "abstract": "Identifying the optimal diagnostic test and hardware system instance to infer\nreliability characteristics using field data is challenging, especially when\nconstrained by fixed budgets and minimal maintenance cycles. Active Learning\n(AL) has shown promise for parameter inference with limited data and budget\nconstraints in machine learning/deep learning tasks. However, AL for\nreliability model parameter inference remains underexplored for repairable\nhardware systems. It requires specialized AL Acquisition Functions (AFs) that\nconsider hardware aging and the fact that a hardware system consists of\nmultiple sub-systems, which may undergo only partial testing during a given\ndiagnostic test. To address these challenges, we propose a relaxed Mixed\nInteger Semidefinite Program (MISDP) AL AF that incorporates Diagnostic\nCoverage (DC), Fisher Information Matrices (FIMs), and diagnostic testing\nbudgets. Furthermore, we design empirical-based simulation experiments focusing\non two diagnostic testing scenarios: (1) partial tests of a hardware system\nwith overlapping subsystem coverage, and (2) partial tests where one diagnostic\ntest fully subsumes the subsystem coverage of another. We evaluate our proposed\napproach against the most widely used AL AF in the literature (entropy), as\nwell as several intuitive AL AFs tailored for reliability model parameter\ninference. Our proposed AF ranked best on average among the alternative AFs\nacross 6,000 experimental configurations, with respect to Area Under the Curve\n(AUC) of the Absolute Total Expected Event Error (ATEER) and Mean Squared Error\n(MSE) curves, with statistical significance calculated at a 0.05 alpha level\nusing a Friedman hypothesis test.",
    "categories": [
      "stat.AP",
      "cs.LG"
    ],
    "authors": [
      "Michael Potter",
      "Beyza Kalkanlı",
      "Deniz Erdoğmuş",
      "Michael Everett"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16311v1",
    "title": "Structured-Noise Masked Modeling for Video, Audio and Beyond",
    "abstract": "Masked modeling has emerged as a powerful self-supervised learning framework,\nbut existing methods largely rely on random masking, disregarding the\nstructural properties of different modalities. In this work, we introduce\nstructured noise-based masking, a simple yet effective approach that naturally\naligns with the spatial, temporal, and spectral characteristics of video and\naudio data. By filtering white noise into distinct color noise distributions,\nwe generate structured masks that preserve modality-specific patterns without\nrequiring handcrafted heuristics or access to the data. Our approach improves\nthe performance of masked video and audio modeling frameworks without any\ncomputational overhead. Extensive experiments demonstrate that structured noise\nmasking achieves consistent improvement over random masking for standard and\nadvanced masked modeling methods, highlighting the importance of modality-aware\nmasking strategies for representation learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SD"
    ],
    "authors": [
      "Aritra Bhowmik",
      "Fida Mohammad Thoker",
      "Carlos Hinojosa",
      "Bernard Ghanem",
      "Cees G. M. Snoek"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16309v1",
    "title": "Rapid patient-specific neural networks for intraoperative X-ray to volume registration",
    "abstract": "The integration of artificial intelligence in image-guided interventions\nholds transformative potential, promising to extract 3D geometric and\nquantitative information from conventional 2D imaging modalities during complex\nprocedures. Achieving this requires the rapid and precise alignment of 2D\nintraoperative images (e.g., X-ray) with 3D preoperative volumes (e.g., CT,\nMRI). However, current 2D/3D registration methods fail across the broad\nspectrum of procedures dependent on X-ray guidance: traditional optimization\ntechniques require custom parameter tuning for each subject, whereas neural\nnetworks trained on small datasets do not generalize to new patients or require\nlabor-intensive manual annotations, increasing clinical burden and precluding\napplication to new anatomical targets. To address these challenges, we present\nxvr, a fully automated framework for training patient-specific neural networks\nfor 2D/3D registration. xvr uses physics-based simulation to generate abundant\nhigh-quality training data from a patient's own preoperative volumetric\nimaging, thereby overcoming the inherently limited ability of supervised models\nto generalize to new patients and procedures. Furthermore, xvr requires only 5\nminutes of training per patient, making it suitable for emergency interventions\nas well as planned procedures. We perform the largest evaluation of a 2D/3D\nregistration algorithm on real X-ray data to date and find that xvr robustly\ngeneralizes across a diverse dataset comprising multiple anatomical structures,\nimaging modalities, and hospitals. Across surgical tasks, xvr achieves\nsubmillimeter-accurate registration at intraoperative speeds, improving upon\nexisting methods by an order of magnitude. xvr is released as open-source\nsoftware freely available at https://github.com/eigenvivek/xvr.",
    "categories": [
      "eess.IV",
      "cs.CV",
      "physics.med-ph"
    ],
    "authors": [
      "Vivek Gopalakrishnan",
      "Neel Dey",
      "David-Dimitris Chlorogiannis",
      "Andrew Abumoussa",
      "Anna M. Larson",
      "Darren B. Orbach",
      "Sarah Frisken",
      "Polina Golland"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16307v1",
    "title": "Speeding up design and making to reduce time-to-project and time-to-market: an AI-Enhanced approach in engineering education",
    "abstract": "This paper explores the integration of AI tools, such as ChatGPT and GitHub\nCopilot, in the Software Architecture for Embedded Systems course. AI-supported\nworkflows enabled students to rapidly prototype complex projects, emphasizing\nreal-world applications like SLAM robotics. Results demon-started enhanced\nproblem-solving, faster development, and more sophisticated outcomes, with AI\naugmenting but not replacing human decision-making.",
    "categories": [
      "cs.AI",
      "I.2; K.3"
    ],
    "authors": [
      "Giovanni Adorni",
      "Daniele Grosso"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16304v2",
    "title": "Bridging Technology and Humanities: Evaluating the Impact of Large Language Models on Social Sciences Research with DeepSeek-R1",
    "abstract": "In recent years, the development of Large Language Models (LLMs) has made\nsignificant breakthroughs in the field of natural language processing and has\ngradually been applied to the field of humanities and social sciences research.\nLLMs have a wide range of application value in the field of humanities and\nsocial sciences because of its strong text understanding, generation and\nreasoning capabilities. In humanities and social sciences research, LLMs can\nanalyze large-scale text data and make inferences.\n  This article analyzes the large language model DeepSeek-R1 from seven\naspects: low-resource language translation, educational question-answering,\nstudent writing improvement in higher education, logical reasoning, educational\nmeasurement and psychometrics, public health policy analysis, and art\neducation.Then we compare the answers given by DeepSeek-R1 in the seven aspects\nwith the answers given by o1-preview. DeepSeek-R1 performs well in the\nhumanities and social sciences, answering most questions correctly and\nlogically, and can give reasonable analysis processes and explanations.\nCompared with o1-preview, it can automatically generate reasoning processes and\nprovide more detailed explanations, which is suitable for beginners or people\nwho need to have a detailed understanding of this knowledge, while o1-preview\nis more suitable for quick reading.\n  Through analysis, it is found that LLM has broad application potential in the\nfield of humanities and social sciences, and shows great advantages in\nimproving text analysis efficiency, language communication and other fields.\nLLM's powerful language understanding and generation capabilities enable it to\ndeeply explore complex problems in the field of humanities and social sciences,\nand provide innovative tools for academic research and practical applications.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "authors": [
      "Peiran Gu",
      "Fuhao Duan",
      "Wenhao Li",
      "Bochen Xu",
      "Ying Cai",
      "Teng Yao",
      "Chenxun Zhuo",
      "Tianming Liu",
      "Bao Ge"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16302v1",
    "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
    "abstract": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "authors": [
      "Zeqiang Lai",
      "Yunfei Zhao",
      "Zibo Zhao",
      "Haolin Liu",
      "Fuyun Wang",
      "Huiwen Shi",
      "Xianghui Yang",
      "Qinxiang Lin",
      "Jinwei Huang",
      "Yuhong Liu",
      "Jie Jiang",
      "Chunchao Guo",
      "Xiangyu Yue"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16290v1",
    "title": "Diffusion-augmented Graph Contrastive Learning for Collaborative Filter",
    "abstract": "Graph-based collaborative filtering has been established as a prominent\napproach in recommendation systems, leveraging the inherent graph topology of\nuser-item interactions to model high-order connectivity patterns and enhance\nrecommendation performance. Recent advances in Graph Contrastive Learning (GCL)\nhave demonstrated promising potential to alleviate data sparsity issues by\nimproving representation learning through contrastive view generation and\nmutual information maximization. However, existing approaches lack effective\ndata augmentation strategies. Structural augmentation risks distorting\nfundamental graph topology, while feature-level perturbation techniques\npredominantly employ uniform noise scales that fail to account for\nnode-specific characteristics. To solve these challenges, we propose\nDiffusion-augmented Contrastive Learning (DGCL), an innovative framework that\nintegrates diffusion models with contrastive learning for enhanced\ncollaborative filtering. Our approach employs a diffusion process that learns\nnode-specific Gaussian distributions of representations, thereby generating\nsemantically consistent yet diversified contrastive views through reverse\ndiffusion sampling. DGCL facilitates adaptive data augmentation based on\nreconstructed representations, considering both semantic coherence and\nnode-specific features. In addition, it explores unrepresented regions of the\nlatent sparse feature space, thereby enriching the diversity of contrastive\nviews. Extensive experimental results demonstrate the effectiveness of DGCL on\nthree public datasets.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Fan Huang",
      "Wei Wang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16289v1",
    "title": "SceneMI: Motion In-betweening for Modeling Human-Scene Interactions",
    "abstract": "Modeling human-scene interactions (HSI) is essential for understanding and\nsimulating everyday human behaviors. Recent approaches utilizing generative\nmodeling have made progress in this domain; however, they are limited in\ncontrollability and flexibility for real-world applications. To address these\nchallenges, we propose reformulating the HSI modeling problem as Scene-aware\nMotion In-betweening -- a more tractable and practical task. We introduce\nSceneMI, a framework that supports several practical applications, including\nkeyframe-guided character animation in 3D scenes and enhancing the motion\nquality of imperfect HSI data. SceneMI employs dual scene descriptors to\ncomprehensively encode global and local scene context. Furthermore, our\nframework leverages the inherent denoising nature of diffusion models to\ngeneralize on noisy keyframes. Experimental results demonstrate SceneMI's\neffectiveness in scene-aware keyframe in-betweening and generalization to the\nreal-world GIMO dataset, where motions and scenes are acquired by noisy IMU\nsensors and smartphones. We further showcase SceneMI's applicability in HSI\nreconstruction from monocular videos.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Inwoo Hwang",
      "Bing Zhou",
      "Young Min Kim",
      "Jian Wang",
      "Chuan Guo"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16286v1",
    "title": "Explainable Graph-theoretical Machine Learning: with Application to Alzheimer's Disease Prediction",
    "abstract": "Alzheimer's disease (AD) affects 50 million people worldwide and is projected\nto overwhelm 152 million by 2050. AD is characterized by cognitive decline due\npartly to disruptions in metabolic brain connectivity. Thus, early and accurate\ndetection of metabolic brain network impairments is crucial for AD management.\nChief to identifying such impairments is FDG-PET data. Despite advancements,\nmost graph-based studies using FDG-PET data rely on group-level analysis or\nthresholding. Yet, group-level analysis can veil individual differences and\nthresholding may overlook weaker but biologically critical brain connections.\nAdditionally, machine learning-based AD prediction largely focuses on\nunivariate outcomes, such as disease status. Here, we introduce explainable\ngraph-theoretical machine learning (XGML), a framework employing kernel density\nestimation and dynamic time warping to construct individual metabolic brain\ngraphs that capture the distance between pair-wise brain regions and identify\nsubgraphs most predictive of multivariate AD-related outcomes. Using FDG-PET\ndata from the Alzheimer's Disease Neuroimaging Initiative, XGML builds\nmetabolic brain graphs and uncovers subgraphs predictive of eight AD-related\ncognitive scores in new subjects. XGML shows robust performance, particularly\nfor predicting scores measuring learning, memory, language, praxis, and\norientation, such as CDRSB ($r = 0.74$), ADAS11 ($r = 0.73$), and ADAS13 ($r =\n0.71$). Moreover, XGML unveils key edges jointly but differentially predictive\nof several AD-related outcomes; they may serve as potential network biomarkers\nfor assessing overall cognitive decline. Together, we show the promise of\ngraph-theoretical machine learning in biomarker discovery and disease\nprediction and its potential to improve our understanding of network neural\nmechanisms underlying AD.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Narmina Baghirova",
      "Duy-Thanh Vũ",
      "Duy-Cat Can",
      "Christelle Schneuwly Diaz",
      "Julien Bodlet",
      "Guillaume Blanc",
      "Georgi Hrusanov",
      "Bernard Ries",
      "Oliver Y. Chén"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16284v1",
    "title": "PSA-MIL: A Probabilistic Spatial Attention-Based Multiple Instance Learning for Whole Slide Image Classification",
    "abstract": "Whole Slide Images (WSIs) are high-resolution digital scans widely used in\nmedical diagnostics. WSI classification is typically approached using Multiple\nInstance Learning (MIL), where the slide is partitioned into tiles treated as\ninterconnected instances. While attention-based MIL methods aim to identify the\nmost informative tiles, they often fail to fully exploit the spatial\nrelationships among them, potentially overlooking intricate tissue structures\ncrucial for accurate diagnosis. To address this limitation, we propose\nProbabilistic Spatial Attention MIL (PSA-MIL), a novel attention-based MIL\nframework that integrates spatial context into the attention mechanism through\nlearnable distance-decayed priors, formulated within a probabilistic\ninterpretation of self-attention as a posterior distribution. This formulation\nenables a dynamic inference of spatial relationships during training,\neliminating the need for predefined assumptions often imposed by previous\napproaches. Additionally, we suggest a spatial pruning strategy for the\nposterior, effectively reducing self-attention's quadratic complexity. To\nfurther enhance spatial modeling, we introduce a diversity loss that encourages\nvariation among attention heads, ensuring each captures distinct spatial\nrepresentations. Together, PSA-MIL enables a more data-driven and adaptive\nintegration of spatial context, moving beyond predefined constraints. We\nachieve state-of-the-art performance across both contextual and non-contextual\nbaselines, while significantly reducing computational costs.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Sharon Peled",
      "Yosef E. Maruvka",
      "Moti Freiman"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16282v1",
    "title": "Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language Model",
    "abstract": "Generalized few-shot 3D point cloud segmentation (GFS-PCS) adapts models to\nnew classes with few support samples while retaining base class segmentation.\nExisting GFS-PCS methods enhance prototypes via interacting with support or\nquery features but remain limited by sparse knowledge from few-shot samples.\nMeanwhile, 3D vision-language models (3D VLMs), generalizing across open-world\nnovel classes, contain rich but noisy novel class knowledge. In this work, we\nintroduce a GFS-PCS framework that synergizes dense but noisy pseudo-labels\nfrom 3D VLMs with precise yet sparse few-shot samples to maximize the strengths\nof both, named GFS-VL. Specifically, we present a prototype-guided pseudo-label\nselection to filter low-quality regions, followed by an adaptive infilling\nstrategy that combines knowledge from pseudo-label contexts and few-shot\nsamples to adaptively label the filtered, unlabeled areas. Additionally, we\ndesign a novel-base mix strategy to embed few-shot samples into training\nscenes, preserving essential context for improved novel class learning.\nMoreover, recognizing the limited diversity in current GFS-PCS benchmarks, we\nintroduce two challenging benchmarks with diverse novel classes for\ncomprehensive generalization evaluation. Experiments validate the effectiveness\nof our framework across models and datasets. Our approach and benchmarks\nprovide a solid foundation for advancing GFS-PCS in the real world. The code is\nat https://github.com/ZhaochongAn/GFS-VL",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Zhaochong An",
      "Guolei Sun",
      "Yun Liu",
      "Runjia Li",
      "Junlin Han",
      "Ender Konukoglu",
      "Serge Belongie"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16278v2",
    "title": "Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on Compressed Spatial Tokens",
    "abstract": "Recent advancements in large language models and their multi-modal extensions\nhave demonstrated the effectiveness of unifying generation and understanding\nthrough autoregressive next-token prediction. However, despite the critical\nrole of 3D structural generation and understanding (3D GU) in AI for science,\nthese tasks have largely evolved independently, with autoregressive methods\nremaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified\nframework that seamlessly integrates 3D GU tasks via autoregressive prediction.\nAt its core, Uni-3DAR employs a novel hierarchical tokenization that compresses\n3D space using an octree, leveraging the inherent sparsity of 3D structures. It\nthen applies an additional tokenization for fine-grained structural details,\ncapturing key attributes such as atom types and precise spatial coordinates in\nmicroscopic 3D structures. We further propose two optimizations to enhance\nefficiency and effectiveness. The first is a two-level subtree compression\nstrategy, which reduces the octree token sequence by up to 8x. The second is a\nmasked next-token prediction mechanism tailored for dynamically varying token\npositions, significantly boosting model performance. By combining these\nstrategies, Uni-3DAR successfully unifies diverse 3D GU tasks within a single\nautoregressive framework. Extensive experiments across multiple microscopic 3D\nGU tasks, including molecules, proteins, polymers, and crystals, validate its\neffectiveness and versatility. Notably, Uni-3DAR surpasses previous\nstate-of-the-art diffusion models by a substantial margin, achieving up to\n256\\% relative improvement while delivering inference speeds up to 21.8x\nfaster. The code is publicly available at\nhttps://github.com/dptech-corp/Uni-3DAR.",
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "q-bio.BM"
    ],
    "authors": [
      "Shuqi Lu",
      "Haowei Lin",
      "Lin Yao",
      "Zhifeng Gao",
      "Xiaohong Ji",
      "Weinan E",
      "Linfeng Zhang",
      "Guolin Ke"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16271v1",
    "title": "Rethinking Robustness in Machine Learning: A Posterior Agreement Approach",
    "abstract": "The robustness of algorithms against covariate shifts is a fundamental\nproblem with critical implications for the deployment of machine learning\nalgorithms in the real world. Current evaluation methods predominantly match\nthe robustness definition to that of standard generalization, relying on\nstandard metrics like accuracy-based scores, which, while designed for\nperformance assessment, lack a theoretical foundation encompassing their\napplication in estimating robustness to distribution shifts. In this work, we\nset the desiderata for a robustness metric, and we propose a novel principled\nframework for the robustness assessment problem that directly follows the\nPosterior Agreement (PA) theory of model validation. Specifically, we extend\nthe PA framework to the covariate shift setting by proposing a PA metric for\nrobustness evaluation in supervised classification tasks. We assess the\nsoundness of our metric in controlled environments and through an empirical\nrobustness analysis in two different covariate shift scenarios: adversarial\nlearning and domain generalization. We illustrate the suitability of PA by\nevaluating several models under different nature and magnitudes of shift, and\nproportion of affected observations. The results show that the PA metric\nprovides a sensible and consistent analysis of the vulnerabilities in learning\nalgorithms, even in the presence of few perturbed observations.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "João Borges S. Carvalho",
      "Alessandro Torcinovich",
      "Victor Jimenez Rodriguez",
      "Antonio E. Cinà",
      "Carlos Cotrini",
      "Lea Schönherr",
      "Joachim M. Buhmann"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16264v1",
    "title": "Do image and video quality metrics model low-level human vision?",
    "abstract": "Image and video quality metrics, such as SSIM, LPIPS, and VMAF, are aimed to\npredict the perceived quality of the evaluated content and are often claimed to\nbe \"perceptual\". Yet, few metrics directly model human visual perception, and\nmost rely on hand-crafted formulas or training datasets to achieve alignment\nwith perceptual data. In this paper, we propose a set of tests for\nfull-reference quality metrics that examine their ability to model several\naspects of low-level human vision: contrast sensitivity, contrast masking, and\ncontrast matching. The tests are meant to provide additional scrutiny for newly\nproposed metrics. We use our tests to analyze 33 existing image and video\nquality metrics and find their strengths and weaknesses, such as the ability of\nLPIPS and MS-SSIM to predict contrast masking and poor performance of VMAF in\nthis task. We further find that the popular SSIM metric overemphasizes\ndifferences in high spatial frequencies, but its multi-scale counterpart,\nMS-SSIM, addresses this shortcoming. Such findings cannot be easily made using\nexisting evaluation protocols.",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.MM"
    ],
    "authors": [
      "Dounia Hammou",
      "Yancheng Cai",
      "Pavan Madhusudanarao",
      "Christos G. Bampis",
      "Rafał K. Mantiuk"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16263v1",
    "title": "From Monocular Vision to Autonomous Action: Guiding Tumor Resection via 3D Reconstruction",
    "abstract": "Surgical automation requires precise guidance and understanding of the scene.\nCurrent methods in the literature rely on bulky depth cameras to create maps of\nthe anatomy, however this does not translate well to space-limited clinical\napplications. Monocular cameras are small and allow minimally invasive\nsurgeries in tight spaces but additional processing is required to generate 3D\nscene understanding. We propose a 3D mapping pipeline that uses only RGB images\nto create segmented point clouds of the target anatomy. To ensure the most\nprecise reconstruction, we compare different structure from motion algorithms'\nperformance on mapping the central airway obstructions, and test the pipeline\non a downstream task of tumor resection. In several metrics, including\npost-procedure tissue model evaluation, our pipeline performs comparably to\nRGB-D cameras and, in some cases, even surpasses their performance. These\npromising results demonstrate that automation guidance can be achieved in\nminimally invasive procedures with monocular cameras. This study is a step\ntoward the complete autonomy of surgical robots.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Ayberk Acar",
      "Mariana Smith",
      "Lidia Al-Zogbi",
      "Tanner Watts",
      "Fangjie Li",
      "Hao Li",
      "Nural Yilmaz",
      "Paul Maria Scheikl",
      "Jesse F. d'Almeida",
      "Susheela Sharma",
      "Lauren Branscombe",
      "Tayfun Efe Ertop",
      "Robert J. Webster III",
      "Ipek Oguz",
      "Alan Kuntz",
      "Axel Krieger",
      "Jie Ying Wu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16260v1",
    "title": "Chain of Functions: A Programmatic Pipeline for Fine-Grained Chart Reasoning Data",
    "abstract": "Visual reasoning is crucial for multimodal large language models (MLLMs) to\naddress complex chart queries, yet high-quality rationale data remains scarce.\nExisting methods leveraged (M)LLMs for data generation, but direct prompting\noften yields limited precision and diversity. In this paper, we propose\n\\textit{Chain of Functions (CoF)}, a novel programmatic reasoning data\ngeneration pipeline that utilizes freely-explored reasoning paths as\nsupervision to ensure data precision and diversity. Specifically, it starts\nwith human-free exploration among the atomic functions (e.g., maximum data and\narithmetic operations) to generate diverse function chains, which are then\ntranslated into linguistic rationales and questions with only a moderate\nopen-sourced LLM. \\textit{CoF} provides multiple benefits: 1) Precision:\nfunction-governed generation reduces hallucinations compared to freeform\ngeneration; 2) Diversity: enumerating function chains enables varied question\ntaxonomies; 3) Explainability: function chains serve as built-in rationales,\nallowing fine-grained evaluation beyond overall accuracy; 4) Practicality:\neliminating reliance on extremely large models. Employing \\textit{CoF}, we\nconstruct the \\textit{ChartCoF} dataset, with 1.4k complex reasoning Q\\&A for\nfine-grained analysis and 50k Q\\&A for reasoning enhancement. The fine-grained\nevaluation on \\textit{ChartCoF} reveals varying performance across question\ntaxonomies for each MLLM, and the experiments also show that finetuning with\n\\textit{ChartCoF} achieves state-of-the-art performance among same-scale MLLMs\non widely used benchmarks. Furthermore, the novel paradigm of function-governed\nrationale generation in \\textit{CoF} could inspire broader applications beyond\ncharts.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Zijian Li",
      "Jingjing Fu",
      "Lei Song",
      "Jiang Bian",
      "Jun Zhang",
      "Rui Wang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16257v1",
    "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language Models",
    "abstract": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Keda Tao",
      "Haoxuan You",
      "Yang Sui",
      "Can Qin",
      "Huan Wang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16254v1",
    "title": "M2N2V2: Multi-Modal Unsupervised and Training-free Interactive Segmentation",
    "abstract": "We present Markov Map Nearest Neighbor V2 (M2N2V2), a novel and simple, yet\neffective approach which leverages depth guidance and attention maps for\nunsupervised and training-free point-prompt-based interactive segmentation.\nFollowing recent trends in supervised multimodal approaches, we carefully\nintegrate depth as an additional modality to create novel depth-guided\nMarkov-maps. Furthermore, we observe occasional segment size fluctuations in\nM2N2 during the interactive process, which can decrease the overall mIoU's. To\nmitigate this problem, we model the prompting as a sequential process and\npropose a novel adaptive score function which considers the previous\nsegmentation and the current prompt point in order to prevent unreasonable\nsegment size changes. Using Stable Diffusion 2 and Depth Anything V2 as\nbackbones, we empirically show that our proposed M2N2V2 significantly improves\nthe Number of Clicks (NoC) and mIoU compared to M2N2 in all datasets except\nthose from the medical domain. Interestingly, our unsupervised approach\nachieves competitive results compared to supervised methods like SAM and\nSimpleClick in the more challenging DAVIS and HQSeg44K datasets in the NoC\nmetric, reducing the gap between supervised and unsupervised methods.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Markus Karmann",
      "Peng-Tao Jiang",
      "Bo Li",
      "Onay Urfalioglu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16252v2",
    "title": "Fin-R1: A Large Language Model for Financial Reasoning through Reinforcement Learning",
    "abstract": "Reasoning large language models are rapidly evolving across various domains.\nHowever, their capabilities in handling complex financial tasks still require\nin-depth exploration. In this paper, we introduce Fin-R1, a reasoning large\nlanguage model specifically designed for the financial sector. Fin-R1 is built\nusing a two-stage architecture, leveraging a financial reasoning dataset\ndistilled and processed based on DeepSeek-R1. Through supervised fine-tuning\n(SFT) and reinforcement learning (RL) training, it demonstrates performance\nclose to DeepSeek-R1 with a parameter size of 7 billion across a range of\nfinancial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA\nand ConvFinQA tasks between those LLMs in our evaluation, surpassing larger\nmodels in other tasks as well. Fin-R1 showcases strong reasoning and\ndecision-making capabilities, providing solutions to various problems\nencountered in the financial domain. Our code is available at\nhttps://github.com/SUFE-AIFLM-Lab/Fin-R1.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Zhaowei Liu",
      "Xin Guo",
      "Fangqi Lou",
      "Lingfeng Zeng",
      "Jinyi Niu",
      "Zixuan Wang",
      "Jiajie Xu",
      "Weige Cai",
      "Ziwei Yang",
      "Xueqian Zhao",
      "Chao Li",
      "Sheng Xu",
      "Dezhi Chen",
      "Yun Chen",
      "Zuo Bai",
      "Liwen Zhang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16251v1",
    "title": "RESFL: An Uncertainty-Aware Framework for Responsible Federated Learning by Balancing Privacy, Fairness and Utility in Autonomous Vehicles",
    "abstract": "Autonomous vehicles (AVs) increasingly rely on Federated Learning (FL) to\nenhance perception models while preserving privacy. However, existing FL\nframeworks struggle to balance privacy, fairness, and robustness, leading to\nperformance disparities across demographic groups. Privacy-preserving\ntechniques like differential privacy mitigate data leakage risks but worsen\nfairness by restricting access to sensitive attributes needed for bias\ncorrection. This work explores the trade-off between privacy and fairness in\nFL-based object detection for AVs and introduces RESFL, an integrated solution\noptimizing both. RESFL incorporates adversarial privacy disentanglement and\nuncertainty-guided fairness-aware aggregation. The adversarial component uses a\ngradient reversal layer to remove sensitive attributes, reducing privacy risks\nwhile maintaining fairness. The uncertainty-aware aggregation employs an\nevidential neural network to weight client updates adaptively, prioritizing\ncontributions with lower fairness disparities and higher confidence. This\nensures robust and equitable FL model updates. We evaluate RESFL on the FACET\ndataset and CARLA simulator, assessing accuracy, fairness, privacy resilience,\nand robustness under varying conditions. RESFL improves detection accuracy,\nreduces fairness disparities, and lowers privacy attack success rates while\ndemonstrating superior robustness to adversarial conditions compared to other\napproaches.",
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.DC",
      "cs.ET"
    ],
    "authors": [
      "Dawood Wasif",
      "Terrence J. Moore",
      "Jin-Hee Cho"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16248v1",
    "title": "AI Agents in Cryptoland: Practical Attacks and No Silver Bullet",
    "abstract": "The integration of AI agents with Web3 ecosystems harnesses their\ncomplementary potential for autonomy and openness, yet also introduces\nunderexplored security risks, as these agents dynamically interact with\nfinancial protocols and immutable smart contracts. This paper investigates the\nvulnerabilities of AI agents within blockchain-based financial ecosystems when\nexposed to adversarial threats in real-world scenarios. We introduce the\nconcept of context manipulation -- a comprehensive attack vector that exploits\nunprotected context surfaces, including input channels, memory modules, and\nexternal data feeds. Through empirical analysis of ElizaOS, a decentralized AI\nagent framework for automated Web3 operations, we demonstrate how adversaries\ncan manipulate context by injecting malicious instructions into prompts or\nhistorical interaction records, leading to unintended asset transfers and\nprotocol violations which could be financially devastating. Our findings\nindicate that prompt-based defenses are insufficient, as malicious inputs can\ncorrupt an agent's stored context, creating cascading vulnerabilities across\ninteractions and platforms. This research highlights the urgent need to develop\nAI agents that are both secure and fiduciarily responsible.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "I.2.7"
    ],
    "authors": [
      "Atharv Singh Patlan",
      "Peiyao Sheng",
      "S. Ashwin Hebbar",
      "Prateek Mittal",
      "Pramod Viswanath"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16247v1",
    "title": "OpenMIBOOD: Open Medical Imaging Benchmarks for Out-Of-Distribution Detection",
    "abstract": "The growing reliance on Artificial Intelligence (AI) in critical domains such\nas healthcare demands robust mechanisms to ensure the trustworthiness of these\nsystems, especially when faced with unexpected or anomalous inputs. This paper\nintroduces the Open Medical Imaging Benchmarks for Out-Of-Distribution\nDetection (OpenMIBOOD), a comprehensive framework for evaluating\nout-of-distribution (OOD) detection methods specifically in medical imaging\ncontexts. OpenMIBOOD includes three benchmarks from diverse medical domains,\nencompassing 14 datasets divided into covariate-shifted in-distribution,\nnear-OOD, and far-OOD categories. We evaluate 24 post-hoc methods across these\nbenchmarks, providing a standardized reference to advance the development and\nfair comparison of OOD detection methods. Results reveal that findings from\nbroad-scale OOD benchmarks in natural image domains do not translate to medical\napplications, underscoring the critical need for such benchmarks in the medical\nfield. By mitigating the risk of exposing AI models to inputs outside their\ntraining distribution, OpenMIBOOD aims to support the advancement of reliable\nand trustworthy AI systems in healthcare. The repository is available at\nhttps://github.com/remic-othr/OpenMIBOOD.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Max Gutbrod",
      "David Rauber",
      "Danilo Weber Nunes",
      "Christoph Palm"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16240v1",
    "title": "Machine learning identifies nullclines in oscillatory dynamical systems",
    "abstract": "We introduce CLINE (Computational Learning and Identification of Nullclines),\na neural network-based method that uncovers the hidden structure of nullclines\nfrom oscillatory time series data. Unlike traditional approaches aiming at\ndirect prediction of system dynamics, CLINE identifies static geometric\nfeatures of the phase space that encode the (non)linear relationships between\nstate variables. It overcomes challenges such as multiple time scales and\nstrong nonlinearities while producing interpretable results convertible into\nsymbolic differential equations. We validate CLINE on various oscillatory\nsystems, showcasing its effectiveness.",
    "categories": [
      "cs.LG",
      "math.DS",
      "nlin.AO",
      "physics.comp-ph"
    ],
    "authors": [
      "Bartosz Prokop",
      "Jimmy Billen",
      "Nikita Frolov",
      "Lendert Gelens"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16233v1",
    "title": "Empirical Analysis of Privacy-Fairness-Accuracy Trade-offs in Federated Learning: A Step Towards Responsible AI",
    "abstract": "Federated Learning (FL) enables collaborative machine learning while\npreserving data privacy but struggles to balance privacy preservation (PP) and\nfairness. Techniques like Differential Privacy (DP), Homomorphic Encryption\n(HE), and Secure Multi-Party Computation (SMC) protect sensitive data but\nintroduce trade-offs. DP enhances privacy but can disproportionately impact\nunderrepresented groups, while HE and SMC mitigate fairness concerns at the\ncost of computational overhead. This work explores the privacy-fairness\ntrade-offs in FL under IID (Independent and Identically Distributed) and\nnon-IID data distributions, benchmarking q-FedAvg, q-MAML, and Ditto on diverse\ndatasets. Our findings highlight context-dependent trade-offs and offer\nguidelines for designing FL systems that uphold responsible AI principles,\nensuring fairness, privacy, and equitable real-world applications.",
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.DC",
      "cs.ET"
    ],
    "authors": [
      "Dawood Wasif",
      "Dian Chen",
      "Sindhuja Madabushi",
      "Nithin Alluru",
      "Terrence J. Moore",
      "Jin-Hee Cho"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16227v1",
    "title": "Flight Testing an Optionally Piloted Aircraft: a Case Study on Trust Dynamics in Human-Autonomy Teaming",
    "abstract": "This paper examines how trust is formed, maintained, or diminished over time\nin the context of human-autonomy teaming with an optionally piloted aircraft.\nWhereas traditional factor-based trust models offer a static representation of\nhuman confidence in technology, here we discuss how variations in the\nunderlying factors lead to variations in trust, trust thresholds, and human\nbehaviours. Over 200 hours of flight test data collected over a multi-year test\ncampaign from 2021 to 2023 were reviewed. The\ndispositional-situational-learned, process-performance-purpose, and IMPACTS\nhomeostasis trust models are applied to illuminate trust trends during nominal\nautonomous flight operations. The results offer promising directions for future\nstudies on trust dynamics and design-for-trust in human-autonomy teaming.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.ET",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Jeremy C. -H. Wang",
      "Ming Hou",
      "David Dunwoody",
      "Marko Ilievski",
      "Justin Tomasi",
      "Edward Chao",
      "Carl Pigeon"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16585v1",
    "title": "Distributed LLMs and Multimodal Large Language Models: A Survey on Advances, Challenges, and Future Directions",
    "abstract": "Language models (LMs) are machine learning models designed to predict\nlinguistic patterns by estimating the probability of word sequences based on\nlarge-scale datasets, such as text. LMs have a wide range of applications in\nnatural language processing (NLP) tasks, including autocomplete and machine\ntranslation. Although larger datasets typically enhance LM performance,\nscalability remains a challenge due to constraints in computational power and\nresources. Distributed computing strategies offer essential solutions for\nimproving scalability and managing the growing computational demand. Further,\nthe use of sensitive datasets in training and deployment raises significant\nprivacy concerns. Recent research has focused on developing decentralized\ntechniques to enable distributed training and inference while utilizing diverse\ncomputational resources and enabling edge AI. This paper presents a survey on\ndistributed solutions for various LMs, including large language models (LLMs),\nvision language models (VLMs), multimodal LLMs (MLLMs), and small language\nmodels (SLMs). While LLMs focus on processing and generating text, MLLMs are\ndesigned to handle multiple modalities of data (e.g., text, images, and audio)\nand to integrate them for broader applications. To this end, this paper reviews\nkey advancements across the MLLM pipeline, including distributed training,\ninference, fine-tuning, and deployment, while also identifying the\ncontributions, limitations, and future areas of improvement. Further, it\ncategorizes the literature based on six primary focus areas of\ndecentralization. Our analysis describes gaps in current methodologies for\nenabling distributed solutions for LMs and outline future research directions,\nemphasizing the need for novel solutions to enhance the robustness and\napplicability of distributed LMs.",
    "categories": [
      "cs.CL",
      "cs.CV",
      "cs.DC",
      "cs.LG"
    ],
    "authors": [
      "Hadi Amini",
      "Md Jueal Mia",
      "Yasaman Saadati",
      "Ahmed Imteaj",
      "Seyedsina Nabavirazavi",
      "Urmish Thakker",
      "Md Zarif Hossain",
      "Awal Ahmed Fime",
      "S. S. Iyengar"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16222v1",
    "title": "Efficient Bayesian Computation Using Plug-and-Play Priors for Poisson Inverse Problems",
    "abstract": "This paper introduces a novel plug-and-play (PnP) Langevin sampling\nmethodology for Bayesian inference in low-photon Poisson imaging problems, a\nchallenging class of problems with significant applications in astronomy,\nmedicine, and biology. PnP Langevin sampling algorithms offer a powerful\nframework for Bayesian image restoration, enabling accurate point estimation as\nwell as advanced inference tasks, including uncertainty quantification and\nvisualization analyses, and empirical Bayesian inference for automatic model\nparameter tuning. However, existing PnP Langevin algorithms are not well-suited\nfor low-photon Poisson imaging due to high solution uncertainty and poor\nregularity properties, such as exploding gradients and non-negativity\nconstraints. To address these challenges, we propose two strategies for\nextending Langevin PnP sampling to Poisson imaging models: (i) an accelerated\nPnP Langevin method that incorporates boundary reflections and a Poisson\nlikelihood approximation and (ii) a mirror sampling algorithm that leverages a\nRiemannian geometry to handle the constraints and the poor regularity of the\nlikelihood without approximations. The effectiveness of these approaches is\ndemonstrated through extensive numerical experiments and comparisons with\nstate-of-the-art methods.",
    "categories": [
      "stat.CO",
      "cs.CV",
      "cs.NA",
      "math.NA",
      "stat.ML",
      "53B21, 60H35, 62F15, 65C40, 65C60, 65J22, 68U10"
    ],
    "authors": [
      "Teresa Klatzer",
      "Savvas Melidonis",
      "Marcelo Pereyra",
      "Konstantinos C. Zygalakis"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16219v1",
    "title": "Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn't",
    "abstract": "Enhancing the reasoning capabilities of large language models (LLMs)\ntypically relies on massive computational resources and extensive datasets,\nlimiting accessibility for resource-constrained settings. Our study\ninvestigates the potential of reinforcement learning (RL) to improve reasoning\nin small LLMs, focusing on a 1.5-billion-parameter model,\nDeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA\nA40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy\nOptimization (GRPO) algorithm and curating a compact, high-quality mathematical\nreasoning dataset, we conducted three experiments to explore model behavior and\nperformance. Our results demonstrate rapid reasoning gains - e.g., AMC23\naccuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing\no1-preview - using only 7,000 samples and a $42 training cost, compared to\nthousands of dollars for baseline models. However, challenges such as\noptimization instability and length constraints emerged with prolonged\ntraining. These findings highlight the efficacy of RL-based fine-tuning for\nsmall LLMs, offering a cost-effective alternative to large-scale approaches. We\nrelease our code and datasets as open-source resources, providing insights into\ntrade-offs and laying a foundation for scalable, reasoning-capable LLMs in\nresource-limited environments. All are available at\nhttps://github.com/knoveleng/open-rs.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Quy-Anh Dang",
      "Chris Ngo"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16218v1",
    "title": "Temporal Score Analysis for Understanding and Correcting Diffusion Artifacts",
    "abstract": "Visual artifacts remain a persistent challenge in diffusion models, even with\ntraining on massive datasets. Current solutions primarily rely on supervised\ndetectors, yet lack understanding of why these artifacts occur in the first\nplace. In our analysis, we identify three distinct phases in the diffusion\ngenerative process: Profiling, Mutation, and Refinement. Artifacts typically\nemerge during the Mutation phase, where certain regions exhibit anomalous score\ndynamics over time, causing abrupt disruptions in the normal evolution pattern.\nThis temporal nature explains why existing methods focusing only on spatial\nuncertainty of the final output fail at effective artifact localization. Based\non these insights, we propose ASCED (Abnormal Score Correction for Enhancing\nDiffusion), that detects artifacts by monitoring abnormal score dynamics during\nthe diffusion process, with a trajectory-aware on-the-fly mitigation strategy\nthat appropriate generation of noise in the detected areas. Unlike most\nexisting methods that apply post hoc corrections, \\eg, by applying a\nnoising-denoising scheme after generation, our mitigation strategy operates\nseamlessly within the existing diffusion process. Extensive experiments\ndemonstrate that our proposed approach effectively reduces artifacts across\ndiverse domains, matching or surpassing existing supervised methods without\nadditional training.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yu Cao",
      "Zengqun Zhao",
      "Ioannis Patras",
      "Shaogang Gong"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16212v1",
    "title": "MathFusion: Enhancing Mathematic Problem-solving of LLM through Instruction Fusion",
    "abstract": "Large Language Models (LLMs) have shown impressive progress in mathematical\nreasoning. While data augmentation is promising to enhance mathematical\nproblem-solving ability, current approaches are predominantly limited to\ninstance-level modifications-such as rephrasing or generating syntactic\nvariations-which fail to capture and leverage the intrinsic relational\nstructures inherent in mathematical knowledge. Inspired by human learning\nprocesses, where mathematical proficiency develops through systematic exposure\nto interconnected concepts, we introduce MathFusion, a novel framework that\nenhances mathematical reasoning through cross-problem instruction synthesis.\nMathFusion implements this through three fusion strategies: (1) sequential\nfusion, which chains related problems to model solution dependencies; (2)\nparallel fusion, which combines analogous problems to reinforce conceptual\nunderstanding; and (3) conditional fusion, which creates context-aware\nselective problems to enhance reasoning flexibility. By applying these\nstrategies, we generate a new dataset, \\textbf{MathFusionQA}, followed by\nfine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental\nresults demonstrate that MathFusion achieves substantial improvements in\nmathematical reasoning while maintaining high data efficiency, boosting\nperformance by 18.0 points in accuracy across diverse benchmarks while\nrequiring only 45K additional synthetic instructions, representing a\nsubstantial improvement over traditional single-instruction approaches. Our\ndatasets, models, and code are publicly available at\nhttps://github.com/QizhiPei/mathfusion.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Qizhi Pei",
      "Lijun Wu",
      "Zhuoshi Pan",
      "Yu Li",
      "Honglin Lin",
      "Chenlin Ming",
      "Xin Gao",
      "Conghui He",
      "Rui Yan"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16207v1",
    "title": "Neural Variable-Order Fractional Differential Equation Networks",
    "abstract": "Neural differential equation models have garnered significant attention in\nrecent years for their effectiveness in machine learning applications.Among\nthese, fractional differential equations (FDEs) have emerged as a promising\ntool due to their ability to capture memory-dependent dynamics, which are often\nchallenging to model with traditional integer-order approaches.While existing\nmodels have primarily focused on constant-order fractional derivatives,\nvariable-order fractional operators offer a more flexible and expressive\nframework for modeling complex memory patterns. In this work, we introduce the\nNeural Variable-Order Fractional Differential Equation network (NvoFDE), a\nnovel neural network framework that integrates variable-order fractional\nderivatives with learnable neural networks.Our framework allows for the\nmodeling of adaptive derivative orders dependent on hidden features, capturing\nmore complex feature-updating dynamics and providing enhanced flexibility. We\nconduct extensive experiments across multiple graph datasets to validate the\neffectiveness of our approach.Our results demonstrate that NvoFDE outperforms\ntraditional constant-order fractional and integer models across a range of\ntasks, showcasing its superior adaptability and performance.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Wenjun Cui",
      "Qiyu Kang",
      "Xuhao Li",
      "Kai Zhao",
      "Wee Peng Tay",
      "Weihua Deng",
      "Yidong Li"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16206v1",
    "title": "Interpretable Neural Causal Models with TRAM-DAGs",
    "abstract": "The ultimate goal of most scientific studies is to understand the underlying\ncausal mechanism between the involved variables. Structural causal models\n(SCMs) are widely used to represent such causal mechanisms. Given an SCM,\ncausal queries on all three levels of Pearl's causal hierarchy can be answered:\n$L_1$ observational, $L_2$ interventional, and $L_3$ counterfactual. An\nessential aspect of modeling the SCM is to model the dependency of each\nvariable on its causal parents. Traditionally this is done by parametric\nstatistical models, such as linear or logistic regression models. This allows\nto handle all kinds of data types and fit interpretable models but bears the\nrisk of introducing a bias. More recently neural causal models came up using\nneural networks (NNs) to model the causal relationships, allowing the\nestimation of nearly any underlying functional form without bias. However,\ncurrent neural causal models are generally restricted to continuous variables\nand do not yield an interpretable form of the causal relationships.\nTransformation models range from simple statistical regressions to complex\nnetworks and can handle continuous, ordinal, and binary data. Here, we propose\nto use TRAMs to model the functional relationships in SCMs allowing us to\nbridge the gap between interpretability and flexibility in causal modeling. We\ncall this method TRAM-DAG and assume currently that the underlying directed\nacyclic graph is known. For the fully observed case, we benchmark TRAM-DAGs\nagainst state-of-the-art statistical and NN-based causal models. We show that\nTRAM-DAGs are interpretable but also achieve equal or superior performance in\nqueries ranging from $L_1$ to $L_3$ in the causal hierarchy. For the continuous\ncase, TRAM-DAGs allow for counterfactual queries for three common causal\nstructures, including unobserved confounding.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Beate Sick",
      "Oliver Dürr"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16203v1",
    "title": "Logic Explanation of AI Classifiers by Categorical Explaining Functors",
    "abstract": "The most common methods in explainable artificial intelligence are post-hoc\ntechniques which identify the most relevant features used by pretrained opaque\nmodels. Some of the most advanced post hoc methods can generate explanations\nthat account for the mutual interactions of input features in the form of logic\nrules. However, these methods frequently fail to guarantee the consistency of\nthe extracted explanations with the model's underlying reasoning. To bridge\nthis gap, we propose a theoretically grounded approach to ensure coherence and\nfidelity of the extracted explanations, moving beyond the limitations of\ncurrent heuristic-based approaches. To this end, drawing from category theory,\nwe introduce an explaining functor which structurally preserves logical\nentailment between the explanation and the opaque model's reasoning. As a proof\nof concept, we validate the proposed theoretical constructions on a synthetic\nbenchmark verifying how the proposed approach significantly mitigates the\ngeneration of contradictory or unfaithful explanations.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Stefano Fioravanti",
      "Francesco Giannini",
      "Paolo Frazzetto",
      "Fabio Zanasi",
      "Pietro Barbiero"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16199v1",
    "title": "Deferring Concept Bottleneck Models: Learning to Defer Interventions to Inaccurate Experts",
    "abstract": "Concept Bottleneck Models (CBMs) are machine learning models that improve\ninterpretability by grounding their predictions on human-understandable\nconcepts, allowing for targeted interventions in their decision-making process.\nHowever, when intervened on, CBMs assume the availability of humans that can\nidentify the need to intervene and always provide correct interventions. Both\nassumptions are unrealistic and impractical, considering labor costs and human\nerror-proneness. In contrast, Learning to Defer (L2D) extends supervised\nlearning by allowing machine learning models to identify cases where a human is\nmore likely to be correct than the model, thus leading to deferring systems\nwith improved performance. In this work, we gain inspiration from L2D and\npropose Deferring CBMs (DCBMs), a novel framework that allows CBMs to learn\nwhen an intervention is needed. To this end, we model DCBMs as a composition of\ndeferring systems and derive a consistent L2D loss to train them. Moreover, by\nrelying on a CBM architecture, DCBMs can explain why defer occurs on the final\ntask. Our results show that DCBMs achieve high predictive performance and\ninterpretability at the cost of deferring more to humans.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Andrea Pugnana",
      "Riccardo Massidda",
      "Francesco Giannini",
      "Pietro Barbiero",
      "Mateo Espinosa Zarlenga",
      "Roberto Pellungrini",
      "Gabriele Dominici",
      "Fosca Giannotti",
      "Davide Bacciu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16195v1",
    "title": "VP-NTK: Exploring the Benefits of Visual Prompting in Differentially Private Data Synthesis",
    "abstract": "Differentially private (DP) synthetic data has become the de facto standard\nfor releasing sensitive data. However, many DP generative models suffer from\nthe low utility of synthetic data, especially for high-resolution images. On\nthe other hand, one of the emerging techniques in parameter efficient\nfine-tuning (PEFT) is visual prompting (VP), which allows well-trained existing\nmodels to be reused for the purpose of adapting to subsequent downstream tasks.\nIn this work, we explore such a phenomenon in constructing captivating\ngenerative models with DP constraints. We show that VP in conjunction with\nDP-NTK, a DP generator that exploits the power of the neural tangent kernel\n(NTK) in training DP generative models, achieves a significant performance\nboost, particularly for high-resolution image datasets, with accuracy improving\nfrom 0.644$\\pm$0.044 to 0.769. Lastly, we perform ablation studies on the\neffect of different parameters that influence the overall performance of\nVP-NTK. Our work demonstrates a promising step forward in improving the utility\nof DP synthetic data, particularly for high-resolution images.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Chia-Yi Hsu",
      "Jia-You Chen",
      "Yu-Lin Tsai",
      "Chih-Hsun Lin",
      "Pin-Yu Chen",
      "Chia-Mu Yu",
      "Chun-Ying Huang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16194v1",
    "title": "Improving Autoregressive Image Generation through Coarse-to-Fine Token Prediction",
    "abstract": "Autoregressive models have shown remarkable success in image generation by\nadapting sequential prediction techniques from language modeling. However,\napplying these approaches to images requires discretizing continuous pixel data\nthrough vector quantization methods like VQ-VAE. To alleviate the quantization\nerrors that existed in VQ-VAE, recent works tend to use larger codebooks.\nHowever, this will accordingly expand vocabulary size, complicating the\nautoregressive modeling task. This paper aims to find a way to enjoy the\nbenefits of large codebooks without making autoregressive modeling more\ndifficult. Through empirical investigation, we discover that tokens with\nsimilar codeword representations produce similar effects on the final generated\nimage, revealing significant redundancy in large codebooks. Based on this\ninsight, we propose to predict tokens from coarse to fine (CTF), realized by\nassigning the same coarse label for similar tokens. Our framework consists of\ntwo stages: (1) an autoregressive model that sequentially predicts coarse\nlabels for each token in the sequence, and (2) an auxiliary model that\nsimultaneously predicts fine-grained labels for all tokens conditioned on their\ncoarse labels. Experiments on ImageNet demonstrate our method's superior\nperformance, achieving an average improvement of 59 points in Inception Score\ncompared to baselines. Notably, despite adding an inference step, our approach\nachieves faster sampling speeds.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Ziyao Guo",
      "Kaipeng Zhang",
      "Michael Qizhe Shieh"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16192v1",
    "title": "Nonparametric Bellman Mappings for Value Iteration in Distributed Reinforcement Learning",
    "abstract": "This paper introduces novel Bellman mappings (B-Maps) for value iteration\n(VI) in distributed reinforcement learning (DRL), where multiple agents operate\nover a network without a centralized fusion node. Each agent constructs its own\nnonparametric B-Map for VI while communicating only with direct neighbors to\nachieve consensus. These B-Maps operate on Q-functions represented in a\nreproducing kernel Hilbert space, enabling a nonparametric formulation that\nallows for flexible, agent-specific basis function design. Unlike existing DRL\nmethods that restrict information exchange to Q-function estimates, the\nproposed framework also enables agents to share basis information in the form\nof covariance matrices, capturing additional structural details. A theoretical\nanalysis establishes linear convergence rates for both Q-function and\ncovariance-matrix estimates toward their consensus values. The optimal learning\nrates for consensus-based updates are dictated by the ratio of the smallest\npositive eigenvalue to the largest one of the network's Laplacian matrix.\nFurthermore, each nodal Q-function estimate is shown to lie very close to the\nfixed point of a centralized nonparametric B-Map, effectively allowing the\nproposed DRL design to approximate the performance of a centralized fusion\ncenter. Numerical experiments on two well-known control problems demonstrate\nthe superior performance of the proposed nonparametric B-Maps compared to prior\nmethods. Notably, the results reveal a counter-intuitive finding: although the\nproposed approach involves greater information exchange -- specifically through\nthe sharing of covariance matrices -- it achieves the desired performance with\nlower cumulative communication cost than existing DRL schemes, highlighting the\ncrucial role of basis information in accelerating the learning process.",
    "categories": [
      "cs.LG",
      "eess.SP"
    ],
    "authors": [
      "Yuki Akiyama",
      "Konstantinos Slavakis"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16191v1",
    "title": "Large Language Models for Water Distribution Systems Modeling and Decision-Making",
    "abstract": "The design, operations, and management of water distribution systems (WDS)\ninvolve complex mathematical models. These models are continually improving due\nto computational advancements, leading to better decision-making and more\nefficient WDS management. However, the significant time and effort required for\nmodeling, programming, and analyzing results remain substantial challenges.\nAnother issue is the professional burden, which confines the interaction with\nmodels, databases, and other sophisticated tools to a small group of experts,\nthereby causing non-technical stakeholders to depend on these experts or make\ndecisions without modeling support. Furthermore, explaining model results is\nchallenging even for experts, as it is often unclear which conditions cause the\nmodel to reach a certain state or recommend a specific policy. The recent\nadvancements in Large Language Models (LLMs) open doors for a new stage in\nhuman-model interaction. This study proposes a framework of plain language\ninteractions with hydraulic and water quality models based on LLM-EPANET\narchitecture. This framework is tested with increasing levels of complexity of\nqueries to study the ability of LLMs to interact with WDS models, run complex\nsimulations, and report simulation results. The performance of the proposed\nframework is evaluated across several categories of queries and hyper-parameter\nconfigurations, demonstrating its potential to enhance decision-making\nprocesses in WDS management.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "authors": [
      "Yinon Goldshtein",
      "Gal Perelman",
      "Assaf Schuster",
      "Avi Ostfeld"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16188v1",
    "title": "CLS-RL: Image Classification with Rule-Based Reinforcement Learning",
    "abstract": "Classification is a core task in machine learning. Recent research has shown\nthat although Multimodal Large Language Models (MLLMs) are initially poor at\nimage classification, fine-tuning them with an adequate amount of data can\nsignificantly enhance their performance, making them comparable to SOTA\nclassification models. However, acquiring large-scale labeled data is\nexpensive. In this paper, we explore few-shot MLLM classification fine-tuning.\nWe found that SFT can cause severe overfitting issues and may even degrade\nperformance over the zero-shot approach. To address this challenge, inspired by\nthe recent successes in rule-based reinforcement learning, we propose CLS-RL,\nwhich uses verifiable signals as reward to fine-tune MLLMs. We discovered that\nCLS-RL outperforms SFT in most datasets and has a much higher average accuracy\non both base-to-new and few-shot learning setting. Moreover, we observed a\nfree-lunch phenomenon for CLS-RL; when models are fine-tuned on a particular\ndataset, their performance on other distinct datasets may also improve over\nzero-shot models, even if those datasets differ in distribution and class\nnames. This suggests that RL-based methods effectively teach models the\nfundamentals of classification. Lastly, inspired by recent works in inference\ntime thinking, we re-examine the `thinking process' during fine-tuning, a\ncritical aspect of RL-based methods, in the context of visual classification.\nWe question whether such tasks require extensive thinking process during\nfine-tuning, proposing that this may actually detract from performance. Based\non this premise, we introduce the No-Thinking-CLS-RL method, which minimizes\nthinking processes during training by setting an equality accuracy reward. Our\nfindings indicate that, with much less fine-tuning time, No-Thinking-CLS-RL\nmethod achieves superior in-domain performance and generalization capabilities\nthan CLS-RL.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Ming Li",
      "Shitian Zhao",
      "Jike Zhong",
      "Yuxiang Lai",
      "Kaipeng Zhang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16187v1",
    "title": "Manifold learning in metric spaces",
    "abstract": "Laplacian-based methods are popular for dimensionality reduction of data\nlying in $\\mathbb{R}^N$. Several theoretical results for these algorithms\ndepend on the fact that the Euclidean distance approximates the geodesic\ndistance on the underlying submanifold which the data are assumed to lie on.\nHowever, for some applications, other metrics, such as the Wasserstein\ndistance, may provide a more appropriate notion of distance than the Euclidean\ndistance. We provide a framework that generalizes the problem of manifold\nlearning to metric spaces and study when a metric satisfies sufficient\nconditions for the pointwise convergence of the graph Laplacian.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Liane Xu",
      "Amit Singer"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16185v1",
    "title": "MapGlue: Multimodal Remote Sensing Image Matching",
    "abstract": "Multimodal remote sensing image (MRSI) matching is pivotal for cross-modal\nfusion, localization, and object detection, but it faces severe challenges due\nto geometric, radiometric, and viewpoint discrepancies across imaging\nmodalities. Existing unimodal datasets lack scale and diversity, limiting deep\nlearning solutions. This paper proposes MapGlue, a universal MRSI matching\nframework, and MapData, a large-scale multimodal dataset addressing these gaps.\nOur contributions are twofold. MapData, a globally diverse dataset spanning 233\nsampling points, offers original images (7,000x5,000 to 20,000x15,000 pixels).\nAfter rigorous cleaning, it provides 121,781 aligned electronic map-visible\nimage pairs (512x512 pixels) with hybrid manual-automated ground truth,\naddressing the scarcity of scalable multimodal benchmarks. MapGlue integrates\nsemantic context with a dual graph-guided mechanism to extract cross-modal\ninvariant features. This structure enables global-to-local interaction,\nenhancing descriptor robustness against modality-specific distortions.\nExtensive evaluations on MapData and five public datasets demonstrate MapGlue's\nsuperiority in matching accuracy under complex conditions, outperforming\nstate-of-the-art methods. Notably, MapGlue generalizes effectively to unseen\nmodalities without retraining, highlighting its adaptability. This work\naddresses longstanding challenges in MRSI matching by combining scalable\ndataset construction with a robust, semantics-driven framework. Furthermore,\nMapGlue shows strong generalization capabilities on other modality matching\ntasks for which it was not specifically trained. The dataset and code are\navailable at https://github.com/PeihaoWu/MapGlue.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Peihao Wu",
      "Yongxiang Yao",
      "Wenfei Zhang",
      "Dong Wei",
      "Yi Wan",
      "Yansheng Li",
      "Yongjun Zhang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16184v1",
    "title": "Accurate Scene Text Recognition with Efficient Model Scaling and Cloze Self-Distillation",
    "abstract": "Scaling architectures have been proven effective for improving Scene Text\nRecognition (STR), but the individual contribution of vision encoder and text\ndecoder scaling remain under-explored. In this work, we present an in-depth\nempirical analysis and demonstrate that, contrary to previous observations,\nscaling the decoder yields significant performance gains, always exceeding\nthose achieved by encoder scaling alone. We also identify label noise as a key\nchallenge in STR, particularly in real-world data, which can limit the\neffectiveness of STR models. To address this, we propose Cloze\nSelf-Distillation (CSD), a method that mitigates label noise by distilling a\nstudent model from context-aware soft predictions and pseudolabels generated by\na teacher model. Additionally, we enhance the decoder architecture by\nintroducing differential cross-attention for STR. Our methodology achieves\nstate-of-the-art performance on 10 out of 11 benchmarks using only real data,\nwhile significantly reducing the parameter size and computational costs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Andrea Maracani",
      "Savas Ozkan",
      "Sijun Cho",
      "Hyowon Kim",
      "Eunchung Noh",
      "Jeongwon Min",
      "Cho Jung Min",
      "Dookun Park",
      "Mete Ozay"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16183v1",
    "title": "Variance-Aware Noisy Training: Hardening DNNs against Unstable Analog Computations",
    "abstract": "The disparity between the computational demands of deep learning and the\ncapabilities of compute hardware is expanding drastically. Although deep\nlearning achieves remarkable performance in countless tasks, its escalating\nrequirements for computational power and energy consumption surpass the\nsustainable limits of even specialized neural processing units, including the\nApple Neural Engine and NVIDIA TensorCores. This challenge is intensified by\nthe slowdown in CMOS scaling.\n  Analog computing presents a promising alternative, offering substantial\nimprovements in energy efficiency by directly manipulating physical quantities\nsuch as current, voltage, charge, or photons. However, it is inherently\nvulnerable to manufacturing variations, nonlinearities, and noise, leading to\ndegraded prediction accuracy. One of the most effective techniques for\nenhancing robustness, Noisy Training, introduces noise during the training\nphase to reinforce the model against disturbances encountered during inference.\nAlthough highly effective, its performance degrades in real-world environments\nwhere noise characteristics fluctuate due to external factors such as\ntemperature variations and temporal drift.\n  This study underscores the necessity of Noisy Training while revealing its\nfundamental limitations in the presence of dynamic noise. To address these\nchallenges, we propose Variance-Aware Noisy Training, a novel approach that\nmitigates performance degradation by incorporating noise schedules which\nemulate the evolving noise conditions encountered during inference. Our method\nsubstantially improves model robustness, without training overhead. We\ndemonstrate a significant increase in robustness, from 72.3\\% with conventional\nNoisy Training to 97.3\\% with Variance-Aware Noisy Training on CIFAR-10 and\nfrom 38.5\\% to 89.9\\% on Tiny ImageNet.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Xiao Wang",
      "Hendrik Borras",
      "Bernhard Klein",
      "Holger Fröning"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16583v1",
    "title": "Explainable AI-Guided Efficient Approximate DNN Generation for Multi-Pod Systolic Arrays",
    "abstract": "Approximate deep neural networks (AxDNNs) are promising for enhancing energy\nefficiency in real-world devices. One of the key contributors behind this\nenhanced energy efficiency in AxDNNs is the use of approximate multipliers.\nUnfortunately, the simulation of approximate multipliers does not usually scale\nwell on CPUs and GPUs. As a consequence, this slows down the overall simulation\nof AxDNNs aimed at identifying the appropriate approximate multipliers to\nachieve high energy efficiency with a minimum accuracy loss. To address this\nproblem, we present a novel XAI-Gen methodology, which leverages the analytical\nmodel of the emerging hardware accelerator (e.g., Google TPU v4) and\nexplainable artificial intelligence (XAI) to precisely identify the\nnon-critical layers for approximation and quickly discover the appropriate\napproximate multipliers for AxDNN layers. Our results show that XAI-Gen\nachieves up to 7x lower energy consumption with only 1-2% accuracy loss. We\nalso showcase the effectiveness of the XAI-Gen approach through a neural\narchitecture search (XAI-NAS) case study. Interestingly, XAI-NAS achieves 40\\%\nhigher energy efficiency with up to 5x less execution time when compared to the\nstate-of-the-art NAS methods for generating AxDNNs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "authors": [
      "Ayesha Siddique",
      "Khurram Khalil",
      "Khaza Anuarul Hoque"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16179v1",
    "title": "Narrowing Class-Wise Robustness Gaps in Adversarial Training",
    "abstract": "Efforts to address declining accuracy as a result of data shifts often\ninvolve various data-augmentation strategies. Adversarial training is one such\nmethod, designed to improve robustness to worst-case distribution shifts caused\nby adversarial examples. While this method can improve robustness, it may also\nhinder generalization to clean examples and exacerbate performance imbalances\nacross different classes. This paper explores the impact of adversarial\ntraining on both overall and class-specific performance, as well as its\nspill-over effects. We observe that enhanced labeling during training boosts\nadversarial robustness by 53.50% and mitigates class imbalances by 5.73%,\nleading to improved accuracy in both clean and adversarial settings compared to\nstandard adversarial training.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Fatemeh Amerehi",
      "Patrick Healy"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16177v1",
    "title": "OccluGaussian: Occlusion-Aware Gaussian Splatting for Large Scene Reconstruction and Rendering",
    "abstract": "In large-scale scene reconstruction using 3D Gaussian splatting, it is common\nto partition the scene into multiple smaller regions and reconstruct them\nindividually. However, existing division methods are occlusion-agnostic,\nmeaning that each region may contain areas with severe occlusions. As a result,\nthe cameras within those regions are less correlated, leading to a low average\ncontribution to the overall reconstruction. In this paper, we propose an\nocclusion-aware scene division strategy that clusters training cameras based on\ntheir positions and co-visibilities to acquire multiple regions. Cameras in\nsuch regions exhibit stronger correlations and a higher average contribution,\nfacilitating high-quality scene reconstruction. We further propose a\nregion-based rendering technique to accelerate large scene rendering, which\nculls Gaussians invisible to the region where the viewpoint is located. Such a\ntechnique significantly speeds up the rendering without compromising quality.\nExtensive experiments on multiple large scenes show that our method achieves\nsuperior reconstruction results with faster rendering speed compared to\nexisting state-of-the-art approaches. Project page:\nhttps://occlugaussian.github.io.",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Shiyong Liu",
      "Xiao Tang",
      "Zhihao Li",
      "Yingfan He",
      "Chongjie Ye",
      "Jianzhuang Liu",
      "Binxiao Huang",
      "Shunbo Zhou",
      "Xiaofei Wu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16167v1",
    "title": "CodeReviewQA: The Code Review Comprehension Assessment for Large Language Models",
    "abstract": "State-of-the-art large language models (LLMs) have demonstrated impressive\ncode generation capabilities but struggle with real-world software engineering\ntasks, such as revising source code to address code reviews, hindering their\npractical use. Code review comments are often implicit, ambiguous, and\ncolloquial, requiring models to grasp both code and human intent. This\nchallenge calls for evaluating large language models' ability to bridge both\ntechnical and conversational contexts. While existing work has employed the\nautomated code refinement (ACR) task to resolve these comments, current\nevaluation methods fall short, relying on text matching metrics that provide\nlimited insight into model failures and remain susceptible to training data\ncontamination. To address these limitations, we introduce a novel evaluation\nbenchmark, $\\textbf{CodeReviewQA}$ that enables us to conduct fine-grained\nassessment of model capabilities and mitigate data contamination risks. In\nCodeReviewQA, we decompose the generation task of code refinement into\n$\\textbf{three essential reasoning steps}$: $\\textit{change type recognition}$\n(CTR), $\\textit{change localisation}$ (CL), and $\\textit{solution\nidentification}$ (SI). Each step is reformulated as multiple-choice questions\nwith varied difficulty levels, enabling precise assessment of model\ncapabilities, while mitigating data contamination risks. Our comprehensive\nevaluation spans 72 recently released large language models on $\\textbf{900\nmanually curated, high-quality examples}$ across nine programming languages.\nOur results show that CodeReviewQA is able to expose specific model weaknesses\nin code review comprehension, disentangled from their generative automated code\nrefinement results.",
    "categories": [
      "cs.SE",
      "cs.CL"
    ],
    "authors": [
      "Hong Yi Lin",
      "Chunhua Liu",
      "Haoyu Gao",
      "Patanamon Thongtanunam",
      "Christoph Treude"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16165v1",
    "title": "Iterative Optimal Attention and Local Model for Single Image Rain Streak Removal",
    "abstract": "High-fidelity imaging is crucial for the successful safety supervision and\nintelligent deployment of vision-based measurement systems (VBMS). It ensures\nhigh-quality imaging in VBMS, which is fundamental for reliable visual\nmeasurement and analysis. However, imaging quality can be significantly\nimpaired by adverse weather conditions, particularly rain, leading to blurred\nimages and reduced contrast. Such impairments increase the risk of inaccurate\nevaluations and misinterpretations in VBMS. To address these limitations, we\npropose an Expectation Maximization Reconstruction Transformer (EMResformer)\nfor single image rain streak removal. The EMResformer retains the key\nself-attention values for feature aggregation, enhancing local features to\nproduce superior image reconstruction. Specifically, we propose an Expectation\nMaximization Block seamlessly integrated into the single image rain streak\nremoval network, enhancing its ability to eliminate superfluous information and\nrestore a cleaner background image. Additionally, to further enhance local\ninformation for improved detail rendition, we introduce a Local Model Residual\nBlock, which integrates two local model blocks along with a sequence of\nconvolutions and activation functions. This integration synergistically\nfacilitates the extraction of more pertinent features for enhanced single image\nrain streak removal. Extensive experiments validate that our proposed\nEMResformer surpasses current state-of-the-art single image rain streak removal\nmethods on both synthetic and real-world datasets, achieving an improved\nbalance between model complexity and single image deraining performance.\nFurthermore, we evaluate the effectiveness of our method in VBMS scenarios,\ndemonstrating that high-quality imaging significantly improves the accuracy and\nreliability of VBMS tasks.",
    "categories": [
      "cs.CV",
      "cs.IR"
    ],
    "authors": [
      "Xiangyu Li",
      "Wanshu Fan",
      "Yue Shen",
      "Cong Wang",
      "Wei Wang",
      "Xin Yang",
      "Qiang Zhang",
      "Dongsheng Zhou"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16163v1",
    "title": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs",
    "abstract": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Shibo Jie",
      "Yehui Tang",
      "Kai Han",
      "Zhi-Hong Deng",
      "Jing Han"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16161v1",
    "title": "Towards Lighter and Robust Evaluation for Retrieval Augmented Generation",
    "abstract": "Large Language Models are prompting us to view more NLP tasks from a\ngenerative perspective. At the same time, they offer a new way of accessing\ninformation, mainly through the RAG framework. While there have been notable\nimprovements for the autoregressive models, overcoming hallucination in the\ngenerated answers remains a continuous problem. A standard solution is to use\ncommercial LLMs, such as GPT4, to evaluate these algorithms. However, such\nframeworks are expensive and not very transparent. Therefore, we propose a\nstudy which demonstrates the interest of open-weight models for evaluating RAG\nhallucination. We develop a lightweight approach using smaller, quantized LLMs\nto provide an accessible and interpretable metric that gives continuous scores\nfor the generated answer with respect to their correctness and faithfulness.\nThis score allows us to question decisions' reliability and explore thresholds\nto develop a new AUC metric as an alternative to correlation with human\njudgment.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "62-08",
      "I.2.7"
    ],
    "authors": [
      "Alex-Razvan Ispas",
      "Charles-Elie Simon",
      "Fabien Caspani",
      "Vincent Guigue"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16159v1",
    "title": "Neural Combinatorial Optimization for Real-World Routing",
    "abstract": "Vehicle Routing Problems (VRPs) are a class of NP-hard problems ubiquitous in\nseveral real-world logistics scenarios that pose significant challenges for\noptimization. Neural Combinatorial Optimization (NCO) has emerged as a\npromising alternative to classical approaches, as it can learn fast heuristics\nto solve VRPs. However, most research works in NCO for VRPs focus on simplified\nsettings, which do not account for asymmetric distances and travel durations\nthat cannot be derived by simple Euclidean distances and unrealistic data\ndistributions, hindering real-world deployment. This work introduces RRNCO\n(Real Routing NCO) to bridge the gap of NCO between synthetic and real-world\nVRPs in the critical aspects of both data and modeling. First, we introduce a\nnew, openly available dataset with real-world data containing a diverse dataset\nof locations, distances, and duration matrices from 100 cities, considering\nrealistic settings with actual routing distances and durations obtained from\nOpen Source Routing Machine (OSRM). Second, we propose a novel approach that\nefficiently processes both node and edge features through contextual gating,\nenabling the construction of more informed node embedding, and we finally\nincorporate an Adaptation Attention Free Module (AAFM) with neural adaptive\nbias mechanisms that effectively integrates not only distance matrices but also\nangular relationships between nodes, allowing our model to capture rich\nstructural information. RRNCO achieves state-of-the-art results in real-world\nVRPs among NCO methods. We make our dataset and code publicly available at\nhttps://github.com/ai4co/real-routing-nco.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jiwoo Son",
      "Zhikai Zhao",
      "Federico Berto",
      "Chuanbo Hua",
      "Changhyun Kwon",
      "Jinkyoo Park"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16158v1",
    "title": "Automatically Generating Chinese Homophone Words to Probe Machine Translation Estimation Systems",
    "abstract": "Evaluating machine translation (MT) of user-generated content (UGC) involves\nunique challenges such as checking whether the nuance of emotions from the\nsource are preserved in the target text. Recent studies have proposed\nemotion-related datasets, frameworks and models to automatically evaluate MT\nquality of Chinese UGC, without relying on reference translations. However,\nwhether these models are robust to the challenge of preserving emotional\nnuances has been left largely unexplored. To address this gap, we introduce a\nnovel method inspired by information theory which generates challenging Chinese\nhomophone words related to emotions, by leveraging the concept of\nself-information. Our approach generates homophones that were observed to cause\ntranslation errors in emotion preservation, and exposes vulnerabilities in MT\nsystems and their evaluation methods when tackling emotional UGC. We evaluate\nthe efficacy of our method using human evaluation for the quality of these\ngenerated homophones, and compare it with an existing one, showing that our\nmethod achieves higher correlation with human judgments. The generated Chinese\nhomophones, along with their manual translations, are utilized to generate\nperturbations and to probe the robustness of existing quality evaluation\nmodels, including models trained using multi-task learning, fine-tuned variants\nof multilingual language models, as well as large language models (LLMs). Our\nresults indicate that LLMs with larger size exhibit higher stability and\nrobustness to such perturbations. We release our data and code for\nreproducibility and further research.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Shenbin Qian",
      "Constantin Orăsan",
      "Diptesh Kanojia",
      "Félix do Carmo"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16153v1",
    "title": "FreeFlux: Understanding and Exploiting Layer-Specific Roles in RoPE-Based MMDiT for Versatile Image Editing",
    "abstract": "The integration of Rotary Position Embedding (RoPE) in Multimodal Diffusion\nTransformer (MMDiT) has significantly enhanced text-to-image generation\nquality. However, the fundamental reliance of self-attention layers on\npositional embedding versus query-key similarity during generation remains an\nintriguing question. We present the first mechanistic analysis of RoPE-based\nMMDiT models (e.g., FLUX), introducing an automated probing strategy that\ndisentangles positional information versus content dependencies by\nstrategically manipulating RoPE during generation. Our analysis reveals\ndistinct dependency patterns that do not straightforwardly correlate with\ndepth, offering new insights into the layer-specific roles in RoPE-based MMDiT.\nBased on these findings, we propose a training-free, task-specific image\nediting framework that categorizes editing tasks into three types:\nposition-dependent editing (e.g., object addition), content\nsimilarity-dependent editing (e.g., non-rigid editing), and region-preserved\nediting (e.g., background replacement). For each type, we design tailored\nkey-value injection strategies based on the characteristics of the editing\ntask. Extensive qualitative and quantitative evaluations demonstrate that our\nmethod outperforms state-of-the-art approaches, particularly in preserving\noriginal semantic content and achieving seamless modifications.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Tianyi Wei",
      "Yifan Zhou",
      "Dongdong Chen",
      "Xingang Pan"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16149v1",
    "title": "Selective Complementary Feature Fusion and Modal Feature Compression Interaction for Brain Tumor Segmentation",
    "abstract": "Efficient modal feature fusion strategy is the key to achieve accurate\nsegmentation of brain glioma. However, due to the specificity of different MRI\nmodes, it is difficult to carry out cross-modal fusion with large differences\nin modal features, resulting in the model ignoring rich feature information. On\nthe other hand, the problem of multi-modal feature redundancy interaction\noccurs in parallel networks due to the proliferation of feature dimensions,\nfurther increase the difficulty of multi-modal feature fusion at the bottom\nend. In order to solve the above problems, we propose a noval complementary\nfeature compression interaction network (CFCI-Net), which realizes the\ncomplementary fusion and compression interaction of multi-modal feature\ninformation with an efficient mode fusion strategy. Firstly, we propose a\nselective complementary feature fusion (SCFF) module, which adaptively fuses\nrich cross-modal feature information by complementary soft selection weights.\nSecondly, a modal feature compression interaction (MFCI) transformer is\nproposed to deal with the multi-mode fusion redundancy problem when the feature\ndimension surges. The MFCI transformer is composed of modal feature compression\n(MFC) and modal feature interaction (MFI) to realize redundancy feature\ncompression and multi-mode feature interactive learning. %In MFI, we propose a\nhierarchical interactive attention mechanism based on multi-head attention.\nEvaluations on the BraTS2019 and BraTS2020 datasets demonstrate that CFCI-Net\nachieves superior results compared to state-of-the-art models. Code:\nhttps://github.com/CDmm0/CFCI-Net",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Dong Chen",
      "Boyue Zhao",
      "Yi Zhang",
      "Meng Zhao"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16148v1",
    "title": "Only a Little to the Left: A Theory-grounded Measure of Political Bias in Large Language Models",
    "abstract": "Prompt-based language models like GPT4 and LLaMa have been used for a wide\nvariety of use cases such as simulating agents, searching for information, or\nfor content analysis. For all of these applications and others, political\nbiases in these models can affect their performance. Several researchers have\nattempted to study political bias in language models using evaluation suites\nbased on surveys, such as the Political Compass Test (PCT), often finding a\nparticular leaning favored by these models. However, there is some variation in\nthe exact prompting techniques, leading to diverging findings and most research\nrelies on constrained-answer settings to extract model responses. Moreover, the\nPolitical Compass Test is not a scientifically valid survey instrument. In this\nwork, we contribute a political bias measured informed by political science\ntheory, building on survey design principles to test a wide variety of input\nprompts, while taking into account prompt sensitivity. We then prompt 11\ndifferent open and commercial models, differentiating between instruction-tuned\nand non-instruction-tuned models, and automatically classify their political\nstances from 88,110 responses. Leveraging this dataset, we compute political\nbias profiles across different prompt variations and find that while PCT\nexaggerates bias in certain models like GPT3.5, measures of political bias are\noften unstable, but generally more left-leaning for instruction-tuned models.",
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "authors": [
      "Mats Faulborn",
      "Indira Sen",
      "Max Pellert",
      "Andreas Spitz",
      "David Garcia"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16144v1",
    "title": "Unify and Triumph: Polyglot, Diverse, and Self-Consistent Generation of Unit Tests with LLMs",
    "abstract": "Large language model (LLM)-based test generation has gained attention in\nsoftware engineering, yet most studies evaluate LLMs' ability to generate unit\ntests in a single attempt for a given language, missing the opportunity to\nleverage LLM diversity for more robust testing. This paper introduces PolyTest,\na novel approach that enhances test generation by exploiting polyglot and\ntemperature-controlled diversity. PolyTest systematically leverages these\nproperties in two complementary ways: (1) Cross-lingual test generation, where\ntests are generated in multiple languages at zero temperature and then unified;\n(2) Diverse test sampling, where multiple test sets are generated within the\nsame language at a higher temperature before unification. A key insight is that\nLLMs can generate diverse yet contradicting tests -- same input, different\nexpected outputs -- across languages and generations. PolyTest mitigates\ninconsistencies by unifying test sets, fostering self-consistency and improving\noverall test quality. Unlike single-language or single-attempt approaches,\nPolyTest enhances testing without requiring on-the-fly execution, making it\nparticularly beneficial for weaker-performing languages. We evaluate PolyTest\non Llama3-70B, GPT-4o, and GPT-3.5 using EvalPlus, generating tests in five\nlanguages (Java, C, Python, JavaScript, and a CSV-based format) at temperature\n0 and sampling multiple sets at temperature 1. We observe that LLMs frequently\ngenerate contradicting tests across settings, and that PolyTest significantly\nimproves test quality across all considered metrics -- number of tests, passing\nrate, statement/branch coverage (up to +9.01%), and mutation score (up to\n+11.23%). Finally, PolyTest outperforms Pynguin in test generation, passing\nrate, and mutation score.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Djamel Eddine Khelladi",
      "Charly Reux",
      "Mathieu Acher"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16582v1",
    "title": "Machine Learning-Based Genomic Linguistic Analysis (Gene Sequence Feature Learning): A Case Study on Predicting Heavy Metal Response Genes in Rice",
    "abstract": "This study explores the application of machine learning-based genetic\nlinguistics for identifying heavy metal response genes in rice (Oryza sativa).\nBy integrating convolutional neural networks and random forest algorithms, we\ndeveloped a hybrid model capable of extracting and learning meaningful features\nfrom gene sequences, such as k-mer frequencies and physicochemical properties.\nThe model was trained and tested on datasets of genes, achieving high\npredictive performance (precision: 0.89, F1-score: 0.82). RNA-seq and qRT-PCR\nexperiments conducted on rice leaves which exposed to Hg0, revealed\ndifferential expression of genes associated with heavy metal responses, which\nvalidated the model's predictions. Co-expression network analysis identified\n103 related genes, and a literature review indicated that these genes are\nhighly likely to be involved in heavy metal-related biological processes. By\nintegrating and comparing the analysis results with those of differentially\nexpressed genes (DEGs), the validity of the new machine learning method was\nfurther demonstrated. This study highlights the efficacy of combining machine\nlearning with genetic linguistics for large-scale gene prediction. It\ndemonstrates a cost-effective and efficient approach for uncovering molecular\nmechanisms underlying heavy metal responses, with potential applications in\ndeveloping stress-tolerant crop varieties.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.GN"
    ],
    "authors": [
      "Ruiqi Yang",
      "Jianxu Wang",
      "Wei Yuan",
      "Xun Wang",
      "Mei Li"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16134v1",
    "title": "Binarized Mamba-Transformer for Lightweight Quad Bayer HybridEVS Demosaicing",
    "abstract": "Quad Bayer demosaicing is the central challenge for enabling the widespread\napplication of Hybrid Event-based Vision Sensors (HybridEVS). Although existing\nlearning-based methods that leverage long-range dependency modeling have\nachieved promising results, their complexity severely limits deployment on\nmobile devices for real-world applications. To address these limitations, we\npropose a lightweight Mamba-based binary neural network designed for efficient\nand high-performing demosaicing of HybridEVS RAW images. First, to effectively\ncapture both global and local dependencies, we introduce a hybrid Binarized\nMamba-Transformer architecture that combines the strengths of the Mamba and\nSwin Transformer architectures. Next, to significantly reduce computational\ncomplexity, we propose a binarized Mamba (Bi-Mamba), which binarizes all\nprojections while retaining the core Selective Scan in full precision. Bi-Mamba\nalso incorporates additional global visual information to enhance global\ncontext and mitigate precision loss. We conduct quantitative and qualitative\nexperiments to demonstrate the effectiveness of BMTNet in both performance and\ncomputational efficiency, providing a lightweight demosaicing solution suited\nfor real-world edge devices. Our codes and models are available at\nhttps://github.com/Clausy9/BMTNet.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Shiyang Zhou",
      "Haijin Zeng",
      "Yunfan Lu",
      "Tong Shao",
      "Ke Tang",
      "Yongyong Chen",
      "Jie Liu",
      "Jingyong Su"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16581v1",
    "title": "Investigating Retrieval-Augmented Generation in Quranic Studies: A Study of 13 Open-Source Large Language Models",
    "abstract": "Accurate and contextually faithful responses are critical when applying large\nlanguage models (LLMs) to sensitive and domain-specific tasks, such as\nanswering queries related to quranic studies. General-purpose LLMs often\nstruggle with hallucinations, where generated responses deviate from\nauthoritative sources, raising concerns about their reliability in religious\ncontexts. This challenge highlights the need for systems that can integrate\ndomain-specific knowledge while maintaining response accuracy, relevance, and\nfaithfulness. In this study, we investigate 13 open-source LLMs categorized\ninto large (e.g., Llama3:70b, Gemma2:27b, QwQ:32b), medium (e.g., Gemma2:9b,\nLlama3:8b), and small (e.g., Llama3.2:3b, Phi3:3.8b). A Retrieval-Augmented\nGeneration (RAG) is used to make up for the problems that come with using\nseparate models. This research utilizes a descriptive dataset of Quranic surahs\nincluding the meanings, historical context, and qualities of the 114 surahs,\nallowing the model to gather relevant knowledge before responding. The models\nare evaluated using three key metrics set by human evaluators: context\nrelevance, answer faithfulness, and answer relevance. The findings reveal that\nlarge models consistently outperform smaller models in capturing query\nsemantics and producing accurate, contextually grounded responses. The\nLlama3.2:3b model, even though it is considered small, does very well on\nfaithfulness (4.619) and relevance (4.857), showing the promise of smaller\narchitectures that have been well optimized. This article examines the\ntrade-offs between model size, computational efficiency, and response quality\nwhile using LLMs in domain-specific applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Zahra Khalila",
      "Arbi Haza Nasution",
      "Winda Monika",
      "Aytug Onan",
      "Yohei Murakami",
      "Yasir Bin Ismail Radi",
      "Noor Mohammad Osmani"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16131v2",
    "title": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for Multilingual Medical Question Answering",
    "abstract": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Feiyang Li",
      "Yingjian Chen",
      "Haoran Liu",
      "Rui Yang",
      "Han Yuan",
      "Yuang Jiang",
      "Tianxiao Li",
      "Edison Marrese Taylor",
      "Hossein Rouhizadeh",
      "Yusuke Iwasawa",
      "Douglas Teodoro",
      "Yutaka Matsuo",
      "Irene Li"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16128v1",
    "title": "Coupling deep and handcrafted features to assess smile genuineness",
    "abstract": "Assessing smile genuineness from video sequences is a vital topic concerned\nwith recognizing facial expression and linking them with the underlying\nemotional states. There have been a number of techniques proposed underpinned\nwith handcrafted features, as well as those that rely on deep learning to\nelaborate the useful features. As both of these approaches have certain\nbenefits and limitations, in this work we propose to combine the features\nlearned by a long short-term memory network with the features handcrafted to\ncapture the dynamics of facial action units. The results of our experiments\nindicate that the proposed solution is more effective than the baseline\ntechniques and it allows for assessing the smile genuineness from video\nsequences in real-time.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Benedykt Pawlus",
      "Bogdan Smolka",
      "Jolanta Kawulok",
      "Michal Kawulok"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16125v1",
    "title": "Uncertainty Meets Diversity: A Comprehensive Active Learning Framework for Indoor 3D Object Detection",
    "abstract": "Active learning has emerged as a promising approach to reduce the substantial\nannotation burden in 3D object detection tasks, spurring several initiatives in\noutdoor environments. However, its application in indoor environments remains\nunexplored. Compared to outdoor 3D datasets, indoor datasets face significant\nchallenges, including fewer training samples per class, a greater number of\nclasses, more severe class imbalance, and more diverse scene types and\nintra-class variances. This paper presents the first study on active learning\nfor indoor 3D object detection, where we propose a novel framework tailored for\nthis task. Our method incorporates two key criteria - uncertainty and diversity\n- to actively select the most ambiguous and informative unlabeled samples for\nannotation. The uncertainty criterion accounts for both inaccurate detections\nand undetected objects, ensuring that the most ambiguous samples are\nprioritized. Meanwhile, the diversity criterion is formulated as a joint\noptimization problem that maximizes the diversity of both object class\ndistributions and scene types, using a new Class-aware Adaptive Prototype (CAP)\nbank. The CAP bank dynamically allocates representative prototypes to each\nclass, helping to capture varying intra-class diversity across different\ncategories. We evaluate our method on SUN RGB-D and ScanNetV2, where it\noutperforms baselines by a significant margin, achieving over 85% of\nfully-supervised performance with just 10% of the annotation budget.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jiangyi Wang",
      "Na Zhao"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16123v1",
    "title": "Distributed Learning over Arbitrary Topology: Linear Speed-Up with Polynomial Transient Time",
    "abstract": "We study a distributed learning problem in which $n$ agents, each with\npotentially heterogeneous local data, collaboratively minimize the sum of their\nlocal cost functions via peer-to-peer communication. We propose a novel\nalgorithm, Spanning Tree Push-Pull (STPP), which employs two spanning trees\nextracted from a general communication graph to distribute both model\nparameters and stochastic gradients. Unlike prior approaches that rely heavily\non spectral gap properties, STPP leverages a more flexible topological\ncharacterization, enabling robust information flow and efficient updates.\nTheoretically, we prove that STPP achieves linear speedup and polynomial\ntransient iteration complexity, up to $O(n^7)$ for smooth nonconvex objectives\nand $\\tilde{O}(n^3)$ for smooth strongly convex objectives, under arbitrary\nnetwork topologies. Moreover, compared with the existing methods, STPP achieves\nfaster convergence rates on sparse and non-regular topologies (e.g., directed\nring) and reduces communication overhead on dense networks (e.g., static\nexponential graph). These results significantly advance the state of the art,\nespecially when $n$ is large. Numerical experiments further demonstrate the\nstrong performance of STPP and confirm the practical relevance of its\ntheoretical convergence rates across various common graph architectures. Our\ncode is available at\nhttps://anonymous.4open.science/r/SpanningTreePushPull-5D3E.",
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "authors": [
      "Runze You",
      "Shi Pu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16120v1",
    "title": "Probabilistic Prompt Distribution Learning for Animal Pose Estimation",
    "abstract": "Multi-species animal pose estimation has emerged as a challenging yet\ncritical task, hindered by substantial visual diversity and uncertainty. This\npaper challenges the problem by efficient prompt learning for Vision-Language\nPretrained (VLP) models, \\textit{e.g.} CLIP, aiming to resolve the\ncross-species generalization problem. At the core of the solution lies in the\nprompt designing, probabilistic prompt modeling and cross-modal adaptation,\nthereby enabling prompts to compensate for cross-modal information and\neffectively overcome large data variances under unbalanced data distribution.\nTo this end, we propose a novel probabilistic prompting approach to fully\nexplore textual descriptions, which could alleviate the diversity issues caused\nby long-tail property and increase the adaptability of prompts on unseen\ncategory instance. Specifically, we first introduce a set of learnable prompts\nand propose a diversity loss to maintain distinctiveness among prompts, thus\nrepresenting diverse image attributes. Diverse textual probabilistic\nrepresentations are sampled and used as the guidance for the pose estimation.\nSubsequently, we explore three different cross-modal fusion strategies at\nspatial level to alleviate the adverse impacts of visual uncertainty. Extensive\nexperiments on multi-species animal pose benchmarks show that our method\nachieves the state-of-the-art performance under both supervised and zero-shot\nsettings. The code is available at https://github.com/Raojiyong/PPAP.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jiyong Rao",
      "Brian Nlong Zhao",
      "Yu Wang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16117v1",
    "title": "Improving Discriminator Guidance in Diffusion Models",
    "abstract": "Discriminator Guidance has become a popular method for efficiently refining\npre-trained Score-Matching Diffusion models. However, in this paper, we\ndemonstrate that the standard implementation of this technique does not\nnecessarily lead to a distribution closer to the real data distribution.\nSpecifically, we show that training the discriminator using Cross-Entropy loss,\nas commonly done, can in fact increase the Kullback-Leibler divergence between\nthe model and target distributions, particularly when the discriminator\noverfits. To address this, we propose a theoretically sound training objective\nfor discriminator guidance that properly minimizes the KL divergence. We\nanalyze its properties and demonstrate empirically across multiple datasets\nthat our proposed method consistently improves over the conventional method by\nproducing samples of higher quality.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Alexandre Verine",
      "Mehdi Inane",
      "Florian Le Bronnec",
      "Benjamin Negrevergne",
      "Yann Chevaleyre"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16112v1",
    "title": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video Streaming",
    "abstract": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN).",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.MM"
    ],
    "authors": [
      "Liming Liu",
      "Jiangkai Wu",
      "Haoyang Wang",
      "Peiheng Wang",
      "Xinggong Zhang",
      "Zongming Guo"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16107v1",
    "title": "Learn to Bid as a Price-Maker Wind Power Producer",
    "abstract": "Wind power producers (WPPs) participating in short-term power markets face\nsignificant imbalance costs due to their non-dispatchable and variable\nproduction. While some WPPs have a large enough market share to influence\nprices with their bidding decisions, existing optimal bidding methods rarely\naccount for this aspect. Price-maker approaches typically model bidding as a\nbilevel optimization problem, but these methods require complex market models,\nestimating other participants' actions, and are computationally demanding. To\naddress these challenges, we propose an online learning algorithm that\nleverages contextual information to optimize WPP bids in the price-maker\nsetting. We formulate the strategic bidding problem as a contextual multi-armed\nbandit, ensuring provable regret minimization. The algorithm's performance is\nevaluated against various benchmark strategies using a numerical simulation of\nthe German day-ahead and real-time markets.",
    "categories": [
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Shobhit Singhal",
      "Marta Fochesato",
      "Liviu Aolaritei",
      "Florian Dörfler"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16106v1",
    "title": "OSLoPrompt: Bridging Low-Supervision Challenges and Open-Set Domain Generalization in CLIP",
    "abstract": "We introduce Low-Shot Open-Set Domain Generalization (LSOSDG), a novel\nparadigm unifying low-shot learning with open-set domain generalization (ODG).\nWhile prompt-based methods using models like CLIP have advanced DG, they falter\nin low-data regimes (e.g., 1-shot) and lack precision in detecting open-set\nsamples with fine-grained semantics related to training classes. To address\nthese challenges, we propose OSLOPROMPT, an advanced prompt-learning framework\nfor CLIP with two core innovations. First, to manage limited supervision across\nsource domains and improve DG, we introduce a domain-agnostic prompt-learning\nmechanism that integrates adaptable domain-specific cues and visually guided\nsemantic attributes through a novel cross-attention module, besides being\nsupported by learnable domain- and class-generic visual prompts to enhance\ncross-modal adaptability. Second, to improve outlier rejection during\ninference, we classify unfamiliar samples as \"unknown\" and train specialized\nprompts with systematically synthesized pseudo-open samples that maintain\nfine-grained relationships to known classes, generated through a targeted query\nstrategy with off-the-shelf foundation models. This strategy enhances feature\nlearning, enabling our model to detect open samples with varied granularity\nmore effectively. Extensive evaluations across five benchmarks demonstrate that\nOSLOPROMPT establishes a new state-of-the-art in LSOSDG, significantly\noutperforming existing methods.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Mohamad Hassan N C",
      "Divyam Gupta",
      "Mainak Singha",
      "Sai Bhargav Rongali",
      "Ankit Jha",
      "Muhammad Haris Khan",
      "Biplab Banerjee"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16096v1",
    "title": "MarkushGrapher: Joint Visual and Textual Recognition of Markush Structures",
    "abstract": "The automated analysis of chemical literature holds promise to accelerate\ndiscovery in fields such as material science and drug development. In\nparticular, search capabilities for chemical structures and Markush structures\n(chemical structure templates) within patent documents are valuable, e.g., for\nprior-art search. Advancements have been made in the automatic extraction of\nchemical structures from text and images, yet the Markush structures remain\nlargely unexplored due to their complex multi-modal nature. In this work, we\npresent MarkushGrapher, a multi-modal approach for recognizing Markush\nstructures in documents. Our method jointly encodes text, image, and layout\ninformation through a Vision-Text-Layout encoder and an Optical Chemical\nStructure Recognition vision encoder. These representations are merged and used\nto auto-regressively generate a sequential graph representation of the Markush\nstructure along with a table defining its variable groups. To overcome the lack\nof real-world training data, we propose a synthetic data generation pipeline\nthat produces a wide range of realistic Markush structures. Additionally, we\npresent M2S, the first annotated benchmark of real-world Markush structures, to\nadvance research on this challenging task. Extensive experiments demonstrate\nthat our approach outperforms state-of-the-art chemistry-specific and\ngeneral-purpose vision-language models in most evaluation settings. Code,\nmodels, and datasets will be available.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Lucas Morin",
      "Valéry Weber",
      "Ahmed Nassar",
      "Gerhard Ingmar Meijer",
      "Luc Van Gool",
      "Yawei Li",
      "Peter Staar"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16580v1",
    "title": "Procrustes Wasserstein Metric: A Modified Benamou-Brenier Approach with Applications to Latent Gaussian Distributions",
    "abstract": "We introduce a modified Benamou-Brenier type approach leading to a\nWasserstein type distance that allows global invariance, specifically,\nisometries, and we show that the problem can be summarized to orthogonal\ntransformations. This distance is defined by penalizing the action with a\ncostless movement of the particle that does not change the direction and speed\nof its trajectory. We show that for Gaussian distribution resume to measuring\nthe Euclidean distance between their ordered vector of eigenvalues and we show\na direct application in recovering Latent Gaussian distributions.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.OC",
      "math.PR",
      "stat.AP",
      "49Q20, 49Q22, 62D10, 62E17, 62E20"
    ],
    "authors": [
      "Kevine Meugang Toukam"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16094v1",
    "title": "Cultural Alignment in Large Language Models Using Soft Prompt Tuning",
    "abstract": "Large Language Model (LLM) alignment conventionally relies on supervised\nfine-tuning or reinforcement learning based alignment frameworks. These methods\ntypically require labeled or preference datasets and involve updating model\nweights to align the LLM with the training objective or reward model.\nMeanwhile, in social sciences such as cross-cultural studies, factor analysis\nis widely used to uncover underlying dimensions or latent variables that\nexplain observed patterns in survey data. The non-differentiable nature of\nthese measurements deriving from survey data renders the former alignment\nmethods infeasible for alignment with cultural dimensions. To overcome this, we\npropose a parameter efficient strategy that combines soft prompt tuning, which\nfreezes the model parameters while modifying the input prompt embeddings, with\nDifferential Evolution (DE), a black-box optimization method for cases where a\ndifferentiable objective is unattainable. This strategy ensures alignment\nconsistency without the need for preference data or model parameter updates,\nsignificantly enhancing efficiency and mitigating overfitting. Our method\ndemonstrates significant improvements in LLama-3-8B-Instruct's cultural\ndimensions across multiple regions, outperforming both the Naive LLM and the\nIn-context Learning (ICL) baseline, and effectively bridges computational\nmodels with human cultural nuances.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Reem I. Masoud",
      "Martin Ferianc",
      "Philip Treleaven",
      "Miguel Rodrigues"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16091v1",
    "title": "AIMI: Leveraging Future Knowledge and Personalization in Sparse Event Forecasting for Treatment Adherence",
    "abstract": "Adherence to prescribed treatments is crucial for individuals with chronic\nconditions to avoid costly or adverse health outcomes. For certain patient\ngroups, intensive lifestyle interventions are vital for enhancing medication\nadherence. Accurate forecasting of treatment adherence can open pathways to\ndeveloping an on-demand intervention tool, enabling timely and personalized\nsupport. With the increasing popularity of smartphones and wearables, it is now\neasier than ever to develop and deploy smart activity monitoring systems.\nHowever, effective forecasting systems for treatment adherence based on\nwearable sensors are still not widely available. We close this gap by proposing\nAdherence Forecasting and Intervention with Machine Intelligence (AIMI). AIMI\nis a knowledge-guided adherence forecasting system that leverages smartphone\nsensors and previous medication history to estimate the likelihood of\nforgetting to take a prescribed medication. A user study was conducted with 27\nparticipants who took daily medications to manage their cardiovascular\ndiseases. We designed and developed CNN and LSTM-based forecasting models with\nvarious combinations of input features and found that LSTM models can forecast\nmedication adherence with an accuracy of 0.932 and an F-1 score of 0.936.\nMoreover, through a series of ablation studies involving convolutional and\nrecurrent neural network architectures, we demonstrate that leveraging known\nknowledge about future and personalized training enhances the accuracy of\nmedication adherence forecasting. Code available:\nhttps://github.com/ab9mamun/AIMI.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Abdullah Mamun",
      "Diane J. Cook",
      "Hassan Ghasemzadeh"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16086v1",
    "title": "Hyperspectral Imaging for Identifying Foreign Objects on Pork Belly",
    "abstract": "Ensuring food safety and quality is critical in the food processing industry,\nwhere the detection of contaminants remains a persistent challenge. This study\npresents an automated solution for detecting foreign objects on pork belly meat\nusing hyperspectral imaging (HSI). A hyperspectral camera was used to capture\ndata across various bands in the near-infrared (NIR) spectrum (900-1700 nm),\nenabling accurate identification of contaminants that are often undetectable\nthrough traditional visual inspection methods. The proposed solution combines\npre-processing techniques with a segmentation approach based on a lightweight\nVision Transformer (ViT) to distinguish contaminants from meat, fat, and\nconveyor belt materials. The adopted strategy demonstrates high detection\naccuracy and training efficiency, while also addressing key industrial\nchallenges such as inherent noise, temperature variations, and spectral\nsimilarity between contaminants and pork belly. Experimental results validate\nthe effectiveness of hyperspectral imaging in enhancing food safety,\nhighlighting its potential for broad real-time applications in automated\nquality control processes.",
    "categories": [
      "cs.CV",
      "cs.LG",
      "I.2.6; I.2.10; J.7"
    ],
    "authors": [
      "Gabriela Ghimpeteanu",
      "Hayat Rajani",
      "Josep Quintana",
      "Rafael Garcia"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16085v1",
    "title": "Allostatic Control of Persistent States in Spiking Neural Networks for perception and computation",
    "abstract": "We introduce a novel model for updating perceptual beliefs about the\nenvironment by extending the concept of Allostasis to the control of internal\nrepresentations. Allostasis is a fundamental regulatory mechanism observed in\nanimal physiology that orchestrates responses to maintain a dynamic equilibrium\nin bodily needs and internal states. In this paper, we focus on an application\nin numerical cognition, where a bump of activity in an attractor network is\nused as a spatial numerical representation. While existing neural networks can\nmaintain persistent states, to date, there is no unified framework for\ndynamically controlling spatial changes in neuronal activity in response to\nenvironmental changes. To address this, we couple a well known allostatic\nmicrocircuit, the Hammel model, with a ring attractor, resulting in a Spiking\nNeural Network architecture that can modulate the location of the bump as a\nfunction of some reference input. This localized activity in turn is used as a\nperceptual belief in a simulated subitization task a quick enumeration process\nwithout counting. We provide a general procedure to fine-tune the model and\ndemonstrate the successful control of the bump location. We also study the\nresponse time in the model with respect to changes in parameters and compare it\nwith biological data. Finally, we analyze the dynamics of the network to\nunderstand the selectivity and specificity of different neurons to distinct\ncategories present in the input. The results of this paper, particularly the\nmechanism for moving persistent states, are not limited to numerical cognition\nbut can be applied to a wide range of tasks involving similar representations.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "authors": [
      "Aung Htet",
      "Alejandro Rodriguez Jimenez",
      "Sarah Hamburg",
      "Alessandro Di Nuovo"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16579v1",
    "title": "World Knowledge from AI Image Generation for Robot Control",
    "abstract": "When interacting with the world robots face a number of difficult questions,\nhaving to make decisions when given under-specified tasks where they need to\nmake choices, often without clearly defined right and wrong answers. Humans, on\nthe other hand, can often rely on their knowledge and experience to fill in the\ngaps. For example, the simple task of organizing newly bought produce into the\nfridge involves deciding where to put each thing individually, how to arrange\nthem together meaningfully, e.g. putting related things together, all while\nthere is no clear right and wrong way to accomplish this task. We could encode\nall this information on how to do such things explicitly into the robots'\nknowledge base, but this can quickly become overwhelming, considering the\nnumber of potential tasks and circumstances the robot could encounter. However,\nimages of the real world often implicitly encode answers to such questions and\ncan show which configurations of objects are meaningful or are usually used by\nhumans. An image of a full fridge can give a lot of information about how\nthings are usually arranged in relation to each other and the full fridge at\nlarge. Modern generative systems are capable of generating plausible images of\nthe real world and can be conditioned on the environment in which the robot\noperates. Here we investigate the idea of using the implicit knowledge about\nthe world of modern generative AI systems given by their ability to generate\nconvincing images of the real world to solve under-specified tasks.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Jonas Krumme",
      "Christoph Zetzsche"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16081v1",
    "title": "OThink-MR1: Stimulating multimodal generalized reasoning capabilities through dynamic reinforcement learning",
    "abstract": "Multimodal Language Models have gained significant traction for their ability\nto process diverse input data types and generate coherent, contextually\nrelevant outputs across various applications. While supervised fine-tuning\n(SFT) has been the predominant approach to enhance MLLM capabilities in\ntask-specific optimization, it often falls short in fostering crucial\ngeneralized reasoning abilities. Despite the potential of reinforcement\nlearning (RL) to address these limitations, it faces two issues: (1) its\ngeneralized capabilities in multimodal tasks remain underexplored. (2) its\ntraining constraints such as constant Kullback-Leibler or clamp strategy easily\nlead to suboptimal bottleneck. To adress these issues, we introduce OThink-MR1,\na framework that extends RL to MLLMs, enabling them to achieve deeper\nunderstanding and reasoning across multimodal tasks. We design a dynamic\nKullback-Leibler strategy that significantly enhances RL performance,\nsurpassing SFT in same-task evaluations. Also, we are the first to reveal that\nRL exhibits remarkable cross-task generalization capabilities, which shows that\nmodels post-trained with RL on one multimodal task can be effectively\ntransfered to another tasks. Finally, extensive experiments demonstrate the\ngreat reasoning ability of our proposed OThink-MR1.",
    "categories": [
      "cs.LG",
      "cs.IR"
    ],
    "authors": [
      "Zhiyuan Liu",
      "Yuting Zhang",
      "Feng Liu",
      "Changwang Zhang",
      "Ying Sun",
      "Jun Wang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16075v1",
    "title": "3-D Image-to-Image Fusion in Lightsheet Microscopy by Two-Step Adversarial Network: Contribution to the FuseMyCells Challenge",
    "abstract": "Lightsheet microscopy is a powerful 3-D imaging technique that addresses\nlimitations of traditional optical and confocal microscopy but suffers from a\nlow penetration depth and reduced image quality at greater depths. Multiview\nlightsheet microscopy improves 3-D resolution by combining multiple views but\nsimultaneously increasing the complexity and the photon budget, leading to\npotential photobleaching and phototoxicity. The FuseMyCells challenge,\norganized in conjunction with the IEEE ISBI 2025 conference, aims to benchmark\ndeep learning-based solutions for fusing high-quality 3-D volumes from single\n3-D views, potentially simplifying procedures and conserving the photon budget.\nIn this work, we propose a contribution to the FuseMyCells challenge based on a\ntwo-step procedure. The first step processes a downsampled version of the image\nto capture the entire region of interest, while the second step uses a\npatch-based approach for high-resolution inference, incorporating adversarial\nloss to enhance visual outcomes. This method addresses challenges related to\nhigh data resolution, the necessity of global context, and the preservation of\nhigh-frequency details. Experimental results demonstrate the effectiveness of\nour approach, highlighting its potential to improve 3-D image fusion quality\nand extend the capabilities of lightsheet microscopy. The average SSIM for the\nnucleus and membranes is greater than 0.85 and 0.91, respectively.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Marek Wodzinski",
      "Henning Müller"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16072v1",
    "title": "Redefining Toxicity: An Objective and Context-Aware Approach for Stress-Level-Based Detection",
    "abstract": "The fundamental problem of toxicity detection lies in the fact that the term\n\"toxicity\" is ill-defined. Such uncertainty causes researchers to rely on\nsubjective and vague data during model training, which leads to non-robust and\ninaccurate results, following the 'garbage in - garbage out' paradigm. This\nstudy introduces a novel, objective, and context-aware framework for toxicity\ndetection, leveraging stress levels as a key determinant of toxicity. We\npropose new definition, metric and training approach as a parts of our\nframework and demonstrate it's effectiveness using a dataset we collected.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Sergey Berezin",
      "Reza Farahbakhsh",
      "Noel Crespi"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16071v1",
    "title": "Tuning LLMs by RAG Principles: Towards LLM-native Memory",
    "abstract": "Memory, additional information beyond the training of large language models\n(LLMs), is crucial to various real-world applications, such as personal\nassistant. The two mainstream solutions to incorporate memory into the\ngeneration process are long-context LLMs and retrieval-augmented generation\n(RAG). In this paper, we first systematically compare these two types of\nsolutions on three renovated/new datasets and show that (1) long-context\nsolutions, although more expensive, shall be easier to capture the big picture\nand better answer queries which require considering the memory as a whole; and\n(2) when the queries concern specific information, RAG solutions shall be more\ncompetitive especially when the keywords can be explicitly matched. Therefore,\nwe propose a novel method RAG-Tuned-LLM which fine-tunes a relative small\n(e.g., 7B) LLM using the data generated following the RAG principles, so it can\ncombine the advantages of both solutions. Extensive experiments on three\ndatasets demonstrate that RAG-Tuned-LLM can beat long-context LLMs and RAG\nmethods across a wide range of query types.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "authors": [
      "Jiale Wei",
      "Shuchi Wu",
      "Ruochen Liu",
      "Xiang Ying",
      "Jingbo Shang",
      "Fangbo Tao"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16069v1",
    "title": "Disentangled and Interpretable Multimodal Attention Fusion for Cancer Survival Prediction",
    "abstract": "To improve the prediction of cancer survival using whole-slide images and\ntranscriptomics data, it is crucial to capture both modality-shared and\nmodality-specific information. However, multimodal frameworks often entangle\nthese representations, limiting interpretability and potentially suppressing\ndiscriminative features. To address this, we propose Disentangled and\nInterpretable Multimodal Attention Fusion (DIMAF), a multimodal framework that\nseparates the intra- and inter-modal interactions within an attention-based\nfusion mechanism to learn distinct modality-specific and modality-shared\nrepresentations. We introduce a loss based on Distance Correlation to promote\ndisentanglement between these representations and integrate Shapley additive\nexplanations to assess their relative contributions to survival prediction. We\nevaluate DIMAF on four public cancer survival datasets, achieving a relative\naverage improvement of 1.85% in performance and 23.7% in disentanglement\ncompared to current state-of-the-art multimodal models. Beyond improved\nperformance, our interpretable framework enables a deeper exploration of the\nunderlying interactions between and within modalities in cancer biology.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Aniek Eijpe",
      "Soufyan Lakbir",
      "Melis Erdal Cesur",
      "Sara P. Oliveira",
      "Sanne Abeln",
      "Wilson Silva"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16068v1",
    "title": "PoseTraj: Pose-Aware Trajectory Control in Video Diffusion",
    "abstract": "Recent advancements in trajectory-guided video generation have achieved\nnotable progress. However, existing models still face challenges in generating\nobject motions with potentially changing 6D poses under wide-range rotations,\ndue to limited 3D understanding. To address this problem, we introduce\nPoseTraj, a pose-aware video dragging model for generating 3D-aligned motion\nfrom 2D trajectories. Our method adopts a novel two-stage pose-aware\npretraining framework, improving 3D understanding across diverse trajectories.\nSpecifically, we propose a large-scale synthetic dataset PoseTraj-10K,\ncontaining 10k videos of objects following rotational trajectories, and enhance\nthe model perception of object pose changes by incorporating 3D bounding boxes\nas intermediate supervision signals. Following this, we fine-tune the\ntrajectory-controlling module on real-world videos, applying an additional\ncamera-disentanglement module to further refine motion accuracy. Experiments on\nvarious benchmark datasets demonstrate that our method not only excels in 3D\npose-aligned dragging for rotational trajectories but also outperforms existing\nbaselines in trajectory accuracy and video quality.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Longbin Ji",
      "Lei Zhong",
      "Pengfei Wei",
      "Changjian Li"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16067v1",
    "title": "Bokehlicious: Photorealistic Bokeh Rendering with Controllable Apertures",
    "abstract": "Bokeh rendering methods play a key role in creating the visually appealing,\nsoftly blurred backgrounds seen in professional photography. While recent\nlearning-based approaches show promising results, generating realistic Bokeh\nwith variable strength remains challenging. Existing methods require additional\ninputs and suffer from unrealistic Bokeh reproduction due to reliance on\nsynthetic data. In this work, we propose Bokehlicious, a highly efficient\nnetwork that provides intuitive control over Bokeh strength through an\nAperture-Aware Attention mechanism, mimicking the physical lens aperture. To\nfurther address the lack of high-quality real-world data, we present RealBokeh,\na novel dataset featuring 23,000 high-resolution (24-MP) images captured by\nprofessional photographers, covering diverse scenes with varied aperture and\nfocal length settings. Evaluations on both our new RealBokeh and established\nBokeh rendering benchmarks show that Bokehlicious consistently outperforms SOTA\nmethods while significantly reducing computational cost and exhibiting strong\nzero-shot generalization. Our method and dataset further extend to defocus\ndeblurring, achieving competitive results on the RealDOF benchmark. Our code\nand data can be found at https://github.com/TimSeizinger/Bokehlicious",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Tim Seizinger",
      "Florin-Alexandru Vasluianu",
      "Marcos V. Conde",
      "Radu Timofte"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16065v1",
    "title": "Shining Yourself: High-Fidelity Ornaments Virtual Try-on with Diffusion Model",
    "abstract": "While virtual try-on for clothes and shoes with diffusion models has gained\nattraction, virtual try-on for ornaments, such as bracelets, rings, earrings,\nand necklaces, remains largely unexplored. Due to the intricate tiny patterns\nand repeated geometric sub-structures in most ornaments, it is much more\ndifficult to guarantee identity and appearance consistency under large pose and\nscale variances between ornaments and models. This paper proposes the task of\nvirtual try-on for ornaments and presents a method to improve the geometric and\nappearance preservation of ornament virtual try-ons. Specifically, we estimate\nan accurate wearing mask to improve the alignments between ornaments and models\nin an iterative scheme alongside the denoising process. To preserve structure\ndetails, we further regularize attention layers to map the reference ornament\nmask to the wearing mask in an implicit way. Experimental results demonstrate\nthat our method successfully wears ornaments from reference images onto target\nmodels, handling substantial differences in scale and pose while preserving\nidentity and achieving realistic visual effects.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yingmao Miao",
      "Zhanpeng Huang",
      "Rui Han",
      "Zibin Wang",
      "Chenhao Lin",
      "Chao Shen"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16064v1",
    "title": "PromptHash: Affinity-Prompted Collaborative Cross-Modal Learning for Adaptive Hashing Retrieval",
    "abstract": "Cross-modal hashing is a promising approach for efficient data retrieval and\nstorage optimization. However, contemporary methods exhibit significant\nlimitations in semantic preservation, contextual integrity, and information\nredundancy, which constrains retrieval efficacy. We present PromptHash, an\ninnovative framework leveraging affinity prompt-aware collaborative learning\nfor adaptive cross-modal hashing. We propose an end-to-end framework for\naffinity-prompted collaborative hashing, with the following fundamental\ntechnical contributions: (i) a text affinity prompt learning mechanism that\npreserves contextual information while maintaining parameter efficiency, (ii)\nan adaptive gated selection fusion architecture that synthesizes State Space\nModel with Transformer network for precise cross-modal feature integration, and\n(iii) a prompt affinity alignment strategy that bridges modal heterogeneity\nthrough hierarchical contrastive learning. To the best of our knowledge, this\nstudy presents the first investigation into affinity prompt awareness within\ncollaborative cross-modal adaptive hash learning, establishing a paradigm for\nenhanced semantic consistency across modalities. Through comprehensive\nevaluation on three benchmark multi-label datasets, PromptHash demonstrates\nsubstantial performance improvements over existing approaches. Notably, on the\nNUS-WIDE dataset, our method achieves significant gains of 18.22% and 18.65% in\nimage-to-text and text-to-image retrieval tasks, respectively. The code is\npublicly available at https://github.com/ShiShuMo/PromptHash.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR",
      "cs.MM"
    ],
    "authors": [
      "Qiang Zou",
      "Shuli Cheng",
      "Jiayi Chen"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16063v1",
    "title": "Two-stage Incomplete Utterance Rewriting on Editing Operation",
    "abstract": "Previous work on Incomplete Utterance Rewriting (IUR) has primarily focused\non generating rewritten utterances based solely on dialogue context, ignoring\nthe widespread phenomenon of coreference and ellipsis in dialogues. To address\nthis issue, we propose a novel framework called TEO (\\emph{Two-stage approach\non Editing Operation}) for IUR, in which the first stage generates editing\noperations and the second stage rewrites incomplete utterances utilizing the\ngenerated editing operations and the dialogue context. Furthermore, an\nadversarial perturbation strategy is proposed to mitigate cascading errors and\nexposure bias caused by the inconsistency between training and inference in the\nsecond stage. Experimental results on three IUR datasets show that our TEO\noutperforms the SOTA models significantly.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Zhiyu Cao",
      "Peifeng Li",
      "Qiaoming Zhu",
      "Yaxin Fan"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16058v1",
    "title": "Landmarks Are Alike Yet Distinct: Harnessing Similarity and Individuality for One-Shot Medical Landmark Detection",
    "abstract": "Landmark detection plays a crucial role in medical imaging applications such\nas disease diagnosis, bone age estimation, and therapy planning. However,\ntraining models for detecting multiple landmarks simultaneously often\nencounters the \"seesaw phenomenon\", where improvements in detecting certain\nlandmarks lead to declines in detecting others. Yet, training a separate model\nfor each landmark increases memory usage and computational overhead. To address\nthese challenges, we propose a novel approach based on the belief that\n\"landmarks are distinct\" by training models with pseudo-labels and template\ndata updated continuously during the training process, where each model is\ndedicated to detecting a single landmark to achieve high accuracy. Furthermore,\ngrounded on the belief that \"landmarks are also alike\", we introduce an\nadapter-based fusion model, combining shared weights with landmark-specific\nweights, to efficiently share model parameters while allowing flexible\nadaptation to individual landmarks. This approach not only significantly\nreduces memory and computational resource requirements but also effectively\nmitigates the seesaw phenomenon in multi-landmark training. Experimental\nresults on publicly available medical image datasets demonstrate that the\nsingle-landmark models significantly outperform traditional multi-point joint\ntraining models in detecting individual landmarks. Although our adapter-based\nfusion model shows slightly lower performance compared to the combined results\nof all single-landmark models, it still surpasses the current state-of-the-art\nmethods while achieving a notable improvement in resource efficiency.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Xu He",
      "Zhen Huang",
      "Qingsong Yao",
      "Xiaoqian Zhou",
      "S. Kevin Zhou"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16057v1",
    "title": "Expert Race: A Flexible Routing Strategy for Scaling Diffusion Transformer with Mixture of Experts",
    "abstract": "Diffusion models have emerged as mainstream framework in visual generation.\nBuilding upon this success, the integration of Mixture of Experts (MoE) methods\nhas shown promise in enhancing model scalability and performance. In this\npaper, we introduce Race-DiT, a novel MoE model for diffusion transformers with\na flexible routing strategy, Expert Race. By allowing tokens and experts to\ncompete together and select the top candidates, the model learns to dynamically\nassign experts to critical tokens. Additionally, we propose per-layer\nregularization to address challenges in shallow layer learning, and router\nsimilarity loss to prevent mode collapse, ensuring better expert utilization.\nExtensive experiments on ImageNet validate the effectiveness of our approach,\nshowcasing significant performance gains while promising scaling properties.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yike Yuan",
      "Ziyu Wang",
      "Zihao Huang",
      "Defa Zhu",
      "Xun Zhou",
      "Jingyi Yu",
      "Qiyang Min"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16056v1",
    "title": "Semantic-Guided Global-Local Collaborative Networks for Lightweight Image Super-Resolution",
    "abstract": "Single-Image Super-Resolution (SISR) plays a pivotal role in enhancing the\naccuracy and reliability of measurement systems, which are integral to various\nvision-based instrumentation and measurement applications. These systems often\nrequire clear and detailed images for precise object detection and recognition.\nHowever, images captured by visual measurement tools frequently suffer from\ndegradation, including blurring and loss of detail, which can impede\nmeasurement accuracy.As a potential remedy, we in this paper propose a\nSemantic-Guided Global-Local Collaborative Network (SGGLC-Net) for lightweight\nSISR. Our SGGLC-Net leverages semantic priors extracted from a pre-trained\nmodel to guide the super-resolution process, enhancing image detail quality\neffectively. Specifically,we propose a Semantic Guidance Module that seamlessly\nintegrates the semantic priors into the super-resolution network, enabling the\nnetwork to more adeptly capture and utilize semantic priors, thereby enhancing\nimage details. To further explore both local and non-local interactions for\nimproved detail rendition,we propose a Global-Local Collaborative Module, which\nfeatures three Global and Local Detail Enhancement Modules, as well as a Hybrid\nAttention Mechanism to work together to efficiently learn more useful features.\nOur extensive experiments show that SGGLC-Net achieves competitive PSNR and\nSSIM values across multiple benchmark datasets, demonstrating higher\nperformance with the multi-adds reduction of 12.81G compared to\nstate-of-the-art lightweight super-resolution approaches. These improvements\nunderscore the potential of our approach to enhance the precision and\neffectiveness of visual measurement systems. Codes are at\nhttps://github.com/fanamber831/SGGLC-Net.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Wanshu Fan",
      "Yue Wang",
      "Cong Wang",
      "Yunzhe Zhang",
      "Wei Wang",
      "Dongsheng Zhou"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16055v1",
    "title": "SALT: Singular Value Adaptation with Low-Rank Transformation",
    "abstract": "The complex nature of medical image segmentation calls for models that are\nspecifically designed to capture detailed, domain-specific features. Large\nfoundation models offer considerable flexibility, yet the cost of fine-tuning\nthese models remains a significant barrier. Parameter-Efficient Fine-Tuning\n(PEFT) methods, such as Low-Rank Adaptation (LoRA), efficiently update model\nweights with low-rank matrices but may suffer from underfitting when the chosen\nrank is insufficient to capture domain-specific nuances. Conversely, full-rank\nSingular Value Decomposition (SVD) based methods provide comprehensive updates\nby modifying all singular values, yet they often lack flexibility and exhibit\nvariable performance across datasets. We propose SALT (Singular Value\nAdaptation with Low-Rank Transformation), a method that selectively adapts the\nmost influential singular values using trainable scale and shift parameters\nwhile complementing this with a low-rank update for the remaining subspace.\nThis hybrid approach harnesses the advantages of both LoRA and SVD, enabling\neffective adaptation without relying on increasing model size or depth.\nEvaluated on 5 challenging medical datasets, ranging from as few as 20 samples\nto 1000, SALT outperforms state-of-the-art PEFT (LoRA and SVD) by 2% to 5% in\nDice with only 3.9% trainable parameters, demonstrating robust adaptation even\nin low-resource settings. The code for SALT is available at:\nhttps://github.com/BioMedIA-MBZUAI/SALT",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Abdelrahman Elsayed",
      "Sarim Hashmi",
      "Mohammed Elseiagy",
      "Hu Wang",
      "Mohammad Yaqub",
      "Ibrahim Almakky"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16051v1",
    "title": "Closer to Ground Truth: Realistic Shape and Appearance Labeled Data Generation for Unsupervised Underwater Image Segmentation",
    "abstract": "Solving fish segmentation in underwater videos, a real-world problem of great\npractical value in marine and aquaculture industry, is a challenging task due\nto the difficulty of the filming environment, poor visibility and limited\nexisting annotated underwater fish data. In order to overcome these obstacles,\nwe introduce a novel two stage unsupervised segmentation approach that requires\nno human annotations and combines artificially created and real images. Our\nmethod generates challenging synthetic training data, by placing virtual fish\nin real-world underwater habitats, after performing fish transformations such\nas Thin Plate Spline shape warping and color Histogram Matching, which\nrealistically integrate synthetic fish into the backgrounds, making the\ngenerated images increasingly closer to the real world data with every stage of\nour approach. While we validate our unsupervised method on the popular DeepFish\ndataset, obtaining a performance close to a fully-supervised SoTA model, we\nfurther show its effectiveness on the specific case of salmon segmentation in\nunderwater videos, for which we introduce DeepSalmon, the largest dataset of\nits kind in the literature (30 GB). Moreover, on both datasets we prove the\ncapability of our approach to boost the performance of the fully-supervised\nSoTA model.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Andrei Jelea",
      "Ahmed Nabil Belbachir",
      "Marius Leordeanu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16048v1",
    "title": "Meta-Learning Neural Mechanisms rather than Bayesian Priors",
    "abstract": "Children acquire language despite being exposed to several orders of\nmagnitude less data than large language models require. Meta-learning has been\nproposed as a way to integrate human-like learning biases into neural-network\narchitectures, combining both the structured generalizations of symbolic models\nwith the scalability of neural-network models. But what does meta-learning\nexactly imbue the model with? We investigate the meta-learning of formal\nlanguages and find that, contrary to previous claims, meta-trained models are\nnot learning simplicity-based priors when meta-trained on datasets organised\naround simplicity. Rather, we find evidence that meta-training imprints neural\nmechanisms (such as counters) into the model, which function like cognitive\nprimitives for the network on downstream tasks. Most surprisingly, we find that\nmeta-training on a single formal language can provide as much improvement to a\nmodel as meta-training on 5000 different formal languages, provided that the\nformal language incentivizes the learning of useful neural mechanisms. Taken\ntogether, our findings provide practical implications for efficient\nmeta-learning paradigms and new theoretical insights into linking symbolic\ntheories and neural mechanisms.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Michael Goodale",
      "Salvador Mascarenhas",
      "Yair Lakretz"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16578v1",
    "title": "SeniorTalk: A Chinese Conversation Dataset with Rich Annotations for Super-Aged Seniors",
    "abstract": "While voice technologies increasingly serve aging populations, current\nsystems exhibit significant performance gaps due to inadequate training data\ncapturing elderly-specific vocal characteristics like presbyphonia and\ndialectal variations. The limited data available on super-aged individuals in\nexisting elderly speech datasets, coupled with overly simple recording styles\nand annotation dimensions, exacerbates this issue. To address the critical\nscarcity of speech data from individuals aged 75 and above, we introduce\nSeniorTalk, a carefully annotated Chinese spoken dialogue dataset. This dataset\ncontains 55.53 hours of speech from 101 natural conversations involving 202\nparticipants, ensuring a strategic balance across gender, region, and age.\nThrough detailed annotation across multiple dimensions, it can support a wide\nrange of speech tasks. We perform extensive experiments on speaker\nverification, speaker diarization, speech recognition, and speech editing\ntasks, offering crucial insights for the development of speech technologies\ntargeting this age group.",
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "authors": [
      "Yang Chen",
      "Hui Wang",
      "Shiyao Wang",
      "Junyang Chen",
      "Jiabei He",
      "Jiaming Zhou",
      "Xi Yang",
      "Yequan Wang",
      "Yonghua Lin",
      "Yong Qin"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16047v2",
    "title": "Temporal-Spatial Attention Network (TSAN) for DoS Attack Detection in Network Traffic",
    "abstract": "Denial-of-Service (DoS) attacks remain a critical threat to network security,\ndisrupting services and causing significant economic losses. Traditional\ndetection methods, including statistical and rule-based models, struggle to\nadapt to evolving attack patterns. To address this challenge, we propose a\nnovel Temporal-Spatial Attention Network (TSAN) architecture for detecting\nDenial of Service (DoS) attacks in network traffic. By leveraging both temporal\nand spatial features of network traffic, our approach captures complex traffic\npatterns and anomalies that traditional methods might miss. The TSAN model\nincorporates transformer-based temporal encoding, convolutional spatial\nencoding, and a cross-attention mechanism to fuse these complementary feature\nspaces. Additionally, we employ multi-task learning with auxiliary tasks to\nenhance the model's robustness. Experimental results on the NSL-KDD dataset\ndemonstrate that TSAN outperforms state-of-the-art models, achieving superior\naccuracy, precision, recall, and F1-score while maintaining computational\nefficiency for real-time deployment. The proposed architecture offers an\noptimal balance between detection accuracy and computational overhead, making\nit highly suitable for real-world network security applications.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Bisola Faith Kayode",
      "Akinyemi Sadeeq Akintola",
      "Oluwole Fagbohun",
      "Egonna Anaesiuba-Bristol",
      "Onyekachukwu Ojumah",
      "Oluwagbade Odimayo",
      "Toyese Oloyede",
      "Aniema Inyang",
      "Teslim Kazeem",
      "Habeeb Alli",
      "Udodirim Ibem Offia",
      "Prisca Chinazor Amajuoyi"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16045v1",
    "title": "Open Science and Artificial Intelligence for supporting the sustainability of the SRC Network: The espSRC case",
    "abstract": "The SKA Observatory (SKAO), a landmark project in radio astronomy, seeks to\naddress fundamental questions in astronomy. To process its immense data output,\napproximately 700 PB/year, a global network of SKA Regional Centres (SR-CNet)\nwill provide the infrastructure, tools, computational power needed for\nscientific analysis and scientific support. The Spanish SRC (espSRC) focuses on\nensuring the sustainability of this network by reducing its environmental\nimpact, integrating green practices into data platforms, and developing Open\nScience technologies to enable reproducible research. This paper discusses and\nsummarizes part of the research and development activities that the team is\nconducting to reduce the SRC energy consumption at the espSRC and SRCNet. The\npaper also discusses fundamental research on trusted repositories to support\nOpen Science practices.",
    "categories": [
      "astro-ph.IM",
      "cs.AI"
    ],
    "authors": [
      "J. Garrido",
      "S. Sánchez-Expósito",
      "A. Ruiz-Falcó",
      "J. Ruedas",
      "M. Á. Mendoza",
      "V. Vázquez",
      "M. Parra",
      "J. Sánchez",
      "I. Labadie",
      "L. Darriba",
      "J. Moldón",
      "M. Rodriguez-Álvarez",
      "J. Díaz",
      "L. Verdes-Montenegro"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16043v1",
    "title": "Incomplete Utterance Rewriting with Editing Operation Guidance and Utterance Augmentation",
    "abstract": "Although existing fashionable generation methods on Incomplete Utterance\nRewriting (IUR) can generate coherent utterances, they often result in the\ninclusion of irrelevant and redundant tokens in rewritten utterances due to\ntheir inability to focus on critical tokens in dialogue context. Furthermore,\nthe limited size of the training datasets also contributes to the insufficient\ntraining of the IUR model. To address the first issue, we propose a multi-task\nlearning framework EO-IUR (Editing Operation-guided Incomplete Utterance\nRewriting) that introduces the editing operation labels generated by sequence\nlabeling module to guide generation model to focus on critical tokens.\nFurthermore, we introduce a token-level heterogeneous graph to represent\ndialogues. To address the second issue, we propose a two-dimensional utterance\naugmentation strategy, namely editing operation-based incomplete utterance\naugmentation and LLM-based historical utterance augmentation. The experimental\nresults on three datasets demonstrate that our EO-IUR outperforms previous\nstate-of-the-art (SOTA) baselines in both open-domain and task-oriented\ndialogue. The code will be available at https://github.com/Dewset/EO-IUR.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Zhiyu Cao",
      "Peifeng Li",
      "Yaxin Fan",
      "Qiaoming Zhu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16041v2",
    "title": "GreenIQ: A Deep Search Platform for Comprehensive Carbon Market Analysis and Automated Report Generation",
    "abstract": "This study introduces GreenIQ, an AI-powered deep search platform designed to\nrevolutionise carbon market intelligence through autonomous analysis and\nautomated report generation. Carbon markets operate across diverse regulatory\nlandscapes, generating vast amounts of heterogeneous data from policy\ndocuments, industry reports, academic literature, and real-time trading\nplatforms. Traditional research approaches remain labour-intensive, slow, and\ndifficult to scale. GreenIQ addresses these limitations through a multi-agent\narchitecture powered by Large Language Models (LLMs), integrating five\nspecialised AI agents: a Main Researcher Agent for intelligent information\nretrieval, a Report Writing Agent for structured synthesis, a Final Reviewer\nAgent for accuracy verification, a Data Visualisation Agent for enhanced\ninterpretability, and a Translator Agent for multilingual adaptation. The\nsystem achieves seamless integration of structured and unstructured information\nwith AI-driven citation verification, ensuring high transparency and\nreliability. GreenIQ delivers a 99.2\\% reduction in processing time and a\n99.7\\% cost reduction compared to traditional research methodologies. A novel\nAI persona-based evaluation framework involving 16 domain-specific AI personas\nhighlights its superior cross-jurisdictional analytical capabilities and\nregulatory insight generation. GreenIQ sets new standards in AI-driven research\nsynthesis, policy analysis, and sustainability finance by streamlining carbon\nmarket research. It offers an efficient and scalable framework for\nenvironmental and financial intelligence, enabling more accurate, timely, and\ncost-effective decision-making in complex regulatory landscapes",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Oluwole Fagbohun",
      "Sai Yashwanth",
      "Akinyemi Sadeeq Akintola",
      "Ifeoluwa Wurola",
      "Lanre Shittu",
      "Aniema Inyang",
      "Oluwatimilehin Odubola",
      "Udodirim Offia",
      "Said Olanrewaju",
      "Ogidan Toluwaleke",
      "Ilemona Abutu",
      "Taiwo Akinbolaji"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16040v1",
    "title": "Evaluating Test-Time Scaling LLMs for Legal Reasoning: OpenAI o1, DeepSeek-R1, and Beyond",
    "abstract": "Recently, Test-Time Scaling Large Language Models (LLMs), such as DeepSeek-R1\nand OpenAI o1, have demonstrated exceptional capabilities across various\ndomains and tasks, particularly in reasoning. While these models have shown\nimpressive performance on general language tasks, their effectiveness in\nspecialized fields like legal remains unclear. To address this, we present a\npreliminary evaluation of LLMs in various legal scenarios, covering both\nChinese and English legal tasks. Our analysis includes 9 LLMs and 17 legal\ntasks, with a focus on newly published and more complex challenges such as\nmulti-defendant legal judgments and legal argument reasoning. Our findings\nindicate that, despite DeepSeek-R1 and OpenAI o1 being among the most powerful\nmodels, their legal reasoning capabilities are still lacking. Specifically,\nthese models score below 80\\% on seven Chinese legal reasoning tasks and below\n80\\% on two English legal reasoning tasks. This suggests that, even among the\nmost advanced reasoning models, legal reasoning abilities remain\nunderdeveloped.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Yaoyao Yu",
      "Leilei Gan",
      "Yinghao Hu",
      "Bin Wei",
      "Kun Kuang",
      "Fei Wu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16036v1",
    "title": "Hybrid-Level Instruction Injection for Video Token Compression in Multi-modal Large Language Models",
    "abstract": "Recent Multi-modal Large Language Models (MLLMs) have been challenged by the\ncomputational overhead resulting from massive video frames, often alleviated\nthrough compression strategies. However, the visual content is not equally\ncontributed to user instructions, existing strategies (\\eg, average pool)\ninevitably lead to the loss of potentially useful information. To tackle this,\nwe propose the Hybrid-level Instruction Injection Strategy for Conditional\nToken Compression in MLLMs (HICom), utilizing the instruction as a condition to\nguide the compression from both local and global levels. This encourages the\ncompression to retain the maximum amount of user-focused information while\nreducing visual tokens to minimize computational burden. Specifically, the\ninstruction condition is injected into the grouped visual tokens at the local\nlevel and the learnable tokens at the global level, and we conduct the\nattention mechanism to complete the conditional compression. From the\nhybrid-level compression, the instruction-relevant visual parts are highlighted\nwhile the temporal-spatial structure is also preserved for easier understanding\nof LLMs. To further unleash the potential of HICom, we introduce a new\nconditional pre-training stage with our proposed dataset HICom-248K.\nExperiments show that our HICom can obtain distinguished video understanding\nability with fewer tokens, increasing the performance by 2.43\\% average on\nthree multiple-choice QA benchmarks and saving 78.8\\% tokens compared with the\nSOTA method. The code is available at https://github.com/lntzm/HICom.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Zhihang Liu",
      "Chen-Wei Xie",
      "Pandeng Li",
      "Liming Zhao",
      "Longxiang Tang",
      "Yun Zheng",
      "Chuanbin Liu",
      "Hongtao Xie"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16032v1",
    "title": "Agentic Keyframe Search for Video Question Answering",
    "abstract": "Video question answering (VideoQA) enables machines to extract and comprehend\nkey information from videos through natural language interaction, which is a\ncritical step towards achieving intelligence. However, the demand for a\nthorough understanding of videos and high computational costs still limit the\nwidespread applications of VideoQA. To address it, we propose Agentic Keyframe\nSearch (AKeyS), a simple yet powerful algorithm for identifying keyframes in\nthe VideoQA task. It can effectively distinguish key information from\nredundant, irrelevant content by leveraging modern language agents to direct\nclassical search algorithms. Specifically, we first segment the video and\norganize it as a tree structure. Then, AKeyS uses a language agent to estimate\nheuristics and movement costs while dynamically expanding nodes. Finally, the\nagent determines if sufficient keyframes have been collected based on\ntermination conditions and provides answers. Extensive experiments on the\nEgoSchema and NExT-QA datasets show that AKeyS outperforms all previous methods\nwith the highest keyframe searching efficiency, which means it can accurately\nidentify key information and conduct effective visual reasoning with minimal\ncomputational overhead. For example, on the EgoSchema subset, it achieves 1.8%\nhigher accuracy while processing only 43.5% of the frames compared to\nVideoTree. We believe that AKeyS represents a significant step towards building\nintelligent agents for video understanding. The code is publicly available at\nhttps://github.com/fansunqi/AKeyS.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Sunqi Fan",
      "Meng-Hao Guo",
      "Shuojin Yang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16031v1",
    "title": "Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging Fabricated Claims with Humorous Content",
    "abstract": "This paper presents the Deceptive Humor Dataset (DHD), a novel resource for\nstudying humor derived from fabricated claims and misinformation. In an era of\nrampant misinformation, understanding how humor intertwines with deception is\nessential. DHD consists of humor-infused comments generated from false\nnarratives, incorporating fabricated claims and manipulated information using\nthe ChatGPT-4o model. Each instance is labeled with a Satire Level, ranging\nfrom 1 for subtle satire to 3 for high-level satire and classified into five\ndistinct Humor Categories: Dark Humor, Irony, Social Commentary, Wordplay, and\nAbsurdity. The dataset spans multiple languages including English, Telugu,\nHindi, Kannada, Tamil, and their code-mixed variants (Te-En, Hi-En, Ka-En,\nTa-En), making it a valuable multilingual benchmark. By introducing DHD, we\nestablish a structured foundation for analyzing humor in deceptive contexts,\npaving the way for a new research direction that explores how humor not only\ninteracts with misinformation but also influences its perception and spread. We\nestablish strong baselines for the proposed dataset, providing a foundation for\nfuture research to benchmark and advance deceptive humor detection models.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Sai Kartheek Reddy Kasu",
      "Shankar Biradar",
      "Sunil Saumya"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16025v1",
    "title": "Single Image Iterative Subject-driven Generation and Editing",
    "abstract": "Personalizing image generation and editing is particularly challenging when\nwe only have a few images of the subject, or even a single image. A common\napproach to personalization is concept learning, which can integrate the\nsubject into existing models relatively quickly, but produces images whose\nquality tends to deteriorate quickly when the number of subject images is\nsmall. Quality can be improved by pre-training an encoder, but training\nrestricts generation to the training distribution, and is time consuming. It is\nstill an open hard challenge to personalize image generation and editing from a\nsingle image without training. Here, we present SISO, a novel, training-free\napproach based on optimizing a similarity score with an input subject image.\nMore specifically, SISO iteratively generates images and optimizes the model\nbased on loss of similarity with the given subject image until a satisfactory\nlevel of similarity is achieved, allowing plug-and-play optimization to any\nimage generator. We evaluated SISO in two tasks, image editing and image\ngeneration, using a diverse data set of personal subjects, and demonstrate\nsignificant improvements over existing methods in image quality, subject\nfidelity, and background preservation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Yair Shpitzer",
      "Gal Chechik",
      "Idan Schwartz"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16024v1",
    "title": "The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement",
    "abstract": "Large language models (LLMs) have recently transformed from text-based\nassistants to autonomous agents capable of planning, reasoning, and iteratively\nimproving their actions. While numerical reward signals and verifiers can\neffectively rank candidate actions, they often provide limited contextual\nguidance. In contrast, natural language feedback better aligns with the\ngenerative capabilities of LLMs, providing richer and more actionable\nsuggestions. However, parsing and implementing this feedback effectively can be\nchallenging for LLM-based agents. In this work, we introduce Critique-Guided\nImprovement (CGI), a novel two-player framework, comprising an actor model that\nexplores an environment and a critic model that generates detailed nature\nlanguage feedback. By training the critic to produce fine-grained assessments\nand actionable revisions, and the actor to utilize these critiques, our\napproach promotes more robust exploration of alternative strategies while\navoiding local optima. Experiments in three interactive environments show that\nCGI outperforms existing baselines by a substantial margin. Notably, even a\nsmall critic model surpasses GPT-4 in feedback quality. The resulting actor\nachieves state-of-the-art performance, demonstrating the power of explicit\niterative guidance to enhance decision-making in LLM-based agents.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Ruihan Yang",
      "Fanghua Ye",
      "Jian Li",
      "Siyu Yuan",
      "Yikai Zhang",
      "Zhaopeng Tu",
      "Xiaolong Li",
      "Deqing Yang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16022v1",
    "title": "Corrective In-Context Learning: Evaluating Self-Correction in Large Language Models",
    "abstract": "In-context learning (ICL) has transformed the use of large language models\n(LLMs) for NLP tasks, enabling few-shot learning by conditioning on labeled\nexamples without finetuning. Despite its effectiveness, ICL is prone to errors,\nespecially for challenging examples. With the goal of improving the performance\nof ICL, we propose corrective in-context learning (CICL), an approach that\nincorporates a model's incorrect predictions alongside ground truth corrections\ninto the prompt, aiming to enhance classification accuracy through\nself-correction. However, contrary to our hypothesis, extensive experiments on\ntext classification tasks demonstrate that CICL consistently underperforms\nstandard ICL, with performance degrading as the proportion of corrections in\nthe prompt increases. Our findings indicate that CICL introduces confusion by\ndisrupting the model's task understanding, rather than refining its\npredictions. Additionally, we observe that presenting harder examples in\nstandard ICL does not improve performance, suggesting that example difficulty\nalone may not be a reliable criterion for effective selection. By presenting\nthese negative results, we provide important insights into the limitations of\nself-corrective mechanisms in LLMs and offer directions for future research.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Mario Sanz-Guerrero",
      "Katharina von der Wense"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16021v2",
    "title": "Autonomous AI imitators increase diversity in homogeneous information ecosystems",
    "abstract": "Recent breakthroughs in large language models (LLMs) have facilitated\nautonomous AI agents capable of imitating human-generated content. This\ntechnological advancement raises fundamental questions about AI's impact on the\ndiversity and democratic value of information ecosystems. We introduce a\nlarge-scale simulation framework to examine AI-based imitation within news, a\ncontext crucial for public discourse. By systematically testing two distinct\nimitation strategies across a range of information environments varying in\ninitial diversity, we demonstrate that AI-generated articles do not uniformly\nhomogenize content. Instead, AI's influence is strongly context-dependent:\nAI-generated content can introduce valuable diversity in originally homogeneous\nnews environments but diminish diversity in initially heterogeneous contexts.\nThese results illustrate that the initial diversity of an information\nenvironment critically shapes AI's impact, challenging assumptions that\nAI-driven imitation uniformly threatens diversity. Instead, when information is\ninitially homogeneous, AI-driven imitation can expand perspectives, styles, and\ntopics. This is especially important in news contexts, where information\ndiversity fosters richer public debate by exposing citizens to alternative\nviewpoints, challenging biases, and preventing narrative monopolies, which is\nessential for a resilient democracy.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "J.4"
    ],
    "authors": [
      "Emil Bakkensen Johansen",
      "Oliver Baumann"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16013v1",
    "title": "GraspCoT: Integrating Physical Property Reasoning for 6-DoF Grasping under Flexible Language Instructions",
    "abstract": "Flexible instruction-guided 6-DoF grasping is a significant yet challenging\ntask for real-world robotic systems. Existing methods utilize the contextual\nunderstanding capabilities of the large language models (LLMs) to establish\nmappings between expressions and targets, allowing robots to comprehend users'\nintentions in the instructions. However, the LLM's knowledge about objects'\nphysical properties remains underexplored despite its tight relevance to\ngrasping. In this work, we propose GraspCoT, a 6-DoF grasp detection framework\nthat integrates a Chain-of-Thought (CoT) reasoning mechanism oriented to\nphysical properties, guided by auxiliary question-answering (QA) tasks.\nParticularly, we design a set of QA templates to enable hierarchical reasoning\nthat includes three stages: target parsing, physical property analysis, and\ngrasp action selection. Moreover, GraspCoT presents a unified multimodal LLM\narchitecture, which encodes multi-view observations of 3D scenes into 3D-aware\nvisual tokens, and then jointly embeds these visual tokens with CoT-derived\ntextual tokens within LLMs to generate grasp pose predictions. Furthermore, we\npresent IntentGrasp, a large-scale benchmark that fills the gap in public\ndatasets for multi-object grasp detection under diverse and indirect verbal\ncommands. Extensive experiments on IntentGrasp demonstrate the superiority of\nour method, with additional validation in real-world robotic applications\nconfirming its practicality. Codes and data will be released.",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Xiaomeng Chu",
      "Jiajun Deng",
      "Guoliang You",
      "Wei Liu",
      "Xingchen Li",
      "Jianmin Ji",
      "Yanyong Zhang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16012v1",
    "title": "GazeSCRNN: Event-based Near-eye Gaze Tracking using a Spiking Neural Network",
    "abstract": "This work introduces GazeSCRNN, a novel spiking convolutional recurrent\nneural network designed for event-based near-eye gaze tracking. Leveraging the\nhigh temporal resolution, energy efficiency, and compatibility of Dynamic\nVision Sensor (DVS) cameras with event-based systems, GazeSCRNN uses a spiking\nneural network (SNN) to address the limitations of traditional gaze-tracking\nsystems in capturing dynamic movements. The proposed model processes event\nstreams from DVS cameras using Adaptive Leaky-Integrate-and-Fire (ALIF) neurons\nand a hybrid architecture optimized for spatio-temporal data. Extensive\nevaluations on the EV-Eye dataset demonstrate the model's accuracy in\npredicting gaze vectors. In addition, we conducted ablation studies to reveal\nthe importance of the ALIF neurons, dynamic event framing, and training\ntechniques, such as Forward-Propagation-Through-Time, in enhancing overall\nsystem performance. The most accurate model achieved a Mean Angle Error (MAE)\nof 6.034{\\deg} and a Mean Pupil Error (MPE) of 2.094 mm. Consequently, this\nwork is pioneering in demonstrating the feasibility of using SNNs for\nevent-based gaze tracking, while shedding light on critical challenges and\nopportunities for further improvement.",
    "categories": [
      "cs.CV",
      "cs.NE"
    ],
    "authors": [
      "Stijn Groenen",
      "Marzieh Hassanshahi Varposhti",
      "Mahyar Shahsavari"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16010v1",
    "title": "Patch-based learning of adaptive Total Variation parameter maps for blind image denoising",
    "abstract": "We consider a patch-based learning approach defined in terms of neural\nnetworks to estimate spatially adaptive regularisation parameter maps for image\ndenoising with weighted Total Variation and test it to situations when the\nnoise distribution is unknown. As an example, we consider situations where\nnoise could be either Gaussian or Poisson and perform preliminary model\nselection by a standard binary classification network. Then, we define a\npatch-based approach where at each image pixel an optimal weighting between TV\nregularisation and the corresponding data fidelity is learned in a supervised\nway using reference natural image patches upon optimisation of SSIM and in a\nsliding window fashion. Extensive numerical results are reported for both noise\nmodels, showing significant improvement w.r.t. results obtained by means of\noptimal scalar regularisation.",
    "categories": [
      "eess.IV",
      "cs.LG",
      "cs.NA",
      "math.NA"
    ],
    "authors": [
      "Claudio Fantasia",
      "Luca Calatroni",
      "Xavier Descombes",
      "Rim Rekik"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16000v1",
    "title": "SenseExpo: Efficient Autonomous Exploration with Prediction Information from Lightweight Neural Networks",
    "abstract": "This paper proposes SenseExpo, an efficient autonomous exploration framework\nbased on a lightweight prediction network, which addresses the limitations of\ntraditional methods in computational overhead and environmental generalization.\nBy integrating Generative Adversarial Networks (GANs), Transformer, and Fast\nFourier Convolution (FFC), we designed a lightweight prediction model with\nmerely 709k parameters. Our smallest model achieves better performance on the\nKTH dataset than U-net (24.5M) and LaMa (51M), delivering PSNR 9.026 and SSIM\n0.718, particularly representing a 38.7% PSNR improvement over the\n51M-parameter LaMa model. Cross-domain testing demonstrates its strong\ngeneralization capability, with an FID score of 161.55 on the HouseExpo\ndataset, significantly outperforming comparable methods. Regarding exploration\nefficiency, on the KTH dataset,SenseExpo demonstrates approximately a 67.9%\ntime reduction in exploration time compared to MapEx. On the MRPB 1.0 dataset,\nSenseExpo achieves 77.1% time reduction roughly compared to MapEx. Deployed as\na plug-and-play ROS node, the framework seamlessly integrates with existing\nnavigation systems, providing an efficient solution for resource-constrained\ndevices.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Haojia Gao",
      "Haohua Que",
      "Hoiian Au",
      "Weihao Shan",
      "Mingkai Liu",
      "Yusen Qin",
      "Lei Mu",
      "Rong Zhao",
      "Xinghua Yang",
      "Qi Wei",
      "Fei Qiao"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15997v1",
    "title": "Automating 3D Dataset Generation with Neural Radiance Fields",
    "abstract": "3D detection is a critical task to understand spatial characteristics of the\nenvironment and is used in a variety of applications including robotics,\naugmented reality, and image retrieval. Training performant detection models\nrequire diverse, precisely annotated, and large scale datasets that involve\ncomplex and expensive creation processes. Hence, there are only few public 3D\ndatasets that are additionally limited in their range of classes. In this work,\nwe propose a pipeline for automatic generation of 3D datasets for arbitrary\nobjects. By utilizing the universal 3D representation and rendering\ncapabilities of Radiance Fields, our pipeline generates high quality 3D models\nfor arbitrary objects. These 3D models serve as input for a synthetic dataset\ngenerator. Our pipeline is fast, easy to use and has a high degree of\nautomation. Our experiments demonstrate, that 3D pose estimation networks,\ntrained with our generated datasets, archive strong performance in typical\napplication scenarios.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "P. Schulz",
      "T. Hempel",
      "A. Al-Hamadi"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15996v1",
    "title": "Animating the Uncaptured: Humanoid Mesh Animation with Video Diffusion Models",
    "abstract": "Animation of humanoid characters is essential in various graphics\napplications, but requires significant time and cost to create realistic\nanimations. We propose an approach to synthesize 4D animated sequences of input\nstatic 3D humanoid meshes, leveraging strong generalized motion priors from\ngenerative video models -- as such video models contain powerful motion\ninformation covering a wide variety of human motions. From an input static 3D\nhumanoid mesh and a text prompt describing the desired animation, we synthesize\na corresponding video conditioned on a rendered image of the 3D mesh. We then\nemploy an underlying SMPL representation to animate the corresponding 3D mesh\naccording to the video-generated motion, based on our motion optimization. This\nenables a cost-effective and accessible solution to enable the synthesis of\ndiverse and realistic 4D animations.",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Marc Benedí San Millán",
      "Angela Dai",
      "Matthias Nießner"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16577v1",
    "title": "Feature selection strategies for optimized heart disease diagnosis using ML and DL models",
    "abstract": "Heart disease remains one of the leading causes of morbidity and mortality\nworldwide, necessitating the development of effective diagnostic tools to\nenable early diagnosis and clinical decision-making. This study evaluates the\nimpact of feature selection techniques Mutual Information (MI), Analysis of\nVariance (ANOVA), and Chi-Square on the predictive performance of various\nmachine learning (ML) and deep learning (DL) models using a dataset of clinical\nindicators for heart disease. Eleven ML/DL models were assessed using metrics\nsuch as precision, recall, AUC score, F1-score, and accuracy. Results indicate\nthat MI outperformed other methods, particularly for advanced models like\nneural networks, achieving the highest accuracy of 82.3% and recall score of\n0.94. Logistic regression (accuracy 82.1%) and random forest (accuracy 80.99%)\nalso demonstrated improved performance with MI. Simpler models such as Naive\nBayes and decision trees achieved comparable results with ANOVA and Chi-Square,\nyielding accuracies of 76.45% and 75.99%, respectively, making them\ncomputationally efficient alternatives. Conversely, k Nearest Neighbors (KNN)\nand Support Vector Machines (SVM) exhibited lower performance, with accuracies\nranging between 51.52% and 54.43%, regardless of the feature selection method.\nThis study provides a comprehensive comparison of feature selection methods for\nheart disease prediction, demonstrating the critical role of feature selection\nin optimizing model performance. The results offer practical guidance for\nselecting appropriate feature selection techniques based on the chosen\nclassification algorithm, contributing to the development of more accurate and\nefficient diagnostic tools for enhanced clinical decision-making in cardiology.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Bilal Ahmad",
      "Jinfu Chen",
      "Haibao Chen"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15990v1",
    "title": "ECKGBench: Benchmarking Large Language Models in E-commerce Leveraging Knowledge Graph",
    "abstract": "Large language models (LLMs) have demonstrated their capabilities across\nvarious NLP tasks. Their potential in e-commerce is also substantial, evidenced\nby practical implementations such as platform search, personalized\nrecommendations, and customer service. One primary concern associated with LLMs\nis their factuality (e.g., hallucination), which is urgent in e-commerce due to\nits significant impact on user experience and revenue. Despite some methods\nproposed to evaluate LLMs' factuality, issues such as lack of reliability, high\nconsumption, and lack of domain expertise leave a gap between effective\nassessment in e-commerce. To bridge the evaluation gap, we propose ECKGBench, a\ndataset specifically designed to evaluate the capacities of LLMs in e-commerce\nknowledge. Specifically, we adopt a standardized workflow to automatically\ngenerate questions based on a large-scale knowledge graph, guaranteeing\nsufficient reliability. We employ the simple question-answering paradigm,\nsubstantially improving the evaluation efficiency by the least input and output\ntokens. Furthermore, we inject abundant e-commerce expertise in each evaluation\nstage, including human annotation, prompt design, negative sampling, and\nverification. Besides, we explore the LLMs' knowledge boundaries in e-commerce\nfrom a novel perspective. Through comprehensive evaluations of several advanced\nLLMs on ECKGBench, we provide meticulous analysis and insights into leveraging\nLLMs for e-commerce.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Langming Liu",
      "Haibin Chen",
      "Yuhao Wang",
      "Yujin Yuan",
      "Shilei Liu",
      "Wenbo Su",
      "Xiangyu Zhao",
      "Bo Zheng"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16575v1",
    "title": "Extract, Match, and Score: An Evaluation Paradigm for Long Question-context-answer Triplets in Financial Analysis",
    "abstract": "The rapid advancement of large language models (LLMs) has sparked widespread\nadoption across diverse applications, making robust evaluation frameworks\ncrucial for assessing their performance. While conventional evaluation metrics\nremain applicable for shorter texts, their efficacy diminishes when evaluating\nthe quality of long-form answers. This limitation is particularly critical in\nreal-world scenarios involving extended questions, extensive context, and\nlong-form answers, such as financial analysis or regulatory compliance. In this\npaper, we use a practical financial use case to illustrate applications that\nhandle \"long question-context-answer triplets\". We construct a real-world\nfinancial dataset comprising long triplets and demonstrate the inadequacies of\ntraditional metrics. To address this, we propose an effective Extract, Match,\nand Score (EMS) evaluation approach tailored to the complexities of long-form\nLLMs' outputs, providing practitioners with a reliable methodology for\nassessing LLMs' performance in complex real-world scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Bo Hu",
      "Han Yuan",
      "Vlad Pandelea",
      "Wuqiong Luo",
      "Yingzhu Zhao",
      "Zheng Ma"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15986v1",
    "title": "SpiLiFormer: Enhancing Spiking Transformers with Lateral Inhibition",
    "abstract": "Spiking Neural Networks (SNNs) based on Transformers have garnered\nsignificant attention due to their superior performance and high energy\nefficiency. However, the spiking attention modules of most existing\nTransformer-based SNNs are adapted from those of analog Transformers, failing\nto fully address the issue of over-allocating attention to irrelevant contexts.\nTo fix this fundamental yet overlooked issue, we propose a Lateral\nInhibition-inspired Spiking Transformer (SpiLiFormer). It emulates the brain's\nlateral inhibition mechanism, guiding the model to enhance attention to\nrelevant tokens while suppressing attention to irrelevant ones. Our model\nachieves state-of-the-art (SOTA) performance across multiple datasets,\nincluding CIFAR-10 (+0.45%), CIFAR-100 (+0.48%), CIFAR10-DVS (+2.70%),\nN-Caltech101 (+1.94%), and ImageNet-1K (+1.6%). Notably, on the ImageNet-1K\ndataset, SpiLiFormer (69.9M parameters, 4 time steps, 384 resolution)\noutperforms E-SpikeFormer (173.0M parameters, 8 time steps, 384 resolution), a\nSOTA spiking Transformer, by 0.46% using only 39% of the parameters and half\nthe time steps. Our code and training checkpoints will be released upon\nacceptance.",
    "categories": [
      "cs.NE",
      "cs.CV"
    ],
    "authors": [
      "Zeqi Zheng",
      "Yanchen Huang",
      "Yingchao Yu",
      "Zizheng Zhu",
      "Junfeng Tang",
      "Zhaofei Yu",
      "Yaochu Jin"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15985v1",
    "title": "Exploring the Reliability of Self-explanation and its Relationship with Classification in Language Model-driven Financial Analysis",
    "abstract": "Language models (LMs) have exhibited exceptional versatility in reasoning and\nin-depth financial analysis through their proprietary information processing\ncapabilities. Previous research focused on evaluating classification\nperformance while often overlooking explainability or pre-conceived that\nrefined explanation corresponds to higher classification accuracy. Using a\npublic dataset in finance domain, we quantitatively evaluated self-explanations\nby LMs, focusing on their factuality and causality. We identified the\nstatistically significant relationship between the accuracy of classifications\nand the factuality or causality of self-explanations. Our study built an\nempirical foundation for approximating classification confidence through\nself-explanations and for optimizing classification via proprietary reasoning.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Han Yuan",
      "Li Zhang",
      "Zheng Ma"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16573v1",
    "title": "AUV Acceleration Prediction Using DVL and Deep Learning",
    "abstract": "Autonomous underwater vehicles (AUVs) are essential for various applications,\nincluding oceanographic surveys, underwater mapping, and infrastructure\ninspections. Accurate and robust navigation are critical to completing these\ntasks. To this end, a Doppler velocity log (DVL) and inertial sensors are fused\ntogether. Recently, a model-based approach demonstrated the ability to extract\nthe vehicle acceleration vector from DVL velocity measurements. Motivated by\nthis advancement, in this paper we present an end-to-end deep learning approach\nto estimate the AUV acceleration vector based on past DVL velocity\nmeasurements. Based on recorded data from sea experiments, we demonstrate that\nthe proposed method improves acceleration vector estimation by more than 65%\ncompared to the model-based approach by using data-driven techniques. As a\nresult of our data-driven approach, we can enhance navigation accuracy and\nreliability in AUV applications, contributing to more efficient and effective\nunderwater missions through improved accuracy and reliability.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Yair Stolero",
      "Itzik Klein"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15984v1",
    "title": "DIPLI: Deep Image Prior Lucky Imaging for Blind Astronomical Image Restoration",
    "abstract": "Contemporary image restoration and super-resolution techniques effectively\nharness deep neural networks, markedly outperforming traditional methods.\nHowever, astrophotography presents unique challenges for deep learning due to\nlimited training data. This work explores hybrid strategies, such as the Deep\nImage Prior (DIP) model, which facilitates blind training but is susceptible to\noverfitting, artifact generation, and instability when handling noisy images.\nWe propose enhancements to the DIP model's baseline performance through several\nadvanced techniques. First, we refine the model to process multiple frames\nconcurrently, employing the Back Projection method and the TVNet model. Next,\nwe adopt a Markov approach incorporating Monte Carlo estimation, Langevin\ndynamics, and a variational input technique to achieve unbiased estimates with\nminimal variance and counteract overfitting effectively. Collectively, these\nmodifications reduce the likelihood of noise learning and mitigate loss\nfunction fluctuations during training, enhancing result stability. We validated\nour algorithm across multiple image sets of astronomical and celestial objects,\nachieving performance that not only mitigates limitations of Lucky Imaging, a\nclassical computer vision technique that remains a standard in astronomical\nimage reconstruction but surpasses the original DIP model, state of the art\ntransformer- and diffusion-based models, underscoring the significance of our\nimprovements.",
    "categories": [
      "cs.CV",
      "astro-ph.IM",
      "cs.AI",
      "eess.IV"
    ],
    "authors": [
      "Suraj Singh",
      "Anastasia Batsheva",
      "Oleg Y. Rogov",
      "Ahmed Bouridane"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15983v1",
    "title": "InhibiDistilbert: Knowledge Distillation for a ReLU and Addition-based Transformer",
    "abstract": "This work explores optimizing transformer-based language models by\nintegrating model compression techniques with inhibitor attention, a novel\nalternative attention mechanism. Inhibitor attention employs Manhattan\ndistances and ReLU activations instead of the matrix multiplications and\nsoftmax activation of the conventional scaled dot-product attention. This shift\noffers potential computational and energy savings while maintaining model\neffectiveness. We propose further adjustments to improve the inhibitor\nmechanism's training efficiency and evaluate its performance on the DistilBERT\narchitecture. Our knowledge distillation experiments indicate that the modified\ninhibitor transformer model can achieve competitive performance on standard NLP\nbenchmarks, including General Language Understanding Evaluation (GLUE) and\nsentiment analysis tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "68T50 (Primary) 68T07, 68Q32 (Secondary)",
      "I.2.6; I.2.7; I.5.1"
    ],
    "authors": [
      "Tony Zhang",
      "Rickard Brännvall"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15979v1",
    "title": "Exploratory Study into Relations between Cognitive Distortions and Emotional Appraisals",
    "abstract": "In recent years, there has been growing interest in studying cognitive\ndistortions and emotional appraisals from both computational and psychological\nperspectives. Despite considerable similarities between emotional reappraisal\nand cognitive reframing as emotion regulation techniques, these concepts have\nlargely been examined in isolation. This research explores the relationship\nbetween cognitive distortions and emotional appraisal dimensions, examining\ntheir potential connections and relevance for future interdisciplinary studies.\nUnder this pretext, we conduct an exploratory computational study, aimed at\ninvestigating the relationship between cognitive distortion and emotional\nappraisals. We show that the patterns of statistically significant\nrelationships between cognitive distortions and appraisal dimensions vary\nacross different distortion categories, giving rise to distinct appraisal\nprofiles for individual distortion classes. Additionally, we analyze the impact\nof cognitive restructuring on appraisal dimensions, exemplifying the emotion\nregulation aspect of cognitive restructuring.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Navneet Agarwal",
      "Kairit Sirts"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15978v1",
    "title": "A Survey on fMRI-based Brain Decoding for Reconstructing Multimodal Stimuli",
    "abstract": "In daily life, we encounter diverse external stimuli, such as images, sounds,\nand videos. As research in multimodal stimuli and neuroscience advances,\nfMRI-based brain decoding has become a key tool for understanding brain\nperception and its complex cognitive processes. Decoding brain signals to\nreconstruct stimuli not only reveals intricate neural mechanisms but also\ndrives progress in AI, disease treatment, and brain-computer interfaces. Recent\nadvancements in neuroimaging and image generation models have significantly\nimproved fMRI-based decoding. While fMRI offers high spatial resolution for\nprecise brain activity mapping, its low temporal resolution and signal noise\npose challenges. Meanwhile, techniques like GANs, VAEs, and Diffusion Models\nhave enhanced reconstructed image quality, and multimodal pre-trained models\nhave boosted cross-modal decoding tasks. This survey systematically reviews\nrecent progress in fMRI-based brain decoding, focusing on stimulus\nreconstruction from passive brain signals. It summarizes datasets, relevant\nbrain regions, and categorizes existing methods by model structure.\nAdditionally, it evaluates model performance and discusses their effectiveness.\nFinally, it identifies key challenges and proposes future research directions,\noffering valuable insights for the field. For more information and resources\nrelated to this survey, visit https://github.com/LpyNow/BrainDecodingImage.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Pengyu Liu",
      "Guohua Dong",
      "Dan Guo",
      "Kun Li",
      "Fengling Li",
      "Xun Yang",
      "Meng Wang",
      "Xiaomin Ying"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15975v1",
    "title": "Acc3D: Accelerating Single Image to 3D Diffusion Models via Edge Consistency Guided Score Distillation",
    "abstract": "We present Acc3D to tackle the challenge of accelerating the diffusion\nprocess to generate 3D models from single images. To derive high-quality\nreconstructions through few-step inferences, we emphasize the critical issue of\nregularizing the learning of score function in states of random noise. To this\nend, we propose edge consistency, i.e., consistent predictions across the high\nsignal-to-noise ratio region, to enhance a pre-trained diffusion model,\nenabling a distillation-based refinement of the endpoint score function.\nBuilding on those distilled diffusion models, we propose an adversarial\naugmentation strategy to further enrich the generation detail and boost overall\ngeneration quality. The two modules complement each other, mutually reinforcing\nto elevate generative performance. Extensive experiments demonstrate that our\nAcc3D not only achieves over a $20\\times$ increase in computational efficiency\nbut also yields notable quality improvements, compared to the\nstate-of-the-arts.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Kendong Liu",
      "Zhiyu Zhu",
      "Hui Liu",
      "Junhui Hou"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15973v1",
    "title": "STOP: Integrated Spatial-Temporal Dynamic Prompting for Video Understanding",
    "abstract": "Pre-trained on tremendous image-text pairs, vision-language models like CLIP\nhave demonstrated promising zero-shot generalization across numerous\nimage-based tasks. However, extending these capabilities to video tasks remains\nchallenging due to limited labeled video data and high training costs. Recent\nvideo prompting methods attempt to adapt CLIP for video tasks by introducing\nlearnable prompts, but they typically rely on a single static prompt for all\nvideo sequences, overlooking the diverse temporal dynamics and spatial\nvariations that exist across frames. This limitation significantly hinders the\nmodel's ability to capture essential temporal information for effective video\nunderstanding. To address this, we propose an integrated Spatial-TempOral\ndynamic Prompting (STOP) model which consists of two complementary modules, the\nintra-frame spatial prompting and inter-frame temporal prompting. Our\nintra-frame spatial prompts are designed to adaptively highlight discriminative\nregions within each frame by leveraging intra-frame attention and temporal\nvariation, allowing the model to focus on areas with substantial temporal\ndynamics and capture fine-grained spatial details. Additionally, to highlight\nthe varying importance of frames for video understanding, we further introduce\ninter-frame temporal prompts, dynamically inserting prompts between frames with\nhigh temporal variance as measured by frame similarity. This enables the model\nto prioritize key frames and enhances its capacity to understand temporal\ndependencies across sequences. Extensive experiments on various video\nbenchmarks demonstrate that STOP consistently achieves superior performance\nagainst state-of-the-art methods. The code is available at\nhttps://github.com/zhoujiahuan1991/CVPR2025-STOP.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Zichen Liu",
      "Kunlun Xu",
      "Bing Su",
      "Xu Zou",
      "Yuxin Peng",
      "Jiahuan Zhou"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15972v1",
    "title": "TVineSynth: A Truncated C-Vine Copula Generator of Synthetic Tabular Data to Balance Privacy and Utility",
    "abstract": "We propose TVineSynth, a vine copula based synthetic tabular data generator,\nwhich is designed to balance privacy and utility, using the vine tree structure\nand its truncation to do the trade-off. Contrary to synthetic data generators\nthat achieve DP by globally adding noise, TVineSynth performs a controlled\napproximation of the estimated data generating distribution, so that it does\nnot suffer from poor utility of the resulting synthetic data for downstream\nprediction tasks. TVineSynth introduces a targeted bias into the vine copula\nmodel that, combined with the specific tree structure of the vine, causes the\nmodel to zero out privacy-leaking dependencies while relying on those that are\nbeneficial for utility. Privacy is here measured with membership (MIA) and\nattribute inference attacks (AIA). Further, we theoretically justify how the\nconstruction of TVineSynth ensures AIA privacy under a natural privacy measure\nfor continuous sensitive attributes. When compared to competitor models, with\nand without DP, on simulated and on real-world data, TVineSynth achieves a\nsuperior privacy-utility balance.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Elisabeth Griesbauer",
      "Claudia Czado",
      "Arnoldo Frigessi",
      "Ingrid Hobæk Haff"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15970v1",
    "title": "V-NAW: Video-based Noise-aware Adaptive Weighting for Facial Expression Recognition",
    "abstract": "Facial Expression Recognition (FER) plays a crucial role in human affective\nanalysis and has been widely applied in computer vision tasks such as\nhuman-computer interaction and psychological assessment. The 8th Affective\nBehavior Analysis in-the-Wild (ABAW) Challenge aims to assess human emotions\nusing the video-based Aff-Wild2 dataset. This challenge includes various tasks,\nincluding the video-based EXPR recognition track, which is our primary focus.\nIn this paper, we demonstrate that addressing label ambiguity and class\nimbalance, which are known to cause performance degradation, can lead to\nmeaningful performance improvements. Specifically, we propose Video-based\nNoise-aware Adaptive Weighting (V-NAW), which adaptively assigns importance to\neach frame in a clip to address label ambiguity and effectively capture\ntemporal variations in facial expressions. Furthermore, we introduce a simple\nand effective augmentation strategy to reduce redundancy between consecutive\nframes, which is a primary cause of overfitting. Through extensive experiments,\nwe validate the effectiveness of our approach, demonstrating significant\nimprovements in video-based FER performance.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "JunGyu Lee",
      "Kunyoung Lee",
      "Haesol Park",
      "Ig-Jae Kim",
      "Gi Pyo Nam"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15969v1",
    "title": "Beyond the Visible: Multispectral Vision-Language Learning for Earth Observation",
    "abstract": "Vision-language models for Earth observation (EO) typically rely on the\nvisual spectrum of data as the only model input, thus failing to leverage the\nrich spectral information available in the multispectral channels recorded by\nsatellites. Therefore, in this paper, we introduce Llama3-MS-CLIP, the first\nvision-language model pre-trained with contrastive learning on a large-scale\nmultispectral dataset and report on the performance gains due to the extended\nspectral range. Furthermore, we present the largest-to-date image-caption\ndataset for multispectral data, consisting of one million Sentinel-2 samples\nand corresponding textual descriptions generated with Llama3-LLaVA-Next and\nOverture Maps data. We develop a scalable captioning pipeline, which is\nvalidated by domain experts. We evaluate Llama3-MS-CLIP on multispectral\nzero-shot image classification and retrieval using three datasets of varying\ncomplexity. Our results demonstrate that Llama3-MS-CLIP significantly\noutperforms other RGB-based approaches, improving classification accuracy by\n6.77% on average and retrieval performance by 4.63% mAP compared to the\nsecond-best model. Our results emphasize the relevance of multispectral\nvision-language learning. We release the image-caption dataset, code, and model\nweights under an open-source license.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Clive Tinashe Marimo",
      "Benedikt Blumenstiel",
      "Maximilian Nitsche",
      "Johannes Jakubik",
      "Thomas Brunschwiler"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16572v1",
    "title": "Efficient ANN-Guided Distillation: Aligning Rate-based Features of Spiking Neural Networks through Hybrid Block-wise Replacement",
    "abstract": "Spiking Neural Networks (SNNs) have garnered considerable attention as a\npotential alternative to Artificial Neural Networks (ANNs). Recent studies have\nhighlighted SNNs' potential on large-scale datasets. For SNN training, two main\napproaches exist: direct training and ANN-to-SNN (ANN2SNN) conversion. To fully\nleverage existing ANN models in guiding SNN learning, either direct ANN-to-SNN\nconversion or ANN-SNN distillation training can be employed. In this paper, we\npropose an ANN-SNN distillation framework from the ANN-to-SNN perspective,\ndesigned with a block-wise replacement strategy for ANN-guided learning. By\ngenerating intermediate hybrid models that progressively align SNN feature\nspaces to those of ANN through rate-based features, our framework naturally\nincorporates rate-based backpropagation as a training method. Our approach\nachieves results comparable to or better than state-of-the-art SNN distillation\nmethods, showing both training and learning efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Shu Yang",
      "Chengting Yu",
      "Lei Liu",
      "Hanzhi Ma",
      "Aili Wang",
      "Erping Li"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15962v1",
    "title": "Information maximization for a broad variety of multi-armed bandit games",
    "abstract": "Information and free-energy maximization are physics principles that provide\ngeneral rules for an agent to optimize actions in line with specific goals and\npolicies. These principles are the building blocks for designing\ndecision-making policies capable of efficient performance with only partial\ninformation. Notably, the information maximization principle has shown\nremarkable success in the classical bandit problem and has recently been shown\nto yield optimal algorithms for Gaussian and sub-Gaussian reward distributions.\nThis article explores a broad extension of physics-based approaches to more\ncomplex and structured bandit problems. To this end, we cover three distinct\ntypes of bandit problems, where information maximization is adapted and leads\nto strong performance. Since the main challenge of information maximization\nlies in avoiding over-exploration, we highlight how information is tailored at\nvarious levels to mitigate this issue, paving the way for more efficient and\nrobust decision-making strategies.",
    "categories": [
      "cs.LG",
      "cond-mat.stat-mech",
      "stat.ML"
    ],
    "authors": [
      "Alex Barbier-Chebbah",
      "Christian L. Vestergaard",
      "Jean-Baptiste Masson"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15953v1",
    "title": "GAN-enhanced Simulation-driven DNN Testing in Absence of Ground Truth",
    "abstract": "The generation of synthetic inputs via simulators driven by search algorithms\nis essential for cost-effective testing of Deep Neural Network (DNN) components\nfor safety-critical systems. However, in many applications, simulators are\nunable to produce the ground-truth data needed for automated test oracles and\nto guide the search process.\n  To tackle this issue, we propose an approach for the generation of inputs for\ncomputer vision DNNs that integrates a generative network to ensure simulator\nfidelity and employs heuristic-based search fitnesses that leverage\ntransformation consistency, noise resistance, surprise adequacy, and\nuncertainty estimation. We compare the performance of our fitnesses with that\nof a traditional fitness function leveraging ground truth; further, we assess\nhow the integration of a GAN not leveraging the ground truth impacts on test\nand retraining effectiveness.\n  Our results suggest that leveraging transformation consistency is the best\noption to generate inputs for both DNN testing and retraining; it maximizes\ninput diversity, spots the inputs leading to worse DNN performance, and leads\nto best DNN performance after retraining. Besides enabling simulator-based\ntesting in the absence of ground truth, our findings pave the way for testing\nsolutions that replace costly simulators with diffusion and large language\nmodels, which might be more affordable than simulators, but cannot generate\nground-truth data.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Mohammed Attaoui",
      "Fabrizio Pastore"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15952v1",
    "title": "Adaptive Group Policy Optimization: Towards Stable Training and Token-Efficient Reasoning",
    "abstract": "Since DeepSeek-R1 popularized, Group Relative Policy Optimization (GRPO) has\nbecome the core part of Reasoning LLMs training. However, we find some\ndeficiency that influences RL stability and inference efficiency. Thus, we\npropose Adaptive Group Policy Optimization (AGPO) which contains two simple but\neffective modifications: a revised advantage estimation method to mitigate\nzero-variance situations; a length-based reward, incentivizing the model to\navoid overthinking. The experiments demonstrate our methods achieve more stable\ntraining and comparable or superior performance with significantly fewer tokens\nin reasoning steps.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Chen Li",
      "Nazhou Liu",
      "Kai Yang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15949v1",
    "title": "CausalCLIPSeg: Unlocking CLIP's Potential in Referring Medical Image Segmentation with Causal Intervention",
    "abstract": "Referring medical image segmentation targets delineating lesions indicated by\ntextual descriptions. Aligning visual and textual cues is challenging due to\ntheir distinct data properties. Inspired by large-scale pre-trained\nvision-language models, we propose CausalCLIPSeg, an end-to-end framework for\nreferring medical image segmentation that leverages CLIP. Despite not being\ntrained on medical data, we enforce CLIP's rich semantic space onto the medical\ndomain by a tailored cross-modal decoding method to achieve text-to-pixel\nalignment. Furthermore, to mitigate confounding bias that may cause the model\nto learn spurious correlations instead of meaningful causal relationships,\nCausalCLIPSeg introduces a causal intervention module which self-annotates\nconfounders and excavates causal features from inputs for segmentation\njudgments. We also devise an adversarial min-max game to optimize causal\nfeatures while penalizing confounding ones. Extensive experiments demonstrate\nthe state-of-the-art performance of our proposed method. Code is available at\nhttps://github.com/WUTCM-Lab/CausalCLIPSeg.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yaxiong Chen",
      "Minghong Wei",
      "Zixuan Zheng",
      "Jingliang Hu",
      "Yilei Shi",
      "Shengwu Xiong",
      "Xiao Xiang Zhu",
      "Lichao Mou"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15948v1",
    "title": "Don't Fight Hallucinations, Use Them: Estimating Image Realism using NLI over Atomic Facts",
    "abstract": "Quantifying the realism of images remains a challenging problem in the field\nof artificial intelligence. For example, an image of Albert Einstein holding a\nsmartphone violates common-sense because modern smartphone were invented after\nEinstein's death. We introduce a novel method for assessing image realism using\nLarge Vision-Language Models (LVLMs) and Natural Language Inference (NLI). Our\napproach is based on the premise that LVLMs may generate hallucinations when\nconfronted with images that defy common sense. Using LVLM to extract atomic\nfacts from these images, we obtain a mix of accurate facts and erroneous\nhallucinations. We proceed by calculating pairwise entailment scores among\nthese facts, subsequently aggregating these values to yield a singular reality\nscore. This process serves to identify contradictions between genuine facts and\nhallucinatory elements, signaling the presence of images that violate common\nsense. Our approach has achieved a new state-of-the-art performance in\nzero-shot mode on the WHOOPS! dataset.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Elisei Rykov",
      "Kseniia Petrushina",
      "Kseniia Titova",
      "Alexander Panchenko",
      "Vasily Konovalov"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15947v1",
    "title": "Unreal-MAP: Unreal-Engine-Based General Platform for Multi-Agent Reinforcement Learning",
    "abstract": "In this paper, we propose Unreal Multi-Agent Playground (Unreal-MAP), an MARL\ngeneral platform based on the Unreal-Engine (UE). Unreal-MAP allows users to\nfreely create multi-agent tasks using the vast visual and physical resources\navailable in the UE community, and deploy state-of-the-art (SOTA) MARL\nalgorithms within them. Unreal-MAP is user-friendly in terms of deployment,\nmodification, and visualization, and all its components are open-source. We\nalso develop an experimental framework compatible with algorithms ranging from\nrule-based to learning-based provided by third-party frameworks. Lastly, we\ndeploy several SOTA algorithms in example tasks developed via Unreal-MAP, and\nconduct corresponding experimental analyses. We believe Unreal-MAP can play an\nimportant role in the MARL field by closely integrating existing algorithms\nwith user-customized tasks, thus advancing the field of MARL.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Tianyi Hu",
      "Qingxu Fu",
      "Zhiqiang Pu",
      "Yuan Wang",
      "Tenghai Qiu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15946v1",
    "title": "Multivariate Time Series Anomaly Detection in Industry 5.0",
    "abstract": "Industry5.0 environments present a critical need for effective anomaly\ndetection methods that can indicate equipment malfunctions, process\ninefficiencies, or potential safety hazards. The ever-increasing sensorization\nof manufacturing lines makes processes more observable, but also poses the\nchallenge of continuously analyzing vast amounts of multivariate time series\ndata. These challenges include data quality since data may contain noise, be\nunlabeled or even mislabeled. A promising approach consists of combining an\nembedding model with other Machine Learning algorithms to enhance the overall\nperformance in detecting anomalies. Moreover, representing time series as\nvectors brings many advantages like higher flexibility and improved ability to\ncapture complex temporal dependencies. We tested our solution in a real\nindustrial use case, using data collected from a Bonfiglioli plant. The results\ndemonstrate that, unlike traditional reconstruction-based autoencoders, which\noften struggle in the presence of sporadic noise, our embedding-based framework\nmaintains high performance across various noise conditions.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Lorenzo Colombi",
      "Michela Vespa",
      "Nicolas Belletti",
      "Matteo Brina",
      "Simon Dahdal",
      "Filippo Tabanelli",
      "Elena Bellodi",
      "Mauro Tortonesi",
      "Cesare Stefanelli",
      "Massimiliano Vignoli"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15944v1",
    "title": "From Chaos to Order: The Atomic Reasoner Framework for Fine-grained Reasoning in Large Language Models",
    "abstract": "Recent advances in large language models (LLMs) have shown remarkable\nprogress, yet their capacity for logical ``slow-thinking'' reasoning persists\nas a critical research frontier. Current inference scaling paradigms suffer\nfrom two fundamental constraints: fragmented thought flows compromising logical\ncoherence, and intensively computational complexity that escalates with search\nspace dimensions. To overcome these limitations, we present \\textbf{Atomic\nReasoner} (\\textbf{AR}), a cognitive inference strategy that enables\nfine-grained reasoning through systematic atomic-level operations. AR\ndecomposes the reasoning process into atomic cognitive units, employing a\ncognitive routing mechanism to dynamically construct reasoning representations\nand orchestrate inference pathways. This systematic methodology implements\nstepwise, structured cognition, which ensures logical coherence while\nsignificantly reducing cognitive load, effectively simulating the cognitive\npatterns observed in human deep thinking processes. Extensive experimental\nresults demonstrate AR's superior reasoning capabilities without the\ncomputational burden of exhaustive solution searches, particularly excelling in\nlinguistic logic puzzles. These findings substantiate AR's effectiveness in\nenhancing LLMs' capacity for robust, long-sequence logical reasoning and\ndeliberation.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Jinyi Liu",
      "Yan Zheng",
      "Rong Cheng",
      "Qiyu Wu",
      "Wei Guo",
      "Fei Ni",
      "Hebin Liang",
      "Yifu Yuan",
      "Hangyu Mao",
      "Fuzheng Zhang",
      "Jianye Hao"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15940v1",
    "title": "UniCrossAdapter: Multimodal Adaptation of CLIP for Radiology Report Generation",
    "abstract": "Automated radiology report generation aims to expedite the tedious and\nerror-prone reporting process for radiologists. While recent works have made\nprogress, learning to align medical images and textual findings remains\nchallenging due to the relative scarcity of labeled medical data. For example,\ndatasets for this task are much smaller than those used for image captioning in\ncomputer vision. In this work, we propose to transfer representations from\nCLIP, a large-scale pre-trained vision-language model, to better capture\ncross-modal semantics between images and texts. However, directly applying CLIP\nis suboptimal due to the domain gap between natural images and radiology. To\nenable efficient adaptation, we introduce UniCrossAdapter, lightweight adapter\nmodules that are incorporated into CLIP and fine-tuned on the target task while\nkeeping base parameters fixed. The adapters are distributed across modalities\nand their interaction to enhance vision-language alignment. Experiments on two\npublic datasets demonstrate the effectiveness of our approach, advancing\nstate-of-the-art in radiology report generation. The proposed transfer learning\nframework provides a means of harnessing semantic knowledge from large-scale\npre-trained models to tackle data-scarce medical vision-language tasks. Code is\navailable at https://github.com/chauncey-tow/MRG-CLIP.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yaxiong Chen",
      "Chuang Du",
      "Chunlei Li",
      "Jingliang Hu",
      "Yilei Shi",
      "Shengwu Xiong",
      "Xiao Xiang Zhu",
      "Lichao Mou"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15937v2",
    "title": "Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical Deployment",
    "abstract": "We propose V-Droid, a mobile GUI task automation agent. Unlike previous\nmobile agents that utilize Large Language Models (LLMs) as generators to\ndirectly generate actions at each step, V-Droid employs LLMs as verifiers to\nevaluate candidate actions before making final decisions. To realize this novel\nparadigm, we introduce a comprehensive framework for constructing\nverifier-driven mobile agents: the discretized action space construction\ncoupled with the prefilling-only workflow to accelerate the verification\nprocess, the pair-wise progress preference training to significantly enhance\nthe verifier's decision-making capabilities, and the scalable human-agent joint\nannotation scheme to efficiently collect the necessary data at scale. V-Droid\nsets a new state-of-the-art task success rate across several public mobile task\nautomation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49% on\nMobileAgentBench, surpassing existing agents by 9.5%, 2.1%, and 9%,\nrespectively. Furthermore, V-Droid achieves an impressively low latency of 0.7\nseconds per step, making it the first mobile agent capable of delivering\nnear-real-time, effective decision-making capabilities.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Gaole Dai",
      "Shiqi Jiang",
      "Ting Cao",
      "Yuanchun Li",
      "Yuqing Yang",
      "Rui Tan",
      "Mo Li",
      "Lili Qiu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.15934v1",
    "title": "SaMam: Style-aware State Space Model for Arbitrary Image Style Transfer",
    "abstract": "Global effective receptive field plays a crucial role for image style\ntransfer (ST) to obtain high-quality stylized results. However, existing ST\nbackbones (e.g., CNNs and Transformers) suffer huge computational complexity to\nachieve global receptive fields. Recently, the State Space Model (SSM),\nespecially the improved variant Mamba, has shown great potential for long-range\ndependency modeling with linear complexity, which offers a approach to resolve\nthe above dilemma. In this paper, we develop a Mamba-based style transfer\nframework, termed SaMam. Specifically, a mamba encoder is designed to\nefficiently extract content and style information. In addition, a style-aware\nmamba decoder is developed to flexibly adapt to various styles. Moreover, to\naddress the problems of local pixel forgetting, channel redundancy and spatial\ndiscontinuity of existing SSMs, we introduce both local enhancement and zigzag\nscan. Qualitative and quantitative results demonstrate that our SaMam\noutperforms state-of-the-art methods in terms of both accuracy and efficiency.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Hongda Liu",
      "Longguang Wang",
      "Ye Zhang",
      "Ziru Yu",
      "Yulan Guo"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15931v1",
    "title": "DnLUT: Ultra-Efficient Color Image Denoising via Channel-Aware Lookup Tables",
    "abstract": "While deep neural networks have revolutionized image denoising capabilities,\ntheir deployment on edge devices remains challenging due to substantial\ncomputational and memory requirements. To this end, we present DnLUT, an\nultra-efficient lookup table-based framework that achieves high-quality color\nimage denoising with minimal resource consumption. Our key innovation lies in\ntwo complementary components: a Pairwise Channel Mixer (PCM) that effectively\ncaptures inter-channel correlations and spatial dependencies in parallel, and a\nnovel L-shaped convolution design that maximizes receptive field coverage while\nminimizing storage overhead. By converting these components into optimized\nlookup tables post-training, DnLUT achieves remarkable efficiency - requiring\nonly 500KB storage and 0.1% energy consumption compared to its CNN contestant\nDnCNN, while delivering 20X faster inference. Extensive experiments demonstrate\nthat DnLUT outperforms all existing LUT-based methods by over 1dB in PSNR,\nestablishing a new state-of-the-art in resource-efficient color image\ndenoising. The project is available at https://github.com/Stephen0808/DnLUT.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Sidi Yang",
      "Binxiao Huang",
      "Yulun Zhang",
      "Dahai Yu",
      "Yujiu Yang",
      "Ngai Wong"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15928v2",
    "title": "Sample-Efficient Bayesian Transfer Learning for Online Machine Parameter Optimization",
    "abstract": "Correctly setting the parameters of a production machine is essential to\nimprove product quality, increase efficiency, and reduce production costs while\nalso supporting sustainability goals. Identifying optimal parameters involves\nan iterative process of producing an object and evaluating its quality.\nMinimizing the number of iterations is, therefore, desirable to reduce the\ncosts associated with unsuccessful attempts. This work introduces a method to\noptimize the machine parameters in the system itself using a Bayesian\noptimization algorithm. By leveraging existing machine data, we use a transfer\nlearning approach in order to identify an optimum with minimal iterations,\nresulting in a cost-effective transfer learning algorithm. We validate our\napproach on a laser machine for cutting sheet metal in the real world.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Philipp Wagner",
      "Tobias Nagel",
      "Philipp Leube",
      "Marco F. Huber"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.15927v1",
    "title": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to Accelerate Diffusion Transformers",
    "abstract": "Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Hui Zhang",
      "Tingwei Gao",
      "Jie Shao",
      "Zuxuan Wu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16567v1",
    "title": "Exploring Deep Learning Models for EEG Neural Decoding",
    "abstract": "Neural decoding is an important method in cognitive neuroscience that aims to\ndecode brain representations from recorded neural activity using a multivariate\nmachine learning model. The THINGS initiative provides a large EEG dataset of\n46 subjects watching rapidly shown images. Here, we test the feasibility of\nusing this method for decoding high-level object features using recent deep\nlearning models. We create a derivative dataset from this of living vs\nnon-living entities test 15 different deep learning models with 5 different\narchitectures and compare to a SOTA linear model. We show that the linear model\nis not able to solve the decoding task, while almost all the deep learning\nmodels are successful, suggesting that in some cases non-linear models are\nneeded to decode neural representations. We also run a comparative study of the\nmodels' performance on individual object categories, and suggest how artificial\nneural networks can be used to study brain activity.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Laurits Dixen",
      "Stefan Heinrich",
      "Paolo Burelli"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15924v1",
    "title": "Towards Automatic Continual Learning: A Self-Adaptive Framework for Continual Instruction Tuning",
    "abstract": "Continual instruction tuning enables large language models (LLMs) to learn\nincrementally while retaining past knowledge, whereas existing methods\nprimarily focus on how to retain old knowledge rather than on selecting which\nnew knowledge to learn. In domain-specific contexts, maintaining data quality\nand managing system constraints remain key challenges. To address these issues,\nwe propose an automated continual instruction tuning framework that dynamically\nfilters incoming data, which identify and reduce redundant data across\nsuccessive updates. Our approach utilizes a small proxy model for efficient\nperplexity-based filtering, and updates the proxy to ensure that the filtering\ncriteria remain aligned with the evolving state of the deployed model. Compared\nto existing static data selection methods, our framework can effectively handle\nincrementally acquired data and shifting distributions. Additionally, it\naddresses practical deployment challenges by enabling seamless model updates,\nsupporting version rollback and incorporating automatic checkpoint evaluation.\nWe evaluated the system in real-world medical scenarios. It reduced\ncomputational costs by 66.7% and improved model performance, and achieved\nautonomous updates, thus demonstrating its effectiveness for automatic\ncontinual instruction tuning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Peiyi Lin",
      "Fukai Zhang",
      "Kai Niu",
      "Hao Fu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16566v1",
    "title": "REVAL: A Comprehension Evaluation on Reliability and Values of Large Vision-Language Models",
    "abstract": "The rapid evolution of Large Vision-Language Models (LVLMs) has highlighted\nthe necessity for comprehensive evaluation frameworks that assess these models\nacross diverse dimensions. While existing benchmarks focus on specific aspects\nsuch as perceptual abilities, cognitive capabilities, and safety against\nadversarial attacks, they often lack the breadth and depth required to provide\na holistic understanding of LVLMs' strengths and limitations. To address this\ngap, we introduce REVAL, a comprehensive benchmark designed to evaluate the\n\\textbf{RE}liability and \\textbf{VAL}ue of LVLMs. REVAL encompasses over 144K\nimage-text Visual Question Answering (VQA) samples, structured into two primary\nsections: Reliability, which assesses truthfulness (\\eg, perceptual accuracy\nand hallucination tendencies) and robustness (\\eg, resilience to adversarial\nattacks, typographic attacks, and image corruption), and Values, which\nevaluates ethical concerns (\\eg, bias and moral understanding), safety issues\n(\\eg, toxicity and jailbreak vulnerabilities), and privacy problems (\\eg,\nprivacy awareness and privacy leakage). We evaluate 26 models, including\nmainstream open-source LVLMs and prominent closed-source models like GPT-4o and\nGemini-1.5-Pro. Our findings reveal that while current LVLMs excel in\nperceptual tasks and toxicity avoidance, they exhibit significant\nvulnerabilities in adversarial scenarios, privacy preservation, and ethical\nreasoning. These insights underscore critical areas for future improvements,\nguiding the development of more secure, reliable, and ethically aligned LVLMs.\nREVAL provides a robust framework for researchers to systematically assess and\ncompare LVLMs, fostering advancements in the field.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jie Zhang",
      "Zheng Yuan",
      "Zhongqi Wang",
      "Bei Yan",
      "Sibo Wang",
      "Xiangkui Cao",
      "Zonghui Guo",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15918v1",
    "title": "Denoising-based Contractive Imitation Learning",
    "abstract": "A fundamental challenge in imitation learning is the \\emph{covariate shift}\nproblem. Existing methods to mitigate covariate shift often require additional\nexpert interactions, access to environment dynamics, or complex adversarial\ntraining, which may not be practical in real-world applications. In this paper,\nwe propose a simple yet effective method (DeCIL) to mitigate covariate shift by\nincorporating a denoising mechanism that enhances the contraction properties of\nthe state transition mapping. Our approach involves training two neural\nnetworks: a dynamics model ( f ) that predicts the next state from the current\nstate, and a joint state-action denoising policy network ( d ) that refines\nthis state prediction via denoising and outputs the corresponding action. We\nprovide theoretical analysis showing that the denoising network acts as a local\ncontraction mapping, reducing the error propagation of the state transition and\nimproving stability. Our method is straightforward to implement and can be\neasily integrated with existing imitation learning frameworks without requiring\nadditional expert data or complex modifications to the training procedure.\nEmpirical results demonstrate that our approach effectively improves success\nrate of various imitation learning tasks under noise perturbation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Macheng Shen",
      "Jishen Peng",
      "Zefang Huang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15917v1",
    "title": "Learning to Efficiently Adapt Foundation Models for Self-Supervised Endoscopic 3D Scene Reconstruction from Any Cameras",
    "abstract": "Accurate 3D scene reconstruction is essential for numerous medical tasks.\nGiven the challenges in obtaining ground truth data, there has been an\nincreasing focus on self-supervised learning (SSL) for endoscopic depth\nestimation as a basis for scene reconstruction. While foundation models have\nshown remarkable progress in visual tasks, their direct application to the\nmedical domain often leads to suboptimal results. However, the visual features\nfrom these models can still enhance endoscopic tasks, emphasizing the need for\nefficient adaptation strategies, which still lack exploration currently. In\nthis paper, we introduce Endo3DAC, a unified framework for endoscopic scene\nreconstruction that efficiently adapts foundation models. We design an\nintegrated network capable of simultaneously estimating depth maps, relative\nposes, and camera intrinsic parameters. By freezing the backbone foundation\nmodel and training only the specially designed Gated Dynamic Vector-Based\nLow-Rank Adaptation (GDV-LoRA) with separate decoder heads, Endo3DAC achieves\nsuperior depth and pose estimation while maintaining training efficiency.\nAdditionally, we propose a 3D scene reconstruction pipeline that optimizes\ndepth maps' scales, shifts, and a few parameters based on our integrated\nnetwork. Extensive experiments across four endoscopic datasets demonstrate that\nEndo3DAC significantly outperforms other state-of-the-art methods while\nrequiring fewer trainable parameters. To our knowledge, we are the first to\nutilize a single network that only requires surgical videos to perform both SSL\ndepth estimation and scene reconstruction tasks. The code will be released upon\nacceptance.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Beilei Cui",
      "Long Bai",
      "Mobarakol Islam",
      "An Wang",
      "Zhiqi Ma",
      "Yiming Huang",
      "Feng Li",
      "Zhen Chen",
      "Zhongliang Jiang",
      "Nassir Navab",
      "Hongliang Ren"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15914v1",
    "title": "Text-Driven Diffusion Model for Sign Language Production",
    "abstract": "We introduce the hfut-lmc team's solution to the SLRTP Sign Production\nChallenge. The challenge aims to generate semantically aligned sign language\npose sequences from text inputs. To this end, we propose a Text-driven\nDiffusion Model (TDM) framework. During the training phase, TDM utilizes an\nencoder to encode text sequences and incorporates them into the diffusion model\nas conditional input to generate sign pose sequences. To guarantee the high\nquality and accuracy of the generated pose sequences, we utilize two key loss\nfunctions. The joint loss function L_{joint} is used to precisely measure and\nminimize the differences between the joint positions of the generated pose\nsequences and those of the ground truth. Similarly, the bone orientation loss\nfunction L_{bone} is instrumental in ensuring that the orientation of the bones\nin the generated poses aligns with the actual, correct orientations. In the\ninference stage, the TDM framework takes on a different yet equally important\ntask. It starts with noisy sequences and, under the strict constraints of the\ntext conditions, gradually refines and generates semantically consistent sign\nlanguage pose sequences. Our carefully designed framework performs well on the\nsign language production task, and our solution achieves a BLEU-1 score of\n20.17, placing second in the challenge.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jiayi He",
      "Xu Wang",
      "Ruobei Zhang",
      "Shengeng Tang",
      "Yaxiong Wang",
      "Lechao Cheng"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15910v1",
    "title": "No Thing, Nothing: Highlighting Safety-Critical Classes for Robust LiDAR Semantic Segmentation in Adverse Weather",
    "abstract": "Existing domain generalization methods for LiDAR semantic segmentation under\nadverse weather struggle to accurately predict \"things\" categories compared to\n\"stuff\" categories. In typical driving scenes, \"things\" categories can be\ndynamic and associated with higher collision risks, making them crucial for\nsafe navigation and planning. Recognizing the importance of \"things\"\ncategories, we identify their performance drop as a serious bottleneck in\nexisting approaches. We observed that adverse weather induces degradation of\nsemantic-level features and both corruption of local features, leading to a\nmisprediction of \"things\" as \"stuff\". To mitigate these corruptions, we suggest\nour method, NTN - segmeNt Things for No-accident. To address semantic-level\nfeature corruption, we bind each point feature to its superclass, preventing\nthe misprediction of things classes into visually dissimilar categories.\nAdditionally, to enhance robustness against local corruption caused by adverse\nweather, we define each LiDAR beam as a local region and propose a\nregularization term that aligns the clean data with its corrupted counterpart\nin feature space. NTN achieves state-of-the-art performance with a +2.6 mIoU\ngain on the SemanticKITTI-to-SemanticSTF benchmark and +7.9 mIoU on the\nSemanticPOSS-to-SemanticSTF benchmark. Notably, NTN achieves a +4.8 and +7.9\nmIoU improvement on \"things\" classes, respectively, highlighting its\neffectiveness.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Junsung Park",
      "Hwijeong Lee",
      "Inha Kang",
      "Hyunjung Shim"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15908v1",
    "title": "Enhancing Close-up Novel View Synthesis via Pseudo-labeling",
    "abstract": "Recent methods, such as Neural Radiance Fields (NeRF) and 3D Gaussian\nSplatting (3DGS), have demonstrated remarkable capabilities in novel view\nsynthesis. However, despite their success in producing high-quality images for\nviewpoints similar to those seen during training, they struggle when generating\ndetailed images from viewpoints that significantly deviate from the training\nset, particularly in close-up views. The primary challenge stems from the lack\nof specific training data for close-up views, leading to the inability of\ncurrent methods to render these views accurately. To address this issue, we\nintroduce a novel pseudo-label-based learning strategy. This approach leverages\npseudo-labels derived from existing training data to provide targeted\nsupervision across a wide range of close-up viewpoints. Recognizing the absence\nof benchmarks for this specific challenge, we also present a new dataset\ndesigned to assess the effectiveness of both current and future methods in this\narea. Our extensive experiments demonstrate the efficacy of our approach.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Jiatong Xia",
      "Libo Sun",
      "Lingqiao Liu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15905v1",
    "title": "Jasmine: Harnessing Diffusion Prior for Self-supervised Depth Estimation",
    "abstract": "In this paper, we propose Jasmine, the first Stable Diffusion (SD)-based\nself-supervised framework for monocular depth estimation, which effectively\nharnesses SD's visual priors to enhance the sharpness and generalization of\nunsupervised prediction. Previous SD-based methods are all supervised since\nadapting diffusion models for dense prediction requires high-precision\nsupervision. In contrast, self-supervised reprojection suffers from inherent\nchallenges (e.g., occlusions, texture-less regions, illumination variance), and\nthe predictions exhibit blurs and artifacts that severely compromise SD's\nlatent priors. To resolve this, we construct a novel surrogate task of hybrid\nimage reconstruction. Without any additional supervision, it preserves the\ndetail priors of SD models by reconstructing the images themselves while\npreventing depth estimation from degradation. Furthermore, to address the\ninherent misalignment between SD's scale and shift invariant estimation and\nself-supervised scale-invariant depth estimation, we build the Scale-Shift GRU.\nIt not only bridges this distribution gap but also isolates the fine-grained\ntexture of SD output against the interference of reprojection loss. Extensive\nexperiments demonstrate that Jasmine achieves SoTA performance on the KITTI\nbenchmark and exhibits superior zero-shot generalization across multiple\ndatasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Jiyuan Wang",
      "Chunyu Lin",
      "Cheng Guan",
      "Lang Nie",
      "Jing He",
      "Haodong Li",
      "Kang Liao",
      "Yao Zhao"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15904v1",
    "title": "From Structured Prompts to Open Narratives: Measuring Gender Bias in LLMs Through Open-Ended Storytelling",
    "abstract": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet concerns persist regarding their tendency to reflect or amplify social\nbiases present in their training data. This study introduces a novel evaluation\nframework to uncover gender biases in LLMs, focusing on their occupational\nnarratives. Unlike previous methods relying on structured scenarios or\ncarefully crafted prompts, our approach leverages free-form storytelling to\nreveal biases embedded in the models. Systematic analyses show an\noverrepresentation of female characters across occupations in six widely used\nLLMs. Additionally, our findings reveal that LLM-generated occupational gender\nrankings align more closely with human stereotypes than actual labor\nstatistics. These insights underscore the need for balanced mitigation\nstrategies to ensure fairness while avoiding the reinforcement of new\nstereotypes.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Evan Chen",
      "Run-Jun Zhan",
      "Yan-Bai Lin",
      "Hung-Hsuan Chen"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16565v1",
    "title": "Gene42: Long-Range Genomic Foundation Model With Dense Attention",
    "abstract": "We introduce Gene42, a novel family of Genomic Foundation Models (GFMs)\ndesigned to manage context lengths of up to 192,000 base pairs (bp) at a\nsingle-nucleotide resolution. Gene42 models utilize a decoder-only\n(LLaMA-style) architecture with a dense self-attention mechanism. Initially\ntrained on fixed-length sequences of 4,096 bp, our models underwent continuous\npretraining to extend the context length to 192,000 bp. This iterative\nextension allowed for the comprehensive processing of large-scale genomic data\nand the capture of intricate patterns and dependencies within the human genome.\nGene42 is the first dense attention model capable of handling such extensive\nlong context lengths in genomics, challenging state-space models that often\nrely on convolutional operators among other mechanisms. Our pretrained models\nexhibit notably low perplexity values and high reconstruction accuracy,\nhighlighting their strong ability to model genomic data. Extensive experiments\non various genomic benchmarks have demonstrated state-of-the-art performance\nacross multiple tasks, including biotype classification, regulatory region\nidentification, chromatin profiling prediction, variant pathogenicity\nprediction, and species classification. The models are publicly available at\nhuggingface.co/inceptionai.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Kirill Vishniakov",
      "Boulbaba Ben Amor",
      "Engin Tekin",
      "Nancy A. ElNaker",
      "Karthik Viswanathan",
      "Aleksandr Medvedev",
      "Aahan Singh",
      "Maryam Nadeem",
      "Mohammad Amaan Sayeed",
      "Praveenkumar Kanithi",
      "Tiago Magalhaes",
      "Natalia Vassilieva",
      "Dwarikanath Mahapatra",
      "Marco Pimentel",
      "and Shadab Khan"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16563v1",
    "title": "Chem42: a Family of chemical Language Models for Target-aware Ligand Generation",
    "abstract": "Revolutionizing drug discovery demands more than just understanding molecular\ninteractions - it requires generative models that can design novel ligands\ntailored to specific biological targets. While chemical Language Models (cLMs)\nhave made strides in learning molecular properties, most fail to incorporate\ntarget-specific insights, restricting their ability to drive de-novo ligand\ngeneration. Chem42, a cutting-edge family of generative chemical Language\nModels, is designed to bridge this gap. By integrating atomic-level\ninteractions with multimodal inputs from Prot42, a complementary protein\nLanguage Model, Chem42 achieves a sophisticated cross-modal representation of\nmolecular structures, interactions, and binding patterns. This innovative\nframework enables the creation of structurally valid, synthetically accessible\nligands with enhanced target specificity. Evaluations across diverse protein\ntargets confirm that Chem42 surpasses existing approaches in chemical validity,\ntarget-aware design, and predicted binding affinity. By reducing the search\nspace of viable drug candidates, Chem42 could accelerate the drug discovery\npipeline, offering a powerful generative AI tool for precision medicine. Our\nChem42 models set a new benchmark in molecule property prediction, conditional\nmolecule generation, and target-aware ligand design. The models are publicly\navailable at huggingface.co/inceptionai.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Aahan Singh",
      "Engin Tekin",
      "Maryam Nadeem",
      "Nancy A. ElNaker",
      "Mohammad Amaan Sayeed",
      "Natalia Vassilieva",
      "Boulbaba Ben Amor"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15902v1",
    "title": "On the Limits of Applying Graph Transformers for Brain Connectome Classification",
    "abstract": "Brain connectomes offer detailed maps of neural connections within the brain.\nRecent studies have proposed novel connectome graph datasets and attempted to\nimprove connectome classification by using graph deep learning. With recent\nadvances demonstrating transformers' ability to model intricate relationships\nand outperform in various domains, this work explores their performance on the\nnovel NeuroGraph benchmark datasets and synthetic variants derived from\nprobabilistically removing edges to simulate noisy data. Our findings suggest\nthat graph transformers offer no major advantage over traditional GNNs on this\ndataset. Furthermore, both traditional and transformer GNN models maintain\naccuracy even with all edges removed, suggesting that the dataset's graph\nstructures may not significantly impact predictions. We propose further\nassessing NeuroGraph as a brain connectome benchmark, emphasizing the need for\nwell-curated datasets and improved preprocessing strategies to obtain\nmeaningful edge connections.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Jose Lara-Rangel",
      "Clare Heinbaugh"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15901v1",
    "title": "A multi-model approach using XAI and anomaly detection to predict asteroid hazards",
    "abstract": "The potential for catastrophic collision makes near-Earth asteroids (NEAs) a\nserious concern. Planetary defense depends on accurately classifying\npotentially hazardous asteroids (PHAs), however the complexity of the data\nhampers conventional techniques. This work offers a sophisticated method for\naccurately predicting hazards by combining machine learning, deep learning,\nexplainable AI (XAI), and anomaly detection. Our approach extracts essential\nparameters like size, velocity, and trajectory from historical and real-time\nasteroid data. A hybrid algorithm improves prediction accuracy by combining\nseveral cutting-edge models. A forecasting module predicts future asteroid\nbehavior, and Monte Carlo simulations evaluate the likelihood of collisions.\nTimely mitigation is made possible by a real-time alarm system that notifies\nworldwide monitoring stations. This technique enhances planetary defense\nefforts by combining real-time alarms with sophisticated predictive modeling.",
    "categories": [
      "astro-ph.EP",
      "astro-ph.IM",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Amit Kumar Mondal",
      "Nafisha Aslam",
      "Prasenjit Maji",
      "Hemanta Kumar Mondal"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15898v1",
    "title": "Reconstructing In-the-Wild Open-Vocabulary Human-Object Interactions",
    "abstract": "Reconstructing human-object interactions (HOI) from single images is\nfundamental in computer vision. Existing methods are primarily trained and\ntested on indoor scenes due to the lack of 3D data, particularly constrained by\nthe object variety, making it challenging to generalize to real-world scenes\nwith a wide range of objects. The limitations of previous 3D HOI datasets were\nprimarily due to the difficulty in acquiring 3D object assets. However, with\nthe development of 3D reconstruction from single images, recently it has become\npossible to reconstruct various objects from 2D HOI images. We therefore\npropose a pipeline for annotating fine-grained 3D humans, objects, and their\ninteractions from single images. We annotated 2.5k+ 3D HOI assets from existing\n2D HOI datasets and built the first open-vocabulary in-the-wild 3D HOI dataset\nOpen3DHOI, to serve as a future test set. Moreover, we design a novel\nGaussian-HOI optimizer, which efficiently reconstructs the spatial interactions\nbetween humans and objects while learning the contact regions. Besides the 3D\nHOI reconstruction, we also propose several new tasks for 3D HOI understanding\nto pave the way for future work. Data and code will be publicly available at\nhttps://wenboran2002.github.io/3dhoi.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Boran Wen",
      "Dingbang Huang",
      "Zichen Zhang",
      "Jiahong Zhou",
      "Jianbin Deng",
      "Jingyu Gong",
      "Yulong Chen",
      "Lizhuang Ma",
      "Yong-Lu Li"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15897v1",
    "title": "Learning 3D Scene Analogies with Neural Contextual Scene Maps",
    "abstract": "Understanding scene contexts is crucial for machines to perform tasks and\nadapt prior knowledge in unseen or noisy 3D environments. As data-driven\nlearning is intractable to comprehensively encapsulate diverse ranges of\nlayouts and open spaces, we propose teaching machines to identify relational\ncommonalities in 3D spaces. Instead of focusing on point-wise or object-wise\nrepresentations, we introduce 3D scene analogies, which are smooth maps between\n3D scene regions that align spatial relationships. Unlike well-studied single\ninstance-level maps, these scene-level maps smoothly link large scene regions,\npotentially enabling unique applications in trajectory transfer in AR/VR, long\ndemonstration transfer for imitation learning, and context-aware object\nrearrangement. To find 3D scene analogies, we propose neural contextual scene\nmaps, which extract descriptor fields summarizing semantic and geometric\ncontexts, and holistically align them in a coarse-to-fine manner for map\nestimation. This approach reduces reliance on individual feature points, making\nit robust to input noise or shape variations. Experiments demonstrate the\neffectiveness of our approach in identifying scene analogies and transferring\ntrajectories or object placements in diverse indoor scenes, indicating its\npotential for robotics and AR/VR applications.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Junho Kim",
      "Gwangtak Bae",
      "Eun Sun Lee",
      "Young Min Kim"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16562v1",
    "title": "Bezier Distillation",
    "abstract": "In Rectified Flow, by obtaining the rectified flow several times, the mapping\nrelationship between distributions can be distilled into a neural network, and\nthe target distribution can be directly predicted by the straight lines of the\nflow. However, during the pairing process of the mapping relationship, a large\namount of error accumulation will occur, resulting in a decrease in performance\nafter multiple rectifications. In the field of flow models, knowledge\ndistillation of multi - teacher diffusion models is also a problem worthy of\ndiscussion in accelerating sampling. I intend to combine multi - teacher\nknowledge distillation with Bezier curves to solve the problem of error\naccumulation. Currently, the related paper is being written by myself.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Ling Feng",
      "SK Yang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15893v1",
    "title": "UniHDSA: A Unified Relation Prediction Approach for Hierarchical Document Structure Analysis",
    "abstract": "Document structure analysis, aka document layout analysis, is crucial for\nunderstanding both the physical layout and logical structure of documents,\nserving information retrieval, document summarization, knowledge extraction,\netc. Hierarchical Document Structure Analysis (HDSA) specifically aims to\nrestore the hierarchical structure of documents created using authoring\nsoftware with hierarchical schemas. Previous research has primarily followed\ntwo approaches: one focuses on tackling specific subtasks of HDSA in isolation,\nsuch as table detection or reading order prediction, while the other adopts a\nunified framework that uses multiple branches or modules, each designed to\naddress a distinct task. In this work, we propose a unified relation prediction\napproach for HDSA, called UniHDSA, which treats various HDSA sub-tasks as\nrelation prediction problems and consolidates relation prediction labels into a\nunified label space. This allows a single relation prediction module to handle\nmultiple tasks simultaneously, whether at a page-level or document-level\nstructure analysis. To validate the effectiveness of UniHDSA, we develop a\nmultimodal end-to-end system based on Transformer architectures. Extensive\nexperimental results demonstrate that our approach achieves state-of-the-art\nperformance on a hierarchical document structure analysis benchmark,\nComp-HRDoc, and competitive results on a large-scale document layout analysis\ndataset, DocLayNet, effectively illustrating the superiority of our method\nacross all sub-tasks.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jiawei Wang",
      "Kai Hu",
      "Qiang Huo"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15892v1",
    "title": "UMIT: Unifying Medical Imaging Tasks via Vision-Language Models",
    "abstract": "With the rapid advancement of deep learning, particularly in the field of\nmedical image analysis, an increasing number of Vision-Language Models (VLMs)\nare being widely applied to solve complex health and biomedical challenges.\nHowever, existing research has primarily focused on specific tasks or single\nmodalities, which limits their applicability and generalization across diverse\nmedical scenarios. To address this challenge, we propose UMIT, a unified\nmulti-modal, multi-task VLM designed specifically for medical imaging tasks.\nUMIT is able to solve various tasks, including visual question answering,\ndisease detection, and medical report generation. In addition, it is applicable\nto multiple imaging modalities (e.g., X-ray, CT and PET), covering a wide range\nof applications from basic diagnostics to complex lesion analysis. Moreover,\nUMIT supports both English and Chinese, expanding its applicability globally\nand ensuring accessibility to healthcare services in different linguistic\ncontexts. To enhance the model's adaptability and task-handling capability, we\ndesign a unique two-stage training strategy and fine-tune UMIT with designed\ninstruction templates. Through extensive empirical evaluation, UMIT outperforms\nprevious methods in five tasks across multiple datasets. The performance of\nUMIT indicates that it can significantly enhance diagnostic accuracy and\nworkflow efficiency, thus providing effective solutions for medical imaging\napplications.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Haiyang Yu",
      "Siyang Yi",
      "Ke Niu",
      "Minghan Zhuo",
      "Bin Li"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15890v1",
    "title": "Time After Time: Deep-Q Effect Estimation for Interventions on When and What to do",
    "abstract": "Problems in fields such as healthcare, robotics, and finance requires\nreasoning about the value both of what decision or action to take and when to\ntake it. The prevailing hope is that artificial intelligence will support such\ndecisions by estimating the causal effect of policies such as how to treat\npatients or how to allocate resources over time. However, existing methods for\nestimating the effect of a policy struggle with \\emph{irregular time}. They\neither discretize time, or disregard the effect of timing policies. We present\na new deep-Q algorithm that estimates the effect of both when and what to do\ncalled Earliest Disagreement Q-Evaluation (EDQ). EDQ makes use of recursion for\nthe Q-function that is compatible with flexible sequence models, such as\ntransformers. EDQ provides accurate estimates under standard assumptions. We\nvalidate the approach through experiments on survival time and tumor growth\ntasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yoav Wald",
      "Mark Goldstein",
      "Yonathan Efroni",
      "Wouter A. C. van Amsterdam",
      "Rajesh Ranganath"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15889v1",
    "title": "LeanTTA: A Backpropagation-Free and Stateless Approach to Quantized Test-Time Adaptation on Edge Devices",
    "abstract": "While there are many advantages to deploying machine learning models on edge\ndevices, the resource constraints of mobile platforms, the dynamic nature of\nthe environment, and differences between the distribution of training versus\nin-the-wild data make such deployments challenging. Current test-time\nadaptation methods are often memory-intensive and not designed to be\nquantization-compatible or deployed on low-resource devices. To address these\nchallenges, we present LeanTTA, a novel backpropagation-free and stateless\nframework for quantized test-time adaptation tailored to edge devices. Our\napproach minimizes computational costs by dynamically updating normalization\nstatistics without backpropagation, which frees LeanTTA from the common pitfall\nof relying on large batches and historical data, making our method robust to\nrealistic deployment scenarios. Our approach is the first to enable further\ncomputational gains by combining partial adaptation with quantized module\nfusion. We validate our framework across sensor modalities, demonstrating\nsignificant improvements over state-of-the-art TTA methods, including a 15.7%\nerror reduction, peak memory usage of only 11.2MB for ResNet18, and fast\nadaptation within an order-of-magnitude of normal inference speeds on-device.\nLeanTTA provides a robust solution for achieving the right trade offs between\naccuracy and system efficiency in edge deployments, addressing the unique\nchallenges posed by limited data and varied operational conditions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Cynthia Dong",
      "Hong Jia",
      "Young D. Kwon",
      "Georgios Rizos",
      "Cecilia Mascolo"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15888v1",
    "title": "Parameters vs. Context: Fine-Grained Control of Knowledge Reliance in Language Models",
    "abstract": "Retrieval-Augmented Generation (RAG) mitigates hallucinations in Large\nLanguage Models (LLMs) by integrating external knowledge. However, conflicts\nbetween parametric knowledge and retrieved context pose challenges,\nparticularly when retrieved information is unreliable or the model's internal\nknowledge is outdated. In such cases, LLMs struggle to determine whether to\nrely more on their own parameters or the conflicted context. To address this,\nwe propose **CK-PLUG**, a plug-and-play method for controlling LLMs' reliance\non parametric and contextual knowledge. We introduce a novel knowledge\nconsistency metric, Confidence Gain, which detects knowledge conflicts by\nmeasuring entropy shifts in token probability distributions after context\ninsertion. CK-PLUG then enables fine-grained control over knowledge preference\nby adjusting the probability distribution of tokens with negative confidence\ngain through a single tuning parameter. Experiments demonstrate CK-PLUG's\nability to significantly regulate knowledge reliance in counterfactual RAG\nscenarios while maintaining generation fluency and knowledge accuracy. For\ninstance, on Llama3-8B, memory recall (MR) of RAG response can be adjusted\nwithin a broad range (9.9%-71.9%), compared to the baseline of 42.1%. Moreover,\nCK-PLUG supports adaptive control based on the model's confidence in both\ninternal and external knowledge, achieving consistent performance improvements\nacross various general RAG tasks. Our code is available at:\n$\\href{https://github.com/byronBBL/CK-PLUG}{\\text{this https URL}}$.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Baolong Bi",
      "Shenghua Liu",
      "Yiwei Wang",
      "Yilong Xu",
      "Junfeng Fang",
      "Lingrui Mei",
      "Xueqi Cheng"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15887v1",
    "title": "DocVideoQA: Towards Comprehensive Understanding of Document-Centric Videos through Question Answering",
    "abstract": "Remote work and online courses have become important methods of knowledge\ndissemination, leading to a large number of document-based instructional\nvideos. Unlike traditional video datasets, these videos mainly feature\nrich-text images and audio that are densely packed with information closely\ntied to the visual content, requiring advanced multimodal understanding\ncapabilities. However, this domain remains underexplored due to dataset\navailability and its inherent complexity. In this paper, we introduce the\nDocVideoQA task and dataset for the first time, comprising 1454 videos across\n23 categories with a total duration of about 828 hours. The dataset is\nannotated with 154k question-answer pairs generated manually and via GPT,\nassessing models' comprehension, temporal awareness, and modality integration\ncapabilities. Initially, we establish a baseline using open-source MLLMs.\nRecognizing the challenges in modality comprehension for document-centric\nvideos, we present DV-LLaMA, a robust video MLLM baseline. Our method enhances\nunimodal feature extraction with diverse instruction-tuning data and employs\ncontrastive learning to strengthen modality integration. Through fine-tuning,\nthe LLM is equipped with audio-visual capabilities, leading to significant\nimprovements in document-centric video understanding. Extensive testing on the\nDocVideoQA dataset shows that DV-LLaMA significantly outperforms existing\nmodels. We'll release the code and dataset to facilitate future research.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Haochen Wang",
      "Kai Hu",
      "Liangcai Gao"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15886v2",
    "title": "Enhancing Zero-Shot Image Recognition in Vision-Language Models through Human-like Concept Guidance",
    "abstract": "In zero-shot image recognition tasks, humans demonstrate remarkable\nflexibility in classifying unseen categories by composing known simpler\nconcepts. However, existing vision-language models (VLMs), despite achieving\nsignificant progress through large-scale natural language supervision, often\nunderperform in real-world applications because of sub-optimal prompt\nengineering and the inability to adapt effectively to target classes. To\naddress these issues, we propose a Concept-guided Human-like Bayesian Reasoning\n(CHBR) framework. Grounded in Bayes' theorem, CHBR models the concept used in\nhuman image recognition as latent variables and formulates this task by summing\nacross potential concepts, weighted by a prior distribution and a likelihood\nfunction. To tackle the intractable computation over an infinite concept space,\nwe introduce an importance sampling algorithm that iteratively prompts large\nlanguage models (LLMs) to generate discriminative concepts, emphasizing\ninter-class differences. We further propose three heuristic approaches\ninvolving Average Likelihood, Confidence Likelihood, and Test Time Augmentation\n(TTA) Likelihood, which dynamically refine the combination of concepts based on\nthe test image. Extensive evaluations across fifteen datasets demonstrate that\nCHBR consistently outperforms existing state-of-the-art zero-shot\ngeneralization methods.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Hui Liu",
      "Wenya Wang",
      "Kecheng Chen",
      "Jie Liu",
      "Yibing Liu",
      "Tiexin Qin",
      "Peisong He",
      "Xinghao Jiang",
      "Haoliang Li"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.16561v1",
    "title": "FutureGen: LLM-RAG Approach to Generate the Future Work of Scientific Article",
    "abstract": "The future work section of a scientific article outlines potential research\ndirections by identifying gaps and limitations of a current study. This section\nserves as a valuable resource for early-career researchers seeking unexplored\nareas and experienced researchers looking for new projects or collaborations.\nIn this study, we generate future work suggestions from key sections of a\nscientific article alongside related papers and analyze how the trends have\nevolved. We experimented with various Large Language Models (LLMs) and\nintegrated Retrieval-Augmented Generation (RAG) to enhance the generation\nprocess. We incorporate a LLM feedback mechanism to improve the quality of the\ngenerated content and propose an LLM-as-a-judge approach for evaluation. Our\nresults demonstrated that the RAG-based approach with LLM feedback outperforms\nother methods evaluated through qualitative and quantitative metrics. Moreover,\nwe conduct a human evaluation to assess the LLM as an extractor and judge. The\ncode and dataset for this project are here, code: HuggingFace",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Ibrahim Al Azher",
      "Miftahul Jannat Mokarrama",
      "Zhishuai Guo",
      "Sagnik Ray Choudhury",
      "Hamed Alhoori"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15880v1",
    "title": "InCo-DPO: Balancing Distribution Shift and Data Quality for Enhanced Preference Optimization",
    "abstract": "Direct Preference Optimization (DPO) optimizes language models to align with\nhuman preferences. Utilizing on-policy samples, generated directly by the\npolicy model, typically results in better performance due to its distribution\nconsistency with the model compared to off-policy samples. This paper\nidentifies the quality of candidate preference samples as another critical\nfactor. While the quality of on-policy data is inherently constrained by the\ncapabilities of the policy model, off-policy data, which can be derived from\ndiverse sources, offers greater potential for quality despite experiencing\ndistribution shifts. However, current research mostly relies on on-policy data\nand neglects the value of off-policy data in terms of data quality, due to the\nchallenge posed by distribution shift. In this paper, we propose InCo-DPO, an\nefficient method for synthesizing preference data by integrating on-policy and\noff-policy data, allowing dynamic adjustments to balance distribution shifts\nand data quality, thus finding an optimal trade-off. Consequently, InCo-DPO\novercomes the limitations of distribution shifts in off-policy data and the\nquality constraints of on-policy data. We evaluated InCo-DPO with the\nAlpaca-Eval 2.0 and Arena-Hard benchmarks. Experimental results demonstrate\nthat our approach not only outperforms both on-policy and off-policy data but\nalso achieves a state-of-the-art win rate of 60.8 on Arena-Hard with the\nvanilla DPO using Gemma-2 model.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Yunan Wang",
      "Jijie Li",
      "Bo-Wen Zhang",
      "Liangdong Wang",
      "Guang Liu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15879v2",
    "title": "Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid Question Answering",
    "abstract": "Non-factoid question-answering (NFQA) poses a significant challenge due to\nits open-ended nature, diverse intents, and the need for multi-aspect\nreasoning, which renders conventional factoid QA approaches, including\nretrieval-augmented generation (RAG), inadequate. Unlike factoid questions,\nnon-factoid questions (NFQs) lack definitive answers and require synthesizing\ninformation from multiple sources across various reasoning dimensions. To\naddress these limitations, we introduce Typed-RAG, a type-aware multi-aspect\ndecomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies\nNFQs into distinct types -- such as debate, experience, and comparison -- and\napplies aspect-based decomposition to refine retrieval and generation\nstrategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and\naggregating the results, Typed-RAG generates more informative and contextually\nrelevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, a benchmark\ndataset covering diverse NFQ types. Experimental results demonstrate that\nTyped-RAG outperforms baselines, thereby highlighting the importance of\ntype-aware decomposition for effective retrieval and generation in NFQA. Our\ncode and dataset are available at https://github.com/TeamNLP/Typed-RAG.",
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "authors": [
      "DongGeon Lee",
      "Ahjeong Park",
      "Hyeri Lee",
      "Hyeonseo Nam",
      "Yunho Maeng"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.15877v1",
    "title": "Repurposing 2D Diffusion Models with Gaussian Atlas for 3D Generation",
    "abstract": "Recent advances in text-to-image diffusion models have been driven by the\nincreasing availability of paired 2D data. However, the development of 3D\ndiffusion models has been hindered by the scarcity of high-quality 3D data,\nresulting in less competitive performance compared to their 2D counterparts. To\naddress this challenge, we propose repurposing pre-trained 2D diffusion models\nfor 3D object generation. We introduce Gaussian Atlas, a novel representation\nthat utilizes dense 2D grids, enabling the fine-tuning of 2D diffusion models\nto generate 3D Gaussians. Our approach demonstrates successful transfer\nlearning from a pre-trained 2D diffusion model to a 2D manifold flattened from\n3D structures. To support model training, we compile GaussianVerse, a\nlarge-scale dataset comprising 205K high-quality 3D Gaussian fittings of\nvarious 3D objects. Our experimental results show that text-to-image diffusion\nmodels can be effectively adapted for 3D content generation, bridging the gap\nbetween 2D and 3D modeling.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Tiange Xiang",
      "Kai Li",
      "Chengjiang Long",
      "Christian Häne",
      "Peihong Guo",
      "Scott Delp",
      "Ehsan Adeli",
      "Li Fei-Fei"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15876v1",
    "title": "DeepPsy-Agent: A Stage-Aware and Deep-Thinking Emotional Support Agent System",
    "abstract": "This paper introduces DeepPsy-Agent, an innovative psychological support\nsystem that combines the three-stage helping theory in psychology with deep\nlearning techniques. The system consists of two core components: (1) a\nmulti-stage response-capable dialogue model (\\textit{deeppsy-chat}), which\nenhances reasoning capabilities through stage-awareness and deep-thinking\nanalysis to generate high-quality responses; and (2) a real-time stage\ntransition detection model that identifies contextual shifts to guide the\ndialogue towards more effective intervention stages. Based on 30,000 real\npsychological hotline conversations, we employ AI-simulated dialogues and\nexpert re-annotation strategies to construct a high-quality multi-turn dialogue\ndataset. Experimental results demonstrate that DeepPsy-Agent outperforms\ngeneral-purpose large language models (LLMs) in key metrics such as problem\nexposure completeness, cognitive restructuring success rate, and action\nadoption rate. Ablation studies further validate the effectiveness of\nstage-awareness and deep-thinking modules, showing that stage information\ncontributes 42.3\\% to performance, while the deep-thinking module increases\nroot-cause identification by 58.3\\% and reduces ineffective suggestions by\n72.1\\%. This system addresses critical challenges in AI-based psychological\nsupport through dynamic dialogue management and deep reasoning, advancing\nintelligent mental health services.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Kai Chen",
      "Zebing Sun"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15875v1",
    "title": "MiLA: Multi-view Intensive-fidelity Long-term Video Generation World Model for Autonomous Driving",
    "abstract": "In recent years, data-driven techniques have greatly advanced autonomous\ndriving systems, but the need for rare and diverse training data remains a\nchallenge, requiring significant investment in equipment and labor. World\nmodels, which predict and generate future environmental states, offer a\npromising solution by synthesizing annotated video data for training. However,\nexisting methods struggle to generate long, consistent videos without\naccumulating errors, especially in dynamic scenes. To address this, we propose\nMiLA, a novel framework for generating high-fidelity, long-duration videos up\nto one minute. MiLA utilizes a Coarse-to-Re(fine) approach to both stabilize\nvideo generation and correct distortion of dynamic objects. Additionally, we\nintroduce a Temporal Progressive Denoising Scheduler and Joint Denoising and\nCorrecting Flow modules to improve the quality of generated videos. Extensive\nexperiments on the nuScenes dataset show that MiLA achieves state-of-the-art\nperformance in video generation quality. For more information, visit the\nproject website: https://github.com/xiaomi-mlab/mila.github.io.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Haiguang Wang",
      "Daqi Liu",
      "Hongwei Xie",
      "Haisong Liu",
      "Enhui Ma",
      "Kaicheng Yu",
      "Limin Wang",
      "Bing Wang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15871v1",
    "title": "MASH-VLM: Mitigating Action-Scene Hallucination in Video-LLMs through Disentangled Spatial-Temporal Representations",
    "abstract": "In this work, we tackle action-scene hallucination in Video Large Language\nModels (Video-LLMs), where models incorrectly predict actions based on the\nscene context or scenes based on observed actions. We observe that existing\nVideo-LLMs often suffer from action-scene hallucination due to two main\nfactors. First, existing Video-LLMs intermingle spatial and temporal features\nby applying an attention operation across all tokens. Second, they use the\nstandard Rotary Position Embedding (RoPE), which causes the text tokens to\noveremphasize certain types of tokens depending on their sequential orders. To\naddress these issues, we introduce MASH-VLM, Mitigating Action-Scene\nHallucination in Video-LLMs through disentangled spatial-temporal\nrepresentations. Our approach includes two key innovations: (1) DST-attention,\na novel attention mechanism that disentangles the spatial and temporal tokens\nwithin the LLM by using masked attention to restrict direct interactions\nbetween the spatial and temporal tokens; (2) Harmonic-RoPE, which extends the\ndimensionality of the positional IDs, allowing the spatial and temporal tokens\nto maintain balanced positions relative to the text tokens. To evaluate the\naction-scene hallucination in Video-LLMs, we introduce the UNSCENE benchmark\nwith 1,320 videos and 4,078 QA pairs. Extensive experiments demonstrate that\nMASH-VLM achieves state-of-the-art results on the UNSCENE benchmark, as well as\non existing video understanding benchmarks.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Kyungho Bae",
      "Jinhyung Kim",
      "Sihaeng Lee",
      "Soonyoung Lee",
      "Gunhee Lee",
      "Jinwoo Choi"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15870v1",
    "title": "FedSAF: A Federated Learning Framework for Enhanced Gastric Cancer Detection and Privacy Preservation",
    "abstract": "Gastric cancer is one of the most commonly diagnosed cancers and has a high\nmortality rate. Due to limited medical resources, developing machine learning\nmodels for gastric cancer recognition provides an efficient solution for\nmedical institutions. However, such models typically require large sample sizes\nfor training and testing, which can challenge patient privacy. Federated\nlearning offers an effective alternative by enabling model training across\nmultiple institutions without sharing sensitive patient data. This paper\naddresses the limited sample size of publicly available gastric cancer data\nwith a modified data processing method. This paper introduces FedSAF, a novel\nfederated learning algorithm designed to improve the performance of existing\nmethods, particularly in non-independent and identically distributed (non-IID)\ndata scenarios. FedSAF incorporates attention-based message passing and the\nFisher Information Matrix to enhance model accuracy, while a model splitting\nfunction reduces computation and transmission costs. Hyperparameter tuning and\nablation studies demonstrate the effectiveness of this new algorithm, showing\nimprovements in test accuracy on gastric cancer datasets, with FedSAF\noutperforming existing federated learning methods like FedAMP, FedAvg, and\nFedProx. The framework's robustness and generalization ability were further\nvalidated across additional datasets (SEED, BOT, FashionMNIST, and CIFAR-10),\nachieving high performance in diverse environments.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Yuxin Miao",
      "Xinyuan Yang",
      "Hongda Fan",
      "Yichun Li",
      "Yishu Hong",
      "Xiechen Guo",
      "Ali Braytee",
      "Weidong Huang",
      "Ali Anaissi"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15868v2",
    "title": "UniCoRN: Latent Diffusion-based Unified Controllable Image Restoration Network across Multiple Degradations",
    "abstract": "Image restoration is essential for enhancing degraded images across computer\nvision tasks. However, most existing methods address only a single type of\ndegradation (e.g., blur, noise, or haze) at a time, limiting their real-world\napplicability where multiple degradations often occur simultaneously. In this\npaper, we propose UniCoRN, a unified image restoration approach capable of\nhandling multiple degradation types simultaneously using a multi-head diffusion\nmodel. Specifically, we uncover the potential of low-level visual cues\nextracted from images in guiding a controllable diffusion model for real-world\nimage restoration and we design a multi-head control network adaptable via a\nmixture-of-experts strategy. We train our model without any prior assumption of\nspecific degradations, through a smartly designed curriculum learning recipe.\nAdditionally, we also introduce MetaRestore, a metalens imaging benchmark\ncontaining images with multiple degradations and artifacts. Extensive\nevaluations on several challenging datasets, including our benchmark,\ndemonstrate that our method achieves significant performance gains and can\nrobustly restore images with severe degradations. Project page:\nhttps://codejaeger.github.io/unicorn-gh",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Debabrata Mandal",
      "Soumitri Chattopadhyay",
      "Guansen Tong",
      "Praneeth Chakravarthula"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.15867v1",
    "title": "TruthLens: Explainable DeepFake Detection for Face Manipulated and Fully Synthetic Data",
    "abstract": "Detecting DeepFakes has become a crucial research area as the widespread use\nof AI image generators enables the effortless creation of face-manipulated and\nfully synthetic content, yet existing methods are often limited to binary\nclassification (real vs. fake) and lack interpretability. To address these\nchallenges, we propose TruthLens, a novel and highly generalizable framework\nfor DeepFake detection that not only determines whether an image is real or\nfake but also provides detailed textual reasoning for its predictions. Unlike\ntraditional methods, TruthLens effectively handles both face-manipulated\nDeepFakes and fully AI-generated content while addressing fine-grained queries\nsuch as \"Does the eyes/nose/mouth look real or fake?\"\n  The architecture of TruthLens combines the global contextual understanding of\nmultimodal large language models like PaliGemma2 with the localized feature\nextraction capabilities of vision-only models like DINOv2. This hybrid design\nleverages the complementary strengths of both models, enabling robust detection\nof subtle manipulations while maintaining interpretability. Extensive\nexperiments on diverse datasets demonstrate that TruthLens outperforms\nstate-of-the-art methods in detection accuracy (by 2-14%) and explainability,\nin both in-domain and cross-data settings, generalizing effectively across\ntraditional and emerging manipulation techniques.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Rohit Kundu",
      "Athula Balachandran",
      "Amit K. Roy-Chowdhury"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15865v1",
    "title": "Active management of battery degradation in wireless sensor network using deep reinforcement learning for group battery replacement",
    "abstract": "Wireless sensor networks (WSNs) have become a promising solution for\nstructural health monitoring (SHM), especially in hard-to-reach or remote\nlocations. Battery-powered WSNs offer various advantages over wired systems,\nhowever limited battery life has always been one of the biggest obstacles in\npractical use of the WSNs, regardless of energy harvesting methods. While\nvarious methods have been studied for battery health management, existing\nmethods exclusively aim to extend lifetime of individual batteries, lacking a\nsystem level view. A consequence of applying such methods is that batteries in\na WSN tend to fail at different times, posing significant difficulty on\nplanning and scheduling of battery replacement trip. This study investigate a\ndeep reinforcement learning (DRL) method for active battery degradation\nmanagement by optimizing duty cycle of WSNs at the system level. This active\nmanagement strategy effectively reduces earlier failure of battery individuals\nwhich enable group replacement without sacrificing WSN performances. A\nsimulated environment based on a real-world WSN setup was developed to train a\nDRL agent and learn optimal duty cycle strategies. The performance of the\nstrategy was validated in a long-term setup with various network sizes,\ndemonstrating its efficiency and scalability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jong-Hyun Jeonga",
      "Hongki Jo",
      "Qiang Zhou",
      "Tahsin Afroz Hoque Nishat",
      "Lang Wu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15861v1",
    "title": "Sequential Spatial-Temporal Network for Interpretable Automatic Ultrasonic Assessment of Fetal Head during labor",
    "abstract": "The intrapartum ultrasound guideline established by ISUOG highlights the\nAngle of Progression (AoP) and Head Symphysis Distance (HSD) as pivotal metrics\nfor assessing fetal head descent and predicting delivery outcomes. Accurate\nmeasurement of the AoP and HSD requires a structured process. This begins with\nidentifying standardized ultrasound planes, followed by the detection of\nspecific anatomical landmarks within the regions of the pubic symphysis and\nfetal head that correlate with the delivery parameters AoP and HSD. Finally,\nthese measurements are derived based on the identified anatomical landmarks.\nAddressing the clinical demands and standard operation process outlined in the\nISUOG guideline, we introduce the Sequential Spatial-Temporal Network (SSTN),\nthe first interpretable model specifically designed for the video of\nintrapartum ultrasound analysis. The SSTN operates by first identifying\nultrasound planes, then segmenting anatomical structures such as the pubic\nsymphysis and fetal head, and finally detecting key landmarks for precise\nmeasurement of HSD and AoP. Furthermore, the cohesive framework leverages\ntask-related information to improve accuracy and reliability. Experimental\nevaluations on clinical datasets demonstrate that SSTN significantly surpasses\nexisting models, reducing the mean absolute error by 18% for AoP and 22% for\nHSD.",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Jie Gan",
      "Zhuonan Liang",
      "Jianan Fan",
      "Lisa Mcguire",
      "Caterina Watson",
      "Jacqueline Spurway",
      "Jillian Clarke",
      "Weidong Cai"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15855v1",
    "title": "VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting Generation with Flexible Pose and Multi-View Joint Modeling",
    "abstract": "We propose VideoRFSplat, a direct text-to-3D model leveraging a video\ngeneration model to generate realistic 3D Gaussian Splatting (3DGS) for\nunbounded real-world scenes. To generate diverse camera poses and unbounded\nspatial extent of real-world scenes, while ensuring generalization to arbitrary\ntext prompts, previous methods fine-tune 2D generative models to jointly model\ncamera poses and multi-view images. However, these methods suffer from\ninstability when extending 2D generative models to joint modeling due to the\nmodality gap, which necessitates additional models to stabilize training and\ninference. In this work, we propose an architecture and a sampling strategy to\njointly model multi-view images and camera poses when fine-tuning a video\ngeneration model. Our core idea is a dual-stream architecture that attaches a\ndedicated pose generation model alongside a pre-trained video generation model\nvia communication blocks, generating multi-view images and camera poses through\nseparate streams. This design reduces interference between the pose and image\nmodalities. Additionally, we propose an asynchronous sampling strategy that\ndenoises camera poses faster than multi-view images, allowing rapidly denoised\nposes to condition multi-view generation, reducing mutual ambiguity and\nenhancing cross-modal consistency. Trained on multiple large-scale real-world\ndatasets (RealEstate10K, MVImgNet, DL3DV-10K, ACID), VideoRFSplat outperforms\nexisting text-to-3D direct generation methods that heavily depend on post-hoc\nrefinement via score distillation sampling, achieving superior results without\nsuch refinement.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Hyojun Go",
      "Byeongjun Park",
      "Hyelin Nam",
      "Byung-Hoon Kim",
      "Hyungjin Chung",
      "Changick Kim"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15853v1",
    "title": "Network Embedding Exploration Tool (NEExT)",
    "abstract": "Many real-world and artificial systems and processes can be represented as\ngraphs. Some examples of such systems include social networks, financial\ntransactions, supply chains, and molecular structures. In many of these cases,\none needs to consider a collection of graphs, rather than a single network.\nThis could be a collection of distinct but related graphs, such as different\nprotein structures or graphs resulting from dynamic processes on the same\nnetwork. Examples of the latter include the evolution of social networks,\ncommunity-induced graphs, or ego-nets around various nodes. A significant\nchallenge commonly encountered is the absence of ground-truth labels for graphs\nor nodes, necessitating the use of unsupervised techniques to analyze such\nsystems. Moreover, even when ground-truth labels are available, many existing\ngraph machine learning methods depend on complex deep learning models,\ncomplicating model explainability and interpretability. To address some of\nthese challenges, we have introduced NEExT (Network Embedding Exploration Tool)\nfor embedding collections of graphs via user-defined node features. The\nadvantages of the framework are twofold: (i) the ability to easily define your\nown interpretable node-based features in view of the task at hand, and (ii)\nfast embedding of graphs provided by the Vectorizers library. In this paper, we\ndemonstrate the usefulness of NEExT on collections of synthetic and real-world\ngraphs. For supervised tasks, we demonstrate that performance in graph\nclassification tasks could be achieved similarly to other state-of-the-art\ntechniques while maintaining model interpretability. Furthermore, our framework\ncan also be used to generate high-quality embeddings in an unsupervised way,\nwhere target variables are not available.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Ashkan Dehghan",
      "Paweł Prałat",
      "François Théberge"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15851v1",
    "title": "Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video Diffusion",
    "abstract": "Animatable head avatar generation typically requires extensive data for\ntraining. To reduce the data requirements, a natural solution is to leverage\nexisting data-free static avatar generation methods, such as pre-trained\ndiffusion models with score distillation sampling (SDS), which align avatars\nwith pseudo ground-truth outputs from the diffusion model. However, directly\ndistilling 4D avatars from video diffusion often leads to over-smooth results\ndue to spatial and temporal inconsistencies in the generated video. To address\nthis issue, we propose Zero-1-to-A, a robust method that synthesizes a spatial\nand temporal consistency dataset for 4D avatar reconstruction using the video\ndiffusion model. Specifically, Zero-1-to-A iteratively constructs video\ndatasets and optimizes animatable avatars in a progressive manner, ensuring\nthat avatar quality increases smoothly and consistently throughout the learning\nprocess. This progressive learning involves two stages: (1) Spatial Consistency\nLearning fixes expressions and learns from front-to-side views, and (2)\nTemporal Consistency Learning fixes views and learns from relaxed to\nexaggerated expressions, generating 4D avatars in a simple-to-complex manner.\nExtensive experiments demonstrate that Zero-1-to-A improves fidelity, animation\nquality, and rendering speed compared to existing diffusion-based methods,\nproviding a solution for lifelike avatar creation. Code is publicly available\nat: https://github.com/ZhenglinZhou/Zero-1-to-A.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Zhou Zhenglin",
      "Ma Fan",
      "Fan Hehe",
      "Chua Tat-Seng"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15850v1",
    "title": "Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey",
    "abstract": "Large Language Models (LLMs) excel in text generation, reasoning, and\ndecision-making, enabling their adoption in high-stakes domains such as\nhealthcare, law, and transportation. However, their reliability is a major\nconcern, as they often produce plausible but incorrect responses. Uncertainty\nquantification (UQ) enhances trustworthiness by estimating confidence in\noutputs, enabling risk mitigation and selective prediction. However,\ntraditional UQ methods struggle with LLMs due to computational constraints and\ndecoding inconsistencies. Moreover, LLMs introduce unique uncertainty sources,\nsuch as input ambiguity, reasoning path divergence, and decoding stochasticity,\nthat extend beyond classical aleatoric and epistemic uncertainty. To address\nthis, we introduce a new taxonomy that categorizes UQ methods based on\ncomputational efficiency and uncertainty dimensions (input, reasoning,\nparameter, and prediction uncertainty). We evaluate existing techniques, assess\ntheir real-world applicability, and identify open challenges, emphasizing the\nneed for scalable, interpretable, and robust UQ approaches to enhance LLM\nreliability.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Xiaoou Liu",
      "Tiejin Chen",
      "Longchao Da",
      "Chacha Chen",
      "Zhen Lin",
      "Hua Wei"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15848v1",
    "title": "Entropy-based Exploration Conduction for Multi-step Reasoning",
    "abstract": "In large language model (LLM) reasoning, multi-step processes have proven\neffective for solving complex tasks. However, the depth of exploration can\nsignificantly affect the reasoning performance. Existing methods to\nautomatically decide the depth often bring high costs and lack flexibility, and\nthus undermine the model's reasoning accuracy. To address these issues, we\npropose Entropy-based Exploration Depth Conduction (Entro-duction), a novel\nmethod that dynamically adjusts the exploration depth during multi-step\nreasoning by monitoring LLM's output entropy and variance entropy. We employ\nthese two metrics to capture the model's current uncertainty and the\nfluctuation of uncertainty across consecutive reasoning steps. Based on the\nobserved changes, the LLM selects whether to deepen, expand or stop exploration\naccording to the probability. In this way, we balance the reasoning accuracy\nand exploration effectiveness. Experimental results across four benchmark\ndatasets demonstrate the efficacy of Entro-duction. We further conduct\nexperiments and analysis on the components of Entro-duction to discuss their\ncontributions to reasoning performance.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Jinghan Zhang",
      "Xiting Wang",
      "Fengran Mo",
      "Yeyang Zhou",
      "Wanfu Gao",
      "Kunpeng Liu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15847v1",
    "title": "Beyond Local Selection: Global Cut Selection for Enhanced Mixed-Integer Programming",
    "abstract": "In mixed-integer programming (MIP) solvers, cutting planes are essential for\nBranch-and-Cut (B&C) algorithms as they reduce the search space and accelerate\nthe solving process. Traditional methods rely on hard-coded heuristics for cut\nplane selection but fail to leverage problem-specific structural features.\nRecent machine learning approaches use neural networks for cut selection but\nfocus narrowly on the efficiency of single-node within the B&C algorithm,\nwithout considering the broader contextual information. To address this, we\npropose Global Cut Selection (GCS), which uses a bipartite graph to represent\nthe search tree and combines graph neural networks with reinforcement learning\nto develop cut selection strategies. Unlike prior methods, GCS applies cutting\nplanes across all nodes, incorporating richer contextual information.\nExperiments show GCS significantly improves solving efficiency for synthetic\nand large-scale real-world MIPs compared to traditional and learning-based\nmethods.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Shuli Zeng",
      "Sijia Zhang",
      "Shaoang Li",
      "Feng Wu",
      "Xiang-Yang Li"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15846v1",
    "title": "What can Off-the-Shelves Large Multi-Modal Models do for Dynamic Scene Graph Generation?",
    "abstract": "Dynamic Scene Graph Generation (DSGG) for videos is a challenging task in\ncomputer vision. While existing approaches often focus on sophisticated\narchitectural design and solely use recall during evaluation, we take a closer\nlook at their predicted scene graphs and discover three critical issues with\nexisting DSGG methods: severe precision-recall trade-off, lack of awareness on\ntriplet importance, and inappropriate evaluation protocols. On the other hand,\nrecent advances of Large Multimodal Models (LMMs) have shown great capabilities\nin video understanding, yet they have not been tested on fine-grained,\nframe-wise understanding tasks like DSGG. In this work, we conduct the first\nsystematic analysis of Video LMMs for performing DSGG. Without relying on\nsophisticated architectural design, we show that LMMs with simple decoder-only\nstructure can be turned into State-of-the-Art scene graph generators that\neffectively overcome the aforementioned issues, while requiring little\nfinetuning (5-10% training data).",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Xuanming Cui",
      "Jaiminkumar Ashokbhai Bhoi",
      "Chionh Wei Peng",
      "Adriel Kuek",
      "Ser Nam Lim"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15845v1",
    "title": "Network-wide Freeway Traffic Estimation Using Sparse Sensor Data: A Dirichlet Graph Auto-Encoder Approach",
    "abstract": "Network-wide Traffic State Estimation (TSE), which aims to infer a complete\nimage of network traffic states with sparsely deployed sensors, plays a vital\nrole in intelligent transportation systems. With the development of data-driven\nmethods, traffic dynamics modeling has advanced significantly. However, TSE\nposes fundamental challenges for data-driven approaches, since historical\npatterns cannot be learned locally at sensor-free segments. Although inductive\ngraph learning shows promise in estimating states at locations without sensor,\nexisting methods typically handle unobserved locations by filling them with\nzeros, introducing bias to the sensitive graph message propagation. The\nrecently proposed Dirichlet Energy-based Feature Propagation (DEFP) method\nachieves State-Of-The-Art (SOTA) performance in unobserved node classification\nby eliminating the need for zero-filling. However, applying it to TSE faces\nthree key challenges: inability to handle directed traffic networks, strong\nassumptions in traffic spatial correlation modeling, and overlooks distinct\npropagation rules of different patterns (e.g., congestion and free flow). We\npropose DGAE, a novel inductive graph representation model that addresses these\nchallenges through theoretically derived DEFP for Directed graph (DEFP4D),\nenhanced spatial representation learning via DEFP4D-guided latent space\nencoding, and physics-guided propagation mechanisms that separately handles\ncongested and free-flow patterns. Experiments on three traffic datasets\ndemonstrate that DGAE outperforms existing SOTA methods and exhibits strong\ncross-city transferability. Furthermore, DEFP4D can serve as a standalone\nlightweight solution, showing superior performance under extremely sparse\nsensor conditions.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Qishen Zhou",
      "Yifan Zhang",
      "Michail A. Makridis",
      "Anastasios Kouvelas",
      "Yibing Wang",
      "Simon Hu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15842v1",
    "title": "FedAWA: Adaptive Optimization of Aggregation Weights in Federated Learning Using Client Vectors",
    "abstract": "Federated Learning (FL) has emerged as a promising framework for distributed\nmachine learning, enabling collaborative model training without sharing local\ndata, thereby preserving privacy and enhancing security. However, data\nheterogeneity resulting from differences across user behaviors, preferences,\nand device characteristics poses a significant challenge for federated\nlearning. Most previous works overlook the adjustment of aggregation weights,\nrelying solely on dataset size for weight assignment, which often leads to\nunstable convergence and reduced model performance. Recently, several studies\nhave sought to refine aggregation strategies by incorporating dataset\ncharacteristics and model alignment. However, adaptively adjusting aggregation\nweights while ensuring data security-without requiring additional proxy\ndata-remains a significant challenge. In this work, we propose Federated\nlearning with Adaptive Weight Aggregation (FedAWA), a novel method that\nadaptively adjusts aggregation weights based on client vectors during the\nlearning process. The client vector captures the direction of model updates,\nreflecting local data variations, and is used to optimize the aggregation\nweight without requiring additional datasets or violating privacy. By assigning\nhigher aggregation weights to local models whose updates align closely with the\nglobal optimization direction, FedAWA enhances the stability and generalization\nof the global model. Extensive experiments under diverse scenarios demonstrate\nthe superiority of our method, providing a promising solution to the challenges\nof data heterogeneity in federated learning.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Changlong Shi",
      "He Zhao",
      "Bingjie Zhang",
      "Mingyuan Zhou",
      "Dandan Guo",
      "Yi Chang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15837v1",
    "title": "Fùxì: A Benchmark for Evaluating Language Models on Ancient Chinese Text Understanding and Generation",
    "abstract": "Ancient Chinese text processing presents unique challenges for large language\nmodels (LLMs) due to its distinct linguistic features, complex structural\nconstraints, and rich cultural context. While existing benchmarks have\nprimarily focused on evaluating comprehension through multiple-choice\nquestions, there remains a critical gap in assessing models' generative\ncapabilities in classical Chinese. We introduce F\\`ux\\`i, a comprehensive\nbenchmark that evaluates both understanding and generation capabilities across\n21 diverse tasks. Our benchmark distinguishes itself through three key\ncontributions: (1) balanced coverage of both comprehension and generation\ntasks, including novel tasks like poetry composition and couplet completion,\n(2) specialized evaluation metrics designed specifically for classical Chinese\ntext generation, combining rule-based verification with fine-tuned LLM\nevaluators, and (3) a systematic assessment framework that considers both\nlinguistic accuracy and cultural authenticity. Through extensive evaluation of\nstate-of-the-art LLMs, we reveal significant performance gaps between\nunderstanding and generation tasks, with models achieving promising results in\ncomprehension but struggling considerably in generation tasks, particularly\nthose requiring deep cultural knowledge and adherence to classical formats. Our\nfindings highlight the current limitations in ancient Chinese text processing\nand provide insights for future model development. The benchmark, evaluation\ntoolkit, and baseline results are publicly available to facilitate research in\nthis domain.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Shangqing Zhao",
      "Yuhao Zhou",
      "Yupei Ren",
      "Zhe Chen",
      "Chenghao Jia",
      "Fang Zhe",
      "Zhaogaung Long",
      "Shu Liu",
      "Man Lan"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15835v1",
    "title": "BARD-GS: Blur-Aware Reconstruction of Dynamic Scenes via Gaussian Splatting",
    "abstract": "3D Gaussian Splatting (3DGS) has shown remarkable potential for static scene\nreconstruction, and recent advancements have extended its application to\ndynamic scenes. However, the quality of reconstructions depends heavily on\nhigh-quality input images and precise camera poses, which are not that trivial\nto fulfill in real-world scenarios. Capturing dynamic scenes with handheld\nmonocular cameras, for instance, typically involves simultaneous movement of\nboth the camera and objects within a single exposure. This combined motion\nfrequently results in image blur that existing methods cannot adequately\nhandle. To address these challenges, we introduce BARD-GS, a novel approach for\nrobust dynamic scene reconstruction that effectively handles blurry inputs and\nimprecise camera poses. Our method comprises two main components: 1) camera\nmotion deblurring and 2) object motion deblurring. By explicitly decomposing\nmotion blur into camera motion blur and object motion blur and modeling them\nseparately, we achieve significantly improved rendering results in dynamic\nregions. In addition, we collect a real-world motion blur dataset of dynamic\nscenes to evaluate our approach. Extensive experiments demonstrate that BARD-GS\neffectively reconstructs high-quality dynamic scenes under realistic\nconditions, significantly outperforming existing methods.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yiren Lu",
      "Yunlai Zhou",
      "Disheng Liu",
      "Tuo Liang",
      "Yu Yin"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15831v1",
    "title": "EDEN: Enhanced Diffusion for High-quality Large-motion Video Frame Interpolation",
    "abstract": "Handling complex or nonlinear motion patterns has long posed challenges for\nvideo frame interpolation. Although recent advances in diffusion-based methods\noffer improvements over traditional optical flow-based approaches, they still\nstruggle to generate sharp, temporally consistent frames in scenarios with\nlarge motion. To address this limitation, we introduce EDEN, an Enhanced\nDiffusion for high-quality large-motion vidEo frame iNterpolation. Our approach\nfirst utilizes a transformer-based tokenizer to produce refined latent\nrepresentations of the intermediate frames for diffusion models. We then\nenhance the diffusion transformer with temporal attention across the process\nand incorporate a start-end frame difference embedding to guide the generation\nof dynamic motion. Extensive experiments demonstrate that EDEN achieves\nstate-of-the-art results across popular benchmarks, including nearly a 10%\nLPIPS reduction on DAVIS and SNU-FILM, and an 8% improvement on DAIN-HD.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Zihao Zhang",
      "Haoran Chen",
      "Haoyu Zhao",
      "Guansong Lu",
      "Yanwei Fu",
      "Hang Xu",
      "Zuxuan Wu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16560v1",
    "title": "Early Prediction of Alzheimer's and Related Dementias: A Machine Learning Approach Utilizing Social Determinants of Health Data",
    "abstract": "Alzheimer's disease and related dementias (AD/ADRD) represent a growing\nhealthcare crisis affecting over 6 million Americans. While genetic factors\nplay a crucial role, emerging research reveals that social determinants of\nhealth (SDOH) significantly influence both the risk and progression of\ncognitive functioning, such as cognitive scores and cognitive decline. This\nreport examines how these social, environmental, and structural factors impact\ncognitive health trajectories, with a particular focus on Hispanic populations,\nwho face disproportionate risk for AD/ADRD. Using data from the Mexican Health\nand Aging Study (MHAS) and its cognitive assessment sub study (Mex-Cog), we\nemployed ensemble of regression trees models to predict 4-year and 9-year\ncognitive scores and cognitive decline based on SDOH. This approach identified\nkey predictive SDOH factors to inform potential multilevel interventions to\naddress cognitive health disparities in this population.",
    "categories": [
      "q-bio.QM",
      "cs.LG"
    ],
    "authors": [
      "Bereket Kindo",
      "Arjee Restar",
      "Anh Tran"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15822v1",
    "title": "Energy-Efficient Federated Learning and Migration in Digital Twin Edge Networks",
    "abstract": "The digital twin edge network (DITEN) is a significant paradigm in the\nsixth-generation wireless system (6G) that aims to organize well-developed\ninfrastructures to meet the requirements of evolving application scenarios.\nHowever, the impact of the interaction between the long-term DITEN maintenance\nand detailed digital twin tasks, which often entail privacy considerations, is\ncommonly overlooked in current research. This paper addresses this issue by\nintroducing a problem of digital twin association and historical data\nallocation for a federated learning (FL) task within DITEN. To achieve this\ngoal, we start by introducing a closed-form function to predict the training\naccuracy of the FL task, referring to it as the data utility. Subsequently, we\ncarry out comprehensive convergence analyses on the proposed FL methodology.\nOur objective is to jointly optimize the data utility of the digital\ntwin-empowered FL task and the energy costs incurred by the long-term DITEN\nmaintenance, encompassing FL model training, data synchronization, and twin\nmigration. To tackle the aforementioned challenge, we present an\noptimization-driven learning algorithm that effectively identifies optimized\nsolutions for the formulated problem. Numerical results demonstrate that our\nproposed algorithm outperforms various baseline approaches.",
    "categories": [
      "cs.NI",
      "cs.LG"
    ],
    "authors": [
      "Yuzhi Zhou",
      "Yaru Fu",
      "Zheng Shi",
      "Howard H. Yang",
      "Kevin Hung",
      "Yan Zhang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15819v1",
    "title": "Control Pneumatic Soft Bending Actuator with Online Learning Pneumatic Physical Reservoir Computing",
    "abstract": "The intrinsic nonlinearities of soft robots present significant control but\nsimultaneously provide them with rich computational potential. Reservoir\ncomputing (RC) has shown effectiveness in online learning systems for\ncontrolling nonlinear systems such as soft actuators. Conventional RC can be\nextended into physical reservoir computing (PRC) by leveraging the nonlinear\ndynamics of soft actuators for computation. This paper introduces a PRC-based\nonline learning framework to control the motion of a pneumatic soft bending\nactuator, utilizing another pneumatic soft actuator as the PRC model. Unlike\nconventional designs requiring two RC models, the proposed control system\nemploys a more compact architecture with a single RC model. Additionally, the\nframework enables zero-shot online learning, addressing limitations of previous\nPRC-based control systems reliant on offline training. Simulations and\nexperiments validated the performance of the proposed system. Experimental\nresults indicate that the PRC model achieved superior control performance\ncompared to a linear model, reducing the root-mean-square error (RMSE) by an\naverage of over 37% in bending motion control tasks. The proposed PRC-based\nonline learning control framework provides a novel approach for harnessing\nphysical systems' inherent nonlinearities to enhance the control of soft\nactuators.",
    "categories": [
      "cs.RO",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Junyi Shen",
      "Tetsuro Miyazaki",
      "Kenji Kawashima"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15818v1",
    "title": "Computation-Efficient and Recognition-Friendly 3D Point Cloud Privacy Protection",
    "abstract": "3D point cloud has been widely used in applications such as self-driving\ncars, robotics, CAD models, etc. To the best of our knowledge, these\napplications raised the issue of privacy leakage in 3D point clouds, which has\nnot been studied well. Different from the 2D image privacy, which is related to\ntexture and 2D geometric structure, the 3D point cloud is texture-less and only\nrelevant to 3D geometric structure. In this work, we defined the 3D point cloud\nprivacy problem and proposed an efficient privacy-preserving framework named\nPointFlowGMM that can support downstream classification and segmentation tasks\nwithout seeing the original data. Using a flow-based generative model, the\npoint cloud is projected into a latent Gaussian mixture distributed subspace.\nWe further designed a novel angular similarity loss to obfuscate the original\ngeometric structure and reduce the model size from 767MB to 120MB without a\ndecrease in recognition performance. The projected point cloud in the latent\nspace is orthogonally rotated randomly to further protect the original\ngeometric structure, the class-to-class relationship is preserved after\nrotation, thus, the protected point cloud can support the recognition task. We\nevaluated our model on multiple datasets and achieved comparable recognition\nresults on encrypted point clouds compared to the original point clouds.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Haotian Ma",
      "Lin Gu",
      "Siyi Wu",
      "Yingying Zhu"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15817v1",
    "title": "Ranking Counterfactual Explanations",
    "abstract": "AI-driven outcomes can be challenging for end-users to understand.\nExplanations can address two key questions: \"Why this outcome?\" (factual) and\n\"Why not another?\" (counterfactual). While substantial efforts have been made\nto formalize factual explanations, a precise and comprehensive study of\ncounterfactual explanations is still lacking. This paper proposes a formal\ndefinition of counterfactual explanations, proving some properties they\nsatisfy, and examining the relationship with factual explanations. Given that\nmultiple counterfactual explanations generally exist for a specific case, we\nalso introduce a rigorous method to rank these counterfactual explanations,\ngoing beyond a simple minimality condition, and to identify the optimal ones.\nOur experiments with 12 real-world datasets highlight that, in most cases, a\nsingle optimal counterfactual explanation emerges. We also demonstrate, via\nthree metrics, that the selected optimal explanation exhibits higher\nrepresentativeness and can explain a broader range of elements than a random\nminimal counterfactual. This result highlights the effectiveness of our\napproach in identifying more robust and comprehensive counterfactual\nexplanations.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Suryani Lim",
      "Henri Prade",
      "Gilles Richard"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15816v1",
    "title": "A Vision Centric Remote Sensing Benchmark",
    "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision-language tasks but their remote sensing (RS) counterpart are relatively\nunder explored. Unlike natural images, RS imagery presents unique challenges\nthat current MLLMs struggle to handle, particularly in visual grounding and\nspatial reasoning. This study investigates the limitations of CLIP-based MLLMs\nin RS, highlighting their failure to differentiate visually distinct yet\nsemantically similar RS images. To address this, we introduce a remote sensing\nmultimodal visual patterns (RSMMVP) benchmark. It is designed to evaluate MLLMs\nin RS tasks by identifying the CLIP-blind pairs, where CLIP-based models\nincorrectly assign high similarity scores to visually distinct RS images.\nThrough a visual question answering (VQA) evaluation, we analyze the\nperformance of state-of-the-art MLLMs, revealing significant limitations in RS\nspecific representation learning. The results provide valuable insights into\nthe weaknesses of CLIP-based visual encoding and offer a foundation for future\nresearch to develop more effective MLLMs tailored for remote sensing\napplications.",
    "categories": [
      "cs.CV",
      "F.2.2; I.2.7"
    ],
    "authors": [
      "Abduljaleel Adejumo",
      "Faegheh Yeganli",
      "Clifford Broni-bediako",
      "Aoran Xiao",
      "Naoto Yokoya",
      "Mennatullah Siam"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15815v1",
    "title": "Attention Pruning: Automated Fairness Repair of Language Models via Surrogate Simulated Annealing",
    "abstract": "This paper explores pruning attention heads as a post-processing bias\nmitigation method for large language models (LLMs). Modern AI systems such as\nLLMs are expanding into sensitive social contexts where fairness concerns\nbecome especially crucial. Since LLMs develop decision-making patterns by\ntraining on massive datasets of human-generated content, they naturally encode\nand perpetuate societal biases. While modifying training datasets and\nalgorithms is expensive and requires significant resources; post-processing\ntechniques-such as selectively deactivating neurons and attention heads in\npre-trained LLMs-can provide feasible and effective approaches to improve\nfairness. However, identifying the optimal subset of parameters to prune\npresents a combinatorial challenge within LLMs' immense parameter space,\nrequiring solutions that efficiently balance competing objectives across the\nfrontiers of model fairness and utility.\n  To address the computational challenges, we explore a search-based program\nrepair approach via randomized simulated annealing. Given the prohibitive\nevaluation costs in billion-parameter LLMs, we develop surrogate deep neural\nnetworks that efficiently model the relationship between attention head states\n(active/inactive) and their corresponding fairness/utility metrics. This allows\nus to perform optimization over the surrogate models and efficiently identify\noptimal subsets of attention heads for selective pruning rather than directly\nsearching through the LLM parameter space. This paper introduces Attention\nPruning, a fairness-aware surrogate simulated annealing approach to prune\nattention heads in LLMs that disproportionately contribute to bias while\nminimally impacting overall model utility. Our experiments show that Attention\nPruning achieves up to $40\\%$ reduction in gender bias and outperforms the\nstate-of-the-art bias mitigation strategies.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Vishnu Asutosh Dasu",
      "Md Rafi ur Rashid",
      "Vipul Gupta",
      "Saeid Tizpaz-Niari",
      "Gang Tan"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15810v1",
    "title": "Big data comparison of quantum invariants",
    "abstract": "We apply big data techniques, including exploratory and topological data\nanalysis, to investigate quantum invariants. More precisely, our study explores\nthe Jones polynomial's structural properties and contrasts its behavior under\nfour principal methods of enhancement: coloring, rank increase,\ncategorification, and leaving the realm of Lie algebras.",
    "categories": [
      "math.GT",
      "cs.LG",
      "math.QA",
      "Primary: 57K16, 62R07, secondary: 57K18, 68P05"
    ],
    "authors": [
      "Daniel Tubbenhauer",
      "Victor Zhang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15809v1",
    "title": "Controlling Avatar Diffusion with Learnable Gaussian Embedding",
    "abstract": "Recent advances in diffusion models have made significant progress in digital\nhuman generation. However, most existing models still struggle to maintain 3D\nconsistency, temporal coherence, and motion accuracy. A key reason for these\nshortcomings is the limited representation ability of commonly used control\nsignals(e.g., landmarks, depth maps, etc.). In addition, the lack of diversity\nin identity and pose variations in public datasets further hinders progress in\nthis area. In this paper, we analyze the shortcomings of current control\nsignals and introduce a novel control signal representation that is\noptimizable, dense, expressive, and 3D consistent. Our method embeds a\nlearnable neural Gaussian onto a parametric head surface, which greatly\nenhances the consistency and expressiveness of diffusion-based head models.\nRegarding the dataset, we synthesize a large-scale dataset with multiple poses\nand identities. In addition, we use real/synthetic labels to effectively\ndistinguish real and synthetic data, minimizing the impact of imperfections in\nsynthetic data on the generated head images. Extensive experiments show that\nour model outperforms existing methods in terms of realism, expressiveness, and\n3D consistency. Our code, synthetic datasets, and pre-trained models will be\nreleased in our project page: https://ustc3dv.github.io/Learn2Control/",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Xuan Gao",
      "Jingtao Zhou",
      "Dongyu Liu",
      "Yuqi Zhou",
      "Juyong Zhang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15808v1",
    "title": "ChatGPT and U(X): A Rapid Review on Measuring the User Experience",
    "abstract": "ChatGPT, powered by a large language model (LLM), has revolutionized everyday\nhuman-computer interaction (HCI) since its 2022 release. While now used by\nmillions around the world, a coherent pathway for evaluating the user\nexperience (UX) ChatGPT offers remains missing. In this rapid review (N = 58),\nI explored how ChatGPT UX has been approached quantitatively so far. I focused\non the independent variables (IVs) manipulated, the dependent variables (DVs)\nmeasured, and the methods used for measurement. Findings reveal trends, gaps,\nand emerging consensus in UX assessments. This work offers a first step towards\nsynthesizing existing approaches to measuring ChatGPT UX, urgent trajectories\nto advance standardization and breadth, and two preliminary frameworks aimed at\nguiding future research and tool development. I seek to elevate the field of\nChatGPT UX by empowering researchers and practitioners in optimizing user\ninteractions with ChatGPT and similar LLM-based systems.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "authors": [
      "Katie Seaborn"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15807v1",
    "title": "Video-VoT-R1: An efficient video inference model integrating image packing and AoE architecture",
    "abstract": "In the field of video-language pretraining, existing models face numerous\nchallenges in terms of inference efficiency and multimodal data processing.\nThis paper proposes a KunLunBaize-VoT-R1 video inference model based on a\nlong-sequence image encoder, along with its training and application methods.\nBy integrating image packing technology, the Autonomy-of-Experts (AoE)\narchitecture, and combining the video of Thought (VoT), a large language model\n(LLM) trained with large-scale reinforcement learning, and multiple training\ntechniques, the efficiency and accuracy of the model in video inference tasks\nare effectively improved. Experiments show that this model performs\noutstandingly in multiple tests, providing a new solution for video-language\nunderstanding.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Cheng Li",
      "Jiexiong Liu",
      "Yixuan Chen",
      "Yanqin Jia"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15804v1",
    "title": "Communication Efficient Federated Learning with Linear Convergence on Heterogeneous Data",
    "abstract": "By letting local clients perform multiple local updates before communicating\nwith a parameter server, modern federated learning algorithms such as FedAvg\ntackle the communication bottleneck problem in distributed learning and have\nfound many successful applications. However, this asynchrony between local\nupdates and communication also leads to a ''client-drift'' problem when the\ndata is heterogeneous (not independent and identically distributed), resulting\nin errors in the final learning result. In this paper, we propose a federated\nlearning algorithm, which is called FedCET, to ensure accurate convergence even\nunder heterogeneous distributions of data across clients. Inspired by the\ndistributed optimization algorithm NIDS, we use learning rates to weight\ninformation received from local clients to eliminate the ''client-drift''. We\nprove that under appropriate learning rates, FedCET can ensure linear\nconvergence to the exact solution. Different from existing algorithms which\nhave to share both gradients and a drift-correction term to ensure accurate\nconvergence under heterogeneous data distributions, FedCET only shares one\nvariable, which significantly reduces communication overhead. Numerical\ncomparison with existing counterpart algorithms confirms the effectiveness of\nFedCET.",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Jie Liu",
      "Yongqiang Wang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15801v1",
    "title": "Disentangling Uncertainties by Learning Compressed Data Representation",
    "abstract": "We study aleatoric and epistemic uncertainty estimation in a learned\nregressive system dynamics model. Disentangling aleatoric uncertainty (the\ninherent randomness of the system) from epistemic uncertainty (the lack of\ndata) is crucial for downstream tasks such as risk-aware control and\nreinforcement learning, efficient exploration, and robust policy transfer.\nWhile existing approaches like Gaussian Processes, Bayesian networks, and model\nensembles are widely adopted, they suffer from either high computational\ncomplexity or inaccurate uncertainty estimation. To address these limitations,\nwe propose the Compressed Data Representation Model (CDRM), a framework that\nlearns a neural network encoding of the data distribution and enables direct\nsampling from the output distribution. Our approach incorporates a novel\ninference procedure based on Langevin dynamics sampling, allowing CDRM to\npredict arbitrary output distributions rather than being constrained to a\nGaussian prior. Theoretical analysis provides the conditions where CDRM\nachieves better memory and computational complexity compared to bin-based\ncompression methods. Empirical evaluations show that CDRM demonstrates a\nsuperior capability to identify aleatoric and epistemic uncertainties\nseparately, achieving AUROCs of 0.8876 and 0.9981 on a single test set\ncontaining a mixture of both uncertainties. Qualitative results further show\nthat CDRM's capability extends to datasets with multimodal output\ndistributions, a challenging scenario where existing methods consistently fail.\nCode and supplementary materials are available at\nhttps://github.com/ryeii/CDRM.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Zhiyu An",
      "Zhibo Hou",
      "Wan Du"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15800v1",
    "title": "Frequency Enhancement for Image Demosaicking",
    "abstract": "Recovering high-frequency textures in image demosaicking remains a\nchallenging issue. While existing methods introduced elaborate spatial learning\nmethods, they still exhibit limited performance. To address this issue, a\nfrequency enhancement approach is proposed. Based on the frequency analysis of\ncolor filter array (CFA)/demosaicked/ground truth images, we propose Dual-path\nFrequency Enhancement Network (DFENet), which reconstructs RGB images in a\ndivide-and-conquer manner through fourier-domain frequency selection. In\nDFENet, two frequency selectors are employed, each selecting a set of frequency\ncomponents for processing along separate paths. One path focuses on generating\nmissing information through detail refinement in spatial domain, while the\nother aims at suppressing undesirable frequencies with the guidance of CFA\nimages in frequency domain. Multi-level frequency supervision with a stagewise\ntraining strategy is employed to further improve the reconstruction\nperformance. With these designs, the proposed DFENet outperforms other\nstate-of-the-art algorithms on different datasets and demonstrates significant\nadvantages on hard cases. Moreover, to better assess algorithms' ability to\nreconstruct high-frequency textures, a new dataset, LineSet37, is contributed,\nwhich consists of 37 artificially designed and generated images. These images\nfeature complex line patterns and are prone to severe visual artifacts like\ncolor moir\\'e after demosaicking. Experiments on LineSet37 offer a more\ntargeted evaluation of performance on challenging cases. The code and dataset\nare available at https://github.com/VelvetReverie/DFENet-demosaicking.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jingyun Liu",
      "Daiqin Yang",
      "Zhenzhong Chen"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15798v1",
    "title": "Mixture of Lookup Experts",
    "abstract": "Mixture-of-Experts (MoE) activates only a subset of experts during inference,\nallowing the model to maintain low inference FLOPs and latency even as the\nparameter count scales up. However, since MoE dynamically selects the experts,\nall the experts need to be loaded into VRAM. Their large parameter size still\nlimits deployment, and offloading, which load experts into VRAM only when\nneeded, significantly increase inference latency. To address this, we propose\nMixture of Lookup Experts (MoLE), a new MoE architecture that is efficient in\nboth communication and VRAM usage. In MoLE, the experts are Feed-Forward\nNetworks (FFNs) during training, taking the output of the embedding layer as\ninput. Before inference, these experts can be re-parameterized as lookup tables\n(LUTs) that retrieves expert outputs based on input ids, and offloaded to\nstorage devices. Therefore, we do not need to perform expert computations\nduring inference. Instead, we directly retrieve the expert's computation\nresults based on input ids and load them into VRAM, and thus the resulting\ncommunication overhead is negligible. Experiments show that, with the same\nFLOPs and VRAM usage, MoLE achieves inference speeds comparable to dense models\nand significantly faster than MoE with experts offloading, while maintaining\nperformance on par with MoE.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Shibo Jie",
      "Yehui Tang",
      "Kai Han",
      "Yitong Li",
      "Duyu Tang",
      "Zhi-Hong Deng",
      "Yunhe Wang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15796v1",
    "title": "Blend the Separated: Mixture of Synergistic Experts for Data-Scarcity Drug-Target Interaction Prediction",
    "abstract": "Drug-target interaction prediction (DTI) is essential in various applications\nincluding drug discovery and clinical application. There are two perspectives\nof input data widely used in DTI prediction: Intrinsic data represents how\ndrugs or targets are constructed, and extrinsic data represents how drugs or\ntargets are related to other biological entities. However, any of the two\nperspectives of input data can be scarce for some drugs or targets, especially\nfor those unpopular or newly discovered. Furthermore, ground-truth labels for\nspecific interaction types can also be scarce. Therefore, we propose the first\nmethod to tackle DTI prediction under input data and/or label scarcity. To make\nour model functional when only one perspective of input data is available, we\ndesign two separate experts to process intrinsic and extrinsic data\nrespectively and fuse them adaptively according to different samples.\nFurthermore, to make the two perspectives complement each other and remedy\nlabel scarcity, two experts synergize with each other in a mutually supervised\nway to exploit the enormous unlabeled data. Extensive experiments on 3\nreal-world datasets under different extents of input data scarcity and/or label\nscarcity demonstrate our model outperforms states of the art significantly and\nsteadily, with a maximum improvement of 53.53%. We also test our model without\nany data scarcity and it still outperforms current methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Xinlong Zhai",
      "Chunchen Wang",
      "Ruijia Wang",
      "Jiazheng Kang",
      "Shujie Li",
      "Boyu Chen",
      "Tengfei Ma",
      "Zikai Zhou",
      "Cheng Yang",
      "Chuan Shi"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15793v1",
    "title": "DNA Bench: When Silence is Smarter -- Benchmarking Over-Reasoning in Reasoning LLMs",
    "abstract": "Test-time scaling has significantly improved large language model\nperformance, enabling deeper reasoning to solve complex problems. However, this\nincreased reasoning capability also leads to excessive token generation and\nunnecessary problem-solving attempts. We introduce Don\\'t Answer Bench (DNA\nBench), a new benchmark designed to evaluate LLMs ability to robustly\nunderstand the tricky reasoning triggers and avoiding unnecessary generation.\nDNA Bench consists of 150 adversarially designed prompts that are easy for\nhumans to understand and respond to, but surprisingly not for many of the\nrecent prominent LLMs. DNA Bench tests models abilities across different\ncapabilities, such as instruction adherence, hallucination avoidance,\nredundancy filtering, and unanswerable question recognition. We evaluate\nreasoning LLMs (RLMs), including DeepSeek-R1, OpenAI O3-mini, Claude-3.7-sonnet\nand compare them against a powerful non-reasoning model, e.g., GPT-4o. Our\nexperiments reveal that RLMs generate up to 70x more tokens than necessary,\noften failing at tasks that simpler non-reasoning models handle efficiently\nwith higher accuracy. Our findings underscore the need for more effective\ntraining and inference strategies in RLMs.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Masoud Hashemi",
      "Oluwanifemi Bamgbose",
      "Sathwik Tejaswi Madhusudhan",
      "Jishnu Sethumadhavan Nair",
      "Aman Tiwari",
      "Vikas Yadav"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15784v1",
    "title": "RL4Med-DDPO: Reinforcement Learning for Controlled Guidance Towards Diverse Medical Image Generation using Vision-Language Foundation Models",
    "abstract": "Vision-Language Foundation Models (VLFM) have shown a tremendous increase in\nperformance in terms of generating high-resolution, photorealistic natural\nimages. While VLFMs show a rich understanding of semantic content across\nmodalities, they often struggle with fine-grained alignment tasks that require\nprecise correspondence between image regions and textual descriptions a\nlimitation in medical imaging, where accurate localization and detection of\nclinical features are essential for diagnosis and analysis. To address this\nissue, we propose a multi-stage architecture where a pre-trained VLFM provides\na cursory semantic understanding, while a reinforcement learning (RL) algorithm\nrefines the alignment through an iterative process that optimizes for\nunderstanding semantic context. The reward signal is designed to align the\nsemantic information of the text with synthesized images. We demonstrate the\neffectiveness of our method on a medical imaging skin dataset where the\ngenerated images exhibit improved generation quality and alignment with prompt\nover the fine-tuned Stable Diffusion. We also show that the synthesized samples\ncould be used to improve disease classifier performance for underrepresented\nsubgroups through augmentation.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Parham Saremi",
      "Amar Kumar",
      "Mohammed Mohammed",
      "Zahra TehraniNasab",
      "Tal Arbel"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15783v1",
    "title": "Grammar and Gameplay-aligned RL for Game Description Generation with LLMs",
    "abstract": "Game Description Generation (GDG) is the task of generating a game\ndescription written in a Game Description Language (GDL) from natural language\ntext. Previous studies have explored generation methods leveraging the\ncontextual understanding capabilities of Large Language Models (LLMs); however,\naccurately reproducing the game features of the game descriptions remains a\nchallenge. In this paper, we propose reinforcement learning-based fine-tuning\nof LLMs for GDG (RLGDG). Our training method simultaneously improves\ngrammatical correctness and fidelity to game concepts by introducing both\ngrammar rewards and concept rewards. Furthermore, we adopt a two-stage training\nstrategy where Reinforcement Learning (RL) is applied following Supervised\nFine-Tuning (SFT). Experimental results demonstrate that our proposed method\nsignificantly outperforms baseline methods using SFT alone.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Tsunehiko Tanaka",
      "Edgar Simo-Serra"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15781v1",
    "title": "UAS Visual Navigation in Large and Unseen Environments via a Meta Agent",
    "abstract": "The aim of this work is to develop an approach that enables Unmanned Aerial\nSystem (UAS) to efficiently learn to navigate in large-scale urban environments\nand transfer their acquired expertise to novel environments. To achieve this,\nwe propose a meta-curriculum training scheme. First, meta-training allows the\nagent to learn a master policy to generalize across tasks. The resulting model\nis then fine-tuned on the downstream tasks. We organize the training curriculum\nin a hierarchical manner such that the agent is guided from coarse to fine\ntowards the target task. In addition, we introduce Incremental Self-Adaptive\nReinforcement learning (ISAR), an algorithm that combines the ideas of\nincremental learning and meta-reinforcement learning (MRL). In contrast to\ntraditional reinforcement learning (RL), which focuses on acquiring a policy\nfor a specific task, MRL aims to learn a policy with fast transfer ability to\nnovel tasks. However, the MRL training process is time consuming, whereas our\nproposed ISAR algorithm achieves faster convergence than the conventional MRL\nalgorithm. We evaluate the proposed methodologies in simulated environments and\ndemonstrate that using this training philosophy in conjunction with the ISAR\nalgorithm significantly improves the convergence speed for navigation in\nlarge-scale cities and the adaptation proficiency in novel environments.",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Yuci Han",
      "Charles Toth",
      "Alper Yilmaz"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15779v1",
    "title": "MobiFuse: Learning Universal Human Mobility Patterns through Cross-domain Data Fusion",
    "abstract": "Human mobility modeling is critical for urban planning and transportation\nmanagement, yet existing datasets often lack the resolution and semantic\nrichness required for comprehensive analysis. To address this, we proposed a\ncross-domain data fusion framework that integrates multi-modal data of distinct\nnature and spatio-temporal resolution, including geographical, mobility,\nsocio-demographic, and traffic information, to construct a privacy-preserving\nand semantically enriched human travel trajectory dataset. This framework is\ndemonstrated through two case studies in Los Angeles (LA) and Egypt, where a\ndomain adaptation algorithm ensures its transferability across diverse urban\ncontexts. Quantitative evaluation shows that the generated synthetic dataset\naccurately reproduces mobility patterns observed in empirical data. Moreover,\nlarge-scale traffic simulations for LA County based on the generated synthetic\ndemand align well with observed traffic. On California's I-405 corridor, the\nsimulation yields a Mean Absolute Percentage Error of 5.85% for traffic volume\nand 4.36% for speed compared to Caltrans PeMS observations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Haoxuan Ma",
      "Xishun Liao",
      "Yifan Liu",
      "Qinhua Jiang",
      "Chris Stanford",
      "Shangqing Cao",
      "Jiaqi Ma"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15778v1",
    "title": "AutoDrive-QA- Automated Generation of Multiple-Choice Questions for Autonomous Driving Datasets Using Large Vision-Language Models",
    "abstract": "In autonomous driving, open-ended question answering often suffers from\nunreliable evaluations because freeform responses require either complex\nmetrics or subjective human judgment. To address this challenge, we introduce\nAutoDrive-QA, an automatic pipeline that converts existing driving QA datasets\n(including DriveLM, NuScenes-QA, and LingoQA) into a structured multiple-choice\nquestion (MCQ) format. This benchmark systematically assesses perception,\nprediction, and planning tasks, providing a standardized and objective\nevaluation framework. AutoDrive-QA employs an automated pipeline that leverages\nlarge language models (LLMs) to generate high-quality, contextually relevant\ndistractors based on domain-specific error patterns commonly found in\nautonomous driving scenarios. To evaluate both general capabilities and\ngeneralization performance, we test the benchmark on three public datasets and\nconduct zero-shot experiments on an unseen dataset. The zero-shot evaluations\nreveal that GPT-4V leads with 69.57% accuracy -- achieving 74.94% in\nPerception, 65.33% in Prediction, and 68.45% in Planning -- demonstrating that\nwhile all models excel in Perception, they struggle in Prediction.\nConsequently, AutoDrive-QA establishes a rigorous, unbiased standard for\nintegrating and evaluating different vision-language models across various\nautonomous driving datasets, thereby improving generalization in this field. We\nrelease all the codes in the AutoDrive-QA GitHub Repository.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Boshra Khalili",
      "Andrew W. Smyth"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15777v1",
    "title": "Line Space Clustering (LSC): Feature-Based Clustering using K-medians and Dynamic Time Warping for Versatility",
    "abstract": "Clustering high-dimensional data is a critical challenge in machine learning\ndue to the curse of dimensionality and the presence of noise. Traditional\nclustering algorithms often fail to capture the intrinsic structures in such\ndata. This paper explores a combination of clustering methods, which we called\nLine Space Clustering (LSC), a representation that transforms data points into\nlines in a newly defined feature space, enabling clustering based on the\nsimilarity of feature value patterns, essentially treating features as\nsequences. LSC employs a combined distance metric that uses Euclidean and\nDynamic Time Warping (DTW) distances, weighted by a parameter {\\alpha},\nallowing flexibility in emphasizing shape or magnitude similarities. We delve\ndeeply into the mechanics of DTW and the Savitzky Golay filter, explaining\ntheir roles in the algorithm. Extensive experiments demonstrate the efficacy of\nLSC on synthetic and real-world datasets, showing that randomly experimenting\nwith time-series optimized methods sometimes might surprisingly work on a\ncomplex dataset, particularly in noisy environments.\n  Source code and experiments are available at:\nhttps://github.com/JoanikijChulev/LSC.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Joanikij Chulev",
      "Angela Mladenovska"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15772v1",
    "title": "Detecting LLM-Written Peer Reviews",
    "abstract": "Editors of academic journals and program chairs of conferences require peer\nreviewers to write their own reviews. However, there is growing concern about\nthe rise of lazy reviewing practices, where reviewers use large language models\n(LLMs) to generate reviews instead of writing them independently. Existing\ntools for detecting LLM-generated content are not designed to differentiate\nbetween fully LLM-generated reviews and those merely polished by an LLM. In\nthis work, we employ a straightforward approach to identify LLM-generated\nreviews - doing an indirect prompt injection via the paper PDF to ask the LLM\nto embed a watermark. Our focus is on presenting watermarking schemes and\nstatistical tests that maintain a bounded family-wise error rate, when a venue\nevaluates multiple reviews, with a higher power as compared to standard methods\nlike Bonferroni correction. These guarantees hold without relying on any\nassumptions about human-written reviews. We also consider various methods for\nprompt injection including font embedding and jailbreaking. We evaluate the\neffectiveness and various tradeoffs of these methods, including different\nreviewer defenses. We find a high success rate in the embedding of our\nwatermarks in LLM-generated reviews across models. We also find that our\napproach is resilient to common reviewer defenses, and that the bounds on error\nrates in our statistical tests hold in practice while having the power to flag\nLLM-generated reviews, while Bonferroni correction is infeasible.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CR"
    ],
    "authors": [
      "Vishisht Rao",
      "Aounon Kumar",
      "Himabindu Lakkaraju",
      "Nihar B. Shah"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15770v1",
    "title": "Nano-3D: Metasurface-Based Neural Depth Imaging",
    "abstract": "Depth imaging is a foundational building block for broad applications, such\nas autonomous driving and virtual/augmented reality. Traditionally, depth\ncameras have relied on time-of-flight sensors or multi-lens systems to achieve\nphysical depth measurements. However, these systems often face a trade-off\nbetween a bulky form factor and imprecise approximations, limiting their\nsuitability for spatially constrained scenarios. Inspired by the emerging\nadvancements of nano-optics, we present Nano-3D, a metasurface-based neural\ndepth imaging solution with an ultra-compact footprint. Nano-3D integrates our\ncustom-fabricated 700 nm thick TiO2 metasurface with a multi-module deep neural\nnetwork to extract precise metric depth information from monocular\nmetasurface-polarized imagery. We demonstrate the effectiveness of Nano-3D with\nboth simulated and physical experiments. We hope the exhibited success paves\nthe way for the community to bridge future graphics systems with emerging\nnanomaterial technologies through novel computational approaches.",
    "categories": [
      "physics.optics",
      "cs.AR",
      "cs.CV"
    ],
    "authors": [
      "Bingxuan Li",
      "Jiahao Wu",
      "Yuan Xu",
      "Yunxiang Zhang",
      "Zezheng Zhu",
      "Nanfang Yu",
      "Qi Sun"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15769v1",
    "title": "Prediction of Permissioned Blockchain Performance for Resource Scaling Configurations",
    "abstract": "Blockchain is increasingly offered as blockchain-as-a-service (BaaS) by cloud\nservice providers. However, configuring BaaS appropriately for optimal\nperformance and reliability resorts to try-and-error. A key challenge is that\nBaaS is often perceived as a ``black-box,'' leading to uncertainties in\nperformance and resource provisioning. Previous studies attempted to address\nthis challenge; however, the impacts of both vertical and horizontal scaling\nremain elusive. To this end, we present machine learning-based models to\npredict network reliability and throughput based on scaling configurations. In\nour evaluation, the models exhibit prediction errors of ~1.9%, which is highly\naccurate and can be applied in the real-world.",
    "categories": [
      "cs.DC",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Seungwoo Jung",
      "Yeonho Yoo",
      "Gyeongsik Yang",
      "Chuck Yoo"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15768v1",
    "title": "Can one size fit all?: Measuring Failure in Multi-Document Summarization Domain Transfer",
    "abstract": "Abstractive multi-document summarization (MDS) is the task of automatically\nsummarizing information in multiple documents, from news articles to\nconversations with multiple speakers. The training approaches for current MDS\nmodels can be grouped into four approaches: end-to-end with special\npre-training (\"direct\"), chunk-then-summarize, extract-then-summarize, and\ninference with GPT-style models. In this work, we evaluate MDS models across\ntraining approaches, domains, and dimensions (reference similarity, quality,\nand factuality), to analyze how and why models trained on one domain can fail\nto summarize documents from another (News, Science, and Conversation) in the\nzero-shot domain transfer setting. We define domain-transfer \"failure\" as a\ndecrease in factuality, higher deviation from the target, and a general\ndecrease in summary quality. In addition to exploring domain transfer for MDS\nmodels, we examine potential issues with applying popular summarization metrics\nout-of-the-box.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Alexandra DeLucia",
      "Mark Dredze"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.16558v1",
    "title": "Advancing Problem-Based Learning in Biomedical Engineering in the Era of Generative AI",
    "abstract": "Problem-Based Learning (PBL) has significantly impacted biomedical\nengineering (BME) education since its introduction in the early 2000s,\neffectively enhancing critical thinking and real-world knowledge application\namong students. With biomedical engineering rapidly converging with artificial\nintelligence (AI), integrating effective AI education into established\ncurricula has become challenging yet increasingly necessary. Recent\nadvancements, including AI's recognition by the 2024 Nobel Prize, have\nhighlighted the importance of training students comprehensively in biomedical\nAI. However, effective biomedical AI education faces substantial obstacles,\nsuch as diverse student backgrounds, limited personalized mentoring,\nconstrained computational resources, and difficulties in safely scaling\nhands-on practical experiments due to privacy and ethical concerns associated\nwith biomedical data. To overcome these issues, we conducted a three-year\n(2021-2023) case study implementing an advanced PBL framework tailored\nspecifically for biomedical AI education, involving 92 undergraduate and 156\ngraduate students from the joint Biomedical Engineering program of Georgia\nInstitute of Technology and Emory University. Our approach emphasizes\ncollaborative, interdisciplinary problem-solving through authentic biomedical\nAI challenges. The implementation led to measurable improvements in learning\noutcomes, evidenced by high research productivity (16 student-authored\npublications), consistently positive peer evaluations, and successful\ndevelopment of innovative computational methods addressing real biomedical\nchallenges. Additionally, we examined the role of generative AI both as a\nteaching subject and an educational support tool within the PBL framework. Our\nstudy presents a practical and scalable roadmap for biomedical engineering\ndepartments aiming to integrate robust AI education into their curricula.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "authors": [
      "Micky C. Nnamdi",
      "J. Ben Tamo",
      "Wenqi Shi",
      "May D. Wang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15766v1",
    "title": "Accelerating Transient CFD through Machine Learning-Based Flow Initialization",
    "abstract": "Transient computational fluid dynamics (CFD) simulations are essential for\nmany industrial applications, but a significant portion of their computational\ncost stems from the time needed to reach statistical steadiness from initial\nconditions. We present a novel machine learning-based initialization method\nthat reduces the cost of this subsequent transient solve substantially,\nachieving a 50% reduction in time-to-convergence compared to traditional\nuniform and potential flow-based initializations. Through a case study in\nautomotive aerodynamics using a 16.7M-cell unsteady RANS simulation, we\nevaluate three ML-based initialization strategies. Two of these strategies are\nrecommended for general use: (1) a physics-informed hybrid method combining ML\npredictions with potential flow solutions, and (2) a more versatile approach\nintegrating ML predictions with uniform flow. Both strategies enable CFD\nsolvers to achieve convergence times comparable to computationally expensive\nsteady RANS initializations, while requiring only seconds of computation. We\ndevelop a robust statistical convergence metric based on windowed\ntime-averaging for performance comparison between initialization strategies.\nNotably, these improvements are achieved using an ML model trained on a\ndifferent dataset of automotive geometries, demonstrating strong generalization\ncapabilities. The proposed methods integrate seamlessly with existing CFD\nworkflows without requiring modifications to the underlying flow solver,\nproviding a practical approach to accelerating industrial CFD simulations\nthrough improved ML-based initialization strategies.",
    "categories": [
      "cs.LG",
      "physics.flu-dyn"
    ],
    "authors": [
      "Peter Sharpe",
      "Rishikesh Ranade",
      "Sanjay Choudhry"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15764v1",
    "title": "Towards Agentic AI Networking in 6G: A Generative Foundation Model-as-Agent Approach",
    "abstract": "The promising potential of AI and network convergence in improving networking\nperformance and enabling new service capabilities has recently attracted\nsignificant interest. Existing network AI solutions, while powerful, are mainly\nbuilt based on the close-loop and passive learning framework, resulting in\nmajor limitations in autonomous solution finding and dynamic environmental\nadaptation. Agentic AI has recently been introduced as a promising solution to\naddress the above limitations and pave the way for true generally intelligent\nand beneficial AI systems. The key idea is to create a networking ecosystem to\nsupport a diverse range of autonomous and embodied AI agents in fulfilling\ntheir goals. In this paper, we focus on the novel challenges and requirements\nof agentic AI networking. We propose AgentNet, a novel framework for supporting\ninteraction, collaborative learning, and knowledge transfer among AI agents. We\nintroduce a general architectural framework of AgentNet and then propose a\ngenerative foundation model (GFM)-based implementation in which multiple\nGFM-as-agents have been created as an interactive knowledge-base to bootstrap\nthe development of embodied AI agents according to different task requirements\nand environmental features. We consider two application scenarios,\ndigital-twin-based industrial automation and metaverse-based infotainment\nsystem, to describe how to apply AgentNet for supporting efficient task-driven\ncollaboration and interaction among AI agents.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "authors": [
      "Yong Xiao",
      "Guangming Shi",
      "Ping Zhang"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15763v1",
    "title": "OffsetOPT: Explicit Surface Reconstruction without Normals",
    "abstract": "Neural surface reconstruction has been dominated by implicit representations\nwith marching cubes for explicit surface extraction. However, those methods\ntypically require high-quality normals for accurate reconstruction. We propose\nOffsetOPT, a method that reconstructs explicit surfaces directly from 3D point\nclouds and eliminates the need for point normals. The approach comprises two\nstages: first, we train a neural network to predict surface triangles based on\nlocal point geometry, given uniformly distributed training point clouds. Next,\nwe apply the frozen network to reconstruct surfaces from unseen point clouds by\noptimizing a per-point offset to maximize the accuracy of triangle predictions.\nCompared to state-of-the-art methods, OffsetOPT not only excels at\nreconstructing overall surfaces but also significantly preserves sharp surface\nfeatures. We demonstrate its accuracy on popular benchmarks, including\nsmall-scale shapes and large-scale open surfaces.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Huan Lei"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15762v1",
    "title": "Dialogic Learning in Child-Robot Interaction: A Hybrid Approach to Personalized Educational Content Generation",
    "abstract": "Dialogic learning fosters motivation and deeper understanding in education\nthrough purposeful and structured dialogues. Foundational models offer a\ntransformative potential for child-robot interactions, enabling the design of\npersonalized, engaging, and scalable interactions. However, their integration\ninto educational contexts presents challenges in terms of ensuring\nage-appropriate and safe content and alignment with pedagogical goals. We\nintroduce a hybrid approach to designing personalized educational dialogues in\nchild-robot interactions. By combining rule-based systems with LLMs for\nselective offline content generation and human validation, the framework\nensures educational quality and developmental appropriateness. We illustrate\nthis approach through a project aimed at enhancing reading motivation, in which\na robot facilitated book-related dialogues.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Elena Malnatsky",
      "Shenghui Wang",
      "Koen V. Hindriks",
      "Mike E. U. Ligthart"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15761v1",
    "title": "GraPLUS: Graph-based Placement Using Semantics for Image Composition",
    "abstract": "We present GraPLUS (Graph-based Placement Using Semantics), a novel framework\nfor plausible object placement in images that leverages scene graphs and large\nlanguage models. Our approach uniquely combines graph-structured scene\nrepresentation with semantic understanding to determine contextually\nappropriate object positions. The framework employs GPT-2 to transform\ncategorical node and edge labels into rich semantic embeddings that capture\nboth definitional characteristics and typical spatial contexts, enabling\nnuanced understanding of object relationships and placement patterns. GraPLUS\nachieves placement accuracy of 92.1% and an FID score of 28.83 on the OPA\ndataset, outperforming state-of-the-art methods by 8.1% while maintaining\ncompetitive visual quality. In human evaluation studies involving 964 samples\nassessed by 19 participants, our method was preferred in 52.1% of cases,\nsignificantly outperforming previous approaches. The framework's key\ninnovations include: (i) leveraging pre-trained scene graph models that\ntransfer knowledge from other domains, (ii) edge-aware graph neural networks\nthat process scene semantics through structured relationships, (iii) a\ncross-modal attention mechanism that aligns categorical embeddings with\nenhanced scene features, and (iv) a multiobjective training strategy\nincorporating semantic consistency constraints.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Mir Mohammad Khaleghi",
      "Mehran Safayani",
      "Abdolreza Mirzaei"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15758v1",
    "title": "ATTENTION2D: Communication Efficient Distributed Self-Attention Mechanism",
    "abstract": "Transformer-based models have emerged as a leading architecture for natural\nlanguage processing, natural language generation, and image generation tasks. A\nfundamental element of the transformer architecture is self-attention, which\nallows the model to capture intricate dependencies within the data. However,\nthe self-attention mechanism also incurs significant computational and memory\ncosts, particularly for long sequences.\n  In this paper, we introduce ATTENTION2D, a novel approach that exploits\nparallelism along two dimensions - query and key/value - of the self-attention\noperation. This method enables efficient distribution and parallelization of\ncomputations across multiple devices. Our approach facilitates asymptotically\nfaster training and inference phases compared to previous methods, without\nrelying on approximations or incurring additional computational or memory\noverheads. Furthermore, unlike existing techniques that struggle to scale with\nan increasing number of processing units, our approach effectively scales with\nadditional processing units.\n  Our experimental results confirm the effectiveness of our method in improving\ncommunication efficiency and scalability. Compared to Ring Attention, our\napproach demonstrated up to a 5x performance boost on a GPT-3-like model using\n64 NVIDIA A100 GPUs across 16 nodes, and up to a 9.4x performance boost on 64\nNVIDIA H100 GPUs across 64 nodes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "authors": [
      "Venmugil Elango"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15754v1",
    "title": "AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration",
    "abstract": "As large language models (LLMs) become increasingly capable, security and\nsafety evaluation are crucial. While current red teaming approaches have made\nstrides in assessing LLM vulnerabilities, they often rely heavily on human\ninput and lack comprehensive coverage of emerging attack vectors. This paper\nintroduces AutoRedTeamer, a novel framework for fully automated, end-to-end red\nteaming against LLMs. AutoRedTeamer combines a multi-agent architecture with a\nmemory-guided attack selection mechanism to enable continuous discovery and\nintegration of new attack vectors. The dual-agent framework consists of a red\nteaming agent that can operate from high-level risk categories alone to\ngenerate and execute test cases and a strategy proposer agent that autonomously\ndiscovers and implements new attacks by analyzing recent research. This modular\ndesign allows AutoRedTeamer to adapt to emerging threats while maintaining\nstrong performance on existing attack vectors. We demonstrate AutoRedTeamer's\neffectiveness across diverse evaluation settings, achieving 20% higher attack\nsuccess rates on HarmBench against Llama-3.1-70B while reducing computational\ncosts by 46% compared to existing approaches. AutoRedTeamer also matches the\ndiversity of human-curated benchmarks in generating test cases, providing a\ncomprehensive, scalable, and continuously evolving framework for evaluating the\nsecurity of AI systems.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Andy Zhou",
      "Kevin Wu",
      "Francesco Pinto",
      "Zhaorun Chen",
      "Yi Zeng",
      "Yu Yang",
      "Shuang Yang",
      "Sanmi Koyejo",
      "James Zou",
      "Bo Li"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15752v1",
    "title": "Using Language Models to Decipher the Motivation Behind Human Behaviors",
    "abstract": "AI presents a novel tool for deciphering the motivations behind human\nbehaviors. We show that by varying prompts to a large language model, we can\nelicit a full range of human behaviors in a variety of different scenarios in\nterms of classic economic games. Then by analyzing which prompts are needed to\nelicit which behaviors, we can infer (decipher) the motivations behind the\nhuman behaviors. We also show how one can analyze the prompts to reveal\nrelationships between the classic economic games, providing new insight into\nwhat different economic scenarios induce people to think about. We also show\nhow this deciphering process can be used to understand differences in the\nbehavioral tendencies of different populations.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Yutong Xie",
      "Qiaozhu Mei",
      "Walter Yuan",
      "Matthew O. Jackson"
    ],
    "published_date": "2025-03-20",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15748v1",
    "title": "PARQ: Piecewise-Affine Regularized Quantization",
    "abstract": "We develop a principled method for quantization-aware training (QAT) of\nlarge-scale machine learning models. Specifically, we show that convex,\npiecewise-affine regularization (PAR) can effectively induce the model\nparameters to cluster towards discrete values. We minimize PAR-regularized loss\nfunctions using an aggregate proximal stochastic gradient method (AProx) and\nprove that it has last-iterate convergence. Our approach provides an\ninterpretation of the straight-through estimator (STE), a widely used heuristic\nfor QAT, as the asymptotic form of PARQ. We conduct experiments to demonstrate\nthat PARQ obtains competitive performance on convolution- and transformer-based\nvision tasks.",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Lisa Jin",
      "Jianhao Ma",
      "Zechun Liu",
      "Andrey Gromov",
      "Aaron Defazio",
      "Lin Xiao"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15742v1",
    "title": "Uncertainty-Aware Diffusion Guided Refinement of 3D Scenes",
    "abstract": "Reconstructing 3D scenes from a single image is a fundamentally ill-posed\ntask due to the severely under-constrained nature of the problem. Consequently,\nwhen the scene is rendered from novel camera views, existing single image to 3D\nreconstruction methods render incoherent and blurry views. This problem is\nexacerbated when the unseen regions are far away from the input camera. In this\nwork, we address these inherent limitations in existing single image-to-3D\nscene feedforward networks. To alleviate the poor performance due to\ninsufficient information beyond the input image's view, we leverage a strong\ngenerative prior in the form of a pre-trained latent video diffusion model, for\niterative refinement of a coarse scene represented by optimizable Gaussian\nparameters. To ensure that the style and texture of the generated images align\nwith that of the input image, we incorporate on-the-fly Fourier-style transfer\nbetween the generated images and the input image. Additionally, we design a\nsemantic uncertainty quantification module that calculates the per-pixel\nentropy and yields uncertainty maps used to guide the refinement process from\nthe most confident pixels while discarding the remaining highly uncertain ones.\nWe conduct extensive experiments on real-world scene datasets, including\nin-domain RealEstate-10K and out-of-domain KITTI-v2, showing that our approach\ncan provide more realistic and high-fidelity novel view synthesis results\ncompared to existing state-of-the-art methods.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Sarosij Bose",
      "Arindam Dutta",
      "Sayak Nag",
      "Junge Zhang",
      "Jiachen Li",
      "Konstantinos Karydis",
      "Amit K. Roy Chowdhury"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15739v1",
    "title": "ECLAIR: Enhanced Clarification for Interactive Responses",
    "abstract": "We present ECLAIR (Enhanced CLArification for Interactive Responses), a novel\nunified and end-to-end framework for interactive disambiguation in enterprise\nAI assistants. ECLAIR generates clarification questions for ambiguous user\nqueries and resolves ambiguity based on the user's response.We introduce a\ngeneralized architecture capable of integrating ambiguity information from\nmultiple downstream agents, enhancing context-awareness in resolving\nambiguities and allowing enterprise specific definition of agents. We further\ndefine agents within our system that provide domain-specific grounding\ninformation. We conduct experiments comparing ECLAIR to few-shot prompting\ntechniques and demonstrate ECLAIR's superior performance in clarification\nquestion generation and ambiguity resolution.",
    "categories": [
      "cs.AI",
      "68T50",
      "I.2.7; H.5.2"
    ],
    "authors": [
      "John Murzaku",
      "Zifan Liu",
      "Md Mehrab Tanjim",
      "Vaishnavi Muppala",
      "Xiang Chen",
      "Yunyao Li"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15737v1",
    "title": "KoGNER: A Novel Framework for Knowledge Graph Distillation on Biomedical Named Entity Recognition",
    "abstract": "Named Entity Recognition (NER) is a fundamental task in Natural Language\nProcessing (NLP) that plays a crucial role in information extraction, question\nanswering, and knowledge-based systems. Traditional deep learning-based NER\nmodels often struggle with domain-specific generalization and suffer from data\nsparsity issues. In this work, we introduce Knowledge Graph distilled for Named\nEntity Recognition (KoGNER), a novel approach that integrates Knowledge Graph\n(KG) distillation into NER models to enhance entity recognition performance.\nOur framework leverages structured knowledge representations from KGs to enrich\ncontextual embeddings, thereby improving entity classification and reducing\nambiguity in entity detection. KoGNER employs a two-step process: (1) Knowledge\nDistillation, where external knowledge sources are distilled into a lightweight\nrepresentation for seamless integration with NER models, and (2) Entity-Aware\nAugmentation, which integrates contextual embeddings that have been enriched\nwith knowledge graph information directly into GNN, thereby improving the\nmodel's ability to understand and represent entity relationships. Experimental\nresults on benchmark datasets demonstrate that KoGNER achieves state-of-the-art\nperformance, outperforming finetuned NER models and LLMs by a significant\nmargin. These findings suggest that leveraging knowledge graphs as auxiliary\ninformation can significantly improve NER accuracy, making KoGNER a promising\ndirection for future research in knowledge-aware NLP.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Heming Zhang",
      "Wenyu Li",
      "Di Huang",
      "Yinjie Tang",
      "Yixin Chen",
      "Philip Payne",
      "Fuhai Li"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15731v1",
    "title": "Graph-Weighted Contrastive Learning for Semi-Supervised Hyperspectral Image Classification",
    "abstract": "Most existing graph-based semi-supervised hyperspectral image classification\nmethods rely on superpixel partitioning techniques. However, they suffer from\nmisclassification of certain pixels due to inaccuracies in superpixel\nboundaries, \\ie, the initial inaccuracies in superpixel partitioning limit\noverall classification performance. In this paper, we propose a novel\ngraph-weighted contrastive learning approach that avoids the use of superpixel\npartitioning and directly employs neural networks to learn hyperspectral image\nrepresentation. Furthermore, while many approaches require all graph nodes to\nbe available during training, our approach supports mini-batch training by\nprocessing only a subset of nodes at a time, reducing computational complexity\nand improving generalization to unseen nodes. Experimental results on three\nwidely-used datasets demonstrate the effectiveness of the proposed approach\ncompared to baselines relying on superpixel partitioning.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yuqing Zhang",
      "Qi Han",
      "Ligeng Wang",
      "Kai Cheng",
      "Bo Wang",
      "Kun Zhan"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15726v1",
    "title": "Reinforcement Learning Environment with LLM-Controlled Adversary in D&D 5th Edition Combat",
    "abstract": "The objective of this study is to design and implement a reinforcement\nlearning (RL) environment using D\\&D 5E combat scenarios to challenge smaller\nRL agents through interaction with a robust adversarial agent controlled by\nadvanced Large Language Models (LLMs) like GPT-4o and LLaMA 3 8B. This research\nemploys Deep Q-Networks (DQN) for the smaller agents, creating a testbed for\nstrategic AI development that also serves as an educational tool by simulating\ndynamic and unpredictable combat scenarios. We successfully integrated\nsophisticated language models into the RL framework, enhancing strategic\ndecision-making processes. Our results indicate that while RL agents generally\noutperform LLM-controlled adversaries in standard metrics, the strategic depth\nprovided by LLMs significantly enhances the overall AI capabilities in this\ncomplex, rule-based setting. The novelty of our approach and its implications\nfor mastering intricate environments and developing adaptive strategies are\ndiscussed, alongside potential innovations in AI-driven interactive\nsimulations. This paper aims to demonstrate how integrating LLMs can create\nmore robust and adaptable AI systems, providing valuable insights for further\nresearch and educational applications.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Joseph Emmanuel DL Dayo",
      "Michel Onasis S. Ogbinar",
      "Prospero C. Naval Jr"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15724v1",
    "title": "Reward Training Wheels: Adaptive Auxiliary Rewards for Robotics Reinforcement Learning",
    "abstract": "Robotics Reinforcement Learning (RL) often relies on carefully engineered\nauxiliary rewards to supplement sparse primary learning objectives to\ncompensate for the lack of large-scale, real-world, trial-and-error data. While\nthese auxiliary rewards accelerate learning, they require significant\nengineering effort, may introduce human biases, and cannot adapt to the robot's\nevolving capabilities during training. In this paper, we introduce Reward\nTraining Wheels (RTW), a teacher-student framework that automates auxiliary\nreward adaptation for robotics RL. To be specific, the RTW teacher dynamically\nadjusts auxiliary reward weights based on the student's evolving capabilities\nto determine which auxiliary reward aspects require more or less emphasis to\nimprove the primary objective. We demonstrate RTW on two challenging robot\ntasks: navigation in highly constrained spaces and off-road vehicle mobility on\nvertically challenging terrain. In simulation, RTW outperforms expert-designed\nrewards by 2.35% in navigation success rate and improves off-road mobility\nperformance by 122.62%, while achieving 35% and 3X faster training efficiency,\nrespectively. Physical robot experiments further validate RTW's effectiveness,\nachieving a perfect success rate (5/5 trials vs. 2/5 for expert-designed\nrewards) and improving vehicle stability with up to 47.4% reduction in\norientation angles.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Linji Wang",
      "Tong Xu",
      "Yuanjie Lu",
      "Xuesu Xiao"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15718v1",
    "title": "Am I eligible? Natural Language Inference for Clinical Trial Patient Recruitment: the Patient's Point of View",
    "abstract": "Recruiting patients to participate in clinical trials can be challenging and\ntime-consuming. Usually, participation in a clinical trial is initiated by a\nhealthcare professional and proposed to the patient. Promoting clinical trials\ndirectly to patients via online recruitment might help to reach them more\nefficiently. In this study, we address the case where a patient is initiating\ntheir own recruitment process and wants to determine whether they are eligible\nfor a given clinical trial, using their own language to describe their medical\nprofile. To study whether this creates difficulties in the patient trial\nmatching process, we design a new dataset and task, Natural Language Inference\nfor Patient Recruitment (NLI4PR), in which patient language profiles must be\nmatched to clinical trials. We create it by adapting the TREC 2022 Clinical\nTrial Track dataset, which provides patients' medical profiles, and rephrasing\nthem manually using patient language. We also use the associated clinical trial\nreports where the patients are either eligible or excluded. We prompt several\nopen-source Large Language Models on our task and achieve from 56.5 to 71.8 of\nF1 score using patient language, against 64.7 to 73.1 for the same task using\nmedical language. When using patient language, we observe only a small loss in\nperformance for the best model, suggesting that having the patient as a\nstarting point could be adopted to help recruit patients for clinical trials.\nThe corpus and code bases are all freely available on our Github and\nHuggingFace repositories.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Mathilde Aguiar",
      "Pierre Zweigenbaum",
      "Nona Naderi"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15712v1",
    "title": "SPNeRF: Open Vocabulary 3D Neural Scene Segmentation with Superpoints",
    "abstract": "Open-vocabulary segmentation, powered by large visual-language models like\nCLIP, has expanded 2D segmentation capabilities beyond fixed classes predefined\nby the dataset, enabling zero-shot understanding across diverse scenes.\nExtending these capabilities to 3D segmentation introduces challenges, as\nCLIP's image-based embeddings often lack the geometric detail necessary for 3D\nscene segmentation. Recent methods tend to address this by introducing\nadditional segmentation models or replacing CLIP with variations trained on\nsegmentation data, which lead to redundancy or loss on CLIP's general language\ncapabilities. To overcome this limitation, we introduce SPNeRF, a NeRF based\nzero-shot 3D segmentation approach that leverages geometric priors. We\nintegrate geometric primitives derived from the 3D scene into NeRF training to\nproduce primitive-wise CLIP features, avoiding the ambiguity of point-wise\nfeatures. Additionally, we propose a primitive-based merging mechanism enhanced\nwith affinity scores. Without relying on additional segmentation models, our\nmethod further explores CLIP's capability for 3D segmentation and achieves\nnotable improvements over original LERF.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Weiwen Hu",
      "Niccolò Parodi",
      "Marcus Zepp",
      "Ingo Feldmann",
      "Oliver Schreer",
      "Peter Eisert"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15708v1",
    "title": "Sustainable Deep Learning-Based Breast Lesion Segmentation: Impact of Breast Region Segmentation on Performance",
    "abstract": "Purpose: Segmentation of the breast lesion in dynamic contrast-enhanced\nmagnetic resonance imaging (DCE-MRI) is an essential step to accurately\ndiagnose and plan treatment and monitor progress. This study aims to highlight\nthe impact of breast region segmentation (BRS) on deep learning-based breast\nlesion segmentation (BLS) in breast DCE-MRI.\n  Methods Using the Stavanger Dataset containing primarily 59 DCE-MRI scans and\nUNet++ as deep learning models, four different process were conducted to\ncompare effect of BRS on BLS. These four approaches included the whole volume\nwithout BRS and with BRS, BRS with the selected lesion slices and lastly\noptimal volume with BRS. Preprocessing methods like augmentation and\noversampling were used to enhance the small dataset, data shape uniformity and\nimprove model performance. Optimal volume size were investigated by a precise\nprocess to ensure that all lesions existed in slices. To evaluate the model, a\nhybrid loss function including dice, focal and cross entropy along with 5-fold\ncross validation method were used and lastly a test dataset which was randomly\nsplit used to evaluate the model performance on unseen data for each of four\nmentioned approaches.\n  Results Results demonstrate that using BRS considerably improved model\nperformance and validation. Significant improvement in last approach -- optimal\nvolume with BRS -- compared to the approach without BRS counting around 50\npercent demonstrating how effective BRS has been in BLS. Moreover, huge\nimprovement in energy consumption, decreasing up to 450 percent, introduces a\ngreen solution toward a more environmentally sustainable approach for future\nwork on large dataset.",
    "categories": [
      "cs.CV",
      "physics.med-ph"
    ],
    "authors": [
      "Sam Narimani",
      "Solveig Roth Hoff",
      "Kathinka Dahli Kurz",
      "Kjell-Inge Gjesdal",
      "Jurgen Geisler",
      "Endre Grovik"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15707v1",
    "title": "Safety Aware Task Planning via Large Language Models in Robotics",
    "abstract": "The integration of large language models (LLMs) into robotic task planning\nhas unlocked better reasoning capabilities for complex, long-horizon workflows.\nHowever, ensuring safety in LLM-driven plans remains a critical challenge, as\nthese models often prioritize task completion over risk mitigation. This paper\nintroduces SAFER (Safety-Aware Framework for Execution in Robotics), a\nmulti-LLM framework designed to embed safety awareness into robotic task\nplanning. SAFER employs a Safety Agent that operates alongside the primary task\nplanner, providing safety feedback. Additionally, we introduce LLM-as-a-Judge,\na novel metric leveraging LLMs as evaluators to quantify safety violations\nwithin generated task plans. Our framework integrates safety feedback at\nmultiple stages of execution, enabling real-time risk assessment, proactive\nerror correction, and transparent safety evaluation. We also integrate a\ncontrol framework using Control Barrier Functions (CBFs) to ensure safety\nguarantees within SAFER's task planning. We evaluated SAFER against\nstate-of-the-art LLM planners on complex long-horizon tasks involving\nheterogeneous robotic agents, demonstrating its effectiveness in reducing\nsafety violations while maintaining task efficiency. We also verify the task\nplanner and safety planner through actual hardware experiments involving\nmultiple robots and a human.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Azal Ahmad Khan",
      "Michael Andrev",
      "Muhammad Ali Murtaza",
      "Sergio Aguilera",
      "Rui Zhang",
      "Jie Ding",
      "Seth Hutchinson",
      "Ali Anwar"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15706v1",
    "title": "Using machine learning to map simulated noisy and laser-limited multidimensional spectra to molecular electronic couplings",
    "abstract": "Two-dimensional electronic spectroscopy (2DES) has enabled significant\ndiscoveries in both biological and synthetic energy-transducing systems.\nAlthough deriving chemical information from 2DES is a complex task, machine\nlearning (ML) offers exciting opportunities to translate complicated\nspectroscopic data into physical insight. Recent studies have found that neural\nnetworks (NNs) can map simulated multidimensional spectra to molecular-scale\nproperties with high accuracy. However, simulations often do not capture\nexperimental factors that influence real spectra, including noise and\nsuboptimal pulse resonance conditions, bringing into question the experimental\nutility of NNs trained on simulated data. Here, we show how factors associated\nwith experimental 2D spectral data influence the ability of NNs to map\nsimulated 2DES spectra onto underlying intermolecular electronic couplings. By\nsystematically introducing multisourced noise into a library of 356000\nsimulated 2D spectra, we show that noise does not hamper NN performance for\nspectra exceeding threshold signal-to-noise ratios (SNR) (> 6.6 if background\nnoise dominates vs. > 2.5 for intensity-dependent noise). In stark contrast to\nhuman-based analyses of 2DES data, we find that the NN accuracy improves\nsignificantly (ca. 84% $\\rightarrow$ 96%) when the data are constrained by the\nbandwidth and center frequency of the pump pulses. This result is consistent\nwith the NN learning the optical trends described by Kasha's theory of\nmolecular excitons. Our findings convey positive prospects for adapting\nsimulation-trained NNs to extract molecular properties from inherently\nimperfect experimental 2DES data. More broadly, we propose that machine-learned\nperspectives of nonlinear spectroscopic data may produce unique and, perhaps,\ncounterintuitive guidelines for experimental design.",
    "categories": [
      "physics.chem-ph",
      "cs.LG",
      "quant-ph"
    ],
    "authors": [
      "Jonathan D. Schultz",
      "Kelsey A. Parker",
      "Bashir Sbaiti",
      "David N. Beratan"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15704v1",
    "title": "Tuning Sequential Monte Carlo Samplers via Greedy Incremental Divergence Minimization",
    "abstract": "The performance of sequential Monte Carlo (SMC) samplers heavily depends on\nthe tuning of the Markov kernels used in the path proposal. For SMC samplers\nwith unadjusted Markov kernels, standard tuning objectives, such as the\nMetropolis-Hastings acceptance rate or the expected-squared jump distance, are\nno longer applicable. While stochastic gradient-based end-to-end optimization\nhas been explored for tuning SMC samplers, they often incur excessive training\ncosts, even for tuning just the kernel step sizes. In this work, we propose a\ngeneral adaptation framework for tuning the Markov kernels in SMC samplers by\nminimizing the incremental Kullback-Leibler (KL) divergence between the\nproposal and target paths. For step size tuning, we provide a gradient- and\ntuning-free algorithm that is generally applicable for kernels such as Langevin\nMonte Carlo (LMC). We further demonstrate the utility of our approach by\nproviding a tailored scheme for tuning \\textit{kinetic} LMC used in SMC\nsamplers. Our implementations are able to obtain a full \\textit{schedule} of\ntuned parameters at the cost of a few vanilla SMC runs, which is a fraction of\ngradient-based approaches.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.CO"
    ],
    "authors": [
      "Kyurae Kim",
      "Zuheng Xu",
      "Jacob R. Gardner",
      "Trevor Campbell"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15703v1",
    "title": "Predicting Multi-Agent Specialization via Task Parallelizability",
    "abstract": "Multi-agent systems often rely on specialized agents with distinct roles\nrather than general-purpose agents that perform the entire task independently.\nHowever, the conditions that govern the optimal degree of specialization remain\npoorly understood. In this work, we propose that specialist teams outperform\ngeneralist ones when environmental constraints limit task parallelizability --\nthe potential to execute task components concurrently. Drawing inspiration from\ndistributed systems, we introduce a heuristic to predict the relative\nefficiency of generalist versus specialist teams by estimating the speed-up\nachieved when two agents perform a task in parallel rather than focus on\ncomplementary subtasks. We validate this heuristic through three multi-agent\nreinforcement learning (MARL) experiments in Overcooked-AI, demonstrating that\nkey factors limiting task parallelizability influence specialization. We also\nobserve that as the state space expands, agents tend to converge on specialist\nstrategies, even when generalist ones are theoretically more efficient,\nhighlighting potential biases in MARL training algorithms. Our findings provide\na principled framework for interpreting specialization given the task and\nenvironment, and introduce a novel benchmark for evaluating whether MARL finds\noptimal strategies.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "authors": [
      "Elizabeth Mieczkowski",
      "Ruaridh Mon-Williams",
      "Neil Bramley",
      "Christopher G. Lucas",
      "Natalia Velez",
      "Thomas L. Griffiths"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15699v1",
    "title": "Representational Similarity via Interpretable Visual Concepts",
    "abstract": "How do two deep neural networks differ in how they arrive at a decision?\nMeasuring the similarity of deep networks has been a long-standing open\nquestion. Most existing methods provide a single number to measure the\nsimilarity of two networks at a given layer, but give no insight into what\nmakes them similar or dissimilar. We introduce an interpretable\nrepresentational similarity method (RSVC) to compare two networks. We use RSVC\nto discover shared and unique visual concepts between two models. We show that\nsome aspects of model differences can be attributed to unique concepts\ndiscovered by one model that are not well represented in the other. Finally, we\nconduct extensive evaluation across different vision model architectures and\ntraining protocols to demonstrate its effectiveness.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "q-bio.NC"
    ],
    "authors": [
      "Neehar Kondapaneni",
      "Oisin Mac Aodha",
      "Pietro Perona"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15697v1",
    "title": "Technical Report for the 5th CLVision Challenge at CVPR: Addressing the Class-Incremental with Repetition using Unlabeled Data -- 4th Place Solution",
    "abstract": "This paper outlines our approach to the 5th CLVision challenge at CVPR, which\naddresses the Class-Incremental with Repetition (CIR) scenario. In contrast to\ntraditional class incremental learning, this novel setting introduces unique\nchallenges and research opportunities, particularly through the integration of\nunlabeled data into the training process. In the CIR scenario, encountered\nclasses may reappear in later learning experiences, and each experience may\ninvolve only a subset of the overall class distribution. Additionally, the\nunlabeled data provided during training may include instances of unseen\nclasses, or irrelevant classes which should be ignored. Our approach focuses on\nretaining previously learned knowledge by utilizing knowledge distillation and\npseudo-labeling techniques. The key characteristic of our method is the\nexploitation of unlabeled data during training, in order to maintain optimal\nperformance on instances of previously encountered categories and reduce the\ndetrimental effects of catastrophic forgetting. Our method achieves an average\naccuracy of 16.68\\% during the pre-selection phase and 21.19% during the final\nevaluation phase, outperforming the baseline accuracy of 9.39%. We provide the\nimplementation code at\nhttps://github.com/panagiotamoraiti/continual-learning-challenge-2024 .",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Panagiota Moraiti",
      "Efstathios Karypidis"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15696v1",
    "title": "Approximation properties of neural ODEs",
    "abstract": "We study the approximation properties of shallow neural networks whose\nactivation function is defined as the flow of a neural ordinary differential\nequation (neural ODE) at the final time of the integration interval. We prove\nthe universal approximation property (UAP) of such shallow neural networks in\nthe space of continuous functions. Furthermore, we investigate the\napproximation properties of shallow neural networks whose parameters are\nrequired to satisfy some constraints. In particular, we constrain the Lipschitz\nconstant of the flow of the neural ODE to increase the stability of the shallow\nneural network, and we restrict the norm of the weight matrices of the linear\nlayers to one to make sure that the restricted expansivity of the flow is not\ncompensated by the increased expansivity of the linear layers. For this\nsetting, we prove approximation bounds that tell us the accuracy to which we\ncan approximate a continuous function with a shallow neural network with such\nconstraints. We prove that the UAP holds if we consider only the constraint on\nthe Lipschitz constant of the flow or the unit norm constraint on the weight\nmatrices of the linear layers.",
    "categories": [
      "math.NA",
      "cs.LG",
      "cs.NA"
    ],
    "authors": [
      "Arturo De Marinis",
      "Davide Murari",
      "Elena Celledoni",
      "Nicola Guglielmi",
      "Brynjulf Owren",
      "Francesco Tudisco"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15693v1",
    "title": "Good Actions Succeed, Bad Actions Generalize: A Case Study on Why RL Generalizes Better",
    "abstract": "Supervised learning (SL) and reinforcement learning (RL) are both widely used\nto train general-purpose agents for complex tasks, yet their generalization\ncapabilities and underlying mechanisms are not yet fully understood. In this\npaper, we provide a direct comparison between SL and RL in terms of zero-shot\ngeneralization. Using the Habitat visual navigation task as a testbed, we\nevaluate Proximal Policy Optimization (PPO) and Behavior Cloning (BC) agents\nacross two levels of generalization: state-goal pair generalization within seen\nenvironments and generalization to unseen environments. Our experiments show\nthat PPO consistently outperforms BC across both zero-shot settings and\nperformance metrics-success rate and SPL. Interestingly, even though additional\noptimal training data enables BC to match PPO's zero-shot performance in SPL,\nit still falls significantly behind in success rate. We attribute this to a\nfundamental difference in how models trained by these algorithms generalize:\nBC-trained models generalize by imitating successful trajectories, whereas\nTD-based RL-trained models generalize through combinatorial experience\nstitching-leveraging fragments of past trajectories (mostly failed ones) to\nconstruct solutions for new tasks. This allows RL to efficiently find solutions\nin vast state space and discover novel strategies beyond the scope of human\nknowledge. Besides providing empirical evidence and understanding, we also\npropose practical guidelines for improving the generalization capabilities of\nRL and SL through algorithm design.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Meng Song"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15686v1",
    "title": "Multi-focal Conditioned Latent Diffusion for Person Image Synthesis",
    "abstract": "The Latent Diffusion Model (LDM) has demonstrated strong capabilities in\nhigh-resolution image generation and has been widely employed for Pose-Guided\nPerson Image Synthesis (PGPIS), yielding promising results. However, the\ncompression process of LDM often results in the deterioration of details,\nparticularly in sensitive areas such as facial features and clothing textures.\nIn this paper, we propose a Multi-focal Conditioned Latent Diffusion (MCLD)\nmethod to address these limitations by conditioning the model on disentangled,\npose-invariant features from these sensitive regions. Our approach utilizes a\nmulti-focal condition aggregation module, which effectively integrates facial\nidentity and texture-specific information, enhancing the model's ability to\nproduce appearance realistic and identity-consistent images. Our method\ndemonstrates consistent identity and appearance generation on the DeepFashion\ndataset and enables flexible person image editing due to its generation\nconsistency. The code is available at https://github.com/jqliu09/mcld.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jiaqi Liu",
      "Jichao Zahng",
      "Paolo Rota",
      "Nicu Sebe"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15685v1",
    "title": "Robotic Paper Wrapping by Learning Force Control",
    "abstract": "Robotic packaging using wrapping paper poses significant challenges due to\nthe material's complex deformation properties. The packaging process itself\ninvolves multiple steps, primarily categorized as folding the paper or creating\ncreases. Small deviations in the robot's arm trajectory or force vector can\nlead to tearing or wrinkling of the paper, exacerbated by the variability in\nmaterial properties.\n  This study introduces a novel framework that combines imitation learning and\nreinforcement learning to enable a robot to perform each step of the packaging\nprocess efficiently. The framework allows the robot to follow approximate\ntrajectories of the tool-center point (TCP) based on human demonstrations while\noptimizing force control parameters to prevent tearing or wrinkling, even with\nvariable wrapping paper materials.\n  The proposed method was validated through ablation studies, which\ndemonstrated successful task completion with a significant reduction in tear\nand wrinkle rates. Furthermore, the force control strategy proved to be\nadaptable across different wrapping paper materials and robust against\nvariations in the size of the target object.",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Hiroki Hanai",
      "Takuya Kiyokawa",
      "Weiwei Wan",
      "Kensuke Harada"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15683v1",
    "title": "The Change You Want To Detect: Semantic Change Detection In Earth Observation With Hybrid Data Generation",
    "abstract": "Bi-temporal change detection at scale based on Very High Resolution (VHR)\nimages is crucial for Earth monitoring. This remains poorly addressed so far:\nmethods either require large volumes of annotated data (semantic case), or are\nlimited to restricted datasets (binary set-ups). Most approaches do not exhibit\nthe versatility required for temporal and spatial adaptation: simplicity in\narchitecture design and pretraining on realistic and comprehensive datasets.\nSynthetic datasets are the key solution but still fail to handle complex and\ndiverse scenes. In this paper, we present HySCDG a generative pipeline for\ncreating a large hybrid semantic change detection dataset that contains both\nreal VHR images and inpainted ones, along with land cover semantic map at both\ndates and the change map. Being semantically and spatially guided, HySCDG\ngenerates realistic images, leading to a comprehensive and hybrid\ntransfer-proof dataset FSC-180k. We evaluate FSC-180k on five change detection\ncases (binary and semantic), from zero-shot to mixed and sequential training,\nand also under low data regime training. Experiments demonstrate that\npretraining on our hybrid dataset leads to a significant performance boost,\noutperforming SyntheWorld, a fully synthetic dataset, in every configuration.\nAll codes, models, and data are available here:\n$\\href{https://yb23.github.io/projects/cywd/}{https://yb23.github.io/projects/cywd/}$.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Benidir Yanis",
      "Gonthier Nicolas",
      "Mallet Clement"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15679v1",
    "title": "Sequential learning based PINNs to overcome temporal domain complexities in unsteady flow past flapping wings",
    "abstract": "For a data-driven and physics combined modelling of unsteady flow systems\nwith moving immersed boundaries, Sundar {\\it et al.} introduced an immersed\nboundary-aware (IBA) framework, combining Physics-Informed Neural Networks\n(PINNs) and the immersed boundary method (IBM). This approach was beneficial\nbecause it avoided case-specific transformations to a body-attached reference\nframe. Building on this, we now address the challenges of long time integration\nin velocity reconstruction and pressure recovery by extending this IBA\nframework with sequential learning strategies. Key difficulties for PINNs in\nlong time integration include temporal sparsity, long temporal domains and rich\nspectral content. To tackle these, a moving boundary-enabled PINN is developed,\nproposing two sequential learning strategies: - a time marching with gradual\nincrease in time domain size, however, this approach struggles with error\naccumulation over long time domains; and - a time decomposition which divides\nthe temporal domain into smaller segments, combined with transfer learning it\neffectively reduces error propagation and computational complexity. The key\nfindings for modelling of incompressible unsteady flows past a flapping airfoil\ninclude: - for quasi-periodic flows, the time decomposition approach with\npreferential spatio-temporal sampling improves accuracy and efficiency for\npressure recovery and aerodynamic load reconstruction, and, - for long time\ndomains, decomposing it into smaller temporal segments and employing multiple\nsub-networks, simplifies the problem ensuring stability and reduced network\nsizes. This study highlights the limitations of traditional PINNs for long time\nintegration of flow-structure interaction problems and demonstrates the\nbenefits of decomposition-based strategies for addressing error accumulation,\ncomputational cost, and complex dynamics.",
    "categories": [
      "physics.flu-dyn",
      "cs.LG"
    ],
    "authors": [
      "Rahul Sundar",
      "Didier Lucor",
      "Sunetra Sarkar"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15676v1",
    "title": "High Temporal Consistency through Semantic Similarity Propagation in Semi-Supervised Video Semantic Segmentation for Autonomous Flight",
    "abstract": "Semantic segmentation from RGB cameras is essential to the perception of\nautonomous flying vehicles. The stability of predictions through the captured\nvideos is paramount to their reliability and, by extension, to the\ntrustworthiness of the agents. In this paper, we propose a lightweight video\nsemantic segmentation approach-suited to onboard real-time inference-achieving\nhigh temporal consistency on aerial data through Semantic Similarity\nPropagation across frames. SSP temporally propagates the predictions of an\nefficient image segmentation model with global registration alignment to\ncompensate for camera movements. It combines the current estimation and the\nprior prediction with linear interpolation using weights computed from the\nfeatures similarities of the two frames. Because data availability is a\nchallenge in this domain, we propose a consistency-aware Knowledge Distillation\ntraining procedure for sparsely labeled datasets with few annotations. Using a\nlarge image segmentation model as a teacher to train the efficient SSP, we\nleverage the strong correlations between labeled and unlabeled frames in the\nsame training videos to obtain high-quality supervision on all frames. KD-SSP\nobtains a significant temporal consistency increase over the base image\nsegmentation model of 12.5% and 6.7% TC on UAVid and RuralScapes respectively,\nwith higher accuracy and comparable inference speed. On these aerial datasets,\nKD-SSP provides a superior segmentation quality and inference speed trade-off\nthan other video methods proposed for general applications and shows\nconsiderably higher consistency. The code will be made publicly available upon\nacceptance.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Cédric Vincent",
      "Taehyoung Kim",
      "Henri Meeß"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15672v1",
    "title": "GASP: Unifying Geometric and Semantic Self-Supervised Pre-training for Autonomous Driving",
    "abstract": "Self-supervised pre-training based on next-token prediction has enabled large\nlanguage models to capture the underlying structure of text, and has led to\nunprecedented performance on a large array of tasks when applied at scale.\nSimilarly, autonomous driving generates vast amounts of spatiotemporal data,\nalluding to the possibility of harnessing scale to learn the underlying\ngeometric and semantic structure of the environment and its evolution over\ntime. In this direction, we propose a geometric and semantic self-supervised\npre-training method, GASP, that learns a unified representation by predicting,\nat any queried future point in spacetime, (1) general occupancy, capturing the\nevolving structure of the 3D scene; (2) ego occupancy, modeling the ego vehicle\npath through the environment; and (3) distilled high-level features from a\nvision foundation model. By modeling geometric and semantic 4D occupancy fields\ninstead of raw sensor measurements, the model learns a structured,\ngeneralizable representation of the environment and its evolution through time.\nWe validate GASP on multiple autonomous driving benchmarks, demonstrating\nsignificant improvements in semantic occupancy forecasting, online mapping, and\nego trajectory prediction. Our results demonstrate that continuous 4D geometric\nand semantic occupancy prediction provides a scalable and effective\npre-training paradigm for autonomous driving. For code and additional\nvisualizations, see \\href{https://research.zenseact.com/publications/gasp/.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "William Ljungbergh",
      "Adam Lilja",
      "Adam Tonderski. Arvid Laveno Ling",
      "Carl Lindström",
      "Willem Verbeke",
      "Junsheng Fu",
      "Christoffer Petersson",
      "Lars Hammarstrand",
      "Michael Felsberg"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15671v1",
    "title": "CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-Consistency from a Single Image",
    "abstract": "Reconstructing clothed humans from a single image is a fundamental task in\ncomputer vision with wide-ranging applications. Although existing monocular\nclothed human reconstruction solutions have shown promising results, they often\nrely on the assumption that the human subject is in an occlusion-free\nenvironment. Thus, when encountering in-the-wild occluded images, these\nalgorithms produce multiview inconsistent and fragmented reconstructions.\nAdditionally, most algorithms for monocular 3D human reconstruction leverage\ngeometric priors such as SMPL annotations for training and inference, which are\nextremely challenging to acquire in real-world applications. To address these\nlimitations, we propose CHROME: Clothed Human Reconstruction with\nOcclusion-Resilience and Multiview-ConsistEncy from a Single Image, a novel\npipeline designed to reconstruct occlusion-resilient 3D humans with multiview\nconsistency from a single occluded image, without requiring either ground-truth\ngeometric prior annotations or 3D supervision. Specifically, CHROME leverages a\nmultiview diffusion model to first synthesize occlusion-free human images from\nthe occluded input, compatible with off-the-shelf pose control to explicitly\nenforce cross-view consistency during synthesis. A 3D reconstruction model is\nthen trained to predict a set of 3D Gaussians conditioned on both the occluded\ninput and synthesized views, aligning cross-view details to produce a cohesive\nand accurate 3D representation. CHROME achieves significant improvements in\nterms of both novel view synthesis (upto 3 db PSNR) and geometric\nreconstruction under challenging conditions.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Arindam Dutta",
      "Meng Zheng",
      "Zhongpai Gao",
      "Benjamin Planche",
      "Anwesha Choudhuri",
      "Terrence Chen",
      "Amit K. Roy-Chowdhury",
      "Ziyan Wu"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15668v1",
    "title": "Model Risk Management for Generative AI In Financial Institutions",
    "abstract": "The success of OpenAI's ChatGPT in 2023 has spurred financial enterprises\ninto exploring Generative AI applications to reduce costs or drive revenue\nwithin different lines of businesses in the Financial Industry. While these\napplications offer strong potential for efficiencies, they introduce new model\nrisks, primarily hallucinations and toxicity. As highly regulated entities,\nfinancial enterprises (primarily large US banks) are obligated to enhance their\nmodel risk framework with additional testing and controls to ensure safe\ndeployment of such applications. This paper outlines the key aspects for model\nrisk management of generative AI model with a special emphasis on additional\npractices required in model validation.",
    "categories": [
      "q-fin.RM",
      "cs.LG"
    ],
    "authors": [
      "Anwesha Bhattacharyya",
      "Ye Yu",
      "Hanyu Yang",
      "Rahul Singh",
      "Tarun Joshi",
      "Jie Chen",
      "Kiran Yalavarthy"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.16557v1",
    "title": "Investigating Cultural Dimensions and Technological Acceptance: The Adoption of Electronic Performance and Tracking Systems in Qatar's Football Sector",
    "abstract": "Qatar's football sector has undergone a substantial technological\ntransformation with the implementation of Electronic Performance and Tracking\nSystems (EPTS). This study examines the impact of cultural and technological\nfactors on EPTS adoption, using Hofstede's Cultural Dimensions Theory and the\nTechnology Acceptance Model (TAM) as theoretical frameworks. An initial\nexploratory study involved ten participants, followed by an expanded dataset\ncomprising thirty stakeholders, including players, coaches, and staff from\nQatari football organizations. Multiple regression analysis was conducted to\nevaluate the relationships between perceived usefulness, perceived ease of use,\npower distance, innovation receptiveness, integration complexity, and overall\nadoption. The results indicate that perceived usefulness, innovation\nreceptiveness, and lower power distance significantly drive EPTS adoption,\nwhile ease of use is marginally significant and integration complexity is\nnon-significant in this sample. These findings provide practical insights for\nsports technology stakeholders in Qatar and emphasize the importance of\naligning cultural considerations with technological readiness for successful\nEPTS integration.",
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "authors": [
      "Abdulaziz Al Mannai"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15667v1",
    "title": "DiffPortrait360: Consistent Portrait Diffusion for 360 View Synthesis",
    "abstract": "Generating high-quality 360-degree views of human heads from single-view\nimages is essential for enabling accessible immersive telepresence applications\nand scalable personalized content creation. While cutting-edge methods for full\nhead generation are limited to modeling realistic human heads, the latest\ndiffusion-based approaches for style-omniscient head synthesis can produce only\nfrontal views and struggle with view consistency, preventing their conversion\ninto true 3D models for rendering from arbitrary angles. We introduce a novel\napproach that generates fully consistent 360-degree head views, accommodating\nhuman, stylized, and anthropomorphic forms, including accessories like glasses\nand hats. Our method builds on the DiffPortrait3D framework, incorporating a\ncustom ControlNet for back-of-head detail generation and a dual appearance\nmodule to ensure global front-back consistency. By training on continuous view\nsequences and integrating a back reference image, our approach achieves robust,\nlocally continuous view synthesis. Our model can be used to produce\nhigh-quality neural radiance fields (NeRFs) for real-time, free-viewpoint\nrendering, outperforming state-of-the-art methods in object synthesis and\n360-degree head generation for very challenging input portraits.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yuming Gu",
      "Phong Tran",
      "Yujian Zheng",
      "Hongyi Xu",
      "Heyuan Li",
      "Adilbek Karmanov",
      "Hao Li"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15666v1",
    "title": "Toward Scalable, Flexible Scene Flow for Point Clouds",
    "abstract": "Scene flow estimation is the task of describing 3D motion between temporally\nsuccessive observations. This thesis aims to build the foundation for building\nscene flow estimators with two important properties: they are scalable, i.e.\nthey improve with access to more data and computation, and they are flexible,\ni.e. they work out-of-the-box in a variety of domains and on a variety of\nmotion patterns without requiring significant hyperparameter tuning.\n  In this dissertation we present several concrete contributions towards this.\nIn Chapter 1 we contextualize scene flow and its prior methods. In Chapter 2 we\npresent a blueprint to build and scale feedforward scene flow estimators\nwithout requiring expensive human annotations via large scale distillation from\npseudolabels provided by strong unsupervised test-time optimization methods. In\nChapter 3 we introduce a benchmark to better measure estimate quality across\ndiverse object types, better bringing into focus what we care about and expect\nfrom scene flow estimators, and use this benchmark to host a public challenge\nthat produced significant progress. In Chapter 4 we present a state-of-the-art\nunsupervised scene flow estimator that introduces a new, full sequence problem\nformulation and exhibits great promise in adjacent domains like 3D point\ntracking. Finally, in Chapter 5 I philosophize about what's next for scene flow\nand its potential future broader impacts.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Kyle Vedder"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15664v1",
    "title": "Enhancing Pancreatic Cancer Staging with Large Language Models: The Role of Retrieval-Augmented Generation",
    "abstract": "Purpose: Retrieval-augmented generation (RAG) is a technology to enhance the\nfunctionality and reliability of large language models (LLMs) by retrieving\nrelevant information from reliable external knowledge (REK). RAG has gained\ninterest in radiology, and we previously reported the utility of NotebookLM, an\nLLM with RAG (RAG-LLM), for lung cancer staging. However, since the comparator\nLLM differed from NotebookLM's internal model, it remained unclear whether its\nadvantage stemmed from RAG or inherent model differences. To better isolate\nRAG's impact and assess its utility across different cancers, we compared\nNotebookLM with its internal LLM, Gemini 2.0 Flash, in a pancreatic cancer\nstaging experiment.\n  Materials and Methods: A summary of Japan's pancreatic cancer staging\nguidelines was used as REK. We compared three groups - REK+/RAG+ (NotebookLM\nwith REK), REK+/RAG- (Gemini 2.0 Flash with REK), and REK-/RAG- (Gemini 2.0\nFlash without REK) - in staging 100 fictional pancreatic cancer cases based on\nCT findings. Staging criteria included TNM classification, local invasion\nfactors, and resectability classification. In REK+/RAG+, retrieval accuracy was\nquantified based on the sufficiency of retrieved REK excerpts.\n  Results: REK+/RAG+ achieved a staging accuracy of 70%, outperforming\nREK+/RAG- (38%) and REK-/RAG- (35%). For TNM classification, REK+/RAG+ attained\n80% accuracy, exceeding REK+/RAG- (55%) and REK-/RAG- (50%). Additionally,\nREK+/RAG+ explicitly presented retrieved REK excerpts, achieving a retrieval\naccuracy of 92%.\n  Conclusion: NotebookLM, a RAG-LLM, outperformed its internal LLM, Gemini 2.0\nFlash, in a pancreatic cancer staging experiment, suggesting that RAG may\nimprove LLM's staging accuracy. Furthermore, its ability to retrieve and\npresent REK excerpts provides transparency for physicians, highlighting its\napplicability for clinical diagnosis and classification.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Hisashi Johno",
      "Yuki Johno",
      "Akitomo Amakawa",
      "Junichi Sato",
      "Ryota Tozuka",
      "Atsushi Komaba",
      "Hiroaki Watanabe",
      "Hiroki Watanabe",
      "Chihiro Goto",
      "Hiroyuki Morisaka",
      "Hiroshi Onishi",
      "Kazunori Nakamoto"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15661v1",
    "title": "UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction",
    "abstract": "Autonomous agents that navigate Graphical User Interfaces (GUIs) to automate\ntasks like document editing and file management can greatly enhance computer\nworkflows. While existing research focuses on online settings, desktop\nenvironments, critical for many professional and everyday tasks, remain\nunderexplored due to data collection challenges and licensing issues. We\nintroduce UI-Vision, the first comprehensive, license-permissive benchmark for\noffline, fine-grained evaluation of computer use agents in real-world desktop\nenvironments. Unlike online benchmarks, UI-Vision provides: (i) dense,\nhigh-quality annotations of human demonstrations, including bounding boxes, UI\nlabels, and action trajectories (clicks, drags, and keyboard inputs) across 83\nsoftware applications, and (ii) three fine-to-coarse grained tasks-Element\nGrounding, Layout Grounding, and Action Prediction-with well-defined metrics to\nrigorously evaluate agents' performance in desktop environments. Our evaluation\nreveals critical limitations in state-of-the-art models like UI-TARS-72B,\nincluding issues with understanding professional software, spatial reasoning,\nand complex actions like drag-and-drop. These findings highlight the challenges\nin developing fully autonomous computer use agents. By releasing UI-Vision as\nopen-source, we aim to advance the development of more capable agents for\nreal-world desktop tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Shravan Nayak",
      "Xiangru Jian",
      "Kevin Qinghong Lin",
      "Juan A. Rodriguez",
      "Montek Kalsi",
      "Rabiul Awal",
      "Nicolas Chapados",
      "M. Tamer Özsu",
      "Aishwarya Agrawal",
      "David Vazquez",
      "Christopher Pal",
      "Perouz Taslakian",
      "Spandana Gella",
      "Sai Rajeswar"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15655v1",
    "title": "R$^2$: A LLM Based Novel-to-Screenplay Generation Framework with Causal Plot Graphs",
    "abstract": "Automatically adapting novels into screenplays is important for the TV, film,\nor opera industries to promote products with low costs. The strong performances\nof large language models (LLMs) in long-text generation call us to propose a\nLLM based framework Reader-Rewriter (R$^2$) for this task. However, there are\ntwo fundamental challenges here. First, the LLM hallucinations may cause\ninconsistent plot extraction and screenplay generation. Second, the\ncausality-embedded plot lines should be effectively extracted for coherent\nrewriting. Therefore, two corresponding tactics are proposed: 1) A\nhallucination-aware refinement method (HAR) to iteratively discover and\neliminate the affections of hallucinations; and 2) a causal plot-graph\nconstruction method (CPC) based on a greedy cycle-breaking algorithm to\nefficiently construct plot lines with event causalities. Recruiting those\nefficient techniques, R$^2$ utilizes two modules to mimic the human screenplay\nrewriting process: The Reader module adopts a sliding window and CPC to build\nthe causal plot graphs, while the Rewriter module generates first the scene\noutlines based on the graphs and then the screenplays. HAR is integrated into\nboth modules for accurate inferences of LLMs. Experimental results demonstrate\nthe superiority of R$^2$, which substantially outperforms three existing\napproaches (51.3%, 22.6%, and 57.1% absolute increases) in pairwise comparison\nat the overall win rate for GPT-4o.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Zefeng Lin",
      "Yi Xiao",
      "Zhiqiang Mo",
      "Qifan Zhang",
      "Jie Wang",
      "Jiayang Chen",
      "Jiajing Zhang",
      "Hui Zhang",
      "Zhengyi Liu",
      "Xianyong Fang",
      "Xiaohua Xu"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15653v1",
    "title": "Transport-Related Surface Detection with Machine Learning: Analyzing Temporal Trends in Madrid and Vienna",
    "abstract": "This study explores the integration of machine learning into urban aerial\nimage analysis, with a focus on identifying infrastructure surfaces for cars\nand pedestrians and analyzing historical trends. It emphasizes the transition\nfrom convolutional architectures to transformer-based pre-trained models,\nunderscoring their potential in global geospatial analysis. A workflow is\npresented for automatically generating geospatial datasets, enabling the\ncreation of semantic segmentation datasets from various sources, including\nWMS/WMTS links, vectorial cartography, and OpenStreetMap (OSM) overpass-turbo\nrequests. The developed code allows a fast dataset generation process for\ntraining machine learning models using openly available data without manual\nlabelling. Using aerial imagery and vectorial data from the respective\ngeographical offices of Madrid and Vienna, two datasets were generated for car\nand pedestrian surface detection. A transformer-based model was trained and\nevaluated for each city, demonstrating good accuracy values. The historical\ntrend analysis involved applying the trained model to earlier images predating\nthe availability of vectorial data 10 to 20 years, successfully identifying\ntemporal trends in infrastructure for pedestrians and cars across different\ncity areas. This technique is applicable for municipal governments to gather\nvaluable data at a minimal cost.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Miguel Ureña Pliego",
      "Rubén Martínez Marín",
      "Nianfang Shi",
      "Takeru Shibayama",
      "Ulrich Leth",
      "Miguel Marchamalo Sacristán"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.16556v1",
    "title": "Reliable Radiologic Skeletal Muscle Area Assessment -- A Biomarker for Cancer Cachexia Diagnosis",
    "abstract": "Cancer cachexia is a common metabolic disorder characterized by severe muscle\natrophy which is associated with poor prognosis and quality of life. Monitoring\nskeletal muscle area (SMA) longitudinally through computed tomography (CT)\nscans, an imaging modality routinely acquired in cancer care, is an effective\nway to identify and track this condition. However, existing tools often lack\nfull automation and exhibit inconsistent accuracy, limiting their potential for\nintegration into clinical workflows. To address these challenges, we developed\nSMAART-AI (Skeletal Muscle Assessment-Automated and Reliable Tool-based on AI),\nan end-to-end automated pipeline powered by deep learning models (nnU-Net 2D)\ntrained on mid-third lumbar level CT images with 5-fold cross-validation,\nensuring generalizability and robustness. SMAART-AI incorporates an\nuncertainty-based mechanism to flag high-error SMA predictions for expert\nreview, enhancing reliability. We combined the SMA, skeletal muscle index, BMI,\nand clinical data to train a multi-layer perceptron (MLP) model designed to\npredict cachexia at the time of cancer diagnosis. Tested on the\ngastroesophageal cancer dataset, SMAART-AI achieved a Dice score of 97.80% +/-\n0.93%, with SMA estimated across all four datasets in this study at a median\nabsolute error of 2.48% compared to manual annotations with SliceOmatic.\nUncertainty metrics-variance, entropy, and coefficient of variation-strongly\ncorrelated with SMA prediction errors (0.83, 0.76, and 0.73 respectively). The\nMLP model predicts cachexia with 79% precision, providing clinicians with a\nreliable tool for early diagnosis and intervention. By combining automation,\naccuracy, and uncertainty awareness, SMAART-AI bridges the gap between research\nand clinical application, offering a transformative approach to managing cancer\ncachexia.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CE",
      "cs.CV"
    ],
    "authors": [
      "Sabeen Ahmed",
      "Nathan Parker",
      "Margaret Park",
      "Daniel Jeong",
      "Lauren Peres",
      "Evan W. Davis",
      "Jennifer B. Permuth",
      "Erin Siegel",
      "Matthew B. Schabath",
      "Yasin Yilmaz",
      "Ghulam Rasool"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15650v1",
    "title": "Survey on Generalization Theory for Graph Neural Networks",
    "abstract": "Message-passing graph neural networks (MPNNs) have emerged as the leading\napproach for machine learning on graphs, attracting significant attention in\nrecent years. While a large set of works explored the expressivity of MPNNs,\ni.e., their ability to separate graphs and approximate functions over them,\ncomparatively less attention has been directed toward investigating their\ngeneralization abilities, i.e., making meaningful predictions beyond the\ntraining data. Here, we systematically review the existing literature on the\ngeneralization abilities of MPNNs. We analyze the strengths and limitations of\nvarious studies in these domains, providing insights into their methodologies\nand findings. Furthermore, we identify potential avenues for future research,\naiming to deepen our understanding of the generalization abilities of MPNNs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Antonis Vasileiou",
      "Stefanie Jegelka",
      "Ron Levie",
      "Christopher Morris"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15648v1",
    "title": "Cancelable Biometric Template Generation Using Random Feature Vector Transformations",
    "abstract": "Cancelable biometric schemes are designed to extract an identity-preserving,\nnon-invertible as well as revocable pseudo-identifier from biometric data.\nRecognition systems need to store only this pseudo-identifier, to avoid\ntampering and/or stealing of original biometric data during the recognition\nprocess. State-of-the-art cancelable schemes generate pseudo-identifiers by\ntransforming the original template using either user-specific salting or\nmany-to-one transformations. In addition to the performance concerns, most of\nsuch schemes are modality-specific and prone to reconstruction attacks as there\nare chances for unauthorized access to security-critical transformation keys. A\nnovel, modality-independent cancelable biometric scheme is proposed to overcome\nthese limitations. In this scheme, a cancelable template (pseudo identifier) is\ngenerated as a distance vector between multiple random transformations of the\nbiometric feature vector. These transformations were done by grouping feature\nvector components based on a set of user-specific random vectors. The proposed\nscheme nullifies the possibility of template reconstruction as the generated\ncancelable template contains only the distance values between the different\nrandom transformations of the feature vector and it does not store any details\nof the biometric template. The recognition performance of the proposed scheme\nis evaluated for face and fingerprint modalities. Equal Error Rate (EER) of 1.5\nis obtained for face and 1.7 is obtained for the fingerprint in the worst case.",
    "categories": [
      "cs.CR",
      "cs.CV"
    ],
    "authors": [
      "Ragendhu Sp",
      "Tony Thomas",
      "Sabu Emmanuel"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15647v1",
    "title": "Multi-Modal Gesture Recognition from Video and Surgical Tool Pose Information via Motion Invariants",
    "abstract": "Recognizing surgical gestures in real-time is a stepping stone towards\nautomated activity recognition, skill assessment, intra-operative assistance,\nand eventually surgical automation. The current robotic surgical systems\nprovide us with rich multi-modal data such as video and kinematics. While some\nrecent works in multi-modal neural networks learn the relationships between\nvision and kinematics data, current approaches treat kinematics information as\nindependent signals, with no underlying relation between tool-tip poses.\nHowever, instrument poses are geometrically related, and the underlying\ngeometry can aid neural networks in learning gesture representation. Therefore,\nwe propose combining motion invariant measures (curvature and torsion) with\nvision and kinematics data using a relational graph network to capture the\nunderlying relations between different data streams. We show that gesture\nrecognition improves when combining invariant signals with tool position,\nachieving 90.3\\% frame-wise accuracy on the JIGSAWS suturing dataset. Our\nresults show that motion invariant signals coupled with position are better\nrepresentations of gesture motion compared to traditional position and\nquaternion representations. Our results highlight the need for geometric-aware\nmodeling of kinematics for gesture recognition.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Jumanh Atoum",
      "Garrison L. H. Johnston",
      "Nabil Simaan",
      "Jie Ying Wu"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15639v1",
    "title": "A Context-Driven Training-Free Network for Lightweight Scene Text Segmentation and Recognition",
    "abstract": "Modern scene text recognition systems often depend on large end-to-end\narchitectures that require extensive training and are prohibitively expensive\nfor real-time scenarios. In such cases, the deployment of heavy models becomes\nimpractical due to constraints on memory, computational resources, and latency.\nTo address these challenges, we propose a novel, training-free plug-and-play\nframework that leverages the strengths of pre-trained text recognizers while\nminimizing redundant computations. Our approach uses context-based\nunderstanding and introduces an attention-based segmentation stage, which\nrefines candidate text regions at the pixel level, improving downstream\nrecognition. Instead of performing traditional text detection that follows a\nblock-level comparison between feature map and source image and harnesses\ncontextual information using pretrained captioners, allowing the framework to\ngenerate word predictions directly from scene context.Candidate texts are\nsemantically and lexically evaluated to get a final score. Predictions that\nmeet or exceed a pre-defined confidence threshold bypass the heavier process of\nend-to-end text STR profiling, ensuring faster inference and cutting down on\nunnecessary computations. Experiments on public benchmarks demonstrate that our\nparadigm achieves performance on par with state-of-the-art systems, yet\nrequires substantially fewer resources.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Ritabrata Chakraborty",
      "Shivakumara Palaiahnakote",
      "Umapada Pal",
      "Cheng-Lin Liu"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15638v1",
    "title": "Using machine learning to measure evidence of students' sensemaking in physics courses",
    "abstract": "In the education system, problem-solving correctness is often inappropriately\nconflated with student learning. Advances in both Physics Education Research\n(PER) and Machine Learning (ML) provide the initial tools to develop a more\nmeaningful and efficient measurement scheme for whether physics students are\nengaging in sensemaking: a learning process of figuring out the how and why for\na particular phenomena. In this work, we contribute such a measurement scheme,\nwhich quantifies the evidence of students' physical sensemaking given their\nwritten explanations for their solutions to physics problems. We outline how\nthe proposed human annotation scheme can be automated into a deployable ML\nmodel using language encoders and shared probabilistic classifiers. The\nprocedure is scalable for a large number of problems and students. We implement\nthree unique language encoders with logistic regression, and provide a\ndeployability analysis on 385 real student explanations from the 2023\nIntroduction to Physics course at Tufts University. Furthermore, we compute\nsensemaking scores for all students, and analyze these measurements alongside\ntheir corresponding problem-solving accuracies. We find no linear relationship\nbetween these two variables, supporting the hypothesis that one is not a\nreliable proxy for the other. We discuss how sensemaking scores can be used\nalongside problem-solving accuracies to provide a more nuanced snapshot of\nstudent performance in physics class.",
    "categories": [
      "physics.ed-ph",
      "cs.LG"
    ],
    "authors": [
      "Kaitlin Gili",
      "Kyle Heuton",
      "Astha Shah",
      "Michael C. Hughes"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15633v1",
    "title": "Vision-Speech Models: Teaching Speech Models to Converse about Images",
    "abstract": "The recent successes of Vision-Language models raise the question of how to\nequivalently imbue a pretrained speech model with vision understanding, an\nimportant milestone towards building a multimodal speech model able to freely\nconverse about images. Building such a conversational Vision-Speech model\nbrings its unique challenges: (i) paired image-speech datasets are much scarcer\nthan their image-text counterparts, (ii) ensuring real-time latency at\ninference is crucial thus bringing compute and memory constraints, and (iii)\nthe model should preserve prosodic features (e.g., speaker tone) which cannot\nbe inferred from text alone. In this work, we introduce MoshiVis, augmenting a\nrecent dialogue speech LLM, Moshi, with visual inputs through lightweight\nadaptation modules. An additional dynamic gating mechanism enables the model to\nmore easily switch between the visual inputs and unrelated conversation topics.\nTo reduce training costs, we design a simple one-stage, parameter-efficient\nfine-tuning pipeline in which we leverage a mixture of image-text (i.e.,\n\"speechless\") and image-speech samples. We evaluate the model on downstream\nvisual understanding tasks with both audio and text prompts, and report\nqualitative samples of interactions with MoshiVis. Our inference code will be\nmade available, as well as the image-speech data used for audio evaluation.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Amélie Royer",
      "Moritz Böhle",
      "Gabriel de Marmiesse",
      "Laurent Mazaré",
      "Neil Zeghidour",
      "Alexandre Défossez",
      "Patrick Pérez"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15629v1",
    "title": "Neural Lyapunov Function Approximation with Self-Supervised Reinforcement Learning",
    "abstract": "Control Lyapunov functions are traditionally used to design a controller\nwhich ensures convergence to a desired state, yet deriving these functions for\nnonlinear systems remains a complex challenge. This paper presents a novel,\nsample-efficient method for neural approximation of nonlinear Lyapunov\nfunctions, leveraging self-supervised Reinforcement Learning (RL) to enhance\ntraining data generation, particularly for inaccurately represented regions of\nthe state space. The proposed approach employs a data-driven World Model to\ntrain Lyapunov functions from off-policy trajectories. The method is validated\non both standard and goal-conditioned robotic tasks, demonstrating faster\nconvergence and higher approximation accuracy compared to the state-of-the-art\nneural Lyapunov approximation baseline. The code is available at:\nhttps://github.com/CAV-Research-Lab/SACLA.git",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CG",
      "cs.LG"
    ],
    "authors": [
      "Luc McCutcheon",
      "Bahman Gharesifard",
      "Saber Fallah"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15625v1",
    "title": "EarthScape: A Multimodal Dataset for Surficial Geologic Mapping and Earth Surface Analysis",
    "abstract": "Surficial geologic mapping is essential for understanding Earth surface\nprocesses, addressing modern challenges such as climate change and national\nsecurity, and supporting common applications in engineering and resource\nmanagement. However, traditional mapping methods are labor-intensive, limiting\nspatial coverage and introducing potential biases. To address these\nlimitations, we introduce EarthScape, a novel, AI-ready multimodal dataset\nspecifically designed for surficial geologic mapping and Earth surface\nanalysis. EarthScape integrates high-resolution aerial RGB and near-infrared\n(NIR) imagery, digital elevation models (DEM), multi-scale DEM-derived terrain\nfeatures, and hydrologic and infrastructure vector data. The dataset provides\ndetailed annotations for seven distinct surficial geologic classes encompassing\nvarious geological processes. We present a comprehensive data processing\npipeline using open-sourced raw data and establish baseline benchmarks using\ndifferent spatial modalities to demonstrate the utility of EarthScape. As a\nliving dataset with a vision for expansion, EarthScape bridges the gap between\ncomputer vision and Earth sciences, offering a valuable resource for advancing\nresearch in multimodal learning, geospatial analysis, and geological mapping.\nOur code is available at https://github.com/masseygeo/earthscape.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Matthew Massey",
      "Abdullah-Al-Zubaer Imran"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15621v1",
    "title": "LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning",
    "abstract": "Recent progress in Multimodal Large Language Models (MLLMs) has highlighted\nthe critical roles of both the visual backbone and the underlying language\nmodel. While prior work has primarily focused on scaling these components to\nbillions of parameters, the trade-offs between model size, architecture, and\nperformance remain underexplored. Additionally, inconsistencies in training\ndata and evaluation protocols have hindered direct comparisons, making it\ndifficult to derive optimal design choices. In this paper, we introduce\nLLaVA-MORE, a new family of MLLMs that integrates recent language models with\ndiverse visual backbones. To ensure fair comparisons, we employ a unified\ntraining protocol applied consistently across all architectures. Our analysis\nsystematically explores both small- and medium-scale LLMs -- including Phi-4,\nLLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and\ninstruction following, while examining the relationship between model size and\nperformance. Beyond evaluating the LLM impact on final results, we conduct a\ncomprehensive study of various visual encoders, ranging from CLIP-based\narchitectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional\nexperiments investigate the effects of increased image resolution and\nvariations in pre-training datasets. Overall, our results provide insights into\nthe design of more effective MLLMs, offering a reproducible evaluation\nframework that facilitates direct comparisons and can guide future model\ndevelopment. Our source code and trained models are publicly available at:\nhttps://github.com/aimagelab/LLaVA-MORE.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "authors": [
      "Federico Cocchi",
      "Nicholas Moratelli",
      "Davide Caffagni",
      "Sara Sarto",
      "Lorenzo Baraldi",
      "Marcella Cornia",
      "Rita Cucchiara"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15620v1",
    "title": "Does Context Matter? ContextualJudgeBench for Evaluating LLM-based Judges in Contextual Settings",
    "abstract": "The large language model (LLM)-as-judge paradigm has been used to meet the\ndemand for a cheap, reliable, and fast evaluation of model outputs during AI\nsystem development and post-deployment monitoring. While judge models -- LLMs\nfinetuned to specialize in assessing and critiquing model outputs -- have been\ntouted as general purpose evaluators, they are typically evaluated only on\nnon-contextual scenarios, such as instruction following. The omission of\ncontextual settings -- those where external information is used as context to\ngenerate an output -- is surprising given the increasing prevalence of\nretrieval-augmented generation (RAG) and summarization use cases. Contextual\nassessment is uniquely challenging, as evaluation often depends on practitioner\npriorities, leading to conditional evaluation criteria (e.g., comparing\nresponses based on factuality and then considering completeness if they are\nequally factual). To address the gap, we propose ContextualJudgeBench, a judge\nbenchmark with 2,000 challenging response pairs across eight splits inspired by\nreal-world contextual evaluation scenarios. We build our benchmark with a\nmulti-pronged data construction pipeline that leverages both existing human\nannotations and model-based perturbations. Our comprehensive study across 11\njudge models and 9 general purpose models, reveals that the contextual\ninformation and its assessment criteria present a significant challenge to even\nstate-of-the-art models. For example, OpenAI's o1, the best-performing model,\nbarely reaches 55% consistent accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Austin Xu",
      "Srijan Bansal",
      "Yifei Ming",
      "Semih Yavuz",
      "Shafiq Joty"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15617v1",
    "title": "CAM-Seg: A Continuous-valued Embedding Approach for Semantic Image Generation",
    "abstract": "Traditional transformer-based semantic segmentation relies on quantized\nembeddings. However, our analysis reveals that autoencoder accuracy on\nsegmentation mask using quantized embeddings (e.g. VQ-VAE) is 8% lower than\ncontinuous-valued embeddings (e.g. KL-VAE). Motivated by this, we propose a\ncontinuous-valued embedding framework for semantic segmentation. By\nreformulating semantic mask generation as a continuous image-to-embedding\ndiffusion process, our approach eliminates the need for discrete latent\nrepresentations while preserving fine-grained spatial and semantic details. Our\nkey contribution includes a diffusion-guided autoregressive transformer that\nlearns a continuous semantic embedding space by modeling long-range\ndependencies in image features. Our framework contains a unified architecture\ncombining a VAE encoder for continuous feature extraction, a diffusion-guided\ntransformer for conditioned embedding generation, and a VAE decoder for\nsemantic mask reconstruction. Our setting facilitates zero-shot domain\nadaptation capabilities enabled by the continuity of the embedding space.\nExperiments across diverse datasets (e.g., Cityscapes and domain-shifted\nvariants) demonstrate state-of-the-art robustness to distribution shifts,\nincluding adverse weather (e.g., fog, snow) and viewpoint variations. Our model\nalso exhibits strong noise resilience, achieving robust performance ($\\approx$\n95% AP compared to baseline) under gaussian noise, moderate motion blur, and\nmoderate brightness/contrast variations, while experiencing only a moderate\nimpact ($\\approx$ 90% AP compared to baseline) from 50% salt and pepper noise,\nsaturation and hue shifts. Code available:\nhttps://github.com/mahmed10/CAMSS.git",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Masud Ahmed",
      "Zahid Hasan",
      "Syed Arefinul Haque",
      "Abu Zaher Md Faridee",
      "Sanjay Purushotham",
      "Suya You",
      "Nirmalya Roy"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15615v1",
    "title": "PEnGUiN: Partially Equivariant Graph NeUral Networks for Sample Efficient MARL",
    "abstract": "Equivariant Graph Neural Networks (EGNNs) have emerged as a promising\napproach in Multi-Agent Reinforcement Learning (MARL), leveraging symmetry\nguarantees to greatly improve sample efficiency and generalization. However,\nreal-world environments often exhibit inherent asymmetries arising from factors\nsuch as external forces, measurement inaccuracies, or intrinsic system biases.\nThis paper introduces \\textit{Partially Equivariant Graph NeUral Networks\n(PEnGUiN)}, a novel architecture specifically designed to address these\nchallenges. We formally identify and categorize various types of partial\nequivariance relevant to MARL, including subgroup equivariance, feature-wise\nequivariance, regional equivariance, and approximate equivariance. We\ntheoretically demonstrate that PEnGUiN is capable of learning both fully\nequivariant (EGNN) and non-equivariant (GNN) representations within a unified\nframework. Through extensive experiments on a range of MARL problems\nincorporating various asymmetries, we empirically validate the efficacy of\nPEnGUiN. Our results consistently demonstrate that PEnGUiN outperforms both\nEGNNs and standard GNNs in asymmetric environments, highlighting their\npotential to improve the robustness and applicability of graph-based MARL\nalgorithms in real-world scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "authors": [
      "Joshua McClellan",
      "Greyson Brothers",
      "Furong Huang",
      "Pratap Tokekar"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15485v1",
    "title": "TULIP: Towards Unified Language-Image Pretraining",
    "abstract": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Zineng Tang",
      "Long Lian",
      "Seun Eisape",
      "XuDong Wang",
      "Roei Herzig",
      "Adam Yala",
      "Alane Suhr",
      "Trevor Darrell",
      "David M. Chan"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15484v1",
    "title": "Value Profiles for Encoding Human Variation",
    "abstract": "Modelling human variation in rating tasks is crucial for enabling AI systems\nfor personalization, pluralistic model alignment, and computational social\nscience. We propose representing individuals using value profiles -- natural\nlanguage descriptions of underlying values compressed from in-context\ndemonstrations -- along with a steerable decoder model to estimate ratings\nconditioned on a value profile or other rater information. To measure the\npredictive information in rater representations, we introduce an\ninformation-theoretic methodology. We find that demonstrations contain the most\ninformation, followed by value profiles and then demographics. However, value\nprofiles offer advantages in terms of scrutability, interpretability, and\nsteerability due to their compressed natural language format. Value profiles\neffectively compress the useful information from demonstrations (>70%\ninformation preservation). Furthermore, clustering value profiles to identify\nsimilarly behaving individuals better explains rater variation than the most\npredictive demographic groupings. Going beyond test set performance, we show\nthat the decoder models interpretably change ratings according to semantic\nprofile differences, are well-calibrated, and can help explain instance-level\ndisagreement by simulating an annotator population. These results demonstrate\nthat value profiles offer novel, predictive ways to describe individual\nvariation beyond demographics or group information.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "authors": [
      "Taylor Sorensen",
      "Pushkar Mishra",
      "Roma Patel",
      "Michael Henry Tessler",
      "Michiel Bakker",
      "Georgina Evans",
      "Iason Gabriel",
      "Noah Goodman",
      "Verena Rieser"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15482v1",
    "title": "Natural Quantization of Neural Networks",
    "abstract": "We propose a natural quantization of a standard neural network, where the\nneurons correspond to qubits and the activation functions are implemented via\nquantum gates and measurements. The simplest quantized neural network\ncorresponds to applying single-qubit rotations, with the rotation angles being\ndependent on the weights and measurement outcomes of the previous layer. This\nrealization has the advantage of being smoothly tunable from the purely\nclassical limit with no quantum uncertainty (thereby reproducing the classical\nneural network exactly) to a quantum case, where superpositions introduce an\nintrinsic uncertainty in the network. We benchmark this architecture on a\nsubset of the standard MNIST dataset and find a regime of \"quantum advantage,\"\nwhere the validation error rate in the quantum realization is smaller than that\nin the classical model. We also consider another approach where quantumness is\nintroduced via weak measurements of ancilla qubits entangled with the neuron\nqubits. This quantum neural network also allows for smooth tuning of the degree\nof quantumness by controlling an entanglement angle, $g$, with $g=\\frac\\pi 2$\nreplicating the classical regime. We find that validation error is also\nminimized within the quantum regime in this approach. We also observe a quantum\ntransition, with sharp loss of the quantum network's ability to learn at a\ncritical point $g_c$. The proposed quantum neural networks are readily\nrealizable in present-day quantum computers on commercial datasets.",
    "categories": [
      "quant-ph",
      "cond-mat.dis-nn",
      "cs.LG"
    ],
    "authors": [
      "Richard Barney",
      "Djamil Lakhdar-Hamina",
      "Victor Galitski"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15481v1",
    "title": "Learning to Play Piano in the Real World",
    "abstract": "Towards the grand challenge of achieving human-level manipulation in robots,\nplaying piano is a compelling testbed that requires strategic, precise, and\nflowing movements. Over the years, several works demonstrated hand-designed\ncontrollers on real world piano playing, while other works evaluated robot\nlearning approaches on simulated piano scenarios. In this paper, we develop the\nfirst piano playing robotic system that makes use of learning approaches while\nalso being deployed on a real world dexterous robot. Specifically, we make use\nof Sim2Real to train a policy in simulation using reinforcement learning before\ndeploying the learned policy on a real world dexterous robot. In our\nexperiments, we thoroughly evaluate the interplay between domain randomization\nand the accuracy of the dynamics model used in simulation. Moreover, we\nevaluate the robot's performance across multiple songs with varying complexity\nto study the generalization of our learned policy. By providing a\nproof-of-concept of learning to play piano in the real world, we want to\nencourage the community to adopt piano playing as a compelling benchmark\ntowards human-level manipulation. We open-source our code and show additional\nvideos at https://lasr.org/research/learning-to-play-piano .",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yves-Simon Zeulner",
      "Sandeep Selvaraj",
      "Roberto Calandra"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15478v1",
    "title": "SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks",
    "abstract": "Large language model (LLM) agents need to perform multi-turn interactions in\nreal-world tasks. However, existing multi-turn RL algorithms for optimizing LLM\nagents fail to perform effective credit assignment over multiple turns while\nleveraging the generalization capabilities of LLMs and it remains unclear how\nto develop such algorithms. To study this, we first introduce a new benchmark,\nColBench, where an LLM agent interacts with a human collaborator over multiple\nturns to solve realistic tasks in backend programming and frontend design.\nBuilding on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with\nStep-WisE Evaluation from Training-time information), that uses a carefully\ndesigned optimization objective to train a critic model with access to\nadditional training-time information. The critic provides step-level rewards\nfor improving the policy model. Our experiments demonstrate that SWEET-RL\nachieves a 6% absolute improvement in success and win rates on ColBench\ncompared to other state-of-the-art multi-turn RL algorithms, enabling\nLlama-3.1-8B to match or exceed the performance of GPT4-o in realistic\ncollaborative content creation.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Yifei Zhou",
      "Song Jiang",
      "Yuandong Tian",
      "Jason Weston",
      "Sergey Levine",
      "Sainbayar Sukhbaatar",
      "Xian Li"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15477v1",
    "title": "What Makes a Reward Model a Good Teacher? An Optimization Perspective",
    "abstract": "The success of Reinforcement Learning from Human Feedback (RLHF) critically\ndepends on the quality of the reward model. While this quality is primarily\nevaluated through accuracy, it remains unclear whether accuracy fully captures\nwhat makes a reward model an effective teacher. We address this question from\nan optimization perspective. First, we prove that regardless of how accurate a\nreward model is, if it induces low reward variance, then the RLHF objective\nsuffers from a flat landscape. Consequently, even a perfectly accurate reward\nmodel can lead to extremely slow optimization, underperforming less accurate\nmodels that induce higher reward variance. We additionally show that a reward\nmodel that works well for one language model can induce low reward variance,\nand thus a flat objective landscape, for another. These results establish a\nfundamental limitation of evaluating reward models solely based on accuracy or\nindependently of the language model they guide. Experiments using models of up\nto 8B parameters corroborate our theory, demonstrating the interplay between\nreward variance, accuracy, and reward maximization rate. Overall, our findings\nhighlight that beyond accuracy, a reward model needs to induce sufficient\nvariance for efficient optimization.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "authors": [
      "Noam Razin",
      "Zixuan Wang",
      "Hubert Strauss",
      "Stanley Wei",
      "Jason D. Lee",
      "Sanjeev Arora"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15475v1",
    "title": "Cube: A Roblox View of 3D Intelligence",
    "abstract": "Foundation models trained on vast amounts of data have demonstrated\nremarkable reasoning and generation capabilities in the domains of text,\nimages, audio and video. Our goal at Roblox is to build such a foundation model\nfor 3D intelligence, a model that can support developers in producing all\naspects of a Roblox experience, from generating 3D objects and scenes to\nrigging characters for animation to producing programmatic scripts describing\nobject behaviors. We discuss three key design requirements for such a 3D\nfoundation model and then present our first step towards building such a model.\nWe expect that 3D geometric shapes will be a core data type and describe our\nsolution for 3D shape tokenizer. We show how our tokenization scheme can be\nused in applications for text-to-shape generation, shape-to-text generation and\ntext-to-scene generation. We demonstrate how these applications can collaborate\nwith existing large language models (LLMs) to perform scene analysis and\nreasoning. We conclude with a discussion outlining our path to building a fully\nunified foundation model for 3D intelligence.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Foundation AI Team",
      "Kiran Bhat",
      "Nishchaie Khanna",
      "Karun Channa",
      "Tinghui Zhou",
      "Yiheng Zhu",
      "Xiaoxia Sun",
      "Charles Shang",
      "Anirudh Sudarshan",
      "Maurice Chu",
      "Daiqing Li",
      "Kangle Deng",
      "Jean-Philippe Fauconnier",
      "Tijmen Verhulsdonck",
      "Maneesh Agrawala",
      "Kayvon Fatahalian",
      "Alexander Weiss",
      "Christian Reiser",
      "Ravi Kiran Chirravuri",
      "Ravali Kandur",
      "Alejandro Pelaez",
      "Akash Garg",
      "Michael Palleschi",
      "Jessica Wang",
      "Skylar Litz",
      "Leon Liu",
      "Anying Li",
      "David Harmon",
      "Derek Liu",
      "Liangjun Feng",
      "Denis Goupil",
      "Lukas Kuczynski",
      "Jihyun Yoon",
      "Naveen Marri",
      "Peiye Zhuang",
      "Yinan Zhang",
      "Brian Yin",
      "Haomiao Jiang",
      "Marcel van Workum",
      "Thomas Lane",
      "Bryce Erickson",
      "Salil Pathare",
      "Kyle Price",
      "Anupam Singh",
      "David Baszucki"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15474v1",
    "title": "Toward task-driven satellite image super-resolution",
    "abstract": "Super-resolution is aimed at reconstructing high-resolution images from\nlow-resolution observations. State-of-the-art approaches underpinned with deep\nlearning allow for obtaining outstanding results, generating images of high\nperceptual quality. However, it often remains unclear whether the reconstructed\ndetails are close to the actual ground-truth information and whether they\nconstitute a more valuable source for image analysis algorithms. In the\nreported work, we address the latter problem, and we present our efforts toward\nlearning super-resolution algorithms in a task-driven way to make them suitable\nfor generating high-resolution images that can be exploited for automated image\nanalysis. In the reported initial research, we propose a methodological\napproach for assessing the existing models that perform computer vision tasks\nin terms of whether they can be used for evaluating super-resolution\nreconstruction algorithms, as well as training them in a task-driven way. We\nsupport our analysis with experimental study and we expect it to establish a\nsolid foundation for selecting appropriate computer vision tasks that will\nadvance the capabilities of real-world super-resolution.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Maciej Ziaja",
      "Pawel Kowaleczko",
      "Daniel Kostrzewa",
      "Nicolas Longépé",
      "Michal Kawulok"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.16554v1",
    "title": "Explainable AI Components for Narrative Map Extraction",
    "abstract": "As narrative extraction systems grow in complexity, establishing user trust\nthrough interpretable and explainable outputs becomes increasingly critical.\nThis paper presents an evaluation of an Explainable Artificial Intelligence\n(XAI) system for narrative map extraction that provides meaningful explanations\nacross multiple levels of abstraction. Our system integrates explanations based\non topical clusters for low-level document relationships, connection\nexplanations for event relationships, and high-level structure explanations for\noverall narrative patterns. In particular, we evaluate the XAI system through a\nuser study involving 10 participants that examined narratives from the 2021\nCuban protests. The analysis of results demonstrates that participants using\nthe explanations made the users trust in the system's decisions, with\nconnection explanations and important event detection proving particularly\neffective at building user confidence. Survey responses indicate that the\nmulti-level explanation approach helped users develop appropriate trust in the\nsystem's narrative extraction capabilities. This work advances the\nstate-of-the-art in explainable narrative extraction while providing practical\ninsights for developing reliable narrative extraction systems that support\neffective human-AI collaboration.",
    "categories": [
      "cs.CL",
      "cs.HC"
    ],
    "authors": [
      "Brian Keith",
      "Fausto German",
      "Eric Krokos",
      "Sarah Joseph",
      "Chris North"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15586v1",
    "title": "How to Train Your Dragon: Automatic Diffusion-Based Rigging for Characters with Diverse Topologies",
    "abstract": "Recent diffusion-based methods have achieved impressive results on animating\nimages of human subjects. However, most of that success has built on\nhuman-specific body pose representations and extensive training with labeled\nreal videos. In this work, we extend the ability of such models to animate\nimages of characters with more diverse skeletal topologies. Given a small\nnumber (3-5) of example frames showing the character in different poses with\ncorresponding skeletal information, our model quickly infers a rig for that\ncharacter that can generate images corresponding to new skeleton poses. We\npropose a procedural data generation pipeline that efficiently samples training\ndata with diverse topologies on the fly. We use it, along with a novel skeleton\nrepresentation, to train our model on articulated shapes spanning a large space\nof textures and topologies. Then during fine-tuning, our model rapidly adapts\nto unseen target characters and generalizes well to rendering new poses, both\nfor realistic and more stylized cartoon appearances. To better evaluate\nperformance on this novel and challenging task, we create the first 2D video\ndataset that contains both humanoid and non-humanoid subjects with per-frame\nkeypoint annotations. With extensive experiments, we demonstrate the superior\nquality of our results. Project page: https://traindragondiffusion.github.io/",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Zeqi Gu",
      "Difan Liu",
      "Timothy Langlois",
      "Matthew Fisher",
      "Abe Davis"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15470v1",
    "title": "EgoDTM: Towards 3D-Aware Egocentric Video-Language Pretraining",
    "abstract": "Egocentric video-language pretraining has significantly advanced video\nrepresentation learning. Humans perceive and interact with a fully 3D world,\ndeveloping spatial awareness that extends beyond text-based understanding.\nHowever, most previous works learn from 1D text or 2D visual cues, such as\nbounding boxes, which inherently lack 3D understanding. To bridge this gap, we\nintroduce EgoDTM, an Egocentric Depth- and Text-aware Model, jointly trained\nthrough large-scale 3D-aware video pretraining and video-text contrastive\nlearning. EgoDTM incorporates a lightweight 3D-aware decoder to efficiently\nlearn 3D-awareness from pseudo depth maps generated by depth estimation models.\nTo further facilitate 3D-aware video pretraining, we enrich the original brief\ncaptions with hand-object visual cues by organically combining several\nfoundation models. Extensive experiments demonstrate EgoDTM's superior\nperformance across diverse downstream tasks, highlighting its superior 3D-aware\nvisual understanding. Our code will be released at\nhttps://github.com/xuboshen/EgoDTM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Boshen Xu",
      "Yuting Mei",
      "Xinbi Liu",
      "Sipeng Zheng",
      "Qin Jin"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15469v2",
    "title": "Dynamic Bi-Elman Attention Networks (DBEAN): Dual-Directional Context-Aware Representation Learning for Enhanced Text Classification",
    "abstract": "Text classification, a fundamental task in natural language processing (NLP),\naims to categorize textual data into predefined labels. Traditional methods\nstruggled with complex linguistic structures and semantic dependencies. The\nadvent of deep learning, particularly recurrent neural networks (RNNs) and\nTransformer-based models, has significantly advanced the field by enabling\nnuanced feature extraction and context-aware predictions. Despite improvements,\nexisting models exhibit limitations in balancing interpretability,\ncomputational efficiency, and long-range contextual understanding. This paper\nproposes the Dynamic Bidirectional Elman with Attention Network (DBEAN), which\nintegrates bidirectional temporal modelling with self-attention mechanisms.\nDBEAN dynamically assigns weights to critical segments of input, improving\ncontextual representation while maintaining computational efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "ZhengLin Lai",
      "MengYao Liao",
      "Dong Xu"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15465v1",
    "title": "FP4DiT: Towards Effective Floating Point Quantization for Diffusion Transformers",
    "abstract": "Diffusion Models (DM) have revolutionized the text-to-image visual generation\nprocess. However, the large computational cost and model footprint of DMs\nhinders practical deployment, especially on edge devices. Post-training\nquantization (PTQ) is a lightweight method to alleviate these burdens without\nthe need for training or fine-tuning. While recent DM PTQ methods achieve W4A8\non integer-based PTQ, two key limitations remain: First, while most existing DM\nPTQ methods evaluate on classical DMs like Stable Diffusion XL, 1.5 or earlier,\nwhich use convolutional U-Nets, newer Diffusion Transformer (DiT) models like\nthe PixArt series, Hunyuan and others adopt fundamentally different transformer\nbackbones to achieve superior image synthesis. Second, integer (INT)\nquantization is prevailing in DM PTQ but doesn't align well with the network\nweight and activation distribution, while Floating-Point Quantization (FPQ) is\nstill under-investigated, yet it holds the potential to better align the weight\nand activation distributions in low-bit settings for DiT. In response, we\nintroduce FP4DiT, a PTQ method that leverages FPQ to achieve W4A6 quantization.\nSpecifically, we extend and generalize the Adaptive Rounding PTQ technique to\nadequately calibrate weight quantization for FPQ and demonstrate that DiT\nactivations depend on input patch data, necessitating robust online activation\nquantization techniques. Experimental results demonstrate that FP4DiT\noutperforms integer-based PTQ at W4A6 and W4A8 precision and generates\nconvincing visual content on PixArt-$\\alpha$, PixArt-$\\Sigma$ and Hunyuan in\nterms of several T2I metrics such as HPSv2 and CLIP.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Ruichen Chen",
      "Keith G. Mills",
      "Di Niu"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15463v2",
    "title": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment",
    "abstract": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our framework's effectiveness,\nadvancing toward truly user-adaptive AI systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Jia-Nan Li",
      "Jian Guan",
      "Songhao Wu",
      "Wei Wu",
      "Rui Yan"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.15457v1",
    "title": "Di$\\mathtt{[M]}$O: Distilling Masked Diffusion Models into One-step Generator",
    "abstract": "Masked Diffusion Models (MDMs) have emerged as a powerful generative modeling\ntechnique. Despite their remarkable results, they typically suffer from slow\ninference with several steps. In this paper, we propose Di$\\mathtt{[M]}$O, a\nnovel approach that distills masked diffusion models into a one-step generator.\nDi$\\mathtt{[M]}$O addresses two key challenges: (1) the intractability of using\nintermediate-step information for one-step generation, which we solve through\ntoken-level distribution matching that optimizes model output logits by an\n'on-policy framework' with the help of an auxiliary model; and (2) the lack of\nentropy in the initial distribution, which we address through a token\ninitialization strategy that injects randomness while maintaining similarity to\nteacher training distribution. We show Di$\\mathtt{[M]}$O's effectiveness on\nboth class-conditional and text-conditional image generation, impressively\nachieving performance competitive to multi-step teacher outputs while\ndrastically reducing inference time. To our knowledge, we are the first to\nsuccessfully achieve one-step distillation of masked diffusion models and the\nfirst to apply discrete distillation to text-to-image generation, opening new\npaths for efficient generative modeling.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yuanzhi Zhu",
      "Xi Wang",
      "Stéphane Lathuilière",
      "Vicky Kalogeiton"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15456v1",
    "title": "Temporal Encoding Strategies for Energy Time Series Prediction",
    "abstract": "In contemporary power systems, energy consumption prediction plays a crucial\nrole in maintaining grid stability and resource allocation enabling power\ncompanies to minimize energy waste and avoid overloading the grid. While there\nare several research works on energy optimization, they often fail to address\nthe complexities of real-time fluctuations and the cyclic pattern of energy\nconsumption. This work proposes a novel approach to enhance the accuracy of\npredictive models by employing sinusoidal encoding on periodic features of\ntime-series data. To demonstrate the increase in performance, several\nstatistical and ensemble machine learning models were trained on an energy\ndemand dataset, using the proposed sinusoidal encoding. The performance of\nthese models was then benchmarked against identical models trained on\ntraditional encoding methods. The results demonstrated a 12.6% improvement of\nRoot Mean Squared Error (from 0.5497 to 0.4802) and a 7.8% increase in the R^2\nscore (from 0.7530 to 0.8118), indicating that the proposed encoding better\ncaptures the cyclic nature of temporal patterns than traditional methods. The\nproposed methodology significantly improves prediction accuracy while\nmaintaining computational efficiency, making it suitable for real-time\napplications in smart grid systems.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Aayam Bansal",
      "Keertan Balaji",
      "Zeus Lalani"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15454v1",
    "title": "Evaluating Bias in Retrieval-Augmented Medical Question-Answering Systems",
    "abstract": "Medical QA systems powered by Retrieval-Augmented Generation (RAG) models\nsupport clinical decision-making but may introduce biases related to race,\ngender, and social determinants of health. We systematically evaluate biases in\nRAG-based LLM by examining demographic-sensitive queries and measuring\nretrieval discrepancies. Using datasets like MMLU and MedMCQA, we analyze\nretrieval overlap and correctness disparities. Our findings reveal substantial\ndemographic disparities within RAG pipelines, emphasizing the critical need for\nretrieval methods that explicitly account for fairness to ensure equitable\nclinical decision-making.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Yuelyu Ji",
      "Hang Zhang",
      "Yanshan Wang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15451v1",
    "title": "MotionStreamer: Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space",
    "abstract": "This paper addresses the challenge of text-conditioned streaming motion\ngeneration, which requires us to predict the next-step human pose based on\nvariable-length historical motions and incoming texts. Existing methods\nstruggle to achieve streaming motion generation, e.g., diffusion models are\nconstrained by pre-defined motion lengths, while GPT-based methods suffer from\ndelayed response and error accumulation problem due to discretized non-causal\ntokenization. To solve these problems, we propose MotionStreamer, a novel\nframework that incorporates a continuous causal latent space into a\nprobabilistic autoregressive model. The continuous latents mitigate information\nloss caused by discretization and effectively reduce error accumulation during\nlong-term autoregressive generation. In addition, by establishing temporal\ncausal dependencies between current and historical motion latents, our model\nfully utilizes the available information to achieve accurate online motion\ndecoding. Experiments show that our method outperforms existing approaches\nwhile offering more applications, including multi-round generation, long-term\ngeneration, and dynamic motion composition. Project Page:\nhttps://zju3dv.github.io/MotionStreamer/",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Lixing Xiao",
      "Shunlin Lu",
      "Huaijin Pi",
      "Ke Fan",
      "Liang Pan",
      "Yueer Zhou",
      "Ziyong Feng",
      "Xiaowei Zhou",
      "Sida Peng",
      "Jingbo Wang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15450v1",
    "title": "SkyLadder: Better and Faster Pretraining via Context Window Scheduling",
    "abstract": "Recent advancements in LLM pretraining have featured ever-expanding context\nwindows to process longer sequences. However, our pilot study reveals that\nmodels pretrained with shorter context windows consistently outperform their\nlong-context counterparts under a fixed token budget. This finding motivates us\nto explore an optimal context window scheduling strategy to better balance\nlong-context capability with pretraining efficiency. To this end, we propose\nSkyLadder, a simple yet effective approach that implements a short-to-long\ncontext window transition. SkyLadder preserves strong standard benchmark\nperformance, while matching or exceeding baseline results on long context\ntasks. Through extensive experiments, we pre-train 1B-parameter models (up to\n32K context) and 3B-parameter models (8K context) on 100B tokens, demonstrating\nthat SkyLadder yields consistent gains of up to 3.7% on common benchmarks,\nwhile achieving up to 22% faster training speeds compared to baselines. The\ncode is at https://github.com/sail-sg/SkyLadder.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Tongyao Zhu",
      "Qian Liu",
      "Haonan Wang",
      "Shiqi Chen",
      "Xiangming Gu",
      "Tianyu Pang",
      "Min-Yen Kan"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15448v1",
    "title": "Reducing Communication Overhead in Federated Learning for Network Anomaly Detection with Adaptive Client Selection",
    "abstract": "Communication overhead in federated learning (FL) poses a significant\nchallenge for network anomaly detection systems, where diverse client\nconfigurations and network conditions impact efficiency and detection accuracy.\nExisting approaches attempt optimization individually but struggle to balance\nreduced overhead with performance. This paper presents an adaptive FL framework\ncombining batch size optimization, client selection, and asynchronous updates\nfor efficient anomaly detection. Using UNSW-NB15 for general network traffic\nand ROAD for automotive networks, our framework reduces communication overhead\nby 97.6% (700.0s to 16.8s) while maintaining comparable accuracy (95.10% vs.\n95.12%). The Mann-Whitney U test confirms significant improvements (p < 0.05).\nProfiling analysis reveals efficiency gains via reduced GPU operations and\nmemory transfers, ensuring robust detection across varying client conditions.",
    "categories": [
      "cs.DC",
      "cs.LG"
    ],
    "authors": [
      "William Marfo",
      "Deepak Tosh",
      "Shirley Moore",
      "Joshua Suetterlein",
      "Joseph Manzano"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15441v1",
    "title": "A discontinuity-capturing neural network with categorical embedding and its application to anisotropic elliptic interface problems",
    "abstract": "In this paper, we propose a discontinuity-capturing shallow neural network\nwith categorical embedding to represent piecewise smooth functions. The network\ncomprises three hidden layers, a discontinuity-capturing layer, a categorical\nembedding layer, and a fully-connected layer. Under such a design, we show that\na piecewise smooth function, even with a large number of pieces, can be\napproximated by a single neural network with high prediction accuracy. We then\nleverage the proposed network model to solve anisotropic elliptic interface\nproblems. The network is trained by minimizing the mean squared error loss of\nthe system. Our results show that, despite its simple and shallow structure,\nthe proposed neural network model exhibits comparable efficiency and accuracy\nto traditional grid-based numerical methods.",
    "categories": [
      "math.NA",
      "cs.LG",
      "cs.NA",
      "41A46, 65C05, 65N99, 68T01"
    ],
    "authors": [
      "Wei-Fan Hu",
      "Te-Sheng Lin",
      "Ming-Chih Lai"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15438v1",
    "title": "VenusFactory: A Unified Platform for Protein Engineering Data Retrieval and Language Model Fine-Tuning",
    "abstract": "Natural language processing (NLP) has significantly influenced scientific\ndomains beyond human language, including protein engineering, where pre-trained\nprotein language models (PLMs) have demonstrated remarkable success. However,\ninterdisciplinary adoption remains limited due to challenges in data\ncollection, task benchmarking, and application. This work presents\nVenusFactory, a versatile engine that integrates biological data retrieval,\nstandardized task benchmarking, and modular fine-tuning of PLMs. VenusFactory\nsupports both computer science and biology communities with choices of both a\ncommand-line execution and a Gradio-based no-code interface, integrating $40+$\nprotein-related datasets and $40+$ popular PLMs. All implementations are\nopen-sourced on https://github.com/tyang816/VenusFactory.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "q-bio.QM"
    ],
    "authors": [
      "Yang Tan",
      "Chen Liu",
      "Jingyuan Gao",
      "Banghao Wu",
      "Mingchen Li",
      "Ruilin Wang",
      "Lingrong Zhang",
      "Huiqun Yu",
      "Guisheng Fan",
      "Liang Hong",
      "Bingxin Zhou"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15436v1",
    "title": "An extensive simulation study evaluating the interaction of resampling techniques across multiple causal discovery contexts",
    "abstract": "Despite the accelerating presence of exploratory causal analysis in modern\nscience and medicine, the available non-experimental methods for validating\ncausal models are not well characterized. One of the most popular methods is to\nevaluate the stability of model features after resampling the data, similar to\nresampling methods for estimating confidence intervals in statistics. Many\naspects of this approach have received little to no attention, however, such as\nwhether the choice of resampling method should depend on the sample size,\nalgorithms being used, or algorithm tuning parameters. We present theoretical\nresults proving that certain resampling methods closely emulate the assignment\nof specific values to algorithm tuning parameters. We also report the results\nof extensive simulation experiments, which verify the theoretical result and\nprovide substantial data to aid researchers in further characterizing\nresampling in the context of causal discovery analysis. Together, the\ntheoretical work and simulation results provide specific guidance on how\nresampling methods and tuning parameters should be selected in practice.",
    "categories": [
      "stat.ME",
      "cs.AI"
    ],
    "authors": [
      "Ritwick Banerjee",
      "Bryan Andrews",
      "Erich Kummerfeld"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15435v1",
    "title": "V2X-DG: Domain Generalization for Vehicle-to-Everything Cooperative Perception",
    "abstract": "LiDAR-based Vehicle-to-Everything (V2X) cooperative perception has\ndemonstrated its impact on the safety and effectiveness of autonomous driving.\nSince current cooperative perception algorithms are trained and tested on the\nsame dataset, the generalization ability of cooperative perception systems\nremains underexplored. This paper is the first work to study the Domain\nGeneralization problem of LiDAR-based V2X cooperative perception (V2X-DG) for\n3D detection based on four widely-used open source datasets: OPV2V, V2XSet,\nV2V4Real and DAIR-V2X. Our research seeks to sustain high performance not only\nwithin the source domain but also across other unseen domains, achieved solely\nthrough training on source domain. To this end, we propose Cooperative Mixup\nAugmentation based Generalization (CMAG) to improve the model generalization\ncapability by simulating the unseen cooperation, which is designed compactly\nfor the domain gaps in cooperative perception. Furthermore, we propose a\nconstraint for the regularization of the robust generalized feature\nrepresentation learning: Cooperation Feature Consistency (CFC), which aligns\nthe intermediately fused features of the generalized cooperation by CMAG and\nthe early fused features of the original cooperation in source domain.\nExtensive experiments demonstrate that our approach achieves significant\nperformance gains when generalizing to other unseen datasets while it also\nmaintains strong performance on the source dataset.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Baolu Li",
      "Zongzhe Xu",
      "Jinlong Li",
      "Xinyu Liu",
      "Jianwu Fang",
      "Xiaopeng Li",
      "Hongkai Yu"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15432v1",
    "title": "Accurate, transferable, and verifiable machine-learned interatomic potentials for layered materials",
    "abstract": "Twisted layered van-der-Waals materials often exhibit unique electronic and\noptical properties absent in their non-twisted counterparts. Unfortunately,\npredicting such properties is hindered by the difficulty in determining the\natomic structure in materials displaying large moir\\'e domains. Here, we\nintroduce a split machine-learned interatomic potential and dataset curation\napproach that separates intralayer and interlayer interactions and\nsignificantly improves model accuracy -- with a tenfold increase in energy and\nforce prediction accuracy relative to conventional models. We further\ndemonstrate that traditional MLIP validation metrics -- force and energy errors\n-- are inadequate for moir\\'e structures and develop a more holistic,\nphysically-motivated metric based on the distribution of stacking\nconfigurations. This metric effectively compares the entirety of large-scale\nmoir\\'e domains between two structures instead of relying on conventional\nmeasures evaluated on smaller commensurate cells. Finally, we establish that\none-dimensional instead of two-dimensional moir\\'e structures can serve as\nefficient surrogate systems for validating MLIPs, allowing for a practical\nmodel validation protocol against explicit DFT calculations. Applying our\nframework to HfS2/GaS bilayers reveals that accurate structural predictions\ndirectly translate into reliable electronic properties. Our model-agnostic\napproach integrates seamlessly with various intralayer and interlayer\ninteraction models, enabling computationally tractable relaxation of moir\\'e\nmaterials, from bilayer to complex multilayers, with rigorously validated\naccuracy.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.LG"
    ],
    "authors": [
      "Johnathan D. Georgaras",
      "Akash Ramdas",
      "Chung Hsuan Shan",
      "Elena Halsted",
      "Berwyn",
      "Tianshu Li",
      "Felipe H. da Jornada"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15426v1",
    "title": "Visual Position Prompt for MLLM based Visual Grounding",
    "abstract": "Although Multimodal Large Language Models (MLLMs) excel at various\nimage-related tasks, they encounter challenges in precisely aligning\ncoordinates with spatial information within images, particularly in\nposition-aware tasks such as visual grounding. This limitation arises from two\nkey factors. First, MLLMs lack explicit spatial references, making it difficult\nto associate textual descriptions with precise image locations. Second, their\nfeature extraction processes prioritize global context over fine-grained\nspatial details, leading to weak localization capability. To address this\nissue, we introduce VPP-LLaVA, an MLLM equipped with Visual Position Prompt\n(VPP) to improve its grounding capability. VPP-LLaVA integrates two\ncomplementary mechanisms. The global VPP overlays learnable, axis-like\nembeddings onto the input image to provide structured spatial cues. The local\nVPP focuses on fine-grained localization by incorporating position-aware\nqueries, which suggests probable object locations. We also introduce a VPP-SFT\ndataset with 0.6M samples, consolidating high-quality visual grounding data\ninto a compact format for efficient model training. Training on this dataset\nwith VPP enhances the model's performance, achieving state-of-the-art results\non standard grounding benchmarks despite using fewer training samples compared\nto other MLLMs like MiniGPT-v2, which rely on much larger datasets ($\\sim$21M\nsamples). The code and VPP-SFT dataset will be available at\nhttps://github.com/WayneTomas/VPP-LLaVA upon acceptance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Wei Tang",
      "Yanpeng Sun",
      "Qinying Gu",
      "Zechao Li"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15421v1",
    "title": "Probing the topology of the space of tokens with structured prompts",
    "abstract": "This article presents a general and flexible method for prompting a large\nlanguage model (LLM) to reveal its (hidden) token input embedding up to\nhomeomorphism. Moreover, this article provides strong theoretical justification\n-- a mathematical proof for generic LLMs -- for why this method should be\nexpected to work. With this method in hand, we demonstrate its effectiveness by\nrecovering the token subspace of Llemma-7B. The results of this paper apply not\nonly to LLMs but also to general nonlinear autoregressive processes.",
    "categories": [
      "math.DG",
      "cs.AI",
      "53Z50, 58Z05",
      "I.2.7"
    ],
    "authors": [
      "Michael Robinson",
      "Sourya Dey",
      "Taisa Kushner"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15420v1",
    "title": "LIFT: Latent Implicit Functions for Task- and Data-Agnostic Encoding",
    "abstract": "Implicit Neural Representations (INRs) are proving to be a powerful paradigm\nin unifying task modeling across diverse data domains, offering key advantages\nsuch as memory efficiency and resolution independence. Conventional deep\nlearning models are typically modality-dependent, often requiring custom\narchitectures and objectives for different types of signals. However, existing\nINR frameworks frequently rely on global latent vectors or exhibit\ncomputational inefficiencies that limit their broader applicability. We\nintroduce LIFT, a novel, high-performance framework that addresses these\nchallenges by capturing multiscale information through meta-learning. LIFT\nleverages multiple parallel localized implicit functions alongside a\nhierarchical latent generator to produce unified latent representations that\nspan local, intermediate, and global features. This architecture facilitates\nsmooth transitions across local regions, enhancing expressivity while\nmaintaining inference efficiency. Additionally, we introduce ReLIFT, an\nenhanced variant of LIFT that incorporates residual connections and expressive\nfrequency encodings. With this straightforward approach, ReLIFT effectively\naddresses the convergence-capacity gap found in comparable methods, providing\nan efficient yet powerful solution to improve capacity and speed up\nconvergence. Empirical results show that LIFT achieves state-of-the-art (SOTA)\nperformance in generative modeling and classification tasks, with notable\nreductions in computational costs. Moreover, in single-task settings, the\nstreamlined ReLIFT architecture proves effective in signal representations and\ninverse problem tasks.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Amirhossein Kazerouni",
      "Soroush Mehraban",
      "Michael Brudno",
      "Babak Taati"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15417v1",
    "title": "Temporal Regularization Makes Your Video Generator Stronger",
    "abstract": "Temporal quality is a critical aspect of video generation, as it ensures\nconsistent motion and realistic dynamics across frames. However, achieving high\ntemporal coherence and diversity remains challenging. In this work, we explore\ntemporal augmentation in video generation for the first time, and introduce\nFluxFlow for initial investigation, a strategy designed to enhance temporal\nquality. Operating at the data level, FluxFlow applies controlled temporal\nperturbations without requiring architectural modifications. Extensive\nexperiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow\nsignificantly improves temporal coherence and diversity across various video\ngeneration models, including U-Net, DiT, and AR-based architectures, while\npreserving spatial fidelity. These findings highlight the potential of temporal\naugmentation as a simple yet effective approach to advancing video generation\nquality.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Harold Haodong Chen",
      "Haojian Huang",
      "Xianfeng Wu",
      "Yexin Liu",
      "Yajing Bai",
      "Wen-Jie Shu",
      "Harry Yang",
      "Ser-Nam Lim"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15415v1",
    "title": "Automated Processing of eXplainable Artificial Intelligence Outputs in Deep Learning Models for Fault Diagnostics of Large Infrastructures",
    "abstract": "Deep Learning (DL) models processing images to recognize the health state of\nlarge infrastructure components can exhibit biases and rely on non-causal\nshortcuts. eXplainable Artificial Intelligence (XAI) can address these issues\nbut manually analyzing explanations generated by XAI techniques is\ntime-consuming and prone to errors. This work proposes a novel framework that\ncombines post-hoc explanations with semi-supervised learning to automatically\nidentify anomalous explanations that deviate from those of correctly classified\nimages and may therefore indicate model abnormal behaviors. This significantly\nreduces the workload for maintenance decision-makers, who only need to manually\nreclassify images flagged as having anomalous explanations. The proposed\nframework is applied to drone-collected images of insulator shells for power\ngrid infrastructure monitoring, considering two different Convolutional Neural\nNetworks (CNNs), GradCAM explanations and Deep Semi-Supervised Anomaly\nDetection. The average classification accuracy on two faulty classes is\nimproved by 8% and maintenance operators are required to manually reclassify\nonly 15% of the images. We compare the proposed framework with a\nstate-of-the-art approach based on the faithfulness metric: the experimental\nresults obtained demonstrate that the proposed framework consistently achieves\nF_1 scores larger than those of the faithfulness-based approach. Additionally,\nthe proposed framework successfully identifies correct classifications that\nresult from non-causal shortcuts, such as the presence of ID tags printed on\ninsulator shells.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Giovanni Floreale",
      "Piero Baraldi",
      "Enrico Zio",
      "Olga Fink"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15414v1",
    "title": "Federated Continual 3D Segmentation With Single-round Communication",
    "abstract": "Federated learning seeks to foster collaboration among distributed clients\nwhile preserving the privacy of their local data. Traditionally, federated\nlearning methods assume a fixed setting in which client data and learning\nobjectives remain constant. However, in real-world scenarios, new clients may\njoin, and existing clients may expand the segmentation label set as task\nrequirements evolve. In such a dynamic federated analysis setup, the\nconventional federated communication strategy of model aggregation per\ncommunication round is suboptimal. As new clients join, this strategy requires\nretraining, linearly increasing communication and computation overhead. It also\nimposes requirements for synchronized communication, which is difficult to\nachieve among distributed clients. In this paper, we propose a federated\ncontinual learning strategy that employs a one-time model aggregation at the\nserver through multi-model distillation. This approach builds and updates the\nglobal model while eliminating the need for frequent server communication. When\nintegrating new data streams or onboarding new clients, this approach\nefficiently reuses previous client models, avoiding the need to retrain the\nglobal model across the entire federation. By minimizing communication load and\nbypassing the need to put unchanged clients online, our approach relaxes\nsynchronization requirements among clients, providing an efficient and scalable\nfederated analysis framework suited for real-world applications. Using\nmulti-class 3D abdominal CT segmentation as an application task, we demonstrate\nthe effectiveness of the proposed approach.",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Can Peng",
      "Qianhui Men",
      "Pramit Saha",
      "Qianye Yang",
      "Cheng Ouyang",
      "J. Alison Noble"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15412v1",
    "title": "Learn Your Scales: Towards Scale-Consistent Generative Novel View Synthesis",
    "abstract": "Conventional depth-free multi-view datasets are captured using a moving\nmonocular camera without metric calibration. The scales of camera positions in\nthis monocular setting are ambiguous. Previous methods have acknowledged scale\nambiguity in multi-view data via various ad-hoc normalization pre-processing\nsteps, but have not directly analyzed the effect of incorrect scene scales on\ntheir application. In this paper, we seek to understand and address the effect\nof scale ambiguity when used to train generative novel view synthesis methods\n(GNVS). In GNVS, new views of a scene or object can be minimally synthesized\ngiven a single image and are, thus, unconstrained, necessitating the use of\ngenerative methods. The generative nature of these models captures all aspects\nof uncertainty, including any uncertainty of scene scales, which act as\nnuisance variables for the task. We study the effect of scene scale ambiguity\nin GNVS when sampled from a single image by isolating its effect on the\nresulting models and, based on these intuitions, define new metrics that\nmeasure the scale inconsistency of generated views. We then propose a framework\nto estimate scene scales jointly with the GNVS model in an end-to-end fashion.\nEmpirically, we show that our method reduces the scale inconsistency of\ngenerated views without the complexity or downsides of previous scale\nnormalization methods. Further, we show that removing this ambiguity improves\ngenerated image quality of the resulting GNVS model.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Fereshteh Forghani",
      "Jason J. Yu",
      "Tristan Aumentado-Armstrong",
      "Konstantinos G. Derpanis",
      "Marcus A. Brubaker"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15407v1",
    "title": "Exploiting Prior Knowledge in Preferential Learning of Individualized Autonomous Vehicle Driving Styles",
    "abstract": "Trajectory planning for automated vehicles commonly employs optimization over\na moving horizon - Model Predictive Control - where the cost function\ncritically influences the resulting driving style. However, finding a suitable\ncost function that results in a driving style preferred by passengers remains\nan ongoing challenge. We employ preferential Bayesian optimization to learn the\ncost function by iteratively querying a passenger's preference. Due to\nincreasing dimensionality of the parameter space, preference learning\napproaches might struggle to find a suitable optimum with a limited number of\nexperiments and expose the passenger to discomfort when exploring the parameter\nspace. We address these challenges by incorporating prior knowledge into the\npreferential Bayesian optimization framework. Our method constructs a virtual\ndecision maker from real-world human driving data to guide parameter sampling.\nIn a simulation experiment, we achieve faster convergence of the\nprior-knowledge-informed learning procedure compared to existing preferential\nBayesian optimization approaches and reduce the number of inadequate driving\nstyles sampled.",
    "categories": [
      "eess.SY",
      "cs.LG",
      "cs.SY"
    ],
    "authors": [
      "Lukas Theiner",
      "Sebastian Hirt",
      "Alexander Steinke",
      "Rolf Findeisen"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15583v1",
    "title": "Efficient Post-Hoc Uncertainty Calibration via Variance-Based Smoothing",
    "abstract": "Since state-of-the-art uncertainty estimation methods are often\ncomputationally demanding, we investigate whether incorporating prior\ninformation can improve uncertainty estimates in conventional deep neural\nnetworks. Our focus is on machine learning tasks where meaningful predictions\ncan be made from sub-parts of the input. For example, in speaker\nclassification, the speech waveform can be divided into sequential patches,\neach containing information about the same speaker. We observe that the\nvariance between sub-predictions serves as a reliable proxy for uncertainty in\nsuch settings. Our proposed variance-based scaling framework produces\ncompetitive uncertainty estimates in classification while being less\ncomputationally demanding and allowing for integration as a post-hoc\ncalibration tool. This approach also leads to a simple extension of deep\nensembles, improving the expressiveness of their predicted distributions.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Fabian Denoodt",
      "José Oramas"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15406v1",
    "title": "Visual Persona: Foundation Model for Full-Body Human Customization",
    "abstract": "We introduce Visual Persona, a foundation model for text-to-image full-body\nhuman customization that, given a single in-the-wild human image, generates\ndiverse images of the individual guided by text descriptions. Unlike prior\nmethods that focus solely on preserving facial identity, our approach captures\ndetailed full-body appearance, aligning with text descriptions for body\nstructure and scene variations. Training this model requires large-scale paired\nhuman data, consisting of multiple images per individual with consistent\nfull-body identities, which is notoriously difficult to obtain. To address\nthis, we propose a data curation pipeline leveraging vision-language models to\nevaluate full-body appearance consistency, resulting in Visual Persona-500K, a\ndataset of 580k paired human images across 100k unique identities. For precise\nappearance transfer, we introduce a transformer encoder-decoder architecture\nadapted to a pre-trained text-to-image diffusion model, which augments the\ninput image into distinct body regions, encodes these regions as local\nappearance features, and projects them into dense identity embeddings\nindependently to condition the diffusion model for synthesizing customized\nimages. Visual Persona consistently surpasses existing approaches, generating\nhigh-quality, customized images from in-the-wild inputs. Extensive ablation\nstudies validate design choices, and we demonstrate the versatility of Visual\nPersona across various downstream tasks.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jisu Nam",
      "Soowon Son",
      "Zhan Xu",
      "Jing Shi",
      "Difan Liu",
      "Feng Liu",
      "Aashish Misraa",
      "Seungryong Kim",
      "Yang Zhou"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15404v1",
    "title": "Improving Adversarial Transferability on Vision Transformers via Forward Propagation Refinement",
    "abstract": "Vision Transformers (ViTs) have been widely applied in various computer\nvision and vision-language tasks. To gain insights into their robustness in\npractical scenarios, transferable adversarial examples on ViTs have been\nextensively studied. A typical approach to improving adversarial\ntransferability is by refining the surrogate model. However, existing work on\nViTs has restricted their surrogate refinement to backward propagation. In this\nwork, we instead focus on Forward Propagation Refinement (FPR) and specifically\nrefine two key modules of ViTs: attention maps and token embeddings. For\nattention maps, we propose Attention Map Diversification (AMD), which\ndiversifies certain attention maps and also implicitly imposes beneficial\ngradient vanishing during backward propagation. For token embeddings, we\npropose Momentum Token Embedding (MTE), which accumulates historical token\nembeddings to stabilize the forward updates in both the Attention and MLP\nblocks. We conduct extensive experiments with adversarial examples transferred\nfrom ViTs to various CNNs and ViTs, demonstrating that our FPR outperforms the\ncurrent best (backward) surrogate refinement by up to 7.0\\% on average. We also\nvalidate its superiority against popular defenses and its compatibility with\nother transfer methods. Codes and appendix are available at\nhttps://github.com/RYC-98/FPR.",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "authors": [
      "Yuchen Ren",
      "Zhengyu Zhao",
      "Chenhao Lin",
      "Bo Yang",
      "Lu Zhou",
      "Zhe Liu",
      "Chao Shen"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15403v1",
    "title": "HQNN-FSP: A Hybrid Classical-Quantum Neural Network for Regression-Based Financial Stock Market Prediction",
    "abstract": "Financial time-series forecasting remains a challenging task due to complex\ntemporal dependencies and market fluctuations. This study explores the\npotential of hybrid quantum-classical approaches to assist in financial trend\nprediction by leveraging quantum resources for improved feature representation\nand learning. A custom Quantum Neural Network (QNN) regressor is introduced,\ndesigned with a novel ansatz tailored for financial applications. Two hybrid\noptimization strategies are proposed: (1) a sequential approach where classical\nrecurrent models (RNN/LSTM) extract temporal dependencies before quantum\nprocessing, and (2) a joint learning framework that optimizes classical and\nquantum parameters simultaneously. Systematic evaluation using TimeSeriesSplit,\nk-fold cross-validation, and predictive error analysis highlights the ability\nof these hybrid models to integrate quantum computing into financial\nforecasting workflows. The findings demonstrate how quantum-assisted learning\ncan contribute to financial modeling, offering insights into the practical role\nof quantum resources in time-series analysis.",
    "categories": [
      "q-fin.ST",
      "cs.LG",
      "quant-ph"
    ],
    "authors": [
      "Prashant Kumar Choudhary",
      "Nouhaila Innan",
      "Muhammad Shafique",
      "Rajeev Singh"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15402v1",
    "title": "Towards efficient keyword spotting using spike-based time difference encoders",
    "abstract": "Keyword spotting in edge devices is becoming increasingly important as\nvoice-activated assistants are widely used. However, its deployment is often\nlimited by the extreme low-power constraints of the target embedded systems.\nHere, we explore the Temporal Difference Encoder (TDE) performance in keyword\nspotting. This recent neuron model encodes the time difference in instantaneous\nfrequency and spike count to perform efficient keyword spotting with\nneuromorphic processors. We use the TIdigits dataset of spoken digits with a\nformant decomposition and rate-based encoding into spikes. We compare three\nSpiking Neural Networks (SNNs) architectures to learn and classify\nspatio-temporal signals. The proposed SNN architectures are made of three\nlayers with variation in its hidden layer composed of either (1) feedforward\nTDE, (2) feedforward Current-Based Leaky Integrate-and-Fire (CuBa-LIF), or (3)\nrecurrent CuBa-LIF neurons. We first show that the spike trains of the\nfrequency-converted spoken digits have a large amount of information in the\ntemporal domain, reinforcing the importance of better exploiting temporal\nencoding for such a task. We then train the three SNNs with the same number of\nsynaptic weights to quantify and compare their performance based on the\naccuracy and synaptic operations. The resulting accuracy of the feedforward TDE\nnetwork (89%) is higher than the feedforward CuBa-LIF network (71%) and close\nto the recurrent CuBa-LIF network (91%). However, the feedforward TDE-based\nnetwork performs 92% fewer synaptic operations than the recurrent CuBa-LIF\nnetwork with the same amount of synapses. In addition, the results of the TDE\nnetwork are highly interpretable and correlated with the frequency and\ntimescale features of the spoken keywords in the dataset. Our findings suggest\nthat the TDE is a promising neuron model for scalable event-driven processing\nof spatio-temporal patterns.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CV",
      "cs.ET"
    ],
    "authors": [
      "Alejandro Pequeño-Zurro",
      "Lyes Khacef",
      "Stefano Panzeri",
      "Elisabetta Chicca"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15390v1",
    "title": "FedSCA: Federated Tuning with Similarity-guided Collaborative Aggregation for Heterogeneous Medical Image Segmentation",
    "abstract": "Transformer-based foundation models (FMs) have recently demonstrated\nremarkable performance in medical image segmentation. However, scaling these\nmodels is challenging due to the limited size of medical image datasets within\nisolated hospitals, where data centralization is restricted due to privacy\nconcerns. These constraints, combined with the data-intensive nature of FMs,\nhinder their broader application. Integrating federated learning (FL) with\nfoundation models (FLFM) fine-tuning offers a potential solution to these\nchallenges by enabling collaborative model training without data sharing, thus\nallowing FMs to take advantage of a diverse pool of sensitive medical image\ndata across hospitals/clients. However, non-independent and identically\ndistributed (non-IID) data among clients, paired with computational and\ncommunication constraints in federated environments, presents an additional\nchallenge that limits further performance improvements and remains inadequately\naddressed in existing studies. In this work, we propose a novel FLFM\nfine-tuning framework, \\underline{\\textbf{Fed}}erated tuning with\n\\underline{\\textbf{S}}imilarity-guided \\underline{\\textbf{C}}ollaborative\n\\underline{\\textbf{A}}ggregation (FedSCA), encompassing all phases of the FL\nprocess. This includes (1) specially designed parameter-efficient fine-tuning\n(PEFT) for local client training to enhance computational efficiency; (2)\npartial low-level adapter transmission for communication efficiency; and (3)\nsimilarity-guided collaborative aggregation (SGCA) on the server side to\naddress non-IID issues. Extensive experiments on three FL benchmarks for\nmedical image segmentation demonstrate the effectiveness of our proposed\nFedSCA, establishing new SOTA performance.",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Yumin Zhang",
      "Yan Gao",
      "Haoran Duan",
      "Hanqing Guo",
      "Tejal Shah",
      "Rajiv Ranjan",
      "Bo Wei"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15386v1",
    "title": "CCDP: Composition of Conditional Diffusion Policies with Guided Sampling",
    "abstract": "Imitation Learning offers a promising approach to learn directly from data\nwithout requiring explicit models, simulations, or detailed task definitions.\nDuring inference, actions are sampled from the learned distribution and\nexecuted on the robot. However, sampled actions may fail for various reasons,\nand simply repeating the sampling step until a successful action is obtained\ncan be inefficient. In this work, we propose an enhanced sampling strategy that\nrefines the sampling distribution to avoid previously unsuccessful actions. We\ndemonstrate that by solely utilizing data from successful demonstrations, our\nmethod can infer recovery actions without the need for additional exploratory\nbehavior or a high-level controller. Furthermore, we leverage the concept of\ndiffusion model decomposition to break down the primary problem (which may\nrequire long-horizon history to manage failures) into multiple smaller, more\nmanageable sub-problems in learning, data collection, and inference, thereby\nenabling the system to adapt to variable failure counts. Our approach yields a\nlow-level controller that dynamically adjusts its sampling space to improve\nefficiency when prior samples fall short. We validate our method across several\ntasks, including door opening with unknown directions, object manipulation, and\nbutton-searching scenarios, demonstrating that our approach outperforms\ntraditional baselines.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Amirreza Razmjoo",
      "Sylvain Calinon",
      "Michael Gienger",
      "Fan Zhang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15374v1",
    "title": "Real-world validation of a multimodal LLM-powered pipeline for High-Accuracy Clinical Trial Patient Matching leveraging EHR data",
    "abstract": "Background: Patient recruitment in clinical trials is hindered by complex\neligibility criteria and labor-intensive chart reviews. Prior research using\ntext-only models have struggled to address this problem in a reliable and\nscalable way due to (1) limited reasoning capabilities, (2) information loss\nfrom converting visual records to text, and (3) lack of a generic EHR\nintegration to extract patient data.\n  Methods: We introduce a broadly applicable, integration-free, LLM-powered\npipeline that automates patient-trial matching using unprocessed documents\nextracted from EHRs. Our approach leverages (1) the new reasoning-LLM paradigm,\nenabling the assessment of even the most complex criteria, (2) visual\ncapabilities of latest LLMs to interpret medical records without lossy\nimage-to-text conversions, and (3) multimodal embeddings for efficient medical\nrecord search. The pipeline was validated on the n2c2 2018 cohort selection\ndataset (288 diabetic patients) and a real-world dataset composed of 485\npatients from 30 different sites matched against 36 diverse trials.\n  Results: On the n2c2 dataset, our method achieved a new state-of-the-art\ncriterion-level accuracy of 93\\%. In real-world trials, the pipeline yielded an\naccuracy of 87\\%, undermined by the difficulty to replicate human\ndecision-making when medical records lack sufficient information. Nevertheless,\nusers were able to review overall eligibility in under 9 minutes per patient on\naverage, representing an 80\\% improvement over traditional manual chart\nreviews.\n  Conclusion: This pipeline demonstrates robust performance in clinical trial\npatient matching without requiring custom integration with site systems or\ntrial-specific tailoring, thereby enabling scalable deployment across sites\nseeking to leverage AI for patient matching.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Anatole Callies",
      "Quentin Bodinier",
      "Philippe Ravaud",
      "Kourosh Davarpanah"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15371v1",
    "title": "Geometrically-Aware One-Shot Skill Transfer of Category-Level Objects",
    "abstract": "Robotic manipulation of unfamiliar objects in new environments is challenging\nand requires extensive training or laborious pre-programming. We propose a new\nskill transfer framework, which enables a robot to transfer complex object\nmanipulation skills and constraints from a single human demonstration. Our\napproach addresses the challenge of skill acquisition and task execution by\nderiving geometric representations from demonstrations focusing on\nobject-centric interactions. By leveraging the Functional Maps (FM) framework,\nwe efficiently map interaction functions between objects and their\nenvironments, allowing the robot to replicate task operations across objects of\nsimilar topologies or categories, even when they have significantly different\nshapes. Additionally, our method incorporates a Task-Space Imitation Algorithm\n(TSIA) which generates smooth, geometrically-aware robot paths to ensure the\ntransferred skills adhere to the demonstrated task constraints. We validate the\neffectiveness and adaptability of our approach through extensive experiments,\ndemonstrating successful skill transfer and task execution in diverse\nreal-world environments without requiring additional training.",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Cristiana de Farias",
      "Luis Figueredo",
      "Riddhiman Laha",
      "Maxime Adjigble",
      "Brahim Tamadazte",
      "Rustam Stolkin",
      "Sami Haddadin",
      "Naresh Marturi"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15369v1",
    "title": "EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language Models",
    "abstract": "While multimodal large language models demonstrate strong performance in\ncomplex reasoning tasks, they pose significant challenges related to model\ncomplexity during deployment, especially for resource-limited devices. In this\npaper, we propose an automatic pruning method for large vision-language models\nto enhance the efficiency of multimodal reasoning. Conventional methods rely on\nthe training data of the original model to select the proper pruning ratio for\ndifferent network components. However, these methods are impractical for large\nvision-language models due to the unaffordable search costs caused by web-scale\ntraining corpus. In contrast, our approach only leverages a small number of\nsamples to search for the desired pruning policy by maximizing its\ngeneralization ability on unknown training data while maintaining the model\naccuracy, which enables the achievement of an optimal trade-off between\naccuracy and efficiency for large visual language models. Specifically, we\nformulate the generalization gap of the pruning strategy using the structural\nrisk minimization principle. Based on both task performance and generalization\ncapability, we iteratively search for the optimal pruning policy within a given\nsearch space and optimize the vision projector to evolve the search space with\nhigher upper bound of performance. We conduct extensive experiments on the\nScienceQA, Vizwiz, MM-vet, and LLaVA-Bench datasets for the task of visual\nquestion answering. Using only 64 samples for pruning policy search,\nEfficientLLaVA achieves an accuracy of 83.05% on ScienceQA, along with a\n$\\times$ 1.8 speedup compared to the dense LLaVA-v1.5-7B model.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yinan Liang",
      "Ziwei Wang",
      "Xiuwei Xu",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15368v1",
    "title": "Online Imitation Learning for Manipulation via Decaying Relative Correction through Teleoperation",
    "abstract": "Teleoperated robotic manipulators enable the collection of demonstration\ndata, which can be used to train control policies through imitation learning.\nHowever, such methods can require significant amounts of training data to\ndevelop robust policies or adapt them to new and unseen tasks. While expert\nfeedback can significantly enhance policy performance, providing continuous\nfeedback can be cognitively demanding and time-consuming for experts. To\naddress this challenge, we propose to use a cable-driven teleoperation system\nwhich can provide spatial corrections with 6 degree of freedom to the\ntrajectories generated by a policy model. Specifically, we propose a correction\nmethod termed Decaying Relative Correction (DRC) which is based upon the\nspatial offset vector provided by the expert and exists temporarily, and which\nreduces the intervention steps required by an expert. Our results demonstrate\nthat DRC reduces the required expert intervention rate by 30\\% compared to a\nstandard absolute corrective method. Furthermore, we show that integrating DRC\nwithin an online imitation learning framework rapidly increases the success\nrate of manipulation tasks such as raspberry harvesting and cloth wiping.",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Cheng Pan",
      "Hung Hon Cheng",
      "Josie Hughes"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15367v1",
    "title": "FedBEns: One-Shot Federated Learning based on Bayesian Ensemble",
    "abstract": "One-Shot Federated Learning (FL) is a recent paradigm that enables multiple\nclients to cooperatively learn a global model in a single round of\ncommunication with a central server. In this paper, we analyze the One-Shot FL\nproblem through the lens of Bayesian inference and propose FedBEns, an\nalgorithm that leverages the inherent multimodality of local loss functions to\nfind better global models. Our algorithm leverages a mixture of Laplace\napproximations for the clients' local posteriors, which the server then\naggregates to infer the global model. We conduct extensive experiments on\nvarious datasets, demonstrating that the proposed method outperforms competing\nbaselines that typically rely on unimodal approximations of the local losses.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Jacopo Talpini",
      "Marco Savi",
      "Giovanni Neglia"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15361v1",
    "title": "Boosting HDR Image Reconstruction via Semantic Knowledge Transfer",
    "abstract": "Recovering High Dynamic Range (HDR) images from multiple Low Dynamic Range\n(LDR) images becomes challenging when the LDR images exhibit noticeable\ndegradation and missing content. Leveraging scene-specific semantic priors\noffers a promising solution for restoring heavily degraded regions. However,\nthese priors are typically extracted from sRGB Standard Dynamic Range (SDR)\nimages, the domain/format gap poses a significant challenge when applying it to\nHDR imaging. To address this issue, we propose a general framework that\ntransfers semantic knowledge derived from SDR domain via self-distillation to\nboost existing HDR reconstruction. Specifically, the proposed framework first\nintroduces the Semantic Priors Guided Reconstruction Model (SPGRM), which\nleverages SDR image semantic knowledge to address ill-posed problems in the\ninitial HDR reconstruction results. Subsequently, we leverage a\nself-distillation mechanism that constrains the color and content information\nwith semantic knowledge, aligning the external outputs between the baseline and\nSPGRM. Furthermore, to transfer the semantic knowledge of the internal\nfeatures, we utilize a semantic knowledge alignment module (SKAM) to fill the\nmissing semantic contents with the complementary masks. Extensive experiments\ndemonstrate that our method can significantly improve the HDR imaging quality\nof existing methods.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Qingsen Yan",
      "Tao Hu",
      "Genggeng Chen",
      "Wei Dong",
      "Yanning Zhang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15358v1",
    "title": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation",
    "abstract": "Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity.",
    "categories": [
      "cs.CL",
      "cs.CV",
      "I.2.7; I.4.m"
    ],
    "authors": [
      "Thomas Pickard",
      "Aline Villavicencio",
      "Maggie Mi",
      "Wei He",
      "Dylan Phelps",
      "Carolina Scarton",
      "Marco Idiart"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15355v1",
    "title": "Robustness of Nonlinear Representation Learning",
    "abstract": "We study the problem of unsupervised representation learning in slightly\nmisspecified settings, and thus formalize the study of robustness of nonlinear\nrepresentation learning. We focus on the case where the mixing is close to a\nlocal isometry in a suitable distance and show based on existing rigidity\nresults that the mixing can be identified up to linear transformations and\nsmall errors. In a second step, we investigate Independent Component Analysis\n(ICA) with observations generated according to $x=f(s)=As+h(s)$ where $A$ is an\ninvertible mixing matrix and $h$ a small perturbation. We show that we can\napproximately recover the matrix $A$ and the independent components. Together,\nthese two results show approximate identifiability of nonlinear ICA with almost\nisometric mixing functions. Those results are a step towards identifiability\nresults for unsupervised representation learning for real-world data that do\nnot follow restrictive model classes.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Simon Buchholz",
      "Bernhard Schölkopf"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15354v1",
    "title": "Optimizing Decomposition for Optimal Claim Verification",
    "abstract": "Current research on the \\textit{Decompose-Then-Verify} paradigm for\nevaluating the factuality of long-form text typically treats decomposition and\nverification in isolation, overlooking their interactions and potential\nmisalignment. We find that existing decomposition policies, typically\nhand-crafted demonstrations, do not align well with downstream verifiers in\nterms of atomicity -- a novel metric quantifying information density -- leading\nto suboptimal verification results. We formulate finding the optimal\ndecomposition policy for optimal verification as a bilevel optimization\nproblem. To approximate a solution for this strongly NP-hard problem, we\npropose dynamic decomposition, a reinforcement learning framework that\nleverages verifier feedback to learn a policy for dynamically decomposing\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\ndecomposition outperforms existing decomposition policies, improving\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\naverage across varying verifiers, datasets, and atomcities of input claims.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yining Lu",
      "Noah Ziems",
      "Hy Dang",
      "Meng Jiang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15352v1",
    "title": "Leveraging Perfect Multimodal Alignment and Gaussian Assumptions for Cross-modal Transfer",
    "abstract": "Multimodal alignment aims to construct a joint latent vector space where two\nmodalities representing the same concept map to the same vector. We formulate\nthis as an inverse problem and show that under certain conditions perfect\nalignment can be achieved. We then address a specific application of alignment\nreferred to as cross-modal transfer. Unsupervised cross-modal transfer aims to\nleverage a model trained with one modality to perform inference on another\nmodality, without any labeled fine-tuning on the new modality. Assuming that\nsemantic classes are represented as a mixture of Gaussians in the latent space,\nwe show how cross-modal transfer can be performed by projecting the data points\nfrom the representation space onto different subspaces representing each\nmodality. Our experiments on synthetic multimodal Gaussian data verify the\neffectiveness of our perfect alignment and cross-modal transfer method. We hope\nthese findings inspire further exploration of the applications of perfect\nalignment and the use of Gaussian models for cross-modal learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "eess.SP"
    ],
    "authors": [
      "Abhi Kamboj",
      "Minh N. Do"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15351v1",
    "title": "SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling with Large Language Models",
    "abstract": "In this paper, we propose Selection and Pooling with Large Language Models\n(SPILL), an intuitive and domain-adaptive method for intent clustering without\nfine-tuning. Existing embeddings-based clustering methods rely on a few labeled\nexamples or unsupervised fine-tuning to optimize results for each new dataset,\nwhich makes them less generalizable to multiple datasets. Our goal is to make\nthese existing embedders more generalizable to new domain datasets without\nfurther fine-tuning. Inspired by our theoretical derivation and simulation\nresults on the effectiveness of sampling and pooling techniques, we view the\nclustering task as a small-scale selection problem. A good solution to this\nproblem is associated with better clustering performance. Accordingly, we\npropose a two-stage approach: First, for each utterance (referred to as the\nseed), we derive its embedding using an existing embedder. Then, we apply a\ndistance metric to select a pool of candidates close to the seed. Because the\nembedder is not optimized for new datasets, in the second stage, we use an LLM\nto further select utterances from these candidates that share the same intent\nas the seed. Finally, we pool these selected candidates with the seed to derive\na refined embedding for the seed. We found that our method generally\noutperforms directly using an embedder, and it achieves comparable results to\nother state-of-the-art studies, even those that use much larger models and\nrequire fine-tuning, showing its strength and efficiency. Our results indicate\nthat our method enables existing embedders to be further improved without\nadditional fine-tuning, making them more adaptable to new domain datasets.\nAdditionally, viewing the clustering task as a small-scale selection problem\ngives the potential of using LLMs to customize clustering tasks according to\nthe user's goals.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "I-Fan Lin",
      "Faegheh Hasibi",
      "Suzan Verberne"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15342v1",
    "title": "TruthLens:A Training-Free Paradigm for DeepFake Detection",
    "abstract": "The proliferation of synthetic images generated by advanced AI models poses\nsignificant challenges in identifying and understanding manipulated visual\ncontent. Current fake image detection methods predominantly rely on binary\nclassification models that focus on accuracy while often neglecting\ninterpretability, leaving users without clear insights into why an image is\ndeemed real or fake. To bridge this gap, we introduce TruthLens, a novel\ntraining-free framework that reimagines deepfake detection as a visual\nquestion-answering (VQA) task. TruthLens utilizes state-of-the-art large\nvision-language models (LVLMs) to observe and describe visual artifacts and\ncombines this with the reasoning capabilities of large language models (LLMs)\nlike GPT-4 to analyze and aggregate evidence into informed decisions. By\nadopting a multimodal approach, TruthLens seamlessly integrates visual and\nsemantic reasoning to not only classify images as real or fake but also provide\ninterpretable explanations for its decisions. This transparency enhances trust\nand provides valuable insights into the artifacts that signal synthetic\ncontent. Extensive evaluations demonstrate that TruthLens outperforms\nconventional methods, achieving high accuracy on challenging datasets while\nmaintaining a strong emphasis on explainability. By reframing deepfake\ndetection as a reasoning-driven process, TruthLens establishes a new paradigm\nin combating synthetic media, combining cutting-edge performance with\ninterpretability to address the growing threats of visual disinformation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Ritabrata Chakraborty",
      "Rajatsubhra Chakraborty",
      "Ali Khaleghi Rahimian",
      "Thomas MacDougall"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15582v1",
    "title": "Hierarchical clustering with maximum density paths and mixture models",
    "abstract": "Hierarchical clustering is an effective and interpretable technique for\nanalyzing structure in data, offering a nuanced understanding by revealing\ninsights at multiple scales and resolutions. It is particularly helpful in\nsettings where the exact number of clusters is unknown, and provides a robust\nframework for exploring complex datasets. Additionally, hierarchical clustering\ncan uncover inner structures within clusters, capturing subtle relationships\nand nested patterns that may be obscured by traditional flat clustering\nmethods. However, existing hierarchical clustering methods struggle with\nhigh-dimensional data, especially when there are no clear density gaps between\nmodes. Our method addresses this limitation by leveraging a two-stage approach,\nfirst employing a Gaussian or Student's t mixture model to overcluster the\ndata, and then hierarchically merging clusters based on the induced density\nlandscape. This approach yields state-of-the-art clustering performance while\nalso providing a meaningful hierarchy, making it a valuable tool for\nexploratory data analysis. Code is available at\nhttps://github.com/ecker-lab/tneb clustering.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Martin Ritzert",
      "Polina Turishcheva",
      "Laura Hansel",
      "Paul Wollenhaupt",
      "Marissa Weis",
      "Alexander Ecker"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15338v1",
    "title": "Solla: Towards a Speech-Oriented LLM That Hears Acoustic Context",
    "abstract": "Large Language Models (LLMs) have recently shown remarkable ability to\nprocess not only text but also multimodal inputs such as speech and audio.\nHowever, most existing models primarily focus on analyzing input signals using\ntext instructions, overlooking scenarios in which speech instructions and audio\nare mixed and serve as inputs to the model. To address these challenges, we\nintroduce Solla, a novel framework designed to understand speech-based\nquestions and hear the acoustic context concurrently. Solla incorporates an\naudio tagging module to effectively identify and represent audio events, as\nwell as an ASR-assisted prediction method to improve comprehension of spoken\ncontent. To rigorously evaluate Solla and other publicly available models, we\npropose a new benchmark dataset called SA-Eval, which includes three tasks:\naudio event classification, audio captioning, and audio question answering.\nSA-Eval has diverse speech instruction with various speaking styles,\nencompassing two difficulty levels, easy and hard, to capture the range of\nreal-world acoustic conditions. Experimental results show that Solla performs\non par with or outperforms baseline models on both the easy and hard test sets,\nunderscoring its effectiveness in jointly understanding speech and audio.",
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.SD"
    ],
    "authors": [
      "Junyi Ao",
      "Dekun Chen",
      "Xiaohai Tian",
      "Wenjie Feng",
      "Jun Zhang",
      "Lu Lu",
      "Yuxuan Wang",
      "Haizhou Li",
      "Zhizheng Wu"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15337v1",
    "title": "Recover and Match: Open-Vocabulary Multi-Label Recognition through Knowledge-Constrained Optimal Transport",
    "abstract": "Identifying multiple novel classes in an image, known as open-vocabulary\nmulti-label recognition, is a challenging task in computer vision. Recent\nstudies explore the transfer of powerful vision-language models such as CLIP.\nHowever, these approaches face two critical challenges: (1) The local semantics\nof CLIP are disrupted due to its global pre-training objectives, resulting in\nunreliable regional predictions. (2) The matching property between image\nregions and candidate labels has been neglected, relying instead on naive\nfeature aggregation such as average pooling, which leads to spurious\npredictions from irrelevant regions. In this paper, we present RAM (Recover And\nMatch), a novel framework that effectively addresses the above issues. To\ntackle the first problem, we propose Ladder Local Adapter (LLA) to enforce\nrefocusing on local regions, recovering local semantics in a memory-friendly\nway. For the second issue, we propose Knowledge-Constrained Optimal Transport\n(KCOT) to suppress meaningless matching to non-GT labels by formulating the\ntask as an optimal transport problem. As a result, RAM achieves\nstate-of-the-art performance on various datasets from three distinct domains,\nand shows great potential to boost the existing methods. Code:\nhttps://github.com/EricTan7/RAM.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Hao Tan",
      "Zichang Tan",
      "Jun Li",
      "Ajian Liu",
      "Jun Wan",
      "Zhen Lei"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.16171v1",
    "title": "Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation",
    "abstract": "Modern text-to-image generative models can inadvertently reproduce\ncopyrighted content memorized in their training data, raising serious concerns\nabout potential copyright infringement. We introduce Guardians of Generation, a\nmodel agnostic inference time framework for dynamic copyright shielding in AI\nimage generation. Our approach requires no retraining or modification of the\ngenerative model weights, instead integrating seamlessly with existing\ndiffusion pipelines. It augments the generation process with an adaptive\nguidance mechanism comprising three components: a detection module, a prompt\nrewriting module, and a guidance adjustment module. The detection module\nmonitors user prompts and intermediate generation steps to identify features\nindicative of copyrighted content before they manifest in the final output. If\nsuch content is detected, the prompt rewriting mechanism dynamically transforms\nthe user's prompt by sanitizing or replacing references that could trigger\ncopyrighted material while preserving the prompt's intended semantics. The\nadaptive guidance module adaptively steers the diffusion process away from\nflagged content by modulating the model's sampling trajectory. Together, these\ncomponents form a robust shield that enables a tunable balance between\npreserving creative fidelity and ensuring copyright compliance. We validate our\nmethod on a variety of generative models such as Stable Diffusion, SDXL, and\nFlux, demonstrating substantial reductions in copyrighted content generation\nwith negligible impact on output fidelity or alignment with user intent. This\nwork provides a practical, plug-and-play safeguard for generative image models,\nenabling more responsible deployment under real-world copyright constraints.\nSource code is available at: https://respailab.github.io/gog",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Soham Roy",
      "Abhishek Mishra",
      "Shirish Karande",
      "Murari Mandal"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15321v1",
    "title": "Euclid Quick Data Release (Q1). Active galactic nuclei identification using diffusion-based inpainting of Euclid VIS images",
    "abstract": "Light emission from galaxies exhibit diverse brightness profiles, influenced\nby factors such as galaxy type, structural features and interactions with other\ngalaxies. Elliptical galaxies feature more uniform light distributions, while\nspiral and irregular galaxies have complex, varied light profiles due to their\nstructural heterogeneity and star-forming activity. In addition, galaxies with\nan active galactic nucleus (AGN) feature intense, concentrated emission from\ngas accretion around supermassive black holes, superimposed on regular galactic\nlight, while quasi-stellar objects (QSO) are the extreme case of the AGN\nemission dominating the galaxy. The challenge of identifying AGN and QSO has\nbeen discussed many times in the literature, often requiring multi-wavelength\nobservations. This paper introduces a novel approach to identify AGN and QSO\nfrom a single image. Diffusion models have been recently developed in the\nmachine-learning literature to generate realistic-looking images of everyday\nobjects. Utilising the spatial resolving power of the Euclid VIS images, we\ncreated a diffusion model trained on one million sources, without using any\nsource pre-selection or labels. The model learns to reconstruct light\ndistributions of normal galaxies, since the population is dominated by them. We\ncondition the prediction of the central light distribution by masking the\ncentral few pixels of each source and reconstruct the light according to the\ndiffusion model. We further use this prediction to identify sources that\ndeviate from this profile by examining the reconstruction error of the few\ncentral pixels regenerated in each source's core. Our approach, solely using\nVIS imaging, features high completeness compared to traditional methods of AGN\nand QSO selection, including optical, near-infrared, mid-infrared, and X-rays.\n[abridged]",
    "categories": [
      "astro-ph.GA",
      "cs.CV"
    ],
    "authors": [
      "Euclid Collaboration",
      "G. Stevens",
      "S. Fotopoulou",
      "M. N. Bremer",
      "T. Matamoro Zatarain",
      "K. Jahnke",
      "B. Margalef-Bentabol",
      "M. Huertas-Company",
      "M. J. Smith",
      "M. Walmsley",
      "M. Salvato",
      "M. Mezcua",
      "A. Paulino-Afonso",
      "M. Siudek",
      "M. Talia",
      "F. Ricci",
      "W. Roster",
      "N. Aghanim",
      "B. Altieri",
      "S. Andreon",
      "H. Aussel",
      "C. Baccigalupi",
      "M. Baldi",
      "S. Bardelli",
      "P. Battaglia",
      "A. Biviano",
      "A. Bonchi",
      "E. Branchini",
      "M. Brescia",
      "J. Brinchmann",
      "S. Camera",
      "G. Cañas-Herrera",
      "V. Capobianco",
      "C. Carbone",
      "J. Carretero",
      "M. Castellano",
      "G. Castignani",
      "S. Cavuoti",
      "K. C. Chambers",
      "A. Cimatti",
      "C. Colodro-Conde",
      "G. Congedo",
      "C. J. Conselice",
      "L. Conversi",
      "Y. Copin",
      "A. Costille",
      "F. Courbin",
      "H. M. Courtois",
      "M. Cropper",
      "A. Da Silva",
      "H. Degaudenzi",
      "G. De Lucia",
      "C. Dolding",
      "H. Dole",
      "M. Douspis",
      "F. Dubath",
      "X. Dupac",
      "S. Dusini",
      "S. Escoffier",
      "M. Farina",
      "S. Ferriol",
      "K. George",
      "C. Giocoli",
      "B. R. Granett",
      "A. Grazian",
      "F. Grupp",
      "S. V. H. Haugan",
      "I. M. Hook",
      "F. Hormuth",
      "A. Hornstrup",
      "P. Hudelot",
      "M. Jhabvala",
      "E. Keihänen",
      "S. Kermiche",
      "A. Kiessling",
      "M. Kilbinger",
      "B. Kubik",
      "M. Kümmel",
      "H. Kurki-Suonio",
      "Q. Le Boulc'h",
      "A. M. C. Le Brun",
      "D. Le Mignant",
      "P. B. Lilje",
      "V. Lindholm",
      "I. Lloro",
      "G. Mainetti",
      "D. Maino",
      "E. Maiorano",
      "O. Marggraf",
      "M. Martinelli",
      "N. Martinet",
      "F. Marulli",
      "R. Massey",
      "S. Maurogordato",
      "H. J. McCracken",
      "E. Medinaceli",
      "S. Mei",
      "M. Melchior",
      "M. Meneghetti",
      "E. Merlin",
      "G. Meylan",
      "A. Mora",
      "M. Moresco",
      "L. Moscardini",
      "R. Nakajima",
      "C. Neissner",
      "S. -M. Niemi",
      "C. Padilla",
      "S. Paltani",
      "F. Pasian",
      "K. Pedersen",
      "W. J. Percival",
      "V. Pettorino",
      "G. Polenta",
      "M. Poncet",
      "L. A. Popa",
      "L. Pozzetti",
      "F. Raison",
      "R. Rebolo",
      "A. Renzi",
      "J. Rhodes",
      "G. Riccio",
      "E. Romelli",
      "M. Roncarelli",
      "R. Saglia",
      "A. G. Sánchez",
      "D. Sapone",
      "J. A. Schewtschenko",
      "M. Schirmer",
      "P. Schneider",
      "T. Schrabback",
      "A. Secroun",
      "S. Serrano",
      "P. Simon",
      "C. Sirignano",
      "G. Sirri",
      "J. Skottfelt",
      "L. Stanco",
      "J. Steinwagner",
      "P. Tallada-Crespí",
      "A. N. Taylor",
      "I. Tereno",
      "S. Toft",
      "R. Toledo-Moreo",
      "F. Torradeflot",
      "I. Tutusaus",
      "L. Valenziano",
      "J. Valiviita",
      "T. Vassallo",
      "G. Verdoes Kleijn",
      "A. Veropalumbo",
      "Y. Wang",
      "J. Weller",
      "A. Zacchei",
      "G. Zamorani",
      "F. M. Zerbi",
      "I. A. Zinchenko",
      "E. Zucca",
      "V. Allevato",
      "M. Ballardini",
      "M. Bolzonella",
      "E. Bozzo",
      "C. Burigana",
      "R. Cabanac",
      "A. Cappi",
      "J. A. Escartin Vigo",
      "L. Gabarra",
      "W. G. Hartley",
      "J. Martín-Fleitas",
      "S. Matthew",
      "R. B. Metcalf",
      "A. Pezzotta",
      "M. Pöntinen",
      "I. Risso",
      "V. Scottez",
      "M. Sereno",
      "M. Tenti",
      "M. Wiesmann",
      "Y. Akrami",
      "S. Alvi",
      "I. T. Andika",
      "S. Anselmi",
      "M. Archidiacono",
      "F. Atrio-Barandela",
      "D. Bertacca",
      "M. Bethermin",
      "L. Bisigello",
      "A. Blanchard",
      "L. Blot",
      "S. Borgani",
      "M. L. Brown",
      "S. Bruton",
      "A. Calabro",
      "F. Caro",
      "T. Castro",
      "F. Cogato",
      "S. Davini",
      "G. Desprez",
      "A. Díaz-Sánchez",
      "J. J. Diaz",
      "S. Di Domizio",
      "J. M. Diego",
      "P. -A. Duc",
      "A. Enia",
      "Y. Fang",
      "A. G. Ferrari",
      "A. Finoguenov",
      "A. Fontana",
      "A. Franco",
      "J. García-Bellido",
      "T. Gasparetto",
      "V. Gautard",
      "E. Gaztanaga",
      "F. Giacomini",
      "F. Gianotti",
      "M. Guidi",
      "C. M. Gutierrez",
      "A. Hall",
      "S. Hemmati",
      "H. Hildebrandt",
      "J. Hjorth",
      "J. J. E. Kajava",
      "Y. Kang",
      "V. Kansal",
      "D. Karagiannis",
      "C. C. Kirkpatrick",
      "S. Kruk",
      "L. Legrand",
      "M. Lembo",
      "F. Lepori",
      "G. Leroy",
      "J. Lesgourgues",
      "L. Leuzzi",
      "T. I. Liaudat",
      "J. Macias-Perez",
      "M. Magliocchetti",
      "F. Mannucci",
      "R. Maoli",
      "C. J. A. P. Martins",
      "L. Maurin",
      "M. Miluzio",
      "P. Monaco",
      "G. Morgante",
      "K. Naidoo",
      "A. Navarro-Alsina",
      "F. Passalacqua",
      "K. Paterson",
      "L. Patrizii",
      "A. Pisani",
      "D. Potter",
      "S. Quai",
      "M. Radovich",
      "P. -F. Rocci",
      "G. Rodighiero",
      "S. Sacquegna",
      "M. Sahlén",
      "D. B. Sanders",
      "E. Sarpa",
      "A. Schneider",
      "M. Schultheis",
      "D. Sciotti",
      "E. Sellentin",
      "F. Shankar",
      "L. C. Smith",
      "K. Tanidis",
      "G. Testera",
      "R. Teyssier",
      "S. Tosi",
      "A. Troja",
      "M. Tucci",
      "C. Valieri",
      "D. Vergani",
      "G. Verza",
      "N. A. Walton"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15300v2",
    "title": "SUM Parts: Benchmarking Part-Level Semantic Segmentation of Urban Meshes",
    "abstract": "Semantic segmentation in urban scene analysis has mainly focused on images or\npoint clouds, while textured meshes - offering richer spatial representation -\nremain underexplored. This paper introduces SUM Parts, the first large-scale\ndataset for urban textured meshes with part-level semantic labels, covering\nabout 2.5 km2 with 21 classes. The dataset was created using our own annotation\ntool, which supports both face- and texture-based annotations with efficient\ninteractive selection. We also provide a comprehensive evaluation of 3D\nsemantic segmentation and interactive annotation methods on this dataset. Our\nproject page is available at https://tudelft3d.github.io/SUMParts/.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Weixiao Gao",
      "Liangliang Nan",
      "Hugo Ledoux"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.15299v1",
    "title": "Inside-Out: Hidden Factual Knowledge in LLMs",
    "abstract": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a\nmodel can internally know an answer perfectly, yet fail to generate it even\nonce, despite large-scale repeated sampling of 1,000 answers. This reveals\nfundamental limitations in the generation capabilities of LLMs, which (3) puts\na practical constraint on scaling test-time compute via repeated answer\nsampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Zorik Gekhman",
      "Eyal Ben David",
      "Hadas Orgad",
      "Eran Ofek",
      "Yonatan Belinkov",
      "Idan Szpector",
      "Jonathan Herzig",
      "Roi Reichart"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15295v1",
    "title": "DCA: Dividing and Conquering Amnesia in Incremental Object Detection",
    "abstract": "Incremental object detection (IOD) aims to cultivate an object detector that\ncan continuously localize and recognize novel classes while preserving its\nperformance on previous classes. Existing methods achieve certain success by\nimproving knowledge distillation and exemplar replay for transformer-based\ndetection frameworks, but the intrinsic forgetting mechanisms remain\nunderexplored. In this paper, we dive into the cause of forgetting and discover\nforgetting imbalance between localization and recognition in transformer-based\nIOD, which means that localization is less-forgetting and can generalize to\nfuture classes, whereas catastrophic forgetting occurs primarily on\nrecognition. Based on these insights, we propose a Divide-and-Conquer Amnesia\n(DCA) strategy, which redesigns the transformer-based IOD into a\nlocalization-then-recognition process. DCA can well maintain and transfer the\nlocalization ability, leaving decoupled fragile recognition to be specially\nconquered. To reduce feature drift in recognition, we leverage semantic\nknowledge encoded in pre-trained language models to anchor class\nrepresentations within a unified feature space across incremental tasks. This\ninvolves designing a duplex classifier fusion and embedding class semantic\nfeatures into the recognition decoding process in the form of queries.\nExtensive experiments validate that our approach achieves state-of-the-art\nperformance, especially for long-term incremental scenarios. For example, under\nthe four-step setting on MS-COCO, our DCA strategy significantly improves the\nfinal AP by 6.9%.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Aoting Zhang",
      "Dongbao Yang",
      "Chang Liu",
      "Xiaopeng Hong",
      "Miao Shang",
      "Yu Zhou"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15294v1",
    "title": "Borsuk-Ulam and Replicable Learning of Large-Margin Halfspaces",
    "abstract": "Recent advances in learning theory have established that, for total concepts,\nlist replicability, global stability, differentially private (DP) learnability,\nand shared-randomness replicability coincide precisely with the finiteness of\nthe Littlestone dimension. Does the same hold for partial concept classes?\n  We answer this question by studying the large-margin half-spaces class, which\nhas bounded Littlestone dimension and is purely DP-learnable and\nshared-randomness replicable even in high dimensions.\n  We prove that the list replicability number of $\\gamma$-margin half-spaces\nsatisfies \\[ \\frac{d}{2} + 1 \\le \\mathrm{LR}(H_{\\gamma}^d) \\le d, \\] which\nincreases with the dimension $d$. This reveals a surprising separation for\npartial concepts: list replicability and global stability do not follow from\nbounded Littlestone dimension, DP-learnability, or shared-randomness\nreplicability.\n  By applying our main theorem, we also answer the following open problems.\n  - We prove that any disambiguation of an infinite-dimensional large-margin\nhalf-space to a total concept class has unbounded Littlestone dimension,\nanswering an open question of Alon et al. (FOCS '21). - We prove that the\nmaximum list-replicability number of any *finite* set of points and homogeneous\nhalf-spaces in $d$-dimensional Euclidean space is $d$, resolving a problem of\nChase et al. (FOCS '23). - We prove that any disambiguation of the Gap Hamming\nDistance problem in the large gap regime has unbounded public-coin randomized\ncommunication complexity. This answers an open problem of Fang et al. (STOC\n'25).\n  We prove the lower bound via a topological argument involving the local\nBorsuk-Ulam theorem of Chase et al. (STOC '24). For the upper bound, we design\na learning rule that relies on certain triangulations of the cross-polytope and\nrecent results on the generalization properties of SVM.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Ari Blondal",
      "Hamed Hatami",
      "Pooya Hatami",
      "Chavdar Lalov",
      "Sivan Tretiak"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15293v1",
    "title": "Test-Time Backdoor Detection for Object Detection Models",
    "abstract": "Object detection models are vulnerable to backdoor attacks, where attackers\npoison a small subset of training samples by embedding a predefined trigger to\nmanipulate prediction. Detecting poisoned samples (i.e., those containing\ntriggers) at test time can prevent backdoor activation. However, unlike image\nclassification tasks, the unique characteristics of object detection --\nparticularly its output of numerous objects -- pose fresh challenges for\nbackdoor detection. The complex attack effects (e.g., \"ghost\" object emergence\nor \"vanishing\" object) further render current defenses fundamentally\ninadequate. To this end, we design TRAnsformation Consistency Evaluation\n(TRACE), a brand-new method for detecting poisoned samples at test time in\nobject detection. Our journey begins with two intriguing observations: (1)\npoisoned samples exhibit significantly more consistent detection results than\nclean ones across varied backgrounds. (2) clean samples show higher detection\nconsistency when introduced to different focal information. Based on these\nphenomena, TRACE applies foreground and background transformations to each test\nsample, then assesses transformation consistency by calculating the variance in\nobjects confidences. TRACE achieves black-box, universal backdoor detection,\nwith extensive experiments showing a 30% improvement in AUROC over\nstate-of-the-art defenses and resistance to adaptive attacks.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Hangtao Zhang",
      "Yichen Wang",
      "Shihui Yan",
      "Chenyu Zhu",
      "Ziqi Zhou",
      "Linshan Hou",
      "Shengshan Hu",
      "Minghui Li",
      "Yanjun Zhang",
      "Leo Yu Zhang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15289v1",
    "title": "TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence Tracing and Relationship Classification",
    "abstract": "LLMs have achieved remarkable fluency and coherence in text generation, yet\ntheir widespread adoption has raised concerns about content reliability and\naccountability. In high-stakes domains such as healthcare, law, and news, it is\ncrucial to understand where and how the content is created. To address this, we\nintroduce the Text pROVEnance (TROVE) challenge, designed to trace each\nsentence of a target text back to specific source sentences within potentially\nlengthy or multi-document inputs. Beyond identifying sources, TROVE annotates\nthe fine-grained relationships (quotation, compression, inference, and others),\nproviding a deep understanding of how each target sentence is formed. To\nbenchmark TROVE, we construct our dataset by leveraging three public datasets\ncovering 11 diverse scenarios (e.g., QA and summarization) in English and\nChinese, spanning source texts of varying lengths (0-5k, 5-10k, 10k+),\nemphasizing the multi-document and long-document settings essential for\nprovenance. To ensure high-quality data, we employ a three-stage annotation\nprocess: sentence retrieval, GPT provenance, and human provenance. We evaluate\n11 LLMs under direct prompting and retrieval-augmented paradigms, revealing\nthat retrieval is essential for robust performance, larger models perform\nbetter in complex relationship classification, and closed-source models often\nlead, yet open-source models show significant promise, particularly with\nretrieval augmentation.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Junnan Zhu",
      "Min Xiao",
      "Yining Wang",
      "Feifei Zhai",
      "Yu Zhou",
      "Chengqing Zong"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15288v1",
    "title": "Beacon2Science: Enhancing STEREO/HI beacon data1 with machine learning for efficient CME tracking",
    "abstract": "Observing and forecasting coronal mass ejections (CME) in real-time is\ncrucial due to the strong geomagnetic storms they can generate that can have a\npotentially damaging effect, for example, on satellites and electrical devices.\nWith its near-real-time availability, STEREO/HI beacon data is the perfect\ncandidate for early forecasting of CMEs. However, previous work concluded that\nCME arrival prediction based on beacon data could not achieve the same accuracy\nas with high-resolution science data due to data gaps and lower quality. We\npresent our novel pipeline entitled ''Beacon2Science'', bridging the gap\nbetween beacon and science data to improve CME tracking. Through this pipeline,\nwe first enhance the quality (signal-to-noise ratio and spatial resolution) of\nbeacon data. We then increase the time resolution of enhanced beacon images\nthrough learned interpolation to match science data's 40-minute resolution. We\nmaximize information coherence between consecutive frames with adapted model\narchitecture and loss functions through the different steps. The improved\nbeacon images are comparable to science data, showing better CME visibility\nthan the original beacon data. Furthermore, we compare CMEs tracked in beacon,\nenhanced beacon, and science images. The tracks extracted from enhanced beacon\ndata are closer to those from science images, with a mean average error of\n$\\sim 0.5 ^\\circ$ of elongation compared to $1^\\circ$ with original beacon\ndata. The work presented in this paper paves the way for its application to\nforthcoming missions such as Vigil and PUNCH.",
    "categories": [
      "physics.space-ph",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Justin Le Louëdec",
      "Maike Bauer",
      "Tanja Amerstorfer",
      "Jackie A. Davies"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.16553v1",
    "title": "A Foundational individual Mobility Prediction Model based on Open-Source Large Language Models",
    "abstract": "Large Language Models (LLMs) are widely applied to domain-specific tasks due\nto their massive general knowledge and remarkable inference capacities. Current\nstudies on LLMs have shown immense potential in applying LLMs to model\nindividual mobility prediction problems. However, most LLM-based mobility\nprediction models only train on specific datasets or use single well-designed\nprompts, leading to difficulty in adapting to different cities and users with\ndiverse contexts. To fill these gaps, this paper proposes a unified fine-tuning\nframework to train a foundational open source LLM-based mobility prediction\nmodel. We conducted extensive experiments on six real-world mobility datasets\nto validate the proposed model. The results showed that the proposed model\nachieved the best performance in prediction accuracy and transferability over\nstate-of-the-art models based on deep learning and LLMs.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Zhenlin Qin",
      "Leizhen Wang",
      "Francisco Camara Pereira",
      "Zhenlinag Ma"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15285v1",
    "title": "PAPI-Reg: Patch-to-Pixel Solution for Efficient Cross-Modal Registration between LiDAR Point Cloud and Camera Image",
    "abstract": "The primary requirement for cross-modal data fusion is the precise alignment\nof data from different sensors. However, the calibration between LiDAR point\nclouds and camera images is typically time-consuming and needs external\ncalibration board or specific environmental features. Cross-modal registration\neffectively solves this problem by aligning the data directly without requiring\nexternal calibration. However, due to the domain gap between the point cloud\nand the image, existing methods rarely achieve satisfactory registration\naccuracy while maintaining real-time performance. To address this issue, we\npropose a framework that projects point clouds into several 2D representations\nfor matching with camera images, which not only leverages the geometric\ncharacteristic of LiDAR point clouds more effectively but also bridge the\ndomain gap between the point cloud and image. Moreover, to tackle the\nchallenges of cross modal differences and the limited overlap between LiDAR\npoint clouds and images in the image matching task, we introduce a multi-scale\nfeature extraction network to effectively extract features from both camera\nimages and the projection maps of LiDAR point cloud. Additionally, we propose a\npatch-to-pixel matching network to provide more effective supervision and\nachieve higher accuracy. We validate the performance of our model through\nexperiments on the KITTI and nuScenes datasets. Our network achieves real-time\nperformance and extremely high registration accuracy. On the KITTI dataset, our\nmodel achieves a registration accuracy rate of over 99\\%.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yuanchao Yue",
      "Zhengxin Li",
      "Wei Zhang",
      "Hui Yuan"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15284v1",
    "title": "EdgeRegNet: Edge Feature-based Multimodal Registration Network between Images and LiDAR Point Clouds",
    "abstract": "Cross-modal data registration has long been a critical task in computer\nvision, with extensive applications in autonomous driving and robotics.\nAccurate and robust registration methods are essential for aligning data from\ndifferent modalities, forming the foundation for multimodal sensor data fusion\nand enhancing perception systems' accuracy and reliability. The registration\ntask between 2D images captured by cameras and 3D point clouds captured by\nLight Detection and Ranging (LiDAR) sensors is usually treated as a visual pose\nestimation problem. High-dimensional feature similarities from different\nmodalities are leveraged to identify pixel-point correspondences, followed by\npose estimation techniques using least squares methods. However, existing\napproaches often resort to downsampling the original point cloud and image data\ndue to computational constraints, inevitably leading to a loss in precision.\nAdditionally, high-dimensional features extracted using different feature\nextractors from various modalities require specific techniques to mitigate\ncross-modal differences for effective matching. To address these challenges, we\npropose a method that uses edge information from the original point clouds and\nimages for cross-modal registration. We retain crucial information from the\noriginal data by extracting edge points and pixels, enhancing registration\naccuracy while maintaining computational efficiency. The use of edge points and\nedge pixels allows us to introduce an attention-based feature exchange block to\neliminate cross-modal disparities. Furthermore, we incorporate an optimal\nmatching layer to improve correspondence identification. We validate the\naccuracy of our method on the KITTI and nuScenes datasets, demonstrating its\nstate-of-the-art performance.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yuanchao Yue",
      "Hui Yuan",
      "Qinglong Miao",
      "Xiaolong Mao",
      "Raouf Hamzaoui",
      "Peter Eisert"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15283v1",
    "title": "TF-TI2I: Training-Free Text-and-Image-to-Image Generation via Multi-Modal Implicit-Context Learning in Text-to-Image Models",
    "abstract": "Text-and-Image-To-Image (TI2I), an extension of Text-To-Image (T2I),\nintegrates image inputs with textual instructions to enhance image generation.\nExisting methods often partially utilize image inputs, focusing on specific\nelements like objects or styles, or they experience a decline in generation\nquality with complex, multi-image instructions. To overcome these challenges,\nwe introduce Training-Free Text-and-Image-to-Image (TF-TI2I), which adapts\ncutting-edge T2I models such as SD3 without the need for additional training.\nOur method capitalizes on the MM-DiT architecture, in which we point out that\ntextual tokens can implicitly learn visual information from vision tokens. We\nenhance this interaction by extracting a condensed visual representation from\nreference images, facilitating selective information sharing through Reference\nContextual Masking -- this technique confines the usage of contextual tokens to\ninstruction-relevant visual information. Additionally, our Winner-Takes-All\nmodule mitigates distribution shifts by prioritizing the most pertinent\nreferences for each vision token. Addressing the gap in TI2I evaluation, we\nalso introduce the FG-TI2I Bench, a comprehensive benchmark tailored for TI2I\nand compatible with existing T2I methods. Our approach shows robust performance\nacross various benchmarks, confirming its effectiveness in handling complex\nimage-generation tasks.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Teng-Fang Hsiao",
      "Bo-Kai Ruan",
      "Yi-Lun Wu",
      "Tzu-Ling Lin",
      "Hong-Han Shuai"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15581v1",
    "title": "Performance-bounded Online Ensemble Learning Method Based on Multi-armed bandits and Its Applications in Real-time Safety Assessment",
    "abstract": "Ensemble learning plays a crucial role in practical applications of online\nlearning due to its enhanced classification performance and adaptable\nadjustment mechanisms. However, most weight allocation strategies in ensemble\nlearning are heuristic, making it challenging to theoretically guarantee that\nthe ensemble classifier outperforms its base classifiers. To address this\nissue, a performance-bounded online ensemble learning method based on\nmulti-armed bandits, named PB-OEL, is proposed in this paper. Specifically,\nmulti-armed bandit with expert advice is incorporated into online ensemble\nlearning, aiming to update the weights of base classifiers and make\npredictions. A theoretical framework is established to bound the performance of\nthe ensemble classifier relative to base classifiers. By setting expert advice\nof bandits, the bound exceeds the performance of any base classifier when the\nlength of data stream is sufficiently large. Additionally, performance bounds\nfor scenarios with limited annotations are also derived. Numerous experiments\non benchmark datasets and a dataset of real-time safety assessment tasks are\nconducted. The experimental results validate the theoretical bound to a certain\nextent and demonstrate that the proposed method outperforms existing\nstate-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Songqiao Hu",
      "Zeyi Liu",
      "Xiao He"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15275v1",
    "title": "Challenges and Trends in Egocentric Vision: A Survey",
    "abstract": "With the rapid development of artificial intelligence technologies and\nwearable devices, egocentric vision understanding has emerged as a new and\nchallenging research direction, gradually attracting widespread attention from\nboth academia and industry. Egocentric vision captures visual and multimodal\ndata through cameras or sensors worn on the human body, offering a unique\nperspective that simulates human visual experiences. This paper provides a\ncomprehensive survey of the research on egocentric vision understanding,\nsystematically analyzing the components of egocentric scenes and categorizing\nthe tasks into four main areas: subject understanding, object understanding,\nenvironment understanding, and hybrid understanding. We explore in detail the\nsub-tasks within each category. We also summarize the main challenges and\ntrends currently existing in the field. Furthermore, this paper presents an\noverview of high-quality egocentric vision datasets, offering valuable\nresources for future research. By summarizing the latest advancements, we\nanticipate the broad applications of egocentric vision technologies in fields\nsuch as augmented reality, virtual reality, and embodied intelligence, and\npropose future research directions based on the latest developments in the\nfield.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Xiang Li",
      "Heqian Qiu",
      "Lanxiao Wang",
      "Hanwen Zhang",
      "Chenghao Qi",
      "Linfeng Han",
      "Huiyu Xiong",
      "Hongliang Li"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15580v1",
    "title": "How Well Can AI Build SD Models?",
    "abstract": "Introduction: As system dynamics (SD) embraces automation, AI offers\nefficiency but risks bias from missing data and flawed models. Models that omit\nmultiple perspectives and data threaten model quality, whether created by\nhumans or with the assistance of AI. To reduce uncertainty about how well AI\ncan build SD models, we introduce two metrics for evaluation of AI-generated\ncausal maps: technical correctness (causal translation) and adherence to\ninstructions (conformance).\n  Approach: We developed an open source project called sd-ai to provide a basis\nfor collaboration in the SD community, aiming to fully harness the potential of\nAI based tools like ChatGPT for dynamic modeling. Additionally, we created an\nevaluation theory along with a comprehensive suite of tests designed to\nevaluate any such tools developed within the sd-ai ecosystem.\n  Results: We tested 11 different LLMs on their ability to do causal\ntranslation as well as conform to user instruction. gpt-4.5-preview was the top\nperformer, scoring 92.9% overall, excelling in both tasks. o1 scored 100% in\ncausal translation. gpt-4o identified all causal links but struggled with\npositive polarity in decreasing terms. While gpt-4.5-preview and o1 are most\naccurate, gpt-4o is the cheapest.\n  Discussion: Causal translation and conformance tests applied to the sd-ai\nengine reveal significant variations across lLLMs, underscoring the need for\ncontinued evaluation to ensure responsible development of AI tools for dynamic\nmodeling. To address this, an open collaboration among tool developers,\nmodelers, and stakeholders is launched to standardize measures for evaluating\nthe capacity of AI tools to improve the modeling process.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "William Schoenberg",
      "Davidson Girard",
      "Saras Chung",
      "Ellen O'Neill",
      "Janet Velasquez",
      "Sara Metcalf"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15272v1",
    "title": "MAMM-Refine: A Recipe for Improving Faithfulness in Generation with Multi-Agent Collaboration",
    "abstract": "Multi-agent collaboration among models has shown promise in reasoning tasks\nbut is underexplored in long-form generation tasks like summarization and\nquestion-answering. We extend multi-agent multi-model reasoning to generation,\nspecifically to improving faithfulness through refinement, i.e., revising\nmodel-generated outputs to remove factual inconsistencies. We investigate how\niterative collaboration among multiple instances and types of large language\nmodels (LLMs) enhances subtasks in the refinement process, such as error\ndetection, critiquing unfaithful sentences, and making corrections based on\ncritiques. We design intrinsic evaluations for each subtask, with our findings\nindicating that both multi-agent (multiple instances) and multi-model (diverse\nLLM types) approaches benefit error detection and critiquing. Additionally,\nreframing critiquing and refinement as reranking rather than generation tasks\nimproves multi-agent performance. We consolidate these insights into a final\n\"recipe\" called Multi-Agent Multi-Model Refinement (MAMM-Refine), where\nmulti-agent and multi-model collaboration significantly boosts performance on\nthree summarization datasets as well as on long-form question answering,\ndemonstrating the effectiveness and generalizability of our recipe.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "David Wan",
      "Justin Chih-Yao Chen",
      "Elias Stengel-Eskin",
      "Mohit Bansal"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15268v1",
    "title": "Do Chains-of-Thoughts of Large Language Models Suffer from Hallucinations, Cognitive Biases, or Phobias in Bayesian Reasoning?",
    "abstract": "Learning to reason and carefully explain arguments is central to students'\ncognitive, mathematical, and computational thinking development. This is\nparticularly challenging in problems under uncertainty and in Bayesian\nreasoning. With the new generation of large language models (LLMs) capable of\nreasoning using Chain-of-Thought (CoT), there is an excellent opportunity to\nlearn with them as they explain their reasoning through a dialogue with their\nartificial internal voice. It is an engaging and excellent opportunity to learn\nBayesian reasoning. Furthermore, given that different LLMs sometimes arrive at\nopposite solutions, CoT generates opportunities for deep learning by detailed\ncomparisons of reasonings. However, unlike humans, we found that they do not\nautonomously explain using ecologically valid strategies like natural\nfrequencies, whole objects, and embodied heuristics. This is unfortunate, as\nthese strategies help humans avoid critical mistakes and have proven\npedagogical value in Bayesian reasoning. In order to overcome these biases and\naid understanding and learning, we included prompts that induce LLMs to use\nthese strategies. We found that LLMs with CoT incorporate them but not\nconsistently. They show persistent biases towards symbolic reasoning and\navoidance or phobia of ecologically valid strategies.",
    "categories": [
      "cs.AI",
      "I.2.0"
    ],
    "authors": [
      "Roberto Araya"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15267v1",
    "title": "Learning to quantify graph nodes",
    "abstract": "Network Quantification is the problem of estimating the class proportions in\nunlabeled subsets of graph nodes. When prior probability shift is at play, this\ntask cannot be effectively addressed by first classifying the nodes and then\ncounting the class predictions. In addition, unlike non-relational\nquantification on i.i.d. datapoints, Network Quantification demands enhanced\nflexibility to capture a broad range of connectivity patterns, resilience to\nthe challenge of heterophily, and efficiency to scale to larger networks. To\nmeet these stringent requirements we introduce XNQ, a novel method that\nsynergizes the flexibility and efficiency of the unsupervised node embeddings\ncomputed by randomized recursive Graph Neural Networks, with an\nExpectation-Maximization algorithm that provides a robust quantification-aware\nadjustment to the output probabilities of a calibrated node classifier. We\nvalidate the design choices underpinning our method through comprehensive\nablation experiments. In an extensive evaluation, we find that our approach\nconsistently and significantly improves on the best Network Quantification\nmethods to date, thereby setting the new state of the art for this challenging\ntask. Simultaneously, it provides a training speed-up of up to 10x-100x over\nother graph learning based methods.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Alessio Micheli",
      "Alejandro Moreo",
      "Marco Podda",
      "Fabrizio Sebastiani",
      "William Simoni",
      "Domenico Tortorella"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15265v1",
    "title": "DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement Learning",
    "abstract": "Triangle meshes play a crucial role in 3D applications for efficient\nmanipulation and rendering. While auto-regressive methods generate structured\nmeshes by predicting discrete vertex tokens, they are often constrained by\nlimited face counts and mesh incompleteness. To address these challenges, we\npropose DeepMesh, a framework that optimizes mesh generation through two key\ninnovations: (1) an efficient pre-training strategy incorporating a novel\ntokenization algorithm, along with improvements in data curation and\nprocessing, and (2) the introduction of Reinforcement Learning (RL) into 3D\nmesh generation to achieve human preference alignment via Direct Preference\nOptimization (DPO). We design a scoring standard that combines human evaluation\nwith 3D metrics to collect preference pairs for DPO, ensuring both visual\nappeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh\ngenerates meshes with intricate details and precise topology, outperforming\nstate-of-the-art methods in both precision and quality. Project page:\nhttps://zhaorw02.github.io/DeepMesh/",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Ruowen Zhao",
      "Junliang Ye",
      "Zhengyi Wang",
      "Guangce Liu",
      "Yiwen Chen",
      "Yikai Wang",
      "Jun Zhu"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15264v1",
    "title": "LEGION: Learning to Ground and Explain for Synthetic Image Detection",
    "abstract": "The rapid advancements in generative technology have emerged as a\ndouble-edged sword. While offering powerful tools that enhance convenience,\nthey also pose significant social concerns. As defenders, current synthetic\nimage detection methods often lack artifact-level textual interpretability and\nare overly focused on image manipulation detection, and current datasets\nusually suffer from outdated generators and a lack of fine-grained annotations.\nIn this paper, we introduce SynthScars, a high-quality and diverse dataset\nconsisting of 12,236 fully synthetic images with human-expert annotations. It\nfeatures 4 distinct image content types, 3 categories of artifacts, and\nfine-grained annotations covering pixel-level segmentation, detailed textual\nexplanations, and artifact category labels. Furthermore, we propose LEGION\n(LEarning to Ground and explain for Synthetic Image detectiON), a multimodal\nlarge language model (MLLM)-based image forgery analysis framework that\nintegrates artifact detection, segmentation, and explanation. Building upon\nthis capability, we further explore LEGION as a controller, integrating it into\nimage refinement pipelines to guide the generation of higher-quality and more\nrealistic images. Extensive experiments show that LEGION outperforms existing\nmethods across multiple benchmarks, particularly surpassing the second-best\ntraditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score.\nMoreover, the refined images generated under its guidance exhibit stronger\nalignment with human preferences. The code, model, and dataset will be\nreleased.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Hengrui Kang",
      "Siwei Wen",
      "Zichen Wen",
      "Junyan Ye",
      "Weijia Li",
      "Peilin Feng",
      "Baichuan Zhou",
      "Bin Wang",
      "Dahua Lin",
      "Linfeng Zhang",
      "Conghui He"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15260v1",
    "title": "DEPT: Deep Extreme Point Tracing for Ultrasound Image Segmentation",
    "abstract": "Automatic medical image segmentation plays a crucial role in computer aided\ndiagnosis. However, fully supervised learning approaches often require\nextensive and labor-intensive annotation efforts. To address this challenge,\nweakly supervised learning methods, particularly those using extreme points as\nsupervisory signals, have the potential to offer an effective solution. In this\npaper, we introduce Deep Extreme Point Tracing (DEPT) integrated with\nFeature-Guided Extreme Point Masking (FGEPM) algorithm for ultrasound image\nsegmentation. Notably, our method generates pseudo labels by identifying the\nlowest-cost path that connects all extreme points on the feature map-based cost\nmatrix. Additionally, an iterative training strategy is proposed to refine\npseudo labels progressively, enabling continuous network improvement.\nExperimental results on two public datasets demonstrate the effectiveness of\nour proposed method. The performance of our method approaches that of the fully\nsupervised method and outperforms several existing weakly supervised methods.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Lei Shi",
      "Xi Fang",
      "Naiyu Wang",
      "Junxing Zhang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15259v1",
    "title": "Fast MLE and MAPE-Based Device Activity Detection for Grant-Free Access via PSCA and PSCA-Net",
    "abstract": "Fast and accurate device activity detection is the critical challenge in\ngrant-free access for supporting massive machine-type communications (mMTC) and\nultra-reliable low-latency communications (URLLC) in 5G and beyond. The\nstate-of-the-art methods have unsatisfactory error rates or computation times.\nTo address these outstanding issues, we propose new maximum likelihood\nestimation (MLE) and maximum a posterior estimation (MAPE) based device\nactivity detection methods for known and unknown pathloss that achieve superior\nerror rate and computation time tradeoffs using optimization and deep learning\ntechniques. Specifically, we investigate four non-convex optimization problems\nfor MLE and MAPE in the two pathloss cases, with one MAPE problem being\nformulated for the first time. For each non-convex problem, we develop an\ninnovative parallel iterative algorithm using the parallel successive convex\napproximation (PSCA) method. Each PSCA-based algorithm allows parallel\ncomputations, uses up to the objective function's second-order information,\nconverges to the problem's stationary points, and has a low per-iteration\ncomputational complexity compared to the state-of-the-art algorithms. Then, for\neach PSCA-based iterative algorithm, we present a deep unrolling neural network\nimplementation, called PSCA-Net, to further reduce the computation time. Each\nPSCA-Net elegantly marries the underlying PSCA-based algorithm's parallel\ncomputation mechanism with the parallelizable neural network architecture and\neffectively optimizes its step sizes based on vast data samples to speed up the\nconvergence. Numerical results demonstrate that the proposed methods can\nsignificantly reduce the error rate and computation time compared to the\nstate-of-the-art methods, revealing their significant values for grant-free\naccess.",
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "authors": [
      "Bowen Tan",
      "Ying Cui"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15250v1",
    "title": "ImputeGAP: A Comprehensive Library for Time Series Imputation",
    "abstract": "With the prevalence of sensor failures, imputation--the process of estimating\nmissing values--has emerged as the cornerstone of time series data preparation.\nWhile numerous imputation algorithms have been developed to address these data\ngaps, existing libraries provide limited support. Furthermore, they often lack\nthe ability to simulate realistic patterns of time series missing data and fail\nto account for the impact of imputation on subsequent downstream analysis.\n  This paper introduces ImputeGAP, a comprehensive library for time series\nimputation that supports a diverse range of imputation methods and modular\nmissing data simulation catering to datasets with varying characteristics. The\nlibrary includes extensive customization options, such as automated\nhyperparameter tuning, benchmarking, explainability, downstream evaluation, and\ncompatibility with popular time series frameworks.",
    "categories": [
      "cs.LG",
      "cs.DB"
    ],
    "authors": [
      "Quentin Nater",
      "Mourad Khayati",
      "Jacques Pasquier"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15248v1",
    "title": "Automated Non-Functional Requirements Generation in Software Engineering with Large Language Models: A Comparative Study",
    "abstract": "Neglecting non-functional requirements (NFRs) early in software development\ncan lead to critical challenges. Despite their importance, NFRs are often\noverlooked or difficult to identify, impacting software quality. To support\nrequirements engineers in eliciting NFRs, we developed a framework that\nleverages Large Language Models (LLMs) to derive quality-driven NFRs from\nfunctional requirements (FRs). Using a custom prompting technique within a\nDeno-based pipeline, the system identifies relevant quality attributes for each\nfunctional requirement and generates corresponding NFRs, aiding systematic\nintegration. A crucial aspect is evaluating the quality and suitability of\nthese generated requirements. Can LLMs produce high-quality NFR suggestions?\nUsing 34 functional requirements - selected as a representative subset of 3,964\nFRs-the LLMs inferred applicable attributes based on the ISO/IEC 25010:2023\nstandard, generating 1,593 NFRs. A horizontal evaluation covered three\ndimensions: NFR validity, applicability of quality attributes, and\nclassification precision. Ten industry software quality evaluators, averaging\n13 years of experience, assessed a subset for relevance and quality. The\nevaluation showed strong alignment between LLM-generated NFRs and expert\nassessments, with median validity and applicability scores of 5.0 (means: 4.63\nand 4.59, respectively) on a 1-5 scale. In the classification task, 80.4% of\nLLM-assigned attributes matched expert choices, with 8.3% near misses and 11.3%\nmismatches. A comparative analysis of eight LLMs highlighted variations in\nperformance, with gemini-1.5-pro exhibiting the highest attribute accuracy,\nwhile llama-3.3-70B achieved higher validity and applicability scores. These\nfindings provide insights into the feasibility of using LLMs for automated NFR\ngeneration and lay the foundation for further exploration of AI-assisted\nrequirements engineering.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Jomar Thomas Almonte",
      "Santhosh Anitha Boominathan",
      "Nathalia Nascimento"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15242v2",
    "title": "BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space Complexity?",
    "abstract": "We introduce BigO(Bench), a novel coding benchmark designed to evaluate the\ncapabilities of generative language models in understanding and generating code\nwith specified time and space complexities. This benchmark addresses the gap in\ncurrent evaluations that often overlook the ability of models to comprehend and\nproduce code constrained by computational complexity. BigO(Bench) includes\ntooling to infer the algorithmic complexity of any Python function from\nprofiling measurements, including human- or LLM-generated solutions.\nBigO(Bench) also includes of set of 3,105 coding problems and 1,190,250\nsolutions from Code Contests annotated with inferred (synthetic) time and space\ncomplexity labels from the complexity framework, as well as corresponding\nruntime and memory footprint values for a large set of input sizes. We present\nresults from evaluating multiple state-of-the-art language models on this\nbenchmark, highlighting their strengths and weaknesses in handling complexity\nrequirements. In particular, token-space reasoning models are unrivaled in code\ngeneration but not in complexity understanding, hinting that they may not\ngeneralize well to tasks for which no reward was given at training time.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CC"
    ],
    "authors": [
      "Pierre Chambon",
      "Baptiste Roziere",
      "Benoit Sagot",
      "Gabriel Synnaeve"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15234v1",
    "title": "CoE: Chain-of-Explanation via Automatic Visual Concept Circuit Description and Polysemanticity Quantification",
    "abstract": "Explainability is a critical factor influencing the wide deployment of deep\nvision models (DVMs). Concept-based post-hoc explanation methods can provide\nboth global and local insights into model decisions. However, current methods\nin this field face challenges in that they are inflexible to automatically\nconstruct accurate and sufficient linguistic explanations for global concepts\nand local circuits. Particularly, the intrinsic polysemanticity in semantic\nVisual Concepts (VCs) impedes the interpretability of concepts and DVMs, which\nis underestimated severely. In this paper, we propose a Chain-of-Explanation\n(CoE) approach to address these issues. Specifically, CoE automates the\ndecoding and description of VCs to construct global concept explanation\ndatasets. Further, to alleviate the effect of polysemanticity on model\nexplainability, we design a concept polysemanticity disentanglement and\nfiltering mechanism to distinguish the most contextually relevant concept\natoms. Besides, a Concept Polysemanticity Entropy (CPE), as a measure of model\ninterpretability, is formulated to quantify the degree of concept uncertainty.\nThe modeling of deterministic concepts is upgraded to uncertain concept atom\ndistributions. Finally, CoE automatically enables linguistic local explanations\nof the decision-making process of DVMs by tracing the concept circuit. GPT-4o\nand human-based experiments demonstrate the effectiveness of CPE and the\nsuperiority of CoE, achieving an average absolute improvement of 36% in terms\nof explainability scores.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Wenlong Yu",
      "Qilong Wang",
      "Chuang Liu",
      "Dong Li",
      "Qinghua Hu"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15235v1",
    "title": "Exploring Large Language Models for Word Games:Who is the Spy?",
    "abstract": "Word games hold significant research value for natural language processing\n(NLP), game theory, and related fields due to their rule-based and situational\nnature. This study explores how large language models (LLMs) can be effectively\ninvolved in word games and proposes a training-free framework. \"Shei Shi Wo Di\"\nor \"Who is the Spy\" in English, is a classic word game. Using this game as an\nexample, we introduce a Chain-of-Thought (CoT)-based scheduling framework to\nenable LLMs to achieve excellent performance in tasks such as inferring role\nwords and disguising their identities. We evaluate the framework's performance\nbased on game success rates and the accuracy of the LLM agents' analytical\nresults. Experimental results affirm the framework's effectiveness,\ndemonstrating notable improvements in LLM performance across multiple datasets.\nThis work highlights the potential of LLMs in mastering situational reasoning\nand social interactions within structured game environments. Our code is\npublicly available at https://github.com/ct-wei/Who-is-The-Spy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Chentian Wei",
      "Jiewei Chen",
      "Jinzhu Xu"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15225v1",
    "title": "A Personalized Data-Driven Generative Model of Human Motion",
    "abstract": "The deployment of autonomous virtual avatars (in extended reality) and robots\nin human group activities - such as rehabilitation therapy, sports, and\nmanufacturing - is expected to increase as these technologies become more\npervasive. Designing cognitive architectures and control strategies to drive\nthese agents requires realistic models of human motion. However, existing\nmodels only provide simplified descriptions of human motor behavior. In this\nwork, we propose a fully data-driven approach, based on Long Short-Term Memory\nneural networks, to generate original motion that captures the unique\ncharacteristics of specific individuals. We validate the architecture using\nreal data of scalar oscillatory motion. Extensive analyses show that our model\neffectively replicates the velocity distribution and amplitude envelopes of the\nindividual it was trained on, remaining different from other individuals, and\noutperforming state-of-the-art models in terms of similarity to human data.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Angelo Di Porzio",
      "Marco Coraggio"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15222v1",
    "title": "Model Hubs and Beyond: Analyzing Model Popularity, Performance, and Documentation",
    "abstract": "With the massive surge in ML models on platforms like Hugging Face, users\noften lose track and struggle to choose the best model for their downstream\ntasks, frequently relying on model popularity indicated by download counts,\nlikes, or recency. We investigate whether this popularity aligns with actual\nmodel performance and how the comprehensiveness of model documentation\ncorrelates with both popularity and performance. In our study, we evaluated a\ncomprehensive set of 500 Sentiment Analysis models on Hugging Face. This\nevaluation involved massive annotation efforts, with human annotators\ncompleting nearly 80,000 annotations, alongside extensive model training and\nevaluation. Our findings reveal that model popularity does not necessarily\ncorrelate with performance. Additionally, we identify critical inconsistencies\nin model card reporting: approximately 80\\% of the models analyzed lack\ndetailed information about the model, training, and evaluation processes.\nFurthermore, about 88\\% of model authors overstate their models' performance in\nthe model cards. Based on our findings, we provide a checklist of guidelines\nfor users to choose good models for downstream tasks.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Pritam Kadasi",
      "Sriman Reddy",
      "Srivathsa Vamsi Chaturvedula",
      "Rudranshu Sen",
      "Agnish Saha",
      "Soumavo Sikdar",
      "Sayani Sarkar",
      "Suhani Mittal",
      "Rohit Jindal",
      "Mayank Singh"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15221v1",
    "title": "A Foundation Model for Patient Behavior Monitoring and Suicide Detection",
    "abstract": "Foundation models (FMs) have achieved remarkable success across various\ndomains, yet their adoption in healthcare remains limited. While significant\nadvances have been made in medical imaging, genetic biomarkers, and time series\nfrom electronic health records, the potential of FMs for patient behavior\nmonitoring through wearable devices remains underexplored. These datasets are\ninherently heterogeneous, multisource, and often exhibit high rates of missing\ndata, posing unique challenges. This paper introduces a novel FM based on a\nmodified vector quantized variational autoencoder (VQ-VAE), specifically\ndesigned to process real-world data from wearable devices. We demonstrate that\nour pretrained FM, trained on a broad cohort of psychiatric patients, performs\ndownstream tasks via its latent representation without fine-tuning on a\nheld-out cohort of suicidal patients. To illustrate this, we develop a\nprobabilistic change-point detection algorithm for suicide detection and\ndemonstrate the FM's effectiveness in predicting emotional states. Our results\nshow that the discrete latent structure of the VQ-VAE outperforms a\nstate-of-the-art Informer architecture in unsupervised suicide detection, while\nmatching its performance in supervised emotion prediction when the latent\ndimensionality is increased, though at the cost of reduced unsupervised\naccuracy. This trade-off highlights the need for future FMs to integrate hybrid\ndiscrete-continuous structures for balanced performance across tasks.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Rodrigo Oliver",
      "Josué Pérez-Sabater",
      "Leire Paz-Arbaizar",
      "Alejandro Lancho",
      "Antonio Artés",
      "Pablo M. Olmos"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15220v2",
    "title": "Entity-aware Cross-lingual Claim Detection for Automated Fact-checking",
    "abstract": "Identifying claims requiring verification is a critical task in automated\nfact-checking, especially given the proliferation of misinformation on social\nmedia platforms. Despite significant progress in the task, there remain open\nchallenges such as dealing with multilingual and multimodal data prevalent in\nonline discourse. Addressing the multilingual challenge, recent efforts have\nfocused on fine-tuning pre-trained multilingual language models. While these\nmodels can handle multiple languages, their ability to effectively transfer\ncross-lingual knowledge for detecting claims spreading on social media remains\nunder-explored. In this paper, we introduce EX-Claim, an entity-aware\ncross-lingual claim detection model that generalizes well to handle claims\nwritten in any language. The model leverages entity information derived from\nnamed entity recognition and entity linking techniques to improve the\nlanguage-level performance of both seen and unseen languages during training.\nExtensive experiments conducted on three datasets from different social media\nplatforms demonstrate that our proposed model significantly outperforms the\nbaselines, across 27 languages, and achieves the highest rate of knowledge\ntransfer, even with limited training data.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Rrubaa Panchendrarajan",
      "Arkaitz Zubiaga"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15211v1",
    "title": "GO-N3RDet: Geometry Optimized NeRF-enhanced 3D Object Detector",
    "abstract": "We propose GO-N3RDet, a scene-geometry optimized multi-view 3D object\ndetector enhanced by neural radiance fields. The key to accurate 3D object\ndetection is in effective voxel representation. However, due to occlusion and\nlack of 3D information, constructing 3D features from multi-view 2D images is\nchallenging. Addressing that, we introduce a unique 3D positional information\nembedded voxel optimization mechanism to fuse multi-view features. To\nprioritize neural field reconstruction in object regions, we also devise a\ndouble importance sampling scheme for the NeRF branch of our detector. We\nadditionally propose an opacity optimization module for precise voxel opacity\nprediction by enforcing multi-view consistency constraints. Moreover, to\nfurther improve voxel density consistency across multiple perspectives, we\nincorporate ray distance as a weighting factor to minimize cumulative ray\nerrors. Our unique modules synergetically form an end-to-end neural model that\nestablishes new state-of-the-art in NeRF-based multi-view 3D detection,\nverified with extensive experiments on ScanNet and ARKITScenes. Code will be\navailable at https://github.com/ZechuanLi/GO-N3RDet.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Zechuan Li",
      "Hongshan Yu",
      "Yihao Ding",
      "Jinhao Qiao",
      "Basim Azam",
      "Naveed Akhtar"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.16550v1",
    "title": "Unified Enhancement of the Generalization and Robustness of Language Models via Bi-Stage Optimization",
    "abstract": "Neural network language models (LMs) are confronted with significant\nchallenges in generalization and robustness. Currently, many studies focus on\nimproving either generalization or robustness in isolation, without methods\naddressing both aspects simultaneously, which presents a significant challenge\nin developing LMs that are both robust and generalized. In this paper, we\npropose a bi-stage optimization framework to uniformly enhance both the\ngeneralization and robustness of LMs, termed UEGR. Specifically, during the\nforward propagation stage, we enrich the output probability distributions of\nadversarial samples by adaptive dropout to generate diverse sub models, and\nincorporate JS divergence and adversarial losses of these output distributions\nto reinforce output stability. During backward propagation stage, we compute\nparameter saliency scores and selectively update only the most critical\nparameters to minimize unnecessary deviations and consolidate the model's\nresilience. Theoretical analysis shows that our framework includes gradient\nregularization to limit the model's sensitivity to input perturbations and\nselective parameter updates to flatten the loss landscape, thus improving both\ngeneralization and robustness. The experimental results show that our method\nsignificantly improves the generalization and robustness of LMs compared to\nother existing methods across 13 publicly available language datasets,\nachieving state-of-the-art (SOTA) performance.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Yudao Sun",
      "Juan Yin",
      "Juan Zhao",
      "Fan Zhang",
      "Yongheng Liu",
      "Hongji Chen"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15210v1",
    "title": "Online federated learning framework for classification",
    "abstract": "In this paper, we develop a novel online federated learning framework for\nclassification, designed to handle streaming data from multiple clients while\nensuring data privacy and computational efficiency. Our method leverages the\ngeneralized distance-weighted discriminant technique, making it robust to both\nhomogeneous and heterogeneous data distributions across clients. In particular,\nwe develop a new optimization algorithm based on the Majorization-Minimization\nprinciple, integrated with a renewable estimation procedure, enabling efficient\nmodel updates without full retraining. We provide a theoretical guarantee for\nthe convergence of our estimator, proving its consistency and asymptotic\nnormality under standard regularity conditions. In addition, we establish that\nour method achieves Bayesian risk consistency, ensuring its reliability for\nclassification tasks in federated environments. We further incorporate\ndifferential privacy mechanisms to enhance data security, protecting client\ninformation while maintaining model performance. Extensive numerical\nexperiments on both simulated and real-world datasets demonstrate that our\napproach delivers high classification accuracy, significant computational\nefficiency gains, and substantial savings in data storage requirements compared\nto existing methods.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Wenxing Guo",
      "Jinhan Xie",
      "Jianya Lu",
      "Bei jiang",
      "Hongsheng Dai",
      "Linglong Kong"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15209v1",
    "title": "Kolmogorov-Arnold Network for Transistor Compact Modeling",
    "abstract": "Neural network (NN)-based transistor compact modeling has recently emerged as\na transformative solution for accelerating device modeling and SPICE circuit\nsimulations. However, conventional NN architectures, despite their widespread\nadoption in state-of-the-art methods, primarily function as black-box problem\nsolvers. This lack of interpretability significantly limits their capacity to\nextract and convey meaningful insights into learned data patterns, posing a\nmajor barrier to their broader adoption in critical modeling tasks. This work\nintroduces, for the first time, Kolmogorov-Arnold network (KAN) for the\ntransistor - a groundbreaking NN architecture that seamlessly integrates\ninterpretability with high precision in physics-based function modeling. We\nsystematically evaluate the performance of KAN and Fourier KAN for FinFET\ncompact modeling, benchmarking them against the golden industry-standard\ncompact model and the widely used MLP architecture. Our results reveal that KAN\nand FKAN consistently achieve superior prediction accuracy for critical figures\nof merit, including gate current, drain charge, and source charge. Furthermore,\nwe demonstrate and improve the unique ability of KAN to derive symbolic\nformulas from learned data patterns - a capability that not only enhances\ninterpretability but also facilitates in-depth transistor analysis and\noptimization. This work highlights the transformative potential of KAN in\nbridging the gap between interpretability and precision in NN-driven transistor\ncompact modeling. By providing a robust and transparent approach to transistor\nmodeling, KAN represents a pivotal advancement for the semiconductor industry\nas it navigates the challenges of advanced technology scaling.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Rodion Novkin",
      "Hussam Amrouch"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15208v1",
    "title": "DiST-4D: Disentangled Spatiotemporal Diffusion with Metric Depth for 4D Driving Scene Generation",
    "abstract": "Current generative models struggle to synthesize dynamic 4D driving scenes\nthat simultaneously support temporal extrapolation and spatial novel view\nsynthesis (NVS) without per-scene optimization. A key challenge lies in finding\nan efficient and generalizable geometric representation that seamlessly\nconnects temporal and spatial synthesis. To address this, we propose DiST-4D,\nthe first disentangled spatiotemporal diffusion framework for 4D driving scene\ngeneration, which leverages metric depth as the core geometric representation.\nDiST-4D decomposes the problem into two diffusion processes: DiST-T, which\npredicts future metric depth and multi-view RGB sequences directly from past\nobservations, and DiST-S, which enables spatial NVS by training only on\nexisting viewpoints while enforcing cycle consistency. This cycle consistency\nmechanism introduces a forward-backward rendering constraint, reducing the\ngeneralization gap between observed and unseen viewpoints. Metric depth is\nessential for both accurate reliable forecasting and accurate spatial NVS, as\nit provides a view-consistent geometric representation that generalizes well to\nunseen perspectives. Experiments demonstrate that DiST-4D achieves\nstate-of-the-art performance in both temporal prediction and NVS tasks, while\nalso delivering competitive performance in planning-related evaluations.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jiazhe Guo",
      "Yikang Ding",
      "Xiwu Chen",
      "Shuo Chen",
      "Bohan Li",
      "Yingshuang Zou",
      "Xiaoyang Lyu",
      "Feiyang Tan",
      "Xiaojuan Qi",
      "Zhiheng Li",
      "Hao Zhao"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15204v1",
    "title": "When Pigs Get Sick: Multi-Agent AI for Swine Disease Detection",
    "abstract": "Swine disease surveillance is critical to the sustainability of global\nagriculture, yet its effectiveness is frequently undermined by limited\nveterinary resources, delayed identification of cases, and variability in\ndiagnostic accuracy. To overcome these barriers, we introduce a novel\nAI-powered, multi-agent diagnostic system that leverages Retrieval-Augmented\nGeneration (RAG) to deliver timely, evidence-based disease detection and\nclinical guidance. By automatically classifying user inputs into either\nKnowledge Retrieval Queries or Symptom-Based Diagnostic Queries, the system\nensures targeted information retrieval and facilitates precise diagnostic\nreasoning. An adaptive questioning protocol systematically collects relevant\nclinical signs, while a confidence-weighted decision fusion mechanism\nintegrates multiple diagnostic hypotheses to generate robust disease\npredictions and treatment recommendations. Comprehensive evaluations\nencompassing query classification, disease diagnosis, and knowledge retrieval\ndemonstrate that the system achieves high accuracy, rapid response times, and\nconsistent reliability. By providing a scalable, AI-driven diagnostic\nframework, this approach enhances veterinary decision-making, advances\nsustainable livestock management practices, and contributes substantively to\nthe realization of global food security.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.MA"
    ],
    "authors": [
      "Tittaya Mairittha",
      "Tanakon Sawanglok",
      "Panuwit Raden",
      "Sorrawit Treesuk"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15202v2",
    "title": "A Unified Framework for Real-Time Failure Handling in Robotics Using Vision-Language Models, Reactive Planner and Behavior Trees",
    "abstract": "Robotic systems often face execution failures due to unexpected obstacles,\nsensor errors, or environmental changes. Traditional failure recovery methods\nrely on predefined strategies or human intervention, making them less\nadaptable. This paper presents a unified failure recovery framework that\ncombines Vision-Language Models (VLMs), a reactive planner, and Behavior Trees\n(BTs) to enable real-time failure handling. Our approach includes pre-execution\nverification, which checks for potential failures before execution, and\nreactive failure handling, which detects and corrects failures during execution\nby verifying existing BT conditions, adding missing preconditions and, when\nnecessary, generating new skills. The framework uses a scene graph for\nstructured environmental perception and an execution history for continuous\nmonitoring, enabling context-aware and adaptive failure handling. We evaluate\nour framework through real-world experiments with an ABB YuMi robot on tasks\nlike peg insertion, object sorting, and drawer placement, as well as in\nAI2-THOR simulator. Compared to using pre-execution and reactive methods\nseparately, our approach achieves higher task success rates and greater\nadaptability. Ablation studies highlight the importance of VLM-based reasoning,\nstructured scene representation, and execution history tracking for effective\nfailure recovery in robotics.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Faseeh Ahmad",
      "Hashim Ismail",
      "Jonathan Styrud",
      "Maj Stenmark",
      "Volker Krueger"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.15579v1",
    "title": "Understanding the Generalization of In-Context Learning in Transformers: An Empirical Study",
    "abstract": "Large language models (LLMs) like GPT-4 and LLaMA-3 utilize the powerful\nin-context learning (ICL) capability of Transformer architecture to learn on\nthe fly from limited examples. While ICL underpins many LLM applications, its\nfull potential remains hindered by a limited understanding of its\ngeneralization boundaries and vulnerabilities. We present a systematic\ninvestigation of transformers' generalization capability with ICL relative to\ntraining data coverage by defining a task-centric framework along three\ndimensions: inter-problem, intra-problem, and intra-task generalization.\nThrough extensive simulation and real-world experiments, encompassing tasks\nsuch as function fitting, API calling, and translation, we find that\ntransformers lack inter-problem generalization with ICL, but excel in\nintra-task and intra-problem generalization. When the training data includes a\ngreater variety of mixed tasks, it significantly enhances the generalization\nability of ICL on unseen tasks and even on known simple tasks. This guides us\nin designing training data to maximize the diversity of tasks covered and to\ncombine different tasks whenever possible, rather than solely focusing on the\ntarget task for testing.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Xingxuan Zhang",
      "Haoran Wang",
      "Jiansheng Li",
      "Yuan Xue",
      "Shikai Guan",
      "Renzhe Xu",
      "Hao Zou",
      "Han Yu",
      "Peng Cui"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15200v1",
    "title": "Partially Observable Reinforcement Learning with Memory Traces",
    "abstract": "Partially observable environments present a considerable computational\nchallenge in reinforcement learning due to the need to consider long histories.\nLearning with a finite window of observations quickly becomes intractable as\nthe window length grows. In this work, we introduce memory traces. Inspired by\neligibility traces, these are compact representations of the history of\nobservations in the form of exponential moving averages. We prove sample\ncomplexity bounds for the problem of offline on-policy evaluation that quantify\nthe value errors achieved with memory traces for the class of Lipschitz\ncontinuous value estimates. We establish a close connection to the window\napproach, and demonstrate that, in certain environments, learning with memory\ntraces is significantly more sample efficient. Finally, we underline the\neffectiveness of memory traces empirically in online reinforcement learning\nexperiments for both value prediction and control.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Onno Eberhard",
      "Michael Muehlebach",
      "Claire Vernade"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15197v1",
    "title": "Detect-and-Guide: Self-regulation of Diffusion Models for Safe Text-to-Image Generation via Guideline Token Optimization",
    "abstract": "Text-to-image diffusion models have achieved state-of-the-art results in\nsynthesis tasks; however, there is a growing concern about their potential\nmisuse in creating harmful content. To mitigate these risks, post-hoc model\nintervention techniques, such as concept unlearning and safety guidance, have\nbeen developed. However, fine-tuning model weights or adapting the hidden\nstates of the diffusion model operates in an uninterpretable way, making it\nunclear which part of the intermediate variables is responsible for unsafe\ngeneration. These interventions severely affect the sampling trajectory when\nerasing harmful concepts from complex, multi-concept prompts, thus hindering\ntheir practical use in real-world settings. In this work, we propose the safe\ngeneration framework Detect-and-Guide (DAG), leveraging the internal knowledge\nof diffusion models to perform self-diagnosis and fine-grained self-regulation\nduring the sampling process. DAG first detects harmful concepts from noisy\nlatents using refined cross-attention maps of optimized tokens, then applies\nsafety guidance with adaptive strength and editing regions to negate unsafe\ngeneration. The optimization only requires a small annotated dataset and can\nprovide precise detection maps with generalizability and concept specificity.\nMoreover, DAG does not require fine-tuning of diffusion models, and therefore\nintroduces no loss to their generation diversity. Experiments on erasing sexual\ncontent show that DAG achieves state-of-the-art safe generation performance,\nbalancing harmfulness mitigation and text-following performance on\nmulti-concept real-world prompts.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Feifei Li",
      "Mi Zhang",
      "Yiming Sun",
      "Min Yang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15195v2",
    "title": "Benchmarking Large Language Models for Handwritten Text Recognition",
    "abstract": "Traditional machine learning models for Handwritten Text Recognition (HTR)\nrely on supervised training, requiring extensive manual annotations, and often\nproduce errors due to the separation between layout and text processing. In\ncontrast, Multimodal Large Language Models (MLLMs) offer a general approach to\nrecognizing diverse handwriting styles without the need for model-specific\ntraining. The study benchmarks various proprietary and open-source LLMs against\nTranskribus models, evaluating their performance on both modern and historical\ndatasets written in English, French, German, and Italian. In addition, emphasis\nis placed on testing the models' ability to autonomously correct previously\ngenerated outputs. Findings indicate that proprietary models, especially Claude\n3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs\nachieve excellent results in recognizing modern handwriting and exhibit a\npreference for the English language due to their pre-training dataset\ncomposition. Comparisons with Transkribus show no consistent advantage for\neither approach. Moreover, LLMs demonstrate limited ability to autonomously\ncorrect errors in zero-shot transcriptions.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Giorgia Crosilla",
      "Lukas Klic",
      "Giovanni Colavizza"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15578v1",
    "title": "Sparseformer: a Transferable Transformer with Multi-granularity Token Sparsification for Medical Time Series Classification",
    "abstract": "Medical time series (MedTS) classification is crucial for improved diagnosis\nin healthcare, and yet it is challenging due to the varying granularity of\npatterns, intricate inter-channel correlation, information redundancy, and\nlabel scarcity. While existing transformer-based models have shown promise in\ntime series analysis, they mainly focus on forecasting and fail to fully\nexploit the distinctive characteristics of MedTS data. In this paper, we\nintroduce Sparseformer, a transformer specifically designed for MedTS\nclassification. We propose a sparse token-based dual-attention mechanism that\nenables global modeling and token compression, allowing dynamic focus on the\nmost informative tokens while distilling redundant features. This mechanism is\nthen applied to the multi-granularity, cross-channel encoding of medical\nsignals, capturing intra- and inter-granularity correlations and inter-channel\nconnections. The sparsification design allows our model to handle heterogeneous\ninputs of varying lengths and channels directly. Further, we introduce an\nadaptive label encoder to address label space misalignment across datasets,\nequipping our model with cross-dataset transferability to alleviate the medical\nlabel scarcity issue. Our model outperforms 12 baselines across seven medical\ndatasets under supervised learning. In the few-shot learning experiments, our\nmodel also achieves superior average results. In addition, the in-domain and\ncross-domain experiments among three diagnostic scenarios demonstrate our\nmodel's zero-shot learning capability. Collectively, these findings underscore\nthe robustness and transferability of our model in various medical\napplications.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Jiexia Ye",
      "Weiqi Zhang",
      "Ziyue Li",
      "Jia Li",
      "Fugee Tsung"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15190v1",
    "title": "Learning Topology Actions for Power Grid Control: A Graph-Based Soft-Label Imitation Learning Approach",
    "abstract": "The rising proportion of renewable energy in the electricity mix introduces\nsignificant operational challenges for power grid operators. Effective power\ngrid management demands adaptive decision-making strategies capable of handling\ndynamic conditions. With the increase in complexity, more and more Deep\nLearning (DL) approaches have been proposed to find suitable grid topologies\nfor congestion management. In this work, we contribute to this research by\nintroducing a novel Imitation Learning (IL) approach that leverages soft labels\nderived from simulated topological action outcomes, thereby capturing multiple\nviable actions per state. Unlike traditional IL methods that rely on hard\nlabels to enforce a single optimal action, our method constructs soft labels\nover actions, by leveraging effective actions that prove suitable in resolving\ngrid congestion. To further enhance decision-making, we integrate Graph Neural\nNetworks (GNNs) to encode the structural properties of power grids, ensuring\nthat the topology-aware representations contribute to better agent performance.\nOur approach significantly outperforms state-of-the-art baselines, all of which\nuse only topological actions, as well as feedforward and GNN-based\narchitectures with hard labels. Most notably, it achieves a 17% better\nperformance compared to the greedy expert agent from which the imitation\ntargets were derived.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Mohamed Hassouna",
      "Clara Holzhüter",
      "Malte Lehna",
      "Matthijs de Jong",
      "Jan Viebahn",
      "Bernhard Sick",
      "Christoph Scholz"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15576v1",
    "title": "A Bird Song Detector for improving bird identification through Deep Learning: a case study from Doñana",
    "abstract": "Passive Acoustic Monitoring with automatic recorders is essential for\necosystem conservation but generates vast unsupervised audio data, posing\nchallenges for extracting meaningful information. Deep Learning techniques\noffer a promising solution. BirdNET, a widely used model for bird\nidentification, has shown success in many study systems but is limited in some\nregions due to biases in its training data. A key challenge in bird species\ndetection is that many recordings either lack target species or contain\noverlapping vocalizations. To overcome these problems, we developed a\nmulti-stage pipeline for automatic bird vocalization identification in Do\\~nana\nNational Park (SW Spain), a region facing significant conservation threats. Our\napproach included a Bird Song Detector to isolate vocalizations and custom\nclassifiers trained with BirdNET embeddings. We manually annotated 461 minutes\nof audio from three habitats across nine locations, yielding 3,749 annotations\nfor 34 classes. Spectrograms facilitated the use of image processing\ntechniques. Applying the Bird Song Detector before classification improved\nspecies identification, as all classification models performed better when\nanalyzing only the segments where birds were detected. Specifically, the\ncombination of the Bird Song Detector and fine-tuned BirdNET compared to the\nbaseline without the Bird Song Detector. Our approach demonstrated the\neffectiveness of integrating a Bird Song Detector with fine-tuned\nclassification models for bird identification at local soundscapes. These\nfindings highlight the need to adapt general-purpose tools for specific\necological challenges, as demonstrated in Do\\~nana. Automatically detecting\nbird species serves for tracking the health status of this threatened\necosystem, given the sensitivity of birds to environmental changes, and helps\nin the design of conservation measures for reducing biodiversity loss",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.NE",
      "I.5.4; I.2.6; I.4.8"
    ],
    "authors": [
      "Alba Márquez-Rodríguez",
      "Miguel Ángel Mohedano-Munoz",
      "Manuel J. Marín-Jiménez",
      "Eduardo Santamaría-García",
      "Giulia Bastianelli",
      "Pedro Jordano",
      "Irene Mendoza"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15185v1",
    "title": "3D Occupancy Prediction with Low-Resolution Queries via Prototype-aware View Transformation",
    "abstract": "The resolution of voxel queries significantly influences the quality of view\ntransformation in camera-based 3D occupancy prediction. However, computational\nconstraints and the practical necessity for real-time deployment require\nsmaller query resolutions, which inevitably leads to an information loss.\nTherefore, it is essential to encode and preserve rich visual details within\nlimited query sizes while ensuring a comprehensive representation of 3D\noccupancy. To this end, we introduce ProtoOcc, a novel occupancy network that\nleverages prototypes of clustered image segments in view transformation to\nenhance low-resolution context. In particular, the mapping of 2D prototypes\nonto 3D voxel queries encodes high-level visual geometries and complements the\nloss of spatial information from reduced query resolutions. Additionally, we\ndesign a multi-perspective decoding strategy to efficiently disentangle the\ndensely compressed visual cues into a high-dimensional 3D occupancy scene.\nExperimental results on both Occ3D and SemanticKITTI benchmarks demonstrate the\neffectiveness of the proposed method, showing clear improvements over the\nbaselines. More importantly, ProtoOcc achieves competitive performance against\nthe baselines even with 75\\% reduced voxel resolution.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Gyeongrok Oh",
      "Sungjune Kim",
      "Heeju Ko",
      "Hyung-gun Chi",
      "Jinkyu Kim",
      "Dongwook Lee",
      "Daehyun Ji",
      "Sungjoon Choi",
      "Sujin Jang",
      "Sangpil Kim"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15182v1",
    "title": "Foundation models may exhibit staged progression in novel CBRN threat disclosure",
    "abstract": "The extent to which foundation models can disclose novel chemical,\nbiological, radiation, and nuclear (CBRN) threats to expert users is unclear\ndue to a lack of test cases. I leveraged the unique opportunity presented by an\nupcoming publication describing a novel catastrophic biothreat - \"Technical\nReport on Mirror Bacteria: Feasibility and Risks\" - to conduct a small\ncontrolled study before it became public. Graduate-trained biologists tasked\nwith predicting the consequences of releasing mirror E. coli showed no\nsignificant differences in rubric-graded accuracy using Claude Sonnet 3.5 new\n(n=10) or web search only (n=2); both groups scored comparably to a web\nbaseline (28 and 43 versus 36). However, Sonnet reasoned correctly when\nprompted by a report author, but a smaller model, Haiku 3.5, failed even with\nauthor guidance (80 versus 5). These results suggest distinct stages of model\ncapability: Haiku is unable to reason about mirror life even with threat-aware\nexpert guidance (Stage 1), while Sonnet correctly reasons only with\nthreat-aware prompting (Stage 2). Continued advances may allow future models to\ndisclose novel CBRN threats to naive experts (Stage 3) or unskilled users\n(Stage 4). While mirror life represents only one case study, monitoring new\nmodels' ability to reason about privately known threats may allow protective\nmeasures to be implemented before widespread disclosure.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "q-bio.OT"
    ],
    "authors": [
      "Kevin M Esvelt"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15177v1",
    "title": "Food Delivery Time Prediction in Indian Cities Using Machine Learning Models",
    "abstract": "Accurate prediction of food delivery times significantly impacts customer\nsatisfaction, operational efficiency, and profitability in food delivery\nservices. However, existing studies primarily utilize static historical data\nand often overlook dynamic, real-time contextual factors crucial for precise\nprediction, particularly in densely populated Indian cities. This research\naddresses these gaps by integrating real-time contextual variables such as\ntraffic density, weather conditions, local events, and geospatial data\n(restaurant and delivery location coordinates) into predictive models. We\nsystematically compare various machine learning algorithms, including Linear\nRegression, Decision Trees, Bagging, Random Forest, XGBoost, and LightGBM, on a\ncomprehensive food delivery dataset specific to Indian urban contexts. Rigorous\ndata preprocessing and feature selection significantly enhanced model\nperformance. Experimental results demonstrate that the LightGBM model achieves\nsuperior predictive accuracy, with an R2 score of 0.76 and Mean Squared Error\n(MSE) of 20.59, outperforming traditional baseline approaches. Our study thus\nprovides actionable insights for improving logistics strategies in complex\nurban environments. The complete methodology and code are publicly available\nfor reproducibility and further research.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Ananya Garg",
      "Mohmmad Ayaan",
      "Swara Parekh",
      "Vikranth Udandarao"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15176v1",
    "title": "A Review on Large Language Models for Visual Analytics",
    "abstract": "This paper provides a comprehensive review of the integration of Large\nLanguage Models (LLMs) with visual analytics, addressing their foundational\nconcepts, capabilities, and wide-ranging applications. It begins by outlining\nthe theoretical underpinnings of visual analytics and the transformative\npotential of LLMs, specifically focusing on their roles in natural language\nunderstanding, natural language generation, dialogue systems, and text-to-media\ntransformations. The review further investigates how the synergy between LLMs\nand visual analytics enhances data interpretation, visualization techniques,\nand interactive exploration capabilities. Key tools and platforms including\nLIDA, Chat2VIS, Julius AI, and Zoho Analytics, along with specialized\nmultimodal models such as ChartLlama and CharXIV, are critically evaluated. The\npaper discusses their functionalities, strengths, and limitations in supporting\ndata exploration, visualization enhancement, automated reporting, and insight\nextraction. The taxonomy of LLM tasks, ranging from natural language\nunderstanding (NLU), natural language generation (NLG), to dialogue systems and\ntext-to-media transformations, is systematically explored. This review provides\na SWOT analysis of integrating Large Language Models (LLMs) with visual\nanalytics, highlighting strengths like accessibility and flexibility,\nweaknesses such as computational demands and biases, opportunities in\nmultimodal integration and user collaboration, and threats including privacy\nconcerns and skill degradation. It emphasizes addressing ethical considerations\nand methodological improvements for effective integration.",
    "categories": [
      "cs.HC",
      "cs.CL",
      "cs.CV"
    ],
    "authors": [
      "Navya Sonal Agarwal",
      "Sanjay Kumar Sonbhadra"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15172v1",
    "title": "Multi-Agent Actor-Critic with Harmonic Annealing Pruning for Dynamic Spectrum Access Systems",
    "abstract": "Multi-Agent Deep Reinforcement Learning (MADRL) has emerged as a powerful\ntool for optimizing decentralized decision-making systems in complex settings,\nsuch as Dynamic Spectrum Access (DSA). However, deploying deep learning models\non resource-constrained edge devices remains challenging due to their high\ncomputational cost. To address this challenge, in this paper, we present a\nnovel sparse recurrent MARL framework integrating gradual neural network\npruning into the independent actor global critic paradigm. Additionally, we\nintroduce a harmonic annealing sparsity scheduler, which achieves comparable,\nand in certain cases superior, performance to standard linear and polynomial\npruning schedulers at large sparsities. Our experimental investigation\ndemonstrates that the proposed DSA framework can discover superior policies,\nunder diverse training conditions, outperforming conventional DSA, MADRL\nbaselines, and state-of-the-art pruning techniques.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI"
    ],
    "authors": [
      "George Stamatelis",
      "Angelos-Nikolaos Kanatas",
      "George C. Alexandropoulos"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15169v1",
    "title": "Comparing Llama3 and DeepSeekR1 on Biomedical Text Classification Tasks",
    "abstract": "This study compares the performance of two open-source large language models\n(LLMs)-Llama3-70B and DeepSeekR1-distill-Llama3-70B-on six biomedical text\nclassification tasks. Four tasks involve data from social media, while two\ntasks focus on clinical notes from electronic health records, and all\nexperiments were performed in zero-shot settings. Performance metrics,\nincluding precision, recall, and F1 scores, were measured for each task, along\nwith their 95% confidence intervals. Results demonstrated that\nDeepSeekR1-distill-Llama3-70B generally performs better in terms of precision\non most tasks, with mixed results on recall. While the zero-shot LLMs\ndemonstrated high F1 scores for some tasks, they grossly underperformed on\nothers, for data from both sources. The findings suggest that model selection\nshould be guided by the specific requirements of the health-related text\nclassification tasks, particularly when considering the precision-recall\ntrade-offs, and that, in the presence of annotated data, supervised\nclassification approaches may be more reliable than zero-shot LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yuting Guo",
      "Abeed Sarker"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15168v1",
    "title": "World Models in Artificial Intelligence: Sensing, Learning, and Reasoning Like a Child",
    "abstract": "World Models help Artificial Intelligence (AI) predict outcomes, reason about\nits environment, and guide decision-making. While widely used in reinforcement\nlearning, they lack the structured, adaptive representations that even young\nchildren intuitively develop. Advancing beyond pattern recognition requires\ndynamic, interpretable frameworks inspired by Piaget's cognitive development\ntheory. We highlight six key research areas -- physics-informed learning,\nneurosymbolic learning, continual learning, causal inference, human-in-the-loop\nAI, and responsible AI -- as essential for enabling true reasoning in AI. By\nintegrating statistical learning with advances in these areas, AI can evolve\nfrom pattern recognition to genuine understanding, adaptation and reasoning\ncapabilities.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.ET",
      "cs.LG",
      "68T05"
    ],
    "authors": [
      "Javier Del Ser",
      "Jesus L. Lobo",
      "Heimo Müller",
      "Andreas Holzinger"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15167v1",
    "title": "Volumetric Reconstruction From Partial Views for Task-Oriented Grasping",
    "abstract": "Object affordance and volumetric information are essential in devising\neffective grasping strategies under task-specific constraints. This paper\npresents an approach for inferring suitable grasping strategies from limited\npartial views of an object. To achieve this, a recurrent generative adversarial\nnetwork (R-GAN) was proposed by incorporating a recurrent generator with long\nshort-term memory (LSTM) units for it to process a variable number of depth\nscans. To determine object affordances, the AffordPose knowledge dataset is\nutilized as prior knowledge. Affordance retrieving is defined by the volume\nsimilarity measured via Chamfer Distance and action similarities. A Proximal\nPolicy Optimization (PPO) reinforcement learning model is further implemented\nto refine the retrieved grasp strategies for task-oriented grasping. The\nretrieved grasp strategies were evaluated on a dual-arm mobile manipulation\nrobot with an overall grasping accuracy of 89% for four tasks: lift, handle\ngrasp, wrap grasp, and press.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Fujian Yan",
      "Hui Li",
      "Hongsheng He"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15166v1",
    "title": "Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive Learning: Adapting Alignment Calibration to MERU",
    "abstract": "Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "authors": [
      "Àlex Pujol Vidal",
      "Sergio Escalera",
      "Kamal Nasrollahi",
      "Thomas B. Moeslund"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15163v1",
    "title": "Global Group Fairness in Federated Learning via Function Tracking",
    "abstract": "We investigate group fairness regularizers in federated learning, aiming to\ntrain a globally fair model in a distributed setting. Ensuring global fairness\nin distributed training presents unique challenges, as fairness regularizers\ntypically involve probability metrics between distributions across all clients\nand are not naturally separable by client. To address this, we introduce a\nfunction-tracking scheme for the global fairness regularizer based on a Maximum\nMean Discrepancy (MMD), which incurs a small communication overhead. This\nscheme seamlessly integrates into most federated learning algorithms while\npreserving rigorous convergence guarantees, as demonstrated in the context of\nFedAvg. Additionally, when enforcing differential privacy, the kernel-based MMD\nregularization enables straightforward analysis through a change of kernel,\nleveraging an intuitive interpretation of kernel convolution. Numerical\nexperiments confirm our theoretical insights.",
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ME"
    ],
    "authors": [
      "Yves Rychener",
      "Daniel Kuhn",
      "Yifan Hu"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15161v1",
    "title": "UltraFlwr -- An Efficient Federated Medical and Surgical Object Detection Framework",
    "abstract": "Object detection shows promise for medical and surgical applications such as\ncell counting and tool tracking. However, its faces multiple real-world edge\ndeployment challenges including limited high-quality annotated data, data\nsharing restrictions, and computational constraints. In this work, we introduce\nUltraFlwr, a framework for federated medical and surgical object detection. By\nleveraging Federated Learning (FL), UltraFlwr enables decentralized model\ntraining across multiple sites without sharing raw data. To further enhance\nUltraFlwr's efficiency, we propose YOLO-PA, a set of novel Partial Aggregation\n(PA) strategies specifically designed for YOLO models in FL. YOLO-PA\nsignificantly reduces communication overhead by up to 83% per round while\nmaintaining performance comparable to Full Aggregation (FA) strategies. Our\nextensive experiments on BCCD and m2cai16-tool-locations datasets demonstrate\nthat YOLO-PA not only provides better client models compared to client-wise\ncentralized training and FA strategies, but also facilitates efficient training\nand deployment across resource-constrained edge devices. Further, we also\nestablish one of the first benchmarks in federated medical and surgical object\ndetection. This paper advances the feasibility of training and deploying\ndetection models on the edge, making federated object detection more practical\nfor time-critical and resource-constrained medical and surgical applications.\nUltraFlwr is publicly available at https://github.com/KCL-BMEIS/UltraFlwr.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yang Li",
      "Soumya Snigdha Kundu",
      "Maxence Boels",
      "Toktam Mahmoodi",
      "Sebastien Ourselin",
      "Tom Vercauteren",
      "Prokar Dasgupta",
      "Jonathan Shapey",
      "Alejandro Granados"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15156v1",
    "title": "ARC: Anchored Representation Clouds for High-Resolution INR Classification",
    "abstract": "Implicit neural representations (INRs) encode signals in neural network\nweights as a memory-efficient representation, decoupling sampling resolution\nfrom the associated resource costs. Current INR image classification methods\nare demonstrated on low-resolution data and are sensitive to image-space\ntransformations. We attribute these issues to the global, fully-connected MLP\nneural network architecture encoding of current INRs, which lack mechanisms for\nlocal representation: MLPs are sensitive to absolute image location and\nstruggle with high-frequency details. We propose ARC: Anchored Representation\nClouds, a novel INR architecture that explicitly anchors latent vectors locally\nin image-space. By introducing spatial structure to the latent vectors, ARC\ncaptures local image data which in our testing leads to state-of-the-art\nimplicit image classification of both low- and high-resolution images and\nincreased robustness against image-space translation. Code can be found at\nhttps://github.com/JLuij/anchored_representation_clouds.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Joost Luijmes",
      "Alexander Gielisse",
      "Roman Knyazhitskiy",
      "Jan van Gemert"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15150v1",
    "title": "Preference Construction: A Bayesian Interactive Preference Elicitation Framework Based on Monte Carlo Tree Search",
    "abstract": "We present a novel preference learning framework to capture participant\npreferences efficiently within limited interaction rounds. It involves three\nmain contributions. First, we develop a variational Bayesian approach to infer\nthe participant's preference model by estimating posterior distributions and\nmanaging uncertainty from limited information. Second, we propose an adaptive\nquestioning policy that maximizes cumulative uncertainty reduction, formulating\nquestioning as a finite Markov decision process and using Monte Carlo Tree\nSearch to prioritize promising question trajectories. By considering long-term\neffects and leveraging the efficiency of the Bayesian approach, the policy\navoids shortsightedness. Third, we apply the framework to Multiple Criteria\nDecision Aiding, with pairwise comparison as the preference information and an\nadditive value function as the preference model. We integrate the\nreparameterization trick to address high-variance issues, enhancing robustness\nand efficiency. Computational studies on real-world and synthetic datasets\ndemonstrate the framework's practical usability, outperforming baselines in\ncapturing preferences and achieving superior uncertainty reduction within\nlimited interactions.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Yan Wang",
      "Jiapeng Liu",
      "Milosz Kadziński",
      "Xiuwu Liao"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15149v1",
    "title": "Machine learning surrogate models of many-body dispersion interactions in polymer melts",
    "abstract": "Accurate prediction of many-body dispersion (MBD) interactions is essential\nfor understanding the van der Waals forces that govern the behavior of many\ncomplex molecular systems. However, the high computational cost of MBD\ncalculations limits their direct application in large-scale simulations. In\nthis work, we introduce a machine learning surrogate model specifically\ndesigned to predict MBD forces in polymer melts, a system that demands accurate\nMBD description and offers structural advantages for machine learning\napproaches. Our model is based on a trimmed SchNet architecture that\nselectively retains the most relevant atomic connections and incorporates\ntrainable radial basis functions for geometric encoding. We validate our\nsurrogate model on datasets from polyethylene, polypropylene, and polyvinyl\nchloride melts, demonstrating high predictive accuracy and robust\ngeneralization across diverse polymer systems. In addition, the model captures\nkey physical features, such as the characteristic decay behavior of MBD\ninteractions, providing valuable insights for optimizing cutoff strategies.\nCharacterized by high computational efficiency, our surrogate model enables\npractical incorporation of MBD effects into large-scale molecular simulations.",
    "categories": [
      "cs.LG",
      "physics.comp-ph"
    ],
    "authors": [
      "Zhaoxiang Shen",
      "Raúl I. Sosa",
      "Jakub Lengiewicz",
      "Alexandre Tkatchenko",
      "Stéphane P. A. Bordas"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15144v1",
    "title": "PointSFDA: Source-free Domain Adaptation for Point Cloud Completion",
    "abstract": "Conventional methods for point cloud completion, typically trained on\nsynthetic datasets, face significant challenges when applied to\nout-of-distribution real-world scans. In this paper, we propose an effective\nyet simple source-free domain adaptation framework for point cloud completion,\ntermed \\textbf{PointSFDA}. Unlike unsupervised domain adaptation that reduces\nthe domain gap by directly leveraging labeled source data, PointSFDA uses only\na pretrained source model and unlabeled target data for adaptation, avoiding\nthe need for inaccessible source data in practical scenarios. Being the first\nsource-free domain adaptation architecture for point cloud completion, our\nmethod offers two core contributions. First, we introduce a coarse-to-fine\ndistillation solution to explicitly transfer the global geometry knowledge\nlearned from the source dataset. Second, as noise may be introduced due to\ndomain gaps, we propose a self-supervised partial-mask consistency training\nstrategy to learn local geometry information in the target domain. Extensive\nexperiments have validated that our method significantly improves the\nperformance of state-of-the-art networks in cross-domain shape completion. Our\ncode is available at\n\\emph{\\textcolor{magenta}{https://github.com/Starak-x/PointSFDA}}.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Xing He",
      "Zhe Zhu",
      "Liangliang Nan",
      "Honghua Chen",
      "Jing Qin",
      "Mingqiang Wei"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15141v1",
    "title": "Object-Centric Pretraining via Target Encoder Bootstrapping",
    "abstract": "Object-centric representation learning has recently been successfully applied\nto real-world datasets. This success can be attributed to pretrained\nnon-object-centric foundation models, whose features serve as reconstruction\ntargets for slot attention. However, targets must remain frozen throughout the\ntraining, which sets an upper bound on the performance object-centric models\ncan attain. Attempts to update the target encoder by bootstrapping result in\nlarge performance drops, which can be attributed to its lack of object-centric\ninductive biases, causing the object-centric model's encoder to drift away from\nrepresentations useful as reconstruction targets. To address these limitations,\nwe propose Object-CEntric Pretraining by Target Encoder BOotstrapping, a\nself-distillation setup for training object-centric models from scratch, on\nreal-world data, for the first time ever. In OCEBO, the target encoder is\nupdated as an exponential moving average of the object-centric model, thus\nexplicitly being enriched with object-centric inductive biases introduced by\nslot attention while removing the upper bound on performance present in other\nmodels. We mitigate the slot collapse caused by random initialization of the\ntarget encoder by introducing a novel cross-view patch filtering approach that\nlimits the supervision to sufficiently informative patches. When pretrained on\n241k images from COCO, OCEBO achieves unsupervised object discovery performance\ncomparable to that of object-centric models with frozen non-object-centric\ntarget encoders pretrained on hundreds of millions of images. The code and\npretrained models are publicly available at https://github.com/djukicn/ocebo.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Nikola Đukić",
      "Tim Lebailly",
      "Tinne Tuytelaars"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15138v2",
    "title": "VideoGen-of-Thought: Step-by-step generating multi-shot video with minimal manual intervention",
    "abstract": "Current video generation models excel at short clips but fail to produce\ncohesive multi-shot narratives due to disjointed visual dynamics and fractured\nstorylines. Existing solutions either rely on extensive manual\nscripting/editing or prioritize single-shot fidelity over cross-scene\ncontinuity, limiting their practicality for movie-like content. We introduce\nVideoGen-of-Thought (VGoT), a step-by-step framework that automates multi-shot\nvideo synthesis from a single sentence by systematically addressing three core\nchallenges: (1) Narrative Fragmentation: Existing methods lack structured\nstorytelling. We propose dynamic storyline modeling, which first converts the\nuser prompt into concise shot descriptions, then elaborates them into detailed,\ncinematic specifications across five domains (character dynamics, background\ncontinuity, relationship evolution, camera movements, HDR lighting), ensuring\nlogical narrative progression with self-validation. (2) Visual Inconsistency:\nExisting approaches struggle with maintaining visual consistency across shots.\nOur identity-aware cross-shot propagation generates identity-preserving\nportrait (IPP) tokens that maintain character fidelity while allowing trait\nvariations (expressions, aging) dictated by the storyline. (3) Transition\nArtifacts: Abrupt shot changes disrupt immersion. Our adjacent latent\ntransition mechanisms implement boundary-aware reset strategies that process\nadjacent shots' features at transition points, enabling seamless visual flow\nwhile preserving narrative continuity. VGoT generates multi-shot videos that\noutperform state-of-the-art baselines by 20.4% in within-shot face consistency\nand 17.4% in style consistency, while achieving over 100% better cross-shot\nconsistency and 10x fewer manual adjustments than alternatives.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Mingzhe Zheng",
      "Yongqi Xu",
      "Haojian Huang",
      "Xuran Ma",
      "Yexin Liu",
      "Wenjie Shu",
      "Yatian Pang",
      "Feilong Tang",
      "Qifeng Chen",
      "Harry Yang",
      "Ser-Nam Lim"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15133v1",
    "title": "EmoGRACE: Aspect-based emotion analysis for social media data",
    "abstract": "While sentiment analysis has advanced from sentence to aspect-level, i.e.,\nthe identification of concrete terms related to a sentiment, the equivalent\nfield of Aspect-based Emotion Analysis (ABEA) is faced with dataset bottlenecks\nand the increased complexity of emotion classes in contrast to binary\nsentiments. This paper addresses these gaps, by generating a first ABEA\ntraining dataset, consisting of 2,621 English Tweets, and fine-tuning a\nBERT-based model for the ABEA sub-tasks of Aspect Term Extraction (ATE) and\nAspect Emotion Classification (AEC).\n  The dataset annotation process was based on the hierarchical emotion theory\nby Shaver et al. [1] and made use of group annotation and majority voting\nstrategies to facilitate label consistency. The resulting dataset contained\naspect-level emotion labels for Anger, Sadness, Happiness, Fear, and a None\nclass. Using the new ABEA training dataset, the state-of-the-art ABSA model\nGRACE by Luo et al. [2] was fine-tuned for ABEA. The results reflected a\nperformance plateau at an F1-score of 70.1% for ATE and 46.9% for joint ATE and\nAEC extraction. The limiting factors for model performance were broadly\nidentified as the small training dataset size coupled with the increased task\ncomplexity, causing model overfitting and limited abilities to generalize well\non new data.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Christina Zorenböhmer",
      "Sebastian Schmidt",
      "Bernd Resch"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.16549v1",
    "title": "MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical Problems",
    "abstract": "Despite impressive performance across diverse tasks, Multimodal Large\nLanguage Models (MLLMs) have yet to fully demonstrate their potential in visual\nmathematical problem-solving, particularly in accurately perceiving and\ninterpreting diagrams. Inspired by typical processes of humans, we hypothesize\nthat the perception capabilities to extract meaningful information from\ndiagrams is crucial, as it directly impacts subsequent inference processes. To\nvalidate this hypothesis, we developed FlowVerse, a comprehensive benchmark\nthat categorizes all information used during problem-solving into four\ncomponents, which are then combined into six problem versions for evaluation.\nOur preliminary results on FlowVerse reveal that existing MLLMs exhibit\nsubstantial limitations when extracting essential information and reasoned\nproperty from diagrams and performing complex reasoning based on these visual\ninputs. In response, we introduce MathFlow, a modular problem-solving pipeline\nthat decouples perception and inference into distinct stages, thereby\noptimizing each independently. Given the perceptual limitations observed in\ncurrent MLLMs, we trained MathFlow-P-7B as a dedicated perception model.\nExperimental results indicate that MathFlow-P-7B yields substantial performance\ngains when integrated with various closed-source and open-source inference\nmodels. This demonstrates the effectiveness of the MathFlow pipeline and its\ncompatibility to diverse inference frameworks. The FlowVerse benchmark and code\nare available at https://github.com/MathFlow-zju/MathFlow.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Felix Chen",
      "Hangjie Yuan",
      "Yunqiu Xu",
      "Tao Feng",
      "Jun Cen",
      "Pengwei Liu",
      "Zeying Huang",
      "Yi Yang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15130v1",
    "title": "A Foundational Theory for Decentralized Sensory Learning",
    "abstract": "In both neuroscience and artificial intelligence, popular functional\nframeworks and neural network formulations operate by making use of extrinsic\nerror measurements and global learning algorithms. Through a set of conjectures\nbased on evolutionary insights on the origin of cellular adaptive mechanisms,\nwe reinterpret the core meaning of sensory signals to allow the brain to be\ninterpreted as a negative feedback control system, and show how this could lead\nto local learning algorithms without the need for global error correction\nmetrics. Thereby, a sufficiently good minima in sensory activity can be the\ncomplete reward signal of the network, as well as being both necessary and\nsufficient for biological learning to arise. We show that this method of\nlearning was likely already present in the earliest unicellular life forms on\nearth. We show evidence that the same principle holds and scales to\nmulticellular organisms where it in addition can lead to division of labour\nbetween cells. Available evidence shows that the evolution of the nervous\nsystem likely was an adaptation to more effectively communicate intercellular\nsignals to support such division of labour. We therefore propose that the same\nlearning principle that evolved already in the earliest unicellular life forms,\ni.e. negative feedback control of externally and internally generated sensor\nsignals, has simply been scaled up to become a fundament of the learning we see\nin biological brains today. We illustrate diverse biological settings, from the\nearliest unicellular organisms to humans, where this operational principle\nappears to be a plausible interpretation of the meaning of sensor signals in\nbiology, and how this relates to current neuroscientific theories and findings.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "authors": [
      "Linus Mårtensson",
      "Jonas M. D. Enander",
      "Udaya B. Rongala",
      "Henrik Jörntell"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15129v1",
    "title": "Aligning Crowd-sourced Human Feedback for Reinforcement Learning on Code Generation by Large Language Models",
    "abstract": "This paper studies how AI-assisted programming and large language models\n(LLM) improve software developers' ability via AI tools (LLM agents) like\nGithub Copilot and Amazon CodeWhisperer, while integrating human feedback to\nenhance reinforcement learning (RLHF) with crowd-sourced computation to enhance\ntext-to-code generation. Additionally, we demonstrate that our Bayesian\noptimization framework supports AI alignment in code generation by distributing\nthe feedback collection burden, highlighting the value of collecting human\nfeedback of good quality. Our empirical evaluations demonstrate the efficacy of\nthis approach, showcasing how LLM agents can be effectively trained for\nimproved text-to-code generation. Our Bayesian optimization framework can be\ndesigned for general domain-specific languages, promoting the alignment of\nlarge language model capabilities with human feedback in AI-assisted\nprogramming for code generation.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Man Fai Wong",
      "Chee Wei Tan"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15128v1",
    "title": "Increasing the Robustness of the Fine-tuned Multilingual Machine-Generated Text Detectors",
    "abstract": "Since the proliferation of LLMs, there have been concerns about their misuse\nfor harmful content creation and spreading. Recent studies justify such fears,\nproviding evidence of LLM vulnerabilities and high potential of their misuse.\nHumans are no longer able to distinguish between high-quality machine-generated\nand authentic human-written texts. Therefore, it is crucial to develop\nautomated means to accurately detect machine-generated content. It would enable\nto identify such content in online information space, thus providing an\nadditional information about its credibility. This work addresses the problem\nby proposing a robust fine-tuning process of LLMs for the detection task,\nmaking the detectors more robust against obfuscation and more generalizable to\nout-of-distribution data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Dominik Macko",
      "Robert Moro",
      "Ivan Srba"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15126v1",
    "title": "Text-Derived Relational Graph-Enhanced Network for Skeleton-Based Action Segmentation",
    "abstract": "Skeleton-based Temporal Action Segmentation (STAS) aims to segment and\nrecognize various actions from long, untrimmed sequences of human skeletal\nmovements. Current STAS methods typically employ spatio-temporal modeling to\nestablish dependencies among joints as well as frames, and utilize one-hot\nencoding with cross-entropy loss for frame-wise classification supervision.\nHowever, these methods overlook the intrinsic correlations among joints and\nactions within skeletal features, leading to a limited understanding of human\nmovements. To address this, we propose a Text-Derived Relational Graph-Enhanced\nNetwork (TRG-Net) that leverages prior graphs generated by Large Language\nModels (LLM) to enhance both modeling and supervision. For modeling, the\nDynamic Spatio-Temporal Fusion Modeling (DSFM) method incorporates Text-Derived\nJoint Graphs (TJG) with channel- and frame-level dynamic adaptation to\neffectively model spatial relations, while integrating spatio-temporal core\nfeatures during temporal modeling. For supervision, the Absolute-Relative\nInter-Class Supervision (ARIS) method employs contrastive learning between\naction features and text embeddings to regularize the absolute class\ndistributions, and utilizes Text-Derived Action Graphs (TAG) to capture the\nrelative inter-class relationships among action features. Additionally, we\npropose a Spatial-Aware Enhancement Processing (SAEP) method, which\nincorporates random joint occlusion and axial rotation to enhance spatial\ngeneralization. Performance evaluations on four public datasets demonstrate\nthat TRG-Net achieves state-of-the-art results.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Haoyu Ji",
      "Bowen Chen",
      "Weihong Ren",
      "Wenze Huang",
      "Zhihao Yang",
      "Zhiyong Wang",
      "Honghai Liu"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15574v1",
    "title": "Machine Learning Techniques for Multifactor Analysis of National Carbon Dioxide Emissions",
    "abstract": "This paper presents a comprehensive study leveraging Support Vector Machine\n(SVM) regression and Principal Component Regression (PCR) to analyze carbon\ndioxide emissions in a global dataset of 62 countries and their dependence on\nidiosyncratic, country-specific parameters. The objective is to understand the\nfactors contributing to carbon dioxide emissions and identify the most\npredictive elements. The analysis provides country-specific emission estimates,\nhighlighting diverse national trajectories and pinpointing areas for targeted\ninterventions in climate change mitigation, sustainable development, and the\ngrowing carbon credit markets and green finance sector. The study aims to\nsupport policymaking with accurate representations of carbon dioxide emissions,\noffering nuanced information for formulating effective strategies to address\nclimate change while informing initiatives related to carbon trading and\nenvironmentally sustainable investments.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Wenjia Xie",
      "Jinhui Li",
      "Kai Zong",
      "Luis Seco"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15573v1",
    "title": "Neuronal Activation States as Sample Embeddings for Data Selection in Task-Specific Instruction Tuning",
    "abstract": "Task-specific instruction tuning enhances the performance of large language\nmodels (LLMs) on specialized tasks, yet efficiently selecting relevant data for\nthis purpose remains a challenge. Inspired by neural coactivation in the human\nbrain, we propose a novel data selection method called NAS, which leverages\nneuronal activation states as embeddings for samples in the feature space.\nExtensive experiments show that NAS outperforms classical data selection\nmethods in terms of both effectiveness and robustness across different models,\ndatasets, and selection ratios.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Da Ma",
      "Gonghu Shang",
      "Zhi Chen",
      "Libo Qin",
      "Yijie Luo",
      "Lei Pan",
      "Shuai Fan",
      "Lu Chen",
      "Kai Yu"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15124v1",
    "title": "Evaluating ASR Confidence Scores for Automated Error Detection in User-Assisted Correction Interfaces",
    "abstract": "Despite advances in Automatic Speech Recognition (ASR), transcription errors\npersist and require manual correction. Confidence scores, which indicate the\ncertainty of ASR results, could assist users in identifying and correcting\nerrors. This study evaluates the reliability of confidence scores for error\ndetection through a comprehensive analysis of end-to-end ASR models and a user\nstudy with 36 participants. The results show that while confidence scores\ncorrelate with transcription accuracy, their error detection performance is\nlimited. Classifiers frequently miss errors or generate many false positives,\nundermining their practical utility. Confidence-based error detection neither\nimproved correction efficiency nor was perceived as helpful by participants.\nThese findings highlight the limitations of confidence scores and the need for\nmore sophisticated approaches to improve user interaction and explainability of\nASR results.",
    "categories": [
      "cs.HC",
      "cs.CL",
      "cs.SD",
      "eess.AS",
      "I.2.7"
    ],
    "authors": [
      "Korbinian Kuhn",
      "Verena Kersken",
      "Gottfried Zimmermann"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15117v1",
    "title": "Exploring Model Editing for LLM-based Aspect-Based Sentiment Classification",
    "abstract": "Model editing aims at selectively updating a small subset of a neural model's\nparameters with an interpretable strategy to achieve desired modifications. It\ncan significantly reduce computational costs to adapt to large language models\n(LLMs). Given its ability to precisely target critical components within LLMs,\nmodel editing shows great potential for efficient fine-tuning applications. In\nthis work, we investigate model editing to serve an efficient method for\nadapting LLMs to solve aspect-based sentiment classification. Through causal\ninterventions, we trace and determine which neuron hidden states are essential\nfor the prediction of the model. By performing interventions and restorations\non each component of an LLM, we identify the importance of these components for\naspect-based sentiment classification. Our findings reveal that a distinct set\nof mid-layer representations is essential for detecting the sentiment polarity\nof given aspect words. Leveraging these insights, we develop a model editing\napproach that focuses exclusively on these critical parts of the LLM, leading\nto a more efficient method for adapting LLMs. Our in-domain and out-of-domain\nexperiments demonstrate that this approach achieves competitive results\ncompared to the currently strongest methods with significantly fewer trainable\nparameters, highlighting a more efficient and interpretable fine-tuning\nstrategy.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Shichen Li",
      "Zhongqing Wang",
      "Zheyu Zhao",
      "Yue Zhang",
      "Peifeng Li"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15114v1",
    "title": "DeCaFlow: A Deconfounding Causal Generative Model",
    "abstract": "Causal generative models (CGMs) have recently emerged as capable approaches\nto simulate the causal mechanisms generating our observations, enabling causal\ninference. Unfortunately, existing approaches either are overly restrictive,\nassuming the absence of hidden confounders, or lack generality, being tailored\nto a particular query and graph. In this work, we introduce DeCaFlow, a CGM\nthat accounts for hidden confounders in a single amortized training process\nusing only observational data and the causal graph. Importantly, DeCaFlow can\nprovably identify all causal queries with a valid adjustment set or\nsufficiently informative proxy variables. Remarkably, for the first time to our\nknowledge, we show that a confounded counterfactual query is identifiable, and\nthus solvable by DeCaFlow, as long as its interventional counterpart is as\nwell. Our empirical results on diverse settings (including the Ecoli70 dataset,\nwith 3 independent hidden confounders, tens of observed variables and hundreds\nof causal queries) show that DeCaFlow outperforms existing approaches, while\ndemonstrating its out-of-the-box flexibility.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Alejandro Almodóvar",
      "Adrián Javaloy",
      "Juan Parras",
      "Santiago Zazo",
      "Isabel Valera"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15113v1",
    "title": "Reasoning Effort and Problem Complexity: A Scaling Analysis in LLMs",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable text generation\ncapabilities, and recent advances in training paradigms have led to\nbreakthroughs in their reasoning performance. In this work, we investigate how\nthe reasoning effort of such models scales with problem complexity. We use the\ninfinitely scalable Tents puzzle, which has a known linear-time solution, to\nanalyze this scaling behavior. Our results show that reasoning effort scales\nwith problem size, but only up to a critical problem complexity. Beyond this\nthreshold, the reasoning effort does not continue to increase, and may even\ndecrease. This observation highlights a critical limitation in the logical\ncoherence of current LLMs as problem complexity increases, and underscores the\nneed for strategies to improve reasoning scalability. Furthermore, our results\nreveal significant performance differences between current state-of-the-art\nreasoning models when faced with increasingly complex logical puzzles.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Benjamin Estermann",
      "Roger Wattenhofer"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15111v1",
    "title": "FedLWS: Federated Learning with Adaptive Layer-wise Weight Shrinking",
    "abstract": "In Federated Learning (FL), weighted aggregation of local models is conducted\nto generate a new global model, and the aggregation weights are typically\nnormalized to 1. A recent study identifies the global weight shrinking effect\nin FL, indicating an enhancement in the global model's generalization when the\nsum of weights (i.e., the shrinking factor) is smaller than 1, where how to\nlearn the shrinking factor becomes crucial. However, principled approaches to\nthis solution have not been carefully studied from the adequate consideration\nof privacy concerns and layer-wise distinctions. To this end, we propose a\nnovel model aggregation strategy, Federated Learning with Adaptive Layer-wise\nWeight Shrinking (FedLWS), which adaptively designs the shrinking factor in a\nlayer-wise manner and avoids optimizing the shrinking factors on a proxy\ndataset. We initially explored the factors affecting the shrinking factor\nduring the training process. Then we calculate the layer-wise shrinking factors\nby considering the distinctions among each layer of the global model. FedLWS\ncan be easily incorporated with various existing methods due to its\nflexibility. Extensive experiments under diverse scenarios demonstrate the\nsuperiority of our method over several state-of-the-art approaches, providing a\npromising tool for enhancing the global model in FL.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Changlong Shi",
      "Jinmeng Li",
      "He Zhao",
      "Dan dan Guo",
      "Yi Chang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15110v2",
    "title": "GIVEPose: Gradual Intra-class Variation Elimination for RGB-based Category-Level Object Pose Estimation",
    "abstract": "Recent advances in RGBD-based category-level object pose estimation have been\nlimited by their reliance on precise depth information, restricting their\nbroader applicability. In response, RGB-based methods have been developed.\nAmong these methods, geometry-guided pose regression that originated from\ninstance-level tasks has demonstrated strong performance. However, we argue\nthat the NOCS map is an inadequate intermediate representation for\ngeometry-guided pose regression method, as its many-to-one correspondence with\ncategory-level pose introduces redundant instance-specific information,\nresulting in suboptimal results. This paper identifies the intra-class\nvariation problem inherent in pose regression based solely on the NOCS map and\nproposes the Intra-class Variation-Free Consensus (IVFC) map, a novel\ncoordinate representation generated from the category-level consensus model. By\nleveraging the complementary strengths of the NOCS map and the IVFC map, we\nintroduce GIVEPose, a framework that implements Gradual Intra-class Variation\nElimination for category-level object pose estimation. Extensive evaluations on\nboth synthetic and real-world datasets demonstrate that GIVEPose significantly\noutperforms existing state-of-the-art RGB-based approaches, achieving\nsubstantial improvements in category-level object pose estimation. Our code is\navailable at https://github.com/ziqin-h/GIVEPose.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Zinqin Huang",
      "Gu Wang",
      "Chenyangguang Zhang",
      "Ruida Zhang",
      "Xiu Li",
      "Xiangyang Ji"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15108v1",
    "title": "VIPER: Visual Perception and Explainable Reasoning for Sequential Decision-Making",
    "abstract": "While Large Language Models (LLMs) excel at reasoning on text and\nVision-Language Models (VLMs) are highly effective for visual perception,\napplying those models for visual instruction-based planning remains a widely\nopen problem. In this paper, we introduce VIPER, a novel framework for\nmultimodal instruction-based planning that integrates VLM-based perception with\nLLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM\ngenerates textual descriptions of image observations, which are then processed\nby an LLM policy to predict actions based on the task goal. We fine-tune the\nreasoning module using behavioral cloning and reinforcement learning, improving\nour agent's decision-making capabilities. Experiments on the ALFWorld benchmark\nshow that VIPER significantly outperforms state-of-the-art visual\ninstruction-based planners while narrowing the gap with purely text-based\noracles. By leveraging text as an intermediate representation, VIPER also\nenhances explainability, paving the way for a fine-grained analysis of\nperception and reasoning components.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "authors": [
      "Mohamed Salim Aissi",
      "Clemence Grislain",
      "Mohamed Chetouani",
      "Olivier Sigaud",
      "Laure Soulier",
      "Nicolas Thome"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15107v1",
    "title": "Interpretability of Graph Neural Networks to Assert Effects of Global Change Drivers on Ecological Networks",
    "abstract": "Pollinators play a crucial role for plant reproduction, either in natural\necosystem or in human-modified landscape. Global change drivers,including\nclimate change or land use modifications, can alter the plant-pollinator\ninteractions. To assert the potential influence of global change drivers on\npollination, large-scale interactions, climate and land use data are required.\nWhile recent machine learning methods, such as graph neural networks (GNNs),\nallow the analysis of such datasets, interpreting their results can be\nchallenging. We explore existing methods for interpreting GNNs in order to\nhighlight the effects of various environmental covariates on pollination\nnetwork connectivity. A large simulation study is performed to confirm whether\nthese methods can detect the interactive effect between a covariate and a genus\nof plant on connectivity, and whether the application of debiasing techniques\ninfluences the estimation of these effects. An application on the Spipoll\ndataset, with and without accounting for sampling effects, highlights the\npotential impact of land use on network connectivity and shows that accounting\nfor sampling effects partially alters the estimation of these effects.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Emre Anakok",
      "Pierre Barbillon",
      "Colin Fontaine",
      "Elisa Thebault"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15106v2",
    "title": "Distilling 3D distinctive local descriptors for 6D pose estimation",
    "abstract": "Three-dimensional local descriptors are crucial for encoding geometric\nsurface properties, making them essential for various point cloud understanding\ntasks. Among these descriptors, GeDi has demonstrated strong zero-shot 6D pose\nestimation capabilities but remains computationally impractical for real-world\napplications due to its expensive inference process. Can we retain GeDi's\neffectiveness while significantly improving its efficiency? In this paper, we\nexplore this question by introducing a knowledge distillation framework that\ntrains an efficient student model to regress local descriptors from a GeDi\nteacher. Our key contributions include: an efficient large-scale training\nprocedure that ensures robustness to occlusions and partial observations while\noperating under compute and storage constraints, and a novel loss formulation\nthat handles weak supervision from non-distinctive teacher descriptors. We\nvalidate our approach on five BOP Benchmark datasets and demonstrate a\nsignificant reduction in inference time while maintaining competitive\nperformance with existing methods, bringing zero-shot 6D pose estimation closer\nto real-time feasibility. Project Website: https://tev-fbk.github.io/dGeDi/",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Amir Hamza",
      "Andrea Caraffa",
      "Davide Boscaini",
      "Fabio Poiesi"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15105v1",
    "title": "Control, Optimal Transport and Neural Differential Equations in Supervised Learning",
    "abstract": "From the perspective of control theory, neural differential equations (neural\nODEs) have become an important tool for supervised learning. In the fundamental\nwork of Ruiz-Balet and Zuazua (SIAM REVIEW 2023), the authors pose an open\nproblem regarding the connection between control theory, optimal transport\ntheory, and neural differential equations. More precisely, they inquire how one\ncan quantify the closeness of the optimal flows in neural transport equations\nto the true dynamic optimal transport. In this work, we propose a construction\nof neural differential equations that converge to the true dynamic optimal\ntransport in the limit, providing a significant step in solving the formerly\nmentioned open problem.",
    "categories": [
      "math.NA",
      "cs.LG",
      "cs.NA",
      "math.OC"
    ],
    "authors": [
      "Minh-Nhat Phung",
      "Minh-Binh Tran"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15571v1",
    "title": "LLM-Aided Customizable Profiling of Code Data Based On Programming Language Concepts",
    "abstract": "Data profiling is critical in machine learning for generating descriptive\nstatistics, supporting both deeper understanding and downstream tasks like data\nvaluation and curation. This work addresses profiling specifically in the\ncontext of code datasets for Large Language Models (code-LLMs), where data\nquality directly influences tasks such as code generation and summarization.\nCharacterizing code datasets in terms of programming language concepts enables\nbetter insights and targeted data curation. Our proposed methodology decomposes\ncode data profiling into two phases: (1) an offline phase where LLMs are\nleveraged to derive and learn rules for extracting syntactic and semantic\nconcepts across various programming languages, including previously unseen or\nlow-resource languages, and (2) an online deterministic phase applying these\nderived rules for efficient real-time analysis. This hybrid approach is\ncustomizable, extensible to new syntactic and semantic constructs, and scalable\nto multiple languages. Experimentally, our LLM-aided method achieves a mean\naccuracy of 90.33% for syntactic extraction rules and semantic classification\naccuracies averaging 80% and 77% across languages and semantic concepts,\nrespectively.",
    "categories": [
      "cs.SE",
      "cs.ET",
      "cs.IR",
      "cs.LG",
      "cs.PL"
    ],
    "authors": [
      "Pankaj Thorat",
      "Adnan Qidwai",
      "Adrija Dhar",
      "Aishwariya Chakraborty",
      "Anand Eswaran",
      "Hima Patel",
      "Praveen Jayachandran"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15096v1",
    "title": "When the Future Becomes the Past: Taming Temporal Correspondence for Self-supervised Video Representation Learning",
    "abstract": "The past decade has witnessed notable achievements in self-supervised\nlearning for video tasks. Recent efforts typically adopt the Masked Video\nModeling (MVM) paradigm, leading to significant progress on multiple video\ntasks. However, two critical challenges remain: 1) Without human annotations,\nthe random temporal sampling introduces uncertainty, increasing the difficulty\nof model training. 2) Previous MVM methods primarily recover the masked patches\nin the pixel space, leading to insufficient information compression for\ndownstream tasks. To address these challenges jointly, we propose a\nself-supervised framework that leverages Temporal Correspondence for video\nRepresentation learning (T-CoRe). For challenge 1), we propose a sandwich\nsampling strategy that selects two auxiliary frames to reduce reconstruction\nuncertainty in a two-side-squeezing manner. Addressing challenge 2), we\nintroduce an auxiliary branch into a self-distillation architecture to restore\nrepresentations in the latent space, generating high-level semantic\nrepresentations enriched with temporal information. Experiments of T-CoRe\nconsistently present superior performance across several downstream tasks,\ndemonstrating its effectiveness for video representation learning. The code is\navailable at https://github.com/yafeng19/T-CORE.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yang Liu",
      "Qianqian Xu",
      "Peisong Wen",
      "Siran Dai",
      "Qingming Huang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15095v1",
    "title": "Diffusion-Based Forecasting for Uncertainty-Aware Model Predictive Control",
    "abstract": "We propose Diffusion-Informed Model Predictive Control (D-I MPC), a generic\nframework for uncertainty-aware prediction and decision-making in partially\nobservable stochastic systems by integrating diffusion-based time series\nforecasting models in Model Predictive Control algorithms. In our approach, a\ndiffusion-based time series forecasting model is used to probabilistically\nestimate the evolution of the system's stochastic components. These forecasts\nare then incorporated into MPC algorithms to estimate future trajectories and\noptimize action selection under the uncertainty of the future. We evaluate the\nframework on the task of energy arbitrage, where a Battery Energy Storage\nSystem participates in the day-ahead electricity market of the New York state.\nExperimental results indicate that our model-based approach with a\ndiffusion-based forecaster significantly outperforms both implementations with\nclassical forecasting methods and model-free reinforcement learning baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "I.2.6; I.5.1"
    ],
    "authors": [
      "Stelios Zarifis",
      "Ioannis Kordonis",
      "Petros Maragos"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15092v1",
    "title": "Towards Understanding the Safety Boundaries of DeepSeek Models: Evaluation and Findings",
    "abstract": "This study presents the first comprehensive safety evaluation of the DeepSeek\nmodels, focusing on evaluating the safety risks associated with their generated\ncontent. Our evaluation encompasses DeepSeek's latest generation of large\nlanguage models, multimodal large language models, and text-to-image models,\nsystematically examining their performance regarding unsafe content generation.\nNotably, we developed a bilingual (Chinese-English) safety evaluation dataset\ntailored to Chinese sociocultural contexts, enabling a more thorough evaluation\nof the safety capabilities of Chinese-developed models. Experimental results\nindicate that despite their strong general capabilities, DeepSeek models\nexhibit significant safety vulnerabilities across multiple risk dimensions,\nincluding algorithmic discrimination and sexual content. These findings provide\ncrucial insights for understanding and improving the safety of large foundation\nmodels. Our code is available at\nhttps://github.com/NY1024/DeepSeek-Safety-Eval.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Zonghao Ying",
      "Guangyi Zheng",
      "Yongxin Huang",
      "Deyue Zhang",
      "Wenxin Zhang",
      "Quanchen Zou",
      "Aishan Liu",
      "Xianglong Liu",
      "Dacheng Tao"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15091v1",
    "title": "Intelligent Spatial Perception by Building Hierarchical 3D Scene Graphs for Indoor Scenarios with the Help of LLMs",
    "abstract": "This paper addresses the high demand in advanced intelligent robot navigation\nfor a more holistic understanding of spatial environments, by introducing a\nnovel system that harnesses the capabilities of Large Language Models (LLMs) to\nconstruct hierarchical 3D Scene Graphs (3DSGs) for indoor scenarios. The\nproposed framework constructs 3DSGs consisting of a fundamental layer with rich\nmetric-semantic information, an object layer featuring precise point-cloud\nrepresentation of object nodes as well as visual descriptors, and higher layers\nof room, floor, and building nodes. Thanks to the innovative application of\nLLMs, not only object nodes but also nodes of higher layers, e.g., room nodes,\nare annotated in an intelligent and accurate manner. A polling mechanism for\nroom classification using LLMs is proposed to enhance the accuracy and\nreliability of the room node annotation. Thorough numerical experiments\ndemonstrate the system's ability to integrate semantic descriptions with\ngeometric data, creating an accurate and comprehensive representation of the\nenvironment instrumental for context-aware navigation and task planning.",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Yao Cheng",
      "Zhe Han",
      "Fengyang Jiang",
      "Huaizhen Wang",
      "Fengyu Zhou",
      "Qingshan Yin",
      "Lei Wei"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15089v1",
    "title": "Continual Contrastive Learning on Tabular Data with Out of Distribution",
    "abstract": "Out-of-distribution (OOD) prediction remains a significant challenge in\nmachine learning, particularly for tabular data where traditional methods often\nfail to generalize beyond their training distribution. This paper introduces\nTabular Continual Contrastive Learning (TCCL), a novel framework designed to\naddress OOD challenges in tabular data processing. TCCL integrates contrastive\nlearning principles with continual learning mechanisms, featuring a\nthree-component architecture: an Encoder for data transformation, a Decoder for\nrepresentation learning, and a Learner Head. We evaluate TCCL against 14\nbaseline models, including state-of-the-art deep learning approaches and\ngradient-boosted decision trees (GBDT), across eight diverse tabular datasets.\nOur experimental results demonstrate that TCCL consistently outperforms\nexisting methods in both classification and regression tasks on OOD data, with\nparticular strength in handling distribution shifts. These findings suggest\nthat TCCL represents a significant advancement in handling OOD scenarios for\ntabular data.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Achmad Ginanjar",
      "Xue Li",
      "Priyanka Singh",
      "Wen Hua"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15087v1",
    "title": "An Investigation of Beam Density on LiDAR Object Detection Performance",
    "abstract": "Accurate 3D object detection is a critical component of autonomous driving,\nenabling vehicles to perceive their surroundings with precision and make\ninformed decisions. LiDAR sensors, widely used for their ability to provide\ndetailed 3D measurements, are key to achieving this capability. However,\nvariations between training and inference data can cause significant\nperformance drops when object detection models are employed in different sensor\nsettings. One critical factor is beam density, as inference on sparse,\ncost-effective LiDAR sensors is often preferred in real-world applications.\nDespite previous work addressing the beam-density-induced domain gap,\nsubstantial knowledge gaps remain, particularly concerning dense 128-beam\nsensors in cross-domain scenarios. To gain better understanding of the impact\nof beam density on domain gaps, we conduct a comprehensive investigation that\nincludes an evaluation of different object detection architectures. Our\narchitecture evaluation reveals that combining voxel- and point-based\napproaches yields superior cross-domain performance by leveraging the strengths\nof both representations. Building on these findings, we analyze\nbeam-density-induced domain gaps and argue that these domain gaps must be\nevaluated in conjunction with other domain shifts. Contrary to conventional\nbeliefs, our experiments reveal that detectors benefit from training on denser\ndata and exhibit robustness to beam density variations during inference.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Christoph Griesbacher",
      "Christian Fruhwirth-Reisinger"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15082v1",
    "title": "StyleLoco: Generative Adversarial Distillation for Natural Humanoid Robot Locomotion",
    "abstract": "Humanoid robots are anticipated to acquire a wide range of locomotion\ncapabilities while ensuring natural movement across varying speeds and\nterrains. Existing methods encounter a fundamental dilemma in learning humanoid\nlocomotion: reinforcement learning with handcrafted rewards can achieve agile\nlocomotion but produces unnatural gaits, while Generative Adversarial Imitation\nLearning (GAIL) with motion capture data yields natural movements but suffers\nfrom unstable training processes and restricted agility. Integrating these\napproaches proves challenging due to the inherent heterogeneity between expert\npolicies and human motion datasets. To address this, we introduce StyleLoco, a\nnovel two-stage framework that bridges this gap through a Generative\nAdversarial Distillation (GAD) process. Our framework begins by training a\nteacher policy using reinforcement learning to achieve agile and dynamic\nlocomotion. It then employs a multi-discriminator architecture, where distinct\ndiscriminators concurrently extract skills from both the teacher policy and\nmotion capture data. This approach effectively combines the agility of\nreinforcement learning with the natural fluidity of human-like movements while\nmitigating the instability issues commonly associated with adversarial\ntraining. Through extensive simulation and real-world experiments, we\ndemonstrate that StyleLoco enables humanoid robots to perform diverse\nlocomotion tasks with the precision of expertly trained policies and the\nnatural aesthetics of human motion, successfully transferring styles across\ndifferent movement types while maintaining stable locomotion across a broad\nspectrum of command inputs.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Le Ma",
      "Ziyu Meng",
      "Tengyu Liu",
      "Yuhan Li",
      "Ran Song",
      "Wei Zhang",
      "Siyuan Huang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15070v1",
    "title": "MultiBARF: Integrating Imagery of Different Wavelength Regions by Using Neural Radiance Fields",
    "abstract": "Optical sensor applications have become popular through digital\ntransformation. Linking observed data to real-world locations and combining\ndifferent image sensors is essential to make the applications practical and\nefficient. However, data preparation to try different sensor combinations\nrequires high sensing and image processing expertise. To make data preparation\neasier for users unfamiliar with sensing and image processing, we have\ndeveloped MultiBARF. This method replaces the co-registration and geometric\ncalibration by synthesizing pairs of two different sensor images and depth\nimages at assigned viewpoints. Our method extends Bundle Adjusting Neural\nRadiance Fields(BARF), a deep neural network-based novel view synthesis method,\nfor the two imagers. Through experiments on visible light and thermographic\nimages, we demonstrate that our method superimposes two color channels of those\nsensor images on NeRF.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Kana Kurata",
      "Hitoshi Niigaki",
      "Xiaojun Wu",
      "Ryuichi Tanida"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15060v2",
    "title": "Conjuring Positive Pairs for Efficient Unification of Representation Learning and Image Synthesis",
    "abstract": "While representation learning and generative modeling seek to understand\nvisual data, unifying both domains remains unexplored. Recent Unified\nSelf-Supervised Learning (SSL) methods have started to bridge the gap between\nboth paradigms. However, they rely solely on semantic token reconstruction,\nwhich requires an external tokenizer during training -- introducing a\nsignificant overhead. In this work, we introduce Sorcen, a novel unified SSL\nframework, incorporating a synergic Contrastive-Reconstruction objective. Our\nContrastive objective, \"Echo Contrast\", leverages the generative capabilities\nof Sorcen, eliminating the need for additional image crops or augmentations\nduring training. Sorcen \"generates\" an echo sample in the semantic token space,\nforming the contrastive positive pair. Sorcen operates exclusively on\nprecomputed tokens, eliminating the need for an online token transformation\nduring training, thereby significantly reducing computational overhead.\nExtensive experiments on ImageNet-1k demonstrate that Sorcen outperforms the\nprevious Unified SSL SoTA by 0.4%, 1.48 FID, 1.76%, and 1.53% on linear\nprobing, unconditional image generation, few-shot learning, and transfer\nlearning, respectively, while being 60.8% more efficient. Additionally, Sorcen\nsurpasses previous single-crop MIM SoTA in linear probing and achieves SoTA\nperformance in unconditional image generation, highlighting significant\nimprovements and breakthroughs in Unified SSL models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.5.4; I.5.1; I.2.10"
    ],
    "authors": [
      "Imanol G. Estepa",
      "Jesús M. Rodríguez-de-Vera",
      "Ignacio Sarasúa",
      "Bhalaji Nagarajan",
      "Petia Radeva"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.15058v1",
    "title": "Texture-Aware StarGAN for CT data harmonisation",
    "abstract": "Computed Tomography (CT) plays a pivotal role in medical diagnosis; however,\nvariability across reconstruction kernels hinders data-driven approaches, such\nas deep learning models, from achieving reliable and generalized performance.\nTo this end, CT data harmonization has emerged as a promising solution to\nminimize such non-biological variances by standardizing data across different\nsources or conditions. In this context, Generative Adversarial Networks (GANs)\nhave proved to be a powerful framework for harmonization, framing it as a\nstyle-transfer problem. However, GAN-based approaches still face limitations in\ncapturing complex relationships within the images, which are essential for\neffective harmonization. In this work, we propose a novel texture-aware StarGAN\nfor CT data harmonization, enabling one-to-many translations across different\nreconstruction kernels. Although the StarGAN model has been successfully\napplied in other domains, its potential for CT data harmonization remains\nunexplored. Furthermore, our approach introduces a multi-scale texture loss\nfunction that embeds texture information across different spatial and angular\nscales into the harmonization process, effectively addressing kernel-induced\ntexture variations. We conducted extensive experimentation on a publicly\navailable dataset, utilizing a total of 48667 chest CT slices from 197 patients\ndistributed over three different reconstruction kernels, demonstrating the\nsuperiority of our method over the baseline StarGAN.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Francesco Di Feola",
      "Ludovica Pompilio",
      "Cecilia Assolito",
      "Valerio Guarrasi",
      "Paolo Soda"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15057v1",
    "title": "A Data-driven Investigation of Euphemistic Language: Comparing the usage of \"slave\" and \"servant\" in 19th century US newspapers",
    "abstract": "This study investigates the usage of \"slave\" and \"servant\" in the 19th\ncentury US newspapers using computational methods. While both terms were used\nto refer to enslaved African Americans, they were used in distinct ways. In the\nChronicling America corpus, we included possible OCR errors by using FastText\nembedding and excluded text reprints to consider text reprint culture in the\n19th century. Word2vec embedding was used to find semantically close words to\n\"slave\" and \"servant\" and log-odds ratio was calculated to identify\nover-represented discourse words in the Southern and Northern newspapers. We\nfound that \"slave\" is associated with socio-economic, legal, and administrative\nwords, however, \"servant\" is linked to religious words in the Northern\nnewspapers while Southern newspapers associated \"servant\" with domestic and\nfamilial words. We further found that slave discourse words in Southern\nnewspapers are more prevalent in Northern newspapers while servant discourse\nwords from each side are prevalent in their own region. This study contributes\nto the understanding of how newspapers created different discourses around\nenslaved African Americans in the 19th century US.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Jaihyun Park",
      "Ryan Cordell"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15056v1",
    "title": "Single-Step Bidirectional Unpaired Image Translation Using Implicit Bridge Consistency Distillation",
    "abstract": "Unpaired image-to-image translation has seen significant progress since the\nintroduction of CycleGAN. However, methods based on diffusion models or\nSchr\\\"odinger bridges have yet to be widely adopted in real-world applications\ndue to their iterative sampling nature. To address this challenge, we propose a\nnovel framework, Implicit Bridge Consistency Distillation (IBCD), which enables\nsingle-step bidirectional unpaired translation without using adversarial loss.\nIBCD extends consistency distillation by using a diffusion implicit bridge\nmodel that connects PF-ODE trajectories between distributions. Additionally, we\nintroduce two key improvements: 1) distribution matching for consistency\ndistillation and 2) adaptive weighting method based on distillation difficulty.\nExperimental results demonstrate that IBCD achieves state-of-the-art\nperformance on benchmark datasets in a single generation step. Project page\navailable at https://hyn2028.github.io/project_page/IBCD/index.html",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Suhyeon Lee",
      "Kwanyoung Kim",
      "Jong Chul Ye"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15055v1",
    "title": "ELTEX: A Framework for Domain-Driven Synthetic Data Generation",
    "abstract": "We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework\nfor generating high-quality synthetic training data in specialized domains.\nWhile Large Language Models (LLMs) have shown impressive general capabilities,\ntheir performance in specialized domains like cybersecurity remains limited by\nthe scarcity of domain-specific training data. ELTEX addresses this challenge\nby systematically integrating explicit domain indicator extraction with dynamic\nprompting to preserve critical domain knowledge throughout the generation\nprocess. We demonstrate ELTEX's effectiveness in the context of\nblockchain-related cyberattack detection, where we fine-tune Gemma-2B using\nvarious combinations of real and ELTEX-generated data. Our results show that\nthe ELTEX-enhanced model achieves performance competitive with GPT-4 across\nboth standard classification metrics and uncertainty calibration, while\nrequiring significantly fewer computational resources. We release a curated\nsynthetic dataset of social media texts for cyberattack detection in\nblockchain. Our work demonstrates that domain-driven synthetic data generation\ncan effectively bridge the performance gap between resource-efficient models\nand larger architectures in specialized domains.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Arina Razmyslovich",
      "Kseniia Murasheva",
      "Sofia Sedlova",
      "Julien Capitaine",
      "Eugene Dmitriev"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15049v1",
    "title": "HAD-Gen: Human-like and Diverse Driving Behavior Modeling for Controllable Scenario Generation",
    "abstract": "Simulation-based testing has emerged as an essential tool for verifying and\nvalidating autonomous vehicles (AVs). However, contemporary methodologies, such\nas deterministic and imitation learning-based driver models, struggle to\ncapture the variability of human-like driving behavior. Given these challenges,\nwe propose HAD-Gen, a general framework for realistic traffic scenario\ngeneration that simulates diverse human-like driving behaviors. The framework\nfirst clusters the vehicle trajectory data into different driving styles\naccording to safety features. It then employs maximum entropy inverse\nreinforcement learning on each of the clusters to learn the reward function\ncorresponding to each driving style. Using these reward functions, the method\nintegrates offline reinforcement learning pre-training and multi-agent\nreinforcement learning algorithms to obtain general and robust driving\npolicies. Multi-perspective simulation results show that our proposed scenario\ngeneration framework can simulate diverse, human-like driving behaviors with\nstrong generalization capability. The proposed framework achieves a 90.96%\ngoal-reaching rate, an off-road rate of 2.08%, and a collision rate of 6.91% in\nthe generalization test, outperforming prior approaches by over 20% in\ngoal-reaching performance. The source code is released at\nhttps://github.com/RoboSafe-Lab/Sim4AD.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "authors": [
      "Cheng Wang",
      "Lingxin Kong",
      "Massimiliano Tamborski",
      "Stefano V. Albrecht"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15044v1",
    "title": "SPADE: Systematic Prompt Framework for Automated Dialogue Expansion in Machine-Generated Text Detection",
    "abstract": "The increasing capability of large language models (LLMs) to generate\nsynthetic content has heightened concerns about their misuse, driving the\ndevelopment of Machine-Generated Text (MGT) detection models. However, these\ndetectors face significant challenges due to the lack of systematically\ngenerated, high-quality datasets for training. To address this issue, we\npropose five novel data augmentation frameworks for synthetic user dialogue\ngeneration through a structured prompting approach, reducing the costs\nassociated with traditional data collection methods. Our proposed method yields\n14 new dialogue datasets, which we benchmark against seven MGT detection\nmodels. The results demonstrate improved generalization performance when\nutilizing a mixed dataset produced by our proposed augmentation framework.\nFurthermore, considering that real-world agents lack knowledge of future\nopponent utterances, we simulate online dialogue detection and examine the\nrelationship between chat history length and detection accuracy. We also\nbenchmark online detection performance with limited chat history on our\nframeworks. Our open-source datasets can be downloaded from\nhttps://github.com/AngieYYF/SPADE-customer-service-dialogue.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Haoyi Li",
      "Angela Yifei Yuan",
      "Soyeon Caren Han",
      "Christopher Leckie"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15569v1",
    "title": "RAG-based User Profiling for Precision Planning in Mixed-precision Over-the-Air Federated Learning",
    "abstract": "Mixed-precision computing, a widely applied technique in AI, offers a larger\ntrade-off space between accuracy and efficiency. The recent purposed\nMixed-Precision Over-the-Air Federated Learning (MP-OTA-FL) enables clients to\noperate at appropriate precision levels based on their heterogeneous hardware,\ntaking advantages of the larger trade-off space while covering the quantization\noverheads in the mixed-precision modulation scheme for the OTA aggregation\nprocess. A key to further exploring the potential of the MP-OTA-FL framework is\nthe optimization of client precision levels. The choice of precision level\nhinges on multifaceted factors including hardware capability, potential client\ncontribution, and user satisfaction, among which factors can be difficult to\ndefine or quantify.\n  In this paper, we propose a RAG-based User Profiling for precision planning\nframework that integrates retrieval-augmented LLMs and dynamic client profiling\nto optimize satisfaction and contributions. This includes a hybrid interface\nfor gathering device/user insights and an RAG database storing historical\nquantization decisions with feedback. Experiments show that our method boosts\nsatisfaction, energy savings, and global model accuracy in MP-OTA-FL systems.",
    "categories": [
      "cs.LG",
      "cs.HC"
    ],
    "authors": [
      "Jinsheng Yuan",
      "Yun Tang",
      "Weisi Guo"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15036v1",
    "title": "Multivariate Gaussian Topic Modelling: A novel approach to discover topics with greater semantic coherence",
    "abstract": "An important aspect of text mining involves information retrieval in form of\ndiscovery of semantic themes (topics) from documents using topic modelling.\nWhile generative topic models like Latent Dirichlet Allocation (LDA) elegantly\nmodel topics as probability distributions and are useful in identifying latent\ntopics from large document corpora with minimal supervision, they suffer from\ndifficulty in topic interpretability and reduced performance in shorter texts.\nHere we propose a novel Multivariate Gaussian Topic modelling (MGD) approach.\nIn this approach topics are presented as Multivariate Gaussian Distributions\nand documents as Gaussian Mixture Models. Using EM algorithm, the various\nconstituent Multivariate Gaussian Distributions and their corresponding\nparameters are identified. Analysis of the parameters helps identify the\nkeywords having the highest variance and mean contributions to the topic, and\nfrom these key-words topic annotations are carried out. This approach is first\napplied on a synthetic dataset to demonstrate the interpretability benefits\nvis-\\`a-vis LDA. A real-world application of this topic model is demonstrated\nin analysis of risks and hazards at a petrochemical plant by applying the model\non safety incident reports to identify the major latent hazards plaguing the\nplant. This model achieves a higher mean topic coherence of 0.436 vis-\\`a-vis\n0.294 for LDA.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Satyajeet Sahoo",
      "Jhareswar Maiti",
      "Virendra Kumar Tewari"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15035v1",
    "title": "GraspCorrect: Robotic Grasp Correction via Vision-Language Model-Guided Feedback",
    "abstract": "Despite significant advancements in robotic manipulation, achieving\nconsistent and stable grasping remains a fundamental challenge, often limiting\nthe successful execution of complex tasks. Our analysis reveals that even\nstate-of-the-art policy models frequently exhibit unstable grasping behaviors,\nleading to failure cases that create bottlenecks in real-world robotic\napplications. To address these challenges, we introduce GraspCorrect, a\nplug-and-play module designed to enhance grasp performance through\nvision-language model-guided feedback. GraspCorrect employs an iterative visual\nquestion-answering framework with two key components: grasp-guided prompting,\nwhich incorporates task-specific constraints, and object-aware sampling, which\nensures the selection of physically feasible grasp candidates. By iteratively\ngenerating intermediate visual goals and translating them into joint-level\nactions, GraspCorrect significantly improves grasp stability and consistently\nenhances task success rates across existing policy models in the RLBench and\nCALVIN datasets.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "authors": [
      "Sungjae Lee",
      "Yeonjoo Hong",
      "Kwang In Kim"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15029v1",
    "title": "DRoPE: Directional Rotary Position Embedding for Efficient Agent Interaction Modeling",
    "abstract": "Accurate and efficient modeling of agent interactions is essential for\ntrajectory generation, the core of autonomous driving systems. Existing\nmethods, scene-centric, agent-centric, and query-centric frameworks, each\npresent distinct advantages and drawbacks, creating an impossible triangle\namong accuracy, computational time, and memory efficiency. To break this\nlimitation, we propose Directional Rotary Position Embedding (DRoPE), a novel\nadaptation of Rotary Position Embedding (RoPE), originally developed in natural\nlanguage processing. Unlike traditional relative position embedding (RPE),\nwhich introduces significant space complexity, RoPE efficiently encodes\nrelative positions without explicitly increasing complexity but faces inherent\nlimitations in handling angular information due to periodicity. DRoPE overcomes\nthis limitation by introducing a uniform identity scalar into RoPE's 2D rotary\ntransformation, aligning rotation angles with realistic agent headings to\nnaturally encode relative angular information. We theoretically analyze DRoPE's\ncorrectness and efficiency, demonstrating its capability to simultaneously\noptimize trajectory generation accuracy, time complexity, and space complexity.\nEmpirical evaluations compared with various state-of-the-art trajectory\ngeneration models, confirm DRoPE's good performance and significantly reduced\nspace complexity, indicating both theoretical soundness and practical\neffectiveness. The video documentation is available at\nhttps://drope-traj.github.io/.",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Jianbo Zhao",
      "Taiyu Ban",
      "Zhihao Liu",
      "Hangning Zhou",
      "Xiyang Wang",
      "Qibin Zhou",
      "Hailong Qin",
      "Mu Yang",
      "Lei Liu",
      "Bin Li"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15024v1",
    "title": "Forensics-Bench: A Comprehensive Forgery Detection Benchmark Suite for Large Vision Language Models",
    "abstract": "Recently, the rapid development of AIGC has significantly boosted the\ndiversities of fake media spread in the Internet, posing unprecedented threats\nto social security, politics, law, and etc. To detect the ever-increasingly\ndiverse malicious fake media in the new era of AIGC, recent studies have\nproposed to exploit Large Vision Language Models (LVLMs) to design robust\nforgery detectors due to their impressive performance on a wide range of\nmultimodal tasks. However, it still lacks a comprehensive benchmark designed to\ncomprehensively assess LVLMs' discerning capabilities on forgery media. To fill\nthis gap, we present Forensics-Bench, a new forgery detection evaluation\nbenchmark suite to assess LVLMs across massive forgery detection tasks,\nrequiring comprehensive recognition, location and reasoning capabilities on\ndiverse forgeries. Forensics-Bench comprises 63,292 meticulously curated\nmulti-choice visual questions, covering 112 unique forgery detection types from\n5 perspectives: forgery semantics, forgery modalities, forgery tasks, forgery\ntypes and forgery models. We conduct thorough evaluations on 22 open-sourced\nLVLMs and 3 proprietary models GPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet,\nhighlighting the significant challenges of comprehensive forgery detection\nposed by Forensics-Bench. We anticipate that Forensics-Bench will motivate the\ncommunity to advance the frontier of LVLMs, striving for all-around forgery\ndetectors in the era of AIGC. The deliverables will be updated at\nhttps://Forensics-Bench.github.io/.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jin Wang",
      "Chenghui Lv",
      "Xian Li",
      "Shichao Dong",
      "Huadong Li",
      "kelu Yao",
      "Chao Li",
      "Wenqi Shao",
      "Ping Luo"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15023v1",
    "title": "Bridging the Gap: Fusing CNNs and Transformers to Decode the Elegance of Handwritten Arabic Script",
    "abstract": "Handwritten Arabic script recognition is a challenging task due to the\nscript's dynamic letter forms and contextual variations. This paper proposes a\nhybrid approach combining convolutional neural networks (CNNs) and\nTransformer-based architectures to address these complexities. We evaluated\ncustom and fine-tuned models, including EfficientNet-B7 and Vision Transformer\n(ViT-B16), and introduced an ensemble model that leverages confidence-based\nfusion to integrate their strengths. Our ensemble achieves remarkable\nperformance on the IFN/ENIT dataset, with 96.38% accuracy for letter\nclassification and 97.22% for positional classification. The results highlight\nthe complementary nature of CNNs and Transformers, demonstrating their combined\npotential for robust Arabic handwriting recognition. This work advances OCR\nsystems, offering a scalable solution for real-world applications.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Chaouki Boufenar",
      "Mehdi Ayoub Rabiai",
      "Boualem Nadjib Zahaf",
      "Khelil Rafik Ouaras"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15022v1",
    "title": "xMOD: Cross-Modal Distillation for 2D/3D Multi-Object Discovery from 2D motion",
    "abstract": "Object discovery, which refers to the task of localizing objects without\nhuman annotations, has gained significant attention in 2D image analysis.\nHowever, despite this growing interest, it remains under-explored in 3D data,\nwhere approaches rely exclusively on 3D motion, despite its several challenges.\nIn this paper, we present a novel framework that leverages advances in 2D\nobject discovery which are based on 2D motion to exploit the advantages of such\nmotion cues being more flexible and generalizable and to bridge the gap between\n2D and 3D modalities. Our primary contributions are twofold: (i) we introduce\nDIOD-3D, the first baseline for multi-object discovery in 3D data using 2D\nmotion, incorporating scene completion as an auxiliary task to enable dense\nobject localization from sparse input data; (ii) we develop xMOD, a cross-modal\ntraining framework that integrates 2D and 3D data while always using 2D motion\ncues. xMOD employs a teacher-student training paradigm across the two\nmodalities to mitigate confirmation bias by leveraging the domain gap. During\ninference, the model supports both RGB-only and point cloud-only inputs.\nAdditionally, we propose a late-fusion technique tailored to our pipeline that\nfurther enhances performance when both modalities are available at inference.\nWe evaluate our approach extensively on synthetic (TRIP-PD) and challenging\nreal-world datasets (KITTI and Waymo). Notably, our approach yields a\nsubstantial performance improvement compared with the 2D object discovery\nstate-of-the-art on all datasets with gains ranging from +8.7 to +15.1 in F1@50\nscore. The code is available at https://github.com/CEA-LIST/xMOD",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Saad Lahlali",
      "Sandra Kara",
      "Hejer Ammar",
      "Florian Chabot",
      "Nicolas Granger",
      "Hervé Le Borgne",
      "Quoc-Cuong Pham"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15568v1",
    "title": "Mixed precision accumulation for neural network inference guided by componentwise forward error analysis",
    "abstract": "This work proposes a mathematically founded mixed precision accumulation\nstrategy for the inference of neural networks. Our strategy is based on a new\ncomponentwise forward error analysis that explains the propagation of errors in\nthe forward pass of neural networks. Specifically, our analysis shows that the\nerror in each component of the output of a layer is proportional to the\ncondition number of the inner product between the weights and the input,\nmultiplied by the condition number of the activation function. These condition\nnumbers can vary widely from one component to the other, thus creating a\nsignificant opportunity to introduce mixed precision: each component should be\naccumulated in a precision inversely proportional to the product of these\ncondition numbers. We propose a practical algorithm that exploits this\nobservation: it first computes all components in low precision, uses this\noutput to estimate the condition numbers, and recomputes in higher precision\nonly the components associated with large condition numbers. We test our\nalgorithm on various networks and datasets and confirm experimentally that it\ncan significantly improve the cost--accuracy tradeoff compared with uniform\nprecision accumulation baselines.",
    "categories": [
      "cs.LG",
      "cs.NA",
      "math.NA"
    ],
    "authors": [
      "El-Mehdi El Arar",
      "Silviu-Ioan Filip",
      "Theo Mary",
      "Elisa Riccietti"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15019v1",
    "title": "Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene",
    "abstract": "The latest emerged 4D Panoptic Scene Graph (4D-PSG) provides an advanced-ever\nrepresentation for comprehensively modeling the dynamic 4D visual real world.\nUnfortunately, current pioneering 4D-PSG research can primarily suffer from\ndata scarcity issues severely, as well as the resulting out-of-vocabulary\nproblems; also, the pipeline nature of the benchmark generation method can lead\nto suboptimal performance. To address these challenges, this paper investigates\na novel framework for 4D-PSG generation that leverages rich 2D visual scene\nannotations to enhance 4D scene learning. First, we introduce a 4D Large\nLanguage Model (4D-LLM) integrated with a 3D mask decoder for end-to-end\ngeneration of 4D-PSG. A chained SG inference mechanism is further designed to\nexploit LLMs' open-vocabulary capabilities to infer accurate and comprehensive\nobject and relation labels iteratively. Most importantly, we propose a 2D-to-4D\nvisual scene transfer learning framework, where a spatial-temporal scene\ntranscending strategy effectively transfers dimension-invariant features from\nabundant 2D SG annotations to 4D scenes, effectively compensating for data\nscarcity in 4D-PSG. Extensive experiments on the benchmark data demonstrate\nthat we strikingly outperform baseline models by a large margin, highlighting\nthe effectiveness of our method.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Shengqiong Wu",
      "Hao Fei",
      "Jingkang Yang",
      "Xiangtai Li",
      "Juncheng Li",
      "Hanwang Zhang",
      "Tat-seng Chua"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15017v1",
    "title": "Exploiting Diffusion Prior for Real-World Image Dehazing with Unpaired Training",
    "abstract": "Unpaired training has been verified as one of the most effective paradigms\nfor real scene dehazing by learning from unpaired real-world hazy and clear\nimages. Although numerous studies have been proposed, current methods\ndemonstrate limited generalization for various real scenes due to limited\nfeature representation and insufficient use of real-world prior. Inspired by\nthe strong generative capabilities of diffusion models in producing both hazy\nand clear images, we exploit diffusion prior for real-world image dehazing, and\npropose an unpaired framework named Diff-Dehazer. Specifically, we leverage\ndiffusion prior as bijective mapping learners within the CycleGAN, a classic\nunpaired learning framework. Considering that physical priors contain pivotal\nstatistics information of real-world data, we further excavate real-world\nknowledge by integrating physical priors into our framework. Furthermore, we\nintroduce a new perspective for adequately leveraging the representation\nability of diffusion models by removing degradation in image and text\nmodalities, so as to improve the dehazing effect. Extensive experiments on\nmultiple real-world datasets demonstrate the superior performance of our\nmethod. Our code https://github.com/ywxjm/Diff-Dehazer.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yunwei Lan",
      "Zhigao Cui",
      "Chang Liu",
      "Jialun Peng",
      "Nian Wang",
      "Xin Luo",
      "Dong Liu"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15016v1",
    "title": "Manifold Learning for Hyperspectral Images",
    "abstract": "Traditional feature extraction and projection techniques, such as Principal\nComponent Analysis, struggle to adequately represent X-Ray Transmission (XRT)\nMulti-Energy (ME) images, limiting the performance of neural networks in\ndecision-making processes. To address this issue, we propose a method that\napproximates the dataset topology by constructing adjacency graphs using the\nUniform Manifold Approximation and Projection. This approach captures nonlinear\ncorrelations within the data, significantly improving the performance of\nmachine learning algorithms, particularly in processing Hyperspectral Images\n(HSI) from X-ray transmission spectroscopy. This technique not only preserves\nthe global structure of the data but also enhances feature separability,\nleading to more accurate and robust classification results.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Fethi Harkat",
      "Tiphaine Deuberet",
      "Guillaume Gey",
      "Valérie Perrier",
      "Kévin Polisano"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15013v1",
    "title": "Ambient Noise Full Waveform Inversion with Neural Operators",
    "abstract": "Numerical simulations of seismic wave propagation are crucial for\ninvestigating velocity structures and improving seismic hazard assessment.\nHowever, standard methods such as finite difference or finite element are\ncomputationally expensive. Recent studies have shown that a new class of\nmachine learning models, called neural operators, can solve the elastodynamic\nwave equation orders of magnitude faster than conventional methods. Full\nwaveform inversion is a prime beneficiary of the accelerated simulations.\nNeural operators, as end-to-end differentiable operators, combined with\nautomatic differentiation, provide an alternative approach to the adjoint-state\nmethod. Since neural operators do not involve the Born approximation, when used\nfor full waveform inversion they have the potential to include additional\nphases and alleviate cycle-skipping problems present in traditional\nadjoint-state formulations. In this study, we demonstrate the application of\nneural operators for full waveform inversion on a real seismic dataset, which\nconsists of several nodal transects collected across the San Gabriel, Chino,\nand San Bernardino basins in the Los Angeles metropolitan area.",
    "categories": [
      "physics.geo-ph",
      "cs.LG"
    ],
    "authors": [
      "Caifeng Zou",
      "Zachary E. Ross",
      "Robert W. Clayton",
      "Fan-Chi Lin",
      "Kamyar Azizzadenesheli"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15008v1",
    "title": "A Novel Channel Boosted Residual CNN-Transformer with Regional-Boundary Learning for Breast Cancer Detection",
    "abstract": "Recent advancements in detecting tumors using deep learning on breast\nultrasound images (BUSI) have demonstrated significant success. Deep CNNs and\nvision-transformers (ViTs) have demonstrated individually promising initial\nperformance. However, challenges related to model complexity and contrast,\ntexture, and tumor morphology variations introduce uncertainties that hinder\nthe effectiveness of current methods. This study introduces a novel hybrid\nframework, CB-Res-RBCMT, combining customized residual CNNs and new ViT\ncomponents for detailed BUSI cancer analysis. The proposed RBCMT uses stem\nconvolution blocks with CNN Meet Transformer (CMT) blocks, followed by new\nRegional and boundary (RB) feature extraction operations for capturing contrast\nand morphological variations. Moreover, the CMT block incorporates global\ncontextual interactions through multi-head attention, enhancing computational\nefficiency with a lightweight design. Additionally, the customized inverse\nresidual and stem CNNs within the CMT effectively extract local texture\ninformation and handle vanishing gradients. Finally, the new channel-boosted\n(CB) strategy enriches the feature diversity of the limited dataset by\ncombining the original RBCMT channels with transfer learning-based residual\nCNN-generated maps. These diverse channels are processed through a spatial\nattention block for optimal pixel selection, reducing redundancy and improving\nthe discrimination of minor contrast and texture variations. The proposed\nCB-Res-RBCMT achieves an F1-score of 95.57%, accuracy of 95.63%, sensitivity of\n96.42%, and precision of 94.79% on the standard harmonized stringent BUSI\ndataset, outperforming existing ViT and CNN methods. These results demonstrate\nthe versatility of our integrated CNN-Transformer framework in capturing\ndiverse features and delivering superior performance in BUSI cancer diagnosis.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Aamir Mehmood",
      "Yue Hu",
      "Saddam Hussain Khan"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15567v1",
    "title": "Towards Unified Latent Space for 3D Molecular Latent Diffusion Modeling",
    "abstract": "3D molecule generation is crucial for drug discovery and material science,\nrequiring models to process complex multi-modalities, including atom types,\nchemical bonds, and 3D coordinates. A key challenge is integrating these\nmodalities of different shapes while maintaining SE(3) equivariance for 3D\ncoordinates. To achieve this, existing approaches typically maintain separate\nlatent spaces for invariant and equivariant modalities, reducing efficiency in\nboth training and sampling. In this work, we propose \\textbf{U}nified\nVariational \\textbf{A}uto-\\textbf{E}ncoder for \\textbf{3D} Molecular Latent\nDiffusion Modeling (\\textbf{UAE-3D}), a multi-modal VAE that compresses 3D\nmolecules into latent sequences from a unified latent space, while maintaining\nnear-zero reconstruction error. This unified latent space eliminates the\ncomplexities of handling multi-modality and equivariance when performing latent\ndiffusion modeling. We demonstrate this by employing the Diffusion\nTransformer--a general-purpose diffusion model without any molecular inductive\nbias--for latent generation. Extensive experiments on GEOM-Drugs and QM9\ndatasets demonstrate that our method significantly establishes new benchmarks\nin both \\textit{de novo} and conditional 3D molecule generation, achieving\nleading efficiency and quality.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Yanchen Luo",
      "Zhiyuan Liu",
      "Yi Zhao",
      "Sihang Li",
      "Kenji Kawaguchi",
      "Tat-Seng Chua",
      "Xiang Wang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15005v1",
    "title": "Universal Scene Graph Generation",
    "abstract": "Scene graph (SG) representations can neatly and efficiently describe scene\nsemantics, which has driven sustained intensive research in SG generation. In\nthe real world, multiple modalities often coexist, with different types, such\nas images, text, video, and 3D data, expressing distinct characteristics.\nUnfortunately, current SG research is largely confined to single-modality scene\nmodeling, preventing the full utilization of the complementary strengths of\ndifferent modality SG representations in depicting holistic scene semantics. To\nthis end, we introduce Universal SG (USG), a novel representation capable of\nfully characterizing comprehensive semantic scenes from any given combination\nof modality inputs, encompassing modality-invariant and modality-specific\nscenes. Further, we tailor a niche-targeting USG parser, USG-Par, which\neffectively addresses two key bottlenecks of cross-modal object alignment and\nout-of-domain challenges. We design the USG-Par with modular architecture for\nend-to-end USG generation, in which we devise an object associator to relieve\nthe modality gap for cross-modal object alignment. Further, we propose a\ntext-centric scene contrasting learning mechanism to mitigate domain imbalances\nby aligning multimodal objects and relations with textual SGs. Through\nextensive experiments, we demonstrate that USG offers a stronger capability for\nexpressing scene semantics than standalone SGs, and also that our USG-Par\nachieves higher efficacy and performance.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Shengqiong Wu",
      "Hao Fei",
      "Tat-Seng Chua"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15004v1",
    "title": "Semantic Segmentation of Transparent and Opaque Drinking Glasses with the Help of Zero-shot Learning",
    "abstract": "Segmenting transparent structures in images is challenging since they are\ndifficult to distinguish from the background. Common examples are drinking\nglasses, which are a ubiquitous part of our lives and appear in many different\nshapes and sizes. In this work we propose TransCaGNet, a modified version of\nthe zero-shot model CaGNet. We exchange the segmentation backbone with the\narchitecture of Trans4Trans to be capable of segmenting transparent objects.\nSince some glasses are rarely captured, we use zeroshot learning to be able to\ncreate semantic segmentations of glass categories not given during training. We\npropose a novel synthetic dataset covering a diverse set of different\nenvironmental conditions. Additionally we capture a real-world evaluation\ndataset since most applications take place in the real world. Comparing our\nmodel with Zeg-Clip we are able to show that TransCaGNet produces better mean\nIoU and accuracy values while ZegClip outperforms it mostly for unseen classes.\nTo improve the segmentation results, we combine the semantic segmentation of\nthe models with the segmentation results of SAM 2. Our evaluation emphasizes\nthat distinguishing between different classes is challenging for the models due\nto similarity, points of view, or coverings. Taking this behavior into account,\nwe assign glasses multiple possible categories. The modification leads to an\nimprovement up to 13.68% for the mean IoU and up to 17.88% for the mean\naccuracy values on the synthetic dataset. Using our difficult synthetic dataset\nfor training, the models produce even better results on the real-world dataset.\nThe mean IoU is improved up to 5.55% and the mean accuracy up to 5.72% on the\nreal-world dataset.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Annalena Blänsdorf",
      "Tristan Wirth",
      "Arne Rak",
      "Thomas Pöllabauer",
      "Volker Knauthe",
      "Arjan Kuijper"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15003v1",
    "title": "LLM Alignment for the Arabs: A Homogenous Culture or Diverse Ones?",
    "abstract": "Large language models (LLMs) have the potential of being useful tools that\ncan automate tasks and assist humans. However, these models are more fluent in\nEnglish and more aligned with Western cultures, norms, and values.\nArabic-specific LLMs are being developed to better capture the nuances of the\nArabic language, as well as the views of the Arabs. Yet, Arabs are sometimes\nassumed to share the same culture. In this position paper, I discuss the\nlimitations of this assumption and provide preliminary thoughts for how to\nbuild systems that can better represent the cultural diversity within the Arab\nworld. The invalidity of the cultural homogeneity assumption might seem\nobvious, yet, it is widely adopted in developing multilingual and\nArabic-specific LLMs. I hope that this paper will encourage the NLP community\nto be considerate of the cultural diversity within various communities speaking\nthe same language.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Amr Keleg"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15002v1",
    "title": "Scalable Trajectory-User Linking with Dual-Stream Representation Networks",
    "abstract": "Trajectory-user linking (TUL) aims to match anonymous trajectories to the\nmost likely users who generated them, offering benefits for a wide range of\nreal-world spatio-temporal applications. However, existing TUL methods are\nlimited by high model complexity and poor learning of the effective\nrepresentations of trajectories, rendering them ineffective in handling\nlarge-scale user trajectory data. In this work, we propose a novel\n$\\underline{Scal}$abl$\\underline{e}$ Trajectory-User Linking with dual-stream\nrepresentation networks for large-scale $\\underline{TUL}$ problem, named\nScaleTUL. Specifically, ScaleTUL generates two views using temporal and spatial\naugmentations to exploit supervised contrastive learning framework to\neffectively capture the irregularities of trajectories. In each view, a\ndual-stream trajectory encoder, consisting of a long-term encoder and a\nshort-term encoder, is designed to learn unified trajectory representations\nthat fuse different temporal-spatial dependencies. Then, a TUL layer is used to\nassociate the trajectories with the corresponding users in the representation\nspace using a two-stage training model. Experimental results on check-in\nmobility datasets from three real-world cities and the nationwide U.S.\ndemonstrate the superiority of ScaleTUL over state-of-the-art baselines for\nlarge-scale TUL tasks.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Hao Zhang",
      "Wei Chen",
      "Xingyu Zhao",
      "Jianpeng Qi",
      "Guiyuan Jiang",
      "Yanwei Yu"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15001v1",
    "title": "Low-Complexity Patch-based No-Reference Point Cloud Quality Metric exploiting Weighted Structure and Texture Features",
    "abstract": "During the compression, transmission, and rendering of point clouds, various\nartifacts are introduced, affecting the quality perceived by the end user.\nHowever, evaluating the impact of these distortions on the overall quality is a\nchallenging task. This study introduces PST-PCQA, a no-reference point cloud\nquality metric based on a low-complexity, learning-based framework. It\nevaluates point cloud quality by analyzing individual patches, integrating\nlocal and global features to predict the Mean Opinion Score. In summary, the\nprocess involves extracting features from patches, combining them, and using\ncorrelation weights to predict the overall quality. This approach allows us to\nassess point cloud quality without relying on a reference point cloud, making\nit particularly useful in scenarios where reference data is unavailable.\nExperimental tests on three state-of-the-art datasets show good prediction\ncapabilities of PST-PCQA, through the analysis of different feature pooling\nstrategies and its ability to generalize across different datasets. The\nablation study confirms the benefits of evaluating quality on a patch-by-patch\nbasis. Additionally, PST-PCQA's light-weight structure, with a small number of\nparameters to learn, makes it well-suited for real-time applications and\ndevices with limited computational capacity. For reproducibility purposes, we\nmade code, model, and pretrained weights available at\nhttps://github.com/michaelneri/PST-PCQA.",
    "categories": [
      "cs.CV",
      "cs.MM",
      "eess.IV"
    ],
    "authors": [
      "Michael Neri",
      "Federica Battisti"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14998v1",
    "title": "TGV: Tabular Data-Guided Learning of Visual Cardiac Representations",
    "abstract": "Contrastive learning methods in computer vision typically rely on different\nviews of the same image to form pairs. However, in medical imaging, we often\nseek to compare entire patients with different phenotypes rather than just\nmultiple augmentations of one scan. We propose harnessing clinically relevant\ntabular data to identify distinct patient phenotypes and form more meaningful\npairs in a contrastive learning framework. Our method uses tabular attributes\nto guide the training of visual representations, without requiring a joint\nembedding space. We demonstrate its strength using short-axis cardiac MR images\nand clinical attributes from the UK Biobank, where tabular data helps to more\neffectively distinguish between patient subgroups. Evaluation on downstream\ntasks, including fine-tuning and zero-shot prediction of cardiovascular artery\ndiseases and cardiac phenotypes, shows that incorporating tabular data yields\nstronger visual representations than conventional methods that rely solely on\nimage augmentations or combined image-tabular embeddings. Furthermore, we\ndemonstrate that image encoders trained with tabular guidance are capable of\nembedding demographic information in their representations, allowing them to\nuse insights from tabular data for unimodal predictions, making them\nwell-suited to real-world medical settings where extensive clinical annotations\nmay not be routinely available at inference time. The code will be available on\nGitHub.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Marta Hasny",
      "Maxime Di Folco",
      "Keno Bressem",
      "Julia Schnabel"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.16547v1",
    "title": "Empowering Medical Multi-Agents with Clinical Consultation Flow for Dynamic Diagnosis",
    "abstract": "Traditional AI-based healthcare systems often rely on single-modal data,\nlimiting diagnostic accuracy due to incomplete information. However, recent\nadvancements in foundation models show promising potential for enhancing\ndiagnosis combining multi-modal information. While these models excel in static\ntasks, they struggle with dynamic diagnosis, failing to manage multi-turn\ninteractions and often making premature diagnostic decisions due to\ninsufficient persistence in information collection.To address this, we propose\na multi-agent framework inspired by consultation flow and reinforcement\nlearning (RL) to simulate the entire consultation process, integrating multiple\nclinical information for effective diagnosis. Our approach incorporates a\nhierarchical action set, structured from clinic consultation flow and medical\ntextbook, to effectively guide the decision-making process. This strategy\nimproves agent interactions, enabling them to adapt and optimize actions based\non the dynamic state. We evaluated our framework on a public dynamic diagnosis\nbenchmark. The proposed framework evidentially improves the baseline methods\nand achieves state-of-the-art performance compared to existing foundation\nmodel-based methods.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "authors": [
      "Sihan Wang",
      "Suiyang Jiang",
      "Yibo Gao",
      "Boming Wang",
      "Shangqi Gao",
      "Xiahai Zhuang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14996v1",
    "title": "Right Answer, Wrong Score: Uncovering the Inconsistencies of LLM Evaluation in Multiple-Choice Question Answering",
    "abstract": "One of the most widely used tasks to evaluate Large Language Models (LLMs) is\nMultiple-Choice Question Answering (MCQA). While open-ended question answering\ntasks are more challenging to evaluate, MCQA tasks are, in principle, easier to\nassess, as the model's answer is thought to be simple to extract and is\ndirectly compared to a set of predefined choices. However, recent studies have\nstarted to question the reliability of MCQA evaluation, showing that multiple\nfactors can significantly impact the reported performance of LLMs, especially\nwhen the model generates free-form text before selecting one of the answer\nchoices. In this work, we shed light on the inconsistencies of MCQA evaluation\nstrategies, which can lead to inaccurate and misleading model comparisons. We\nsystematically analyze whether existing answer extraction methods are aligned\nwith human judgment, and how they are influenced by answer constraints in the\nprompt across different domains. Our experiments demonstrate that traditional\nevaluation strategies often underestimate LLM capabilities, while LLM-based\nanswer extractors are prone to systematic errors. Moreover, we reveal a\nfundamental trade-off between including format constraints in the prompt to\nsimplify answer extraction and allowing models to generate free-form text to\nimprove reasoning. Our findings call for standardized evaluation methodologies\nand highlight the need for more reliable and consistent MCQA evaluation\npractices.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Francesco Maria Molfese",
      "Luca Moroni",
      "Luca Gioffrè",
      "Alessandro Scirè",
      "Simone Conia",
      "Roberto Navigli"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.16546v1",
    "title": "A Comprehensive Survey on Architectural Advances in Deep CNNs: Challenges, Applications, and Emerging Research Directions",
    "abstract": "Deep Convolutional Neural Networks (CNNs) have significantly advanced deep\nlearning, driving breakthroughs in computer vision, natural language\nprocessing, medical diagnosis, object detection, and speech recognition.\nArchitectural innovations including 1D, 2D, and 3D convolutional models,\ndilated and grouped convolutions, depthwise separable convolutions, and\nattention mechanisms address domain-specific challenges and enhance feature\nrepresentation and computational efficiency. Structural refinements such as\nspatial-channel exploitation, multi-path design, and feature-map enhancement\ncontribute to robust hierarchical feature extraction and improved\ngeneralization, particularly through transfer learning. Efficient preprocessing\nstrategies, including Fourier transforms, structured transforms, low-precision\ncomputation, and weight compression, optimize inference speed and facilitate\ndeployment in resource-constrained environments. This survey presents a unified\ntaxonomy that classifies CNN architectures based on spatial exploitation,\nmulti-path structures, depth, width, dimensionality expansion, channel\nboosting, and attention mechanisms. It systematically reviews CNN applications\nin face recognition, pose estimation, action recognition, text classification,\nstatistical language modeling, disease diagnosis, radiological analysis,\ncryptocurrency sentiment prediction, 1D data processing, video analysis, and\nspeech recognition. In addition to consolidating architectural advancements,\nthe review highlights emerging learning paradigms such as few-shot, zero-shot,\nweakly supervised, federated learning frameworks and future research directions\ninclude hybrid CNN-transformer models, vision-language integration, generative\nlearning, etc. This review provides a comprehensive perspective on CNN's\nevolution from 2015 to 2025, outlining key innovations, challenges, and\nopportunities.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "authors": [
      "Saddam Hussain Khan",
      "Rashid Iqbal"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14991v1",
    "title": "Inspecting the Representation Manifold of Differentially-Private Text",
    "abstract": "Differential Privacy (DP) for text has recently taken the form of text\nparaphrasing using language models and temperature sampling to better balance\nprivacy and utility. However, the geometric distortion of DP regarding the\nstructure and complexity in the representation space remains unexplored. By\nestimating the intrinsic dimension of paraphrased text across varying privacy\nbudgets, we find that word-level methods severely raise the representation\nmanifold, while sentence-level methods produce paraphrases whose manifolds are\ntopologically more consistent with human-written paraphrases. Among\nsentence-level methods, masked paraphrasing, compared to causal paraphrasing,\ndemonstrates superior preservation of structural complexity, suggesting that\nautoregressive generation propagates distortions from unnatural word choices\nthat cascade and inflate the representation space.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Stefan Arnold"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14990v1",
    "title": "Disentangling Modes and Interference in the Spectrogram of Multicomponent Signals",
    "abstract": "In this paper, we investigate how the spectrogram of multicomponent signals\ncan be decomposed into a mode part and an interference part. We explore two\napproaches: (i) a variational method inspired by texture-geometry decomposition\nin image processing, and (ii) a supervised learning approach using a U-Net\narchitecture, trained on a dataset encompassing diverse interference patterns\nand noise conditions. Once the interference component is identified, we explain\nhow it enables us to define a criterion to locally adapt the window length used\nin the definition of the spectrogram, for the sake of improving ridge detection\nin the presence of close modes. Numerical experiments illustrate the advantages\nand limitations of both approaches for spectrogram decomposition, highlighting\ntheir potential for enhancing time-frequency analysis in the presence of strong\ninterference.",
    "categories": [
      "cs.CV",
      "eess.SP"
    ],
    "authors": [
      "Kévin Polisano",
      "Sylvain Meignen",
      "Nils Laurent",
      "Hubert Leterme"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14985v1",
    "title": "ML-Triton, A Multi-Level Compilation and Language Extension to Triton GPU Programming",
    "abstract": "In the era of LLMs, dense operations such as GEMM and MHA are critical\ncomponents. These operations are well-suited for parallel execution using a\ntilebased approach. While traditional GPU programming often relies on low level\ninterfaces like CUDA or SYCL, Triton has emerged as a DSL that offers a more\nuser-friendly and portable alternative by programming at a higher level. The\ncurrent Triton starts at the workgroup (aka threadblock) level, and directly\nlowers to per-thread level. And then attempt to coalesce and amend through a\nseries of passes, promoting information from low-level representation. We\nbelieve this is pre-mature lowering based on the below observations. 1. GPU has\na hierarchical structure both physically and logically. Modern GPUs often\nfeature SIMD units capable of directly operating on tiles on a warp or\nwarpgroup basis, such as blocked load and blocked MMA. 2. Multi-level gradual\nlowering can make compiler decoupled and clean by separating considerations\ninter and intra a logical layer. 3. Kernel developers often need fine control\nto get good performance on the latest hardware. FlashAttention2 advocates\nexplicit data partition between warps to make a performance boost. In this\ncontext, we propose ML-Triton which features multi-level compilation flow and\nprogramming interface. Our approach begins at the workgroup level and\nprogressively lowers to the warp and intrinsic level, implementing a multilevel\nlowering align with the hierarchical nature of GPU. Additionally, we extend\ntriton language to support user-set compiler hint and warp level programming,\nenabling researchers to get good out-of-the box performance without awaiting\ncompiler updates. Experimental results demonstrate that our approach achieves\nperformance above 95% of expert-written kernels on Intel GPU, as measured by\nthe geometric mean.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Dewei Wang",
      "Wei Zhu",
      "Liyang Ling",
      "Ettore Tiotto",
      "Quintin Wang",
      "Whitney Tsang",
      "Julian Opperman",
      "Jacky Deng"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.16545v1",
    "title": "EmpathyAgent: Can Embodied Agents Conduct Empathetic Actions?",
    "abstract": "Empathy is fundamental to human interactions, yet it remains unclear whether\nembodied agents can provide human-like empathetic support. Existing works have\nstudied agents' tasks solving and social interactions abilities, but whether\nagents can understand empathetic needs and conduct empathetic behaviors remains\noverlooked. To address this, we introduce EmpathyAgent, the first benchmark to\nevaluate and enhance agents' empathetic actions across diverse scenarios.\nEmpathyAgent contains 10,000 multimodal samples with corresponding empathetic\ntask plans and three different challenges. To systematically evaluate the\nagents' empathetic actions, we propose an empathy-specific evaluation suite\nthat evaluates the agents' empathy process. We benchmark current models and\nfound that exhibiting empathetic actions remains a significant challenge.\nMeanwhile, we train Llama3-8B using EmpathyAgent and find it can potentially\nenhance empathetic behavior. By establishing a standard benchmark for\nevaluating empathetic actions, we hope to advance research in empathetic\nembodied agents. Our code and data are publicly available at\nhttps://github.com/xinyan-cxy/EmpathyAgent.",
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "authors": [
      "Xinyan Chen",
      "Jiaxin Ge",
      "Hongming Dai",
      "Qiang Zhou",
      "Qiuxuan Feng",
      "Jingtong Hu",
      "Yizhou Wang",
      "Jiaming Liu",
      "Shanghang Zhang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14983v1",
    "title": "Semi-KAN: KAN Provides an Effective Representation for Semi-Supervised Learning in Medical Image Segmentation",
    "abstract": "Deep learning-based medical image segmentation has shown remarkable success;\nhowever, it typically requires extensive pixel-level annotations, which are\nboth expensive and time-intensive. Semi-supervised medical image segmentation\n(SSMIS) offers a viable alternative, driven by advancements in CNNs and ViTs.\nHowever, these networks often rely on single fixed activation functions and\nlinear modeling patterns, limiting their ability to effectively learn robust\nrepresentations. Given the limited availability of labeled date, achieving\nrobust representation learning becomes crucial. Inspired by Kolmogorov-Arnold\nNetworks (KANs), we propose Semi-KAN, which leverages the untapped potential of\nKANs to enhance backbone architectures for representation learning in SSMIS.\nOur findings indicate that: (1) compared to networks with fixed activation\nfunctions, KANs exhibit superior representation learning capabilities with\nfewer parameters, and (2) KANs excel in high-semantic feature spaces. Building\non these insights, we integrate KANs into tokenized intermediate\nrepresentations, applying them selectively at the encoder's bottleneck and the\ndecoder's top layers within a U-Net pipeline to extract high-level semantic\nfeatures. Although learnable activation functions improve feature expansion,\nthey introduce significant computational overhead with only marginal\nperformance gains. To mitigate this, we reduce the feature dimensions and\nemploy horizontal scaling to capture multiple pattern representations.\nFurthermore, we design a multi-branch U-Net architecture with uncertainty\nestimation to effectively learn diverse pattern representations. Extensive\nexperiments on four public datasets demonstrate that Semi-KAN surpasses\nbaseline networks, utilizing fewer KAN layers and lower computational cost,\nthereby underscoring the potential of KANs as a promising approach for SSMIS.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Zanting Ye",
      "Xiaolong Niu",
      "Xuanbin Wu",
      "Wenxiang Yi",
      "Yuan Chang",
      "Lijun Lu"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14980v1",
    "title": "Embedding spatial context in urban traffic forecasting with contrastive pre-training",
    "abstract": "Urban traffic forecasting is a commonly encountered problem, with\nwide-ranging applications in fields such as urban planning, civil engineering\nand transport. In this paper, we study the enhancement of traffic forecasting\nwith pre-training, focusing on spatio-temporal graph methods. While various\nmachine learning methods to solve traffic forecasting problems have been\nexplored and extensively studied, there is a gap of a more contextual approach:\nstudying how relevant non-traffic data can improve prediction performance on\ntraffic forecasting problems. We call this data spatial context. We introduce a\nnovel method of combining road and traffic information through the notion of a\ntraffic quotient graph, a quotient graph formed from road geometry and traffic\nsensors. We also define a way to encode this relationship in the form of a\ngeometric encoder, pre-trained using contrastive learning methods and enhanced\nwith OpenStreetMap data. We introduce and discuss ways to integrate this\ngeometric encoder with existing graph neural network (GNN)-based traffic\nforecasting models, using a contrastive pre-training paradigm. We demonstrate\nthe potential for this hybrid model to improve generalisation and performance\nwith zero additional traffic data. Code for this paper is available at\nhttps://github.com/mattchrlw/forecasting-on-new-roads.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Matthew Low",
      "Arian Prabowo",
      "Hao Xue",
      "Flora Salim"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14979v1",
    "title": "One-Shot Medical Video Object Segmentation via Temporal Contrastive Memory Networks",
    "abstract": "Video object segmentation is crucial for the efficient analysis of complex\nmedical video data, yet it faces significant challenges in data availability\nand annotation. We introduce the task of one-shot medical video object\nsegmentation, which requires separating foreground and background pixels\nthroughout a video given only the mask annotation of the first frame. To\naddress this problem, we propose a temporal contrastive memory network\ncomprising image and mask encoders to learn feature representations, a temporal\ncontrastive memory bank that aligns embeddings from adjacent frames while\npushing apart distant ones to explicitly model inter-frame relationships and\nstores these features, and a decoder that fuses encoded image features and\nmemory readouts for segmentation. We also collect a diverse, multi-source\nmedical video dataset spanning various modalities and anatomies to benchmark\nthis task. Extensive experiments demonstrate state-of-the-art performance in\nsegmenting both seen and unseen structures from a single exemplar, showing\nability to generalize from scarce labels. This highlights the potential to\nalleviate annotation burdens for medical video analysis. Code is available at\nhttps://github.com/MedAITech/TCMN.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yaxiong Chen",
      "Junjian Hu",
      "Chunlei Li",
      "Zixuan Zheng",
      "Jingliang Hu",
      "Yilei Shi",
      "Shengwu Xiong",
      "Xiao Xiang Zhu",
      "Lichao Mou"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14976v2",
    "title": "Application of linear regression method to the deep reinforcement learning in continuous action cases",
    "abstract": "The linear regression (LR) method offers the advantage that optimal\nparameters can be calculated relatively easily, although its representation\ncapability is limited than that of the deep learning technique. To improve deep\nreinforcement learning, the Least Squares Deep Q Network (LS-DQN) method was\nproposed by Levine et al., which combines Deep Q Network (DQN) with LR method.\nHowever, the LS-DQN method assumes that the actions are discrete. In this\nstudy, we propose the Double Least Squares Deep Deterministic Policy Gradient\n(DLS-DDPG) method to address this limitation. This method combines the LR\nmethod with the Deep Deterministic Policy Gradient (DDPG) technique, one of the\nrepresentative deep reinforcement learning algorithms for continuous action\ncases. Numerical experiments conducted in MuJoCo environments showed that the\nLR update improved performance at least in some tasks, although there are\ndifficulties such as the inability to make the regularization terms small.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Hisato Komatsu"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.14975v1",
    "title": "Taming Flow Matching with Unbalanced Optimal Transport into Fast Pansharpening",
    "abstract": "Pansharpening, a pivotal task in remote sensing for fusing high-resolution\npanchromatic and multispectral imagery, has garnered significant research\ninterest. Recent advancements employing diffusion models based on stochastic\ndifferential equations (SDEs) have demonstrated state-of-the-art performance.\nHowever, the inherent multi-step sampling process of SDEs imposes substantial\ncomputational overhead, hindering practical deployment. While existing methods\nadopt efficient samplers, knowledge distillation, or retraining to reduce\nsampling steps (e.g., from 1,000 to fewer steps), such approaches often\ncompromise fusion quality. In this work, we propose the Optimal Transport Flow\nMatching (OTFM) framework, which integrates the dual formulation of unbalanced\noptimal transport (UOT) to achieve one-step, high-quality pansharpening. Unlike\nconventional OT formulations that enforce rigid distribution alignment, UOT\nrelaxes marginal constraints to enhance modeling flexibility, accommodating the\nintrinsic spectral and spatial disparities in remote sensing data. Furthermore,\nwe incorporate task-specific regularization into the UOT objective, enhancing\nthe robustness of the flow model. The OTFM framework enables simulation-free\ntraining and single-step inference while maintaining strict adherence to\npansharpening constraints. Experimental evaluations across multiple datasets\ndemonstrate that OTFM matches or exceeds the performance of previous\nregression-based models and leading diffusion-based methods while only needing\none sampling step. Codes are available at https://github.com/294coder/PAN-OTFM.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Zihan Cao",
      "Yu Zhong",
      "Liang-Jian Deng"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14974v1",
    "title": "Language-based Image Colorization: A Benchmark and Beyond",
    "abstract": "Image colorization aims to bring colors back to grayscale images. Automatic\nimage colorization methods, which requires no additional guidance, struggle to\ngenerate high-quality images due to color ambiguity, and provides limited user\ncontrollability. Thanks to the emergency of cross-modality datasets and models,\nlanguage-based colorization methods are proposed to fully utilize the\nefficiency and flexibly of text descriptions to guide colorization. In view of\nthe lack of a comprehensive review of language-based colorization literature,\nwe conduct a thorough analysis and benchmarking. We first briefly summarize\nexisting automatic colorization methods. Then, we focus on language-based\nmethods and point out their core challenge on cross-modal alignment. We further\ndivide these methods into two categories: one attempts to train a\ncross-modality network from scratch, while the other utilizes the pre-trained\ncross-modality model to establish the textual-visual correspondence. Based on\nthe analyzed limitations of existing language-based methods, we propose a\nsimple yet effective method based on distilled diffusion model. Extensive\nexperiments demonstrate that our simple baseline can produces better results\nthan previous complex methods with 14 times speed up. To the best of our\nknowledge, this is the first comprehensive review and benchmark on\nlanguage-based image colorization field, providing meaningful insights for the\ncommunity. The code is available at https://github.com/lyf1212/Color-Turbo.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yifan Li",
      "Shuai Yang",
      "Jiaying Liu"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14973v1",
    "title": "Behaviour Discovery and Attribution for Explainable Reinforcement Learning",
    "abstract": "Explaining the decisions made by reinforcement learning (RL) agents is\ncritical for building trust and ensuring reliability in real-world\napplications. Traditional approaches to explainability often rely on saliency\nanalysis, which can be limited in providing actionable insights. Recently,\nthere has been growing interest in attributing RL decisions to specific\ntrajectories within a dataset. However, these methods often generalize\nexplanations to long trajectories, potentially involving multiple distinct\nbehaviors. Often, providing multiple more fine grained explanations would\nimprove clarity. In this work, we propose a framework for behavior discovery\nand action attribution to behaviors in offline RL trajectories. Our method\nidentifies meaningful behavioral segments, enabling more precise and granular\nexplanations associated with high level agent behaviors. This approach is\nadaptable across diverse environments with minimal modifications, offering a\nscalable and versatile solution for behavior discovery and attribution for\nexplainable RL.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Rishav Rishav",
      "Somjit Nath",
      "Vincent Michalski",
      "Samira Ebrahimi Kahou"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14966v1",
    "title": "Ultrasound Image-to-Video Synthesis via Latent Dynamic Diffusion Models",
    "abstract": "Ultrasound video classification enables automated diagnosis and has emerged\nas an important research area. However, publicly available ultrasound video\ndatasets remain scarce, hindering progress in developing effective video\nclassification models. We propose addressing this shortage by synthesizing\nplausible ultrasound videos from readily available, abundant ultrasound images.\nTo this end, we introduce a latent dynamic diffusion model (LDDM) to\nefficiently translate static images to dynamic sequences with realistic video\ncharacteristics. We demonstrate strong quantitative results and visually\nappealing synthesized videos on the BUSV benchmark. Notably, training video\nclassification models on combinations of real and LDDM-synthesized videos\nsubstantially improves performance over using real data alone, indicating our\nmethod successfully emulates dynamics critical for discrimination. Our\nimage-to-video approach provides an effective data augmentation solution to\nadvance ultrasound video analysis. Code is available at\nhttps://github.com/MedAITech/U_I2V.",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "authors": [
      "Tingxiu Chen",
      "Yilei Shi",
      "Zixuan Zheng",
      "Bingcong Yan",
      "Jingliang Hu",
      "Xiao Xiang Zhu",
      "Lichao Mou"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14963v1",
    "title": "Continual Multimodal Contrastive Learning",
    "abstract": "Multimodal contrastive learning (MCL) advances in aligning different\nmodalities and generating multimodal representations in a joint space. By\nleveraging contrastive learning across diverse modalities, large-scale\nmultimodal data enhances representational quality. However, a critical yet\noften overlooked challenge remains: multimodal data is rarely collected in a\nsingle process, and training from scratch is computationally expensive.\nInstead, emergent multimodal data can be used to optimize existing models\ngradually, \\textit{i.e.}, models are trained on a sequence of modality pair\ndata. We define this problem as Continual Multimodal Contrastive Learning\n(CMCL), an underexplored yet crucial research direction at the intersection of\nmultimodal and continual learning. In this paper, we formulate CMCL through two\nspecialized principles of stability and plasticity. We theoretically derive a\nnovel optimization-based method, which projects updated gradients from dual\nsides onto subspaces where any gradient is prevented from interfering with the\npreviously learned knowledge. Two upper bounds provide theoretical insights on\nboth stability and plasticity in our solution. Beyond our theoretical\ncontributions, we conduct experiments on multiple datasets by comparing our\nmethod against advanced continual learning baselines. The empirical results\nfurther support our claims and demonstrate the efficacy of our method. The code\nwill be publicly available.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Xiaohao Liu",
      "Xiaobo Xia",
      "See-Kiong Ng",
      "Tat-Seng Chua"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14960v1",
    "title": "Body-Hand Modality Expertized Networks with Cross-attention for Fine-grained Skeleton Action Recognition",
    "abstract": "Skeleton-based Human Action Recognition (HAR) is a vital technology in\nrobotics and human-robot interaction. However, most existing methods\nconcentrate primarily on full-body movements and often overlook subtle hand\nmotions that are critical for distinguishing fine-grained actions. Recent work\nleverages a unified graph representation that combines body, hand, and foot\nkeypoints to capture detailed body dynamics. Yet, these models often blur fine\nhand details due to the disparity between body and hand action characteristics\nand the loss of subtle features during the spatial-pooling. In this paper, we\npropose BHaRNet (Body-Hand action Recognition Network), a novel framework that\naugments a typical body-expert model with a hand-expert model. Our model\njointly trains both streams with an ensemble loss that fosters cooperative\nspecialization, functioning in a manner reminiscent of a Mixture-of-Experts\n(MoE). Moreover, cross-attention is employed via an expertized branch method\nand a pooling-attention module to enable feature-level interactions and\nselectively fuse complementary information. Inspired by MMNet, we also\ndemonstrate the applicability of our approach to multi-modal tasks by\nleveraging RGB information, where body features guide RGB learning to capture\nricher contextual cues. Experiments on large-scale benchmarks (NTU RGB+D 60,\nNTU RGB+D 120, PKU-MMD, and Northwestern-UCLA) demonstrate that BHaRNet\nachieves SOTA accuracies -- improving from 86.4\\% to 93.0\\% in hand-intensive\nactions -- while maintaining fewer GFLOPs and parameters than the relevant\nunified methods.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Seungyeon Cho",
      "Tae-Kyun Kim"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14958v1",
    "title": "Reducing Annotation Burden: Exploiting Image Knowledge for Few-Shot Medical Video Object Segmentation via Spatiotemporal Consistency Relearning",
    "abstract": "Few-shot video object segmentation aims to reduce annotation costs; however,\nexisting methods still require abundant dense frame annotations for training,\nwhich are scarce in the medical domain. We investigate an extremely low-data\nregime that utilizes annotations from only a few video frames and leverages\nexisting labeled images to minimize costly video annotations. Specifically, we\npropose a two-phase framework. First, we learn a few-shot segmentation model\nusing labeled images. Subsequently, to improve performance without full\nsupervision, we introduce a spatiotemporal consistency relearning approach on\nmedical videos that enforces consistency between consecutive frames.\nConstraints are also enforced between the image model and relearning model at\nboth feature and prediction levels. Experiments demonstrate the superiority of\nour approach over state-of-the-art few-shot segmentation methods. Our model\nbridges the gap between abundant annotated medical images and scarce, sparsely\nlabeled medical videos to achieve strong video segmentation performance in this\nlow data regime. Code is available at https://github.com/MedAITech/RAB.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Zixuan Zheng",
      "Yilei Shi",
      "Chunlei Li",
      "Jingliang Hu",
      "Xiao Xiang Zhu",
      "Lichao Mou"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14957v1",
    "title": "Neuro Symbolic Knowledge Reasoning for Procedural Video Question Answering",
    "abstract": "This paper introduces a new video question-answering (VQA) dataset that\nchallenges models to leverage procedural knowledge for complex reasoning. It\nrequires recognizing visual entities, generating hypotheses, and performing\ncontextual, causal, and counterfactual reasoning. To address this, we propose\nneuro symbolic reasoning module that integrates neural networks and LLM-driven\nconstrained reasoning over variables for interpretable answer generation.\nResults show that combining LLMs with structured knowledge reasoning with logic\nenhances procedural reasoning on the STAR benchmark and our dataset. Code and\ndataset at https://github.com/LUNAProject22/KML soon.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Thanh-Son Nguyen",
      "Hong Yang",
      "Tzeh Yuan Neoh",
      "Hao Zhang",
      "Ee Yeo Keat",
      "Basura Fernando"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14955v1",
    "title": "Depth-Aware Range Image-Based Model for Point Cloud Segmentation",
    "abstract": "Point cloud segmentation (PCS) aims to separate points into different and\nmeaningful groups. The task plays an important role in robotics because PCS\nenables robots to understand their physical environments directly. To process\nsparse and large-scale outdoor point clouds in real time, range image-based\nmodels are commonly adopted. However, in a range image, the lack of explicit\ndepth information inevitably causes some separate objects in 3D space to touch\neach other, bringing difficulty for the range image-based models in correctly\nsegmenting the objects. Moreover, previous PCS models are usually derived from\nthe existing color image-based models and unable to make full use of the\nimplicit but ordered depth information inherent in the range image, thereby\nachieving inferior performance. In this paper, we propose Depth-Aware Module\n(DAM) and Fast FMVNet V3. DAM perceives the ordered depth information in the\nrange image by explicitly modelling the interdependence among channels. Fast\nFMVNet V3 incorporates DAM by integrating it into the last block in each\narchitecture stage. Extensive experiments conducted on SemanticKITTI, nuScenes,\nand SemanticPOSS demonstrate that DAM brings a significant improvement for Fast\nFMVNet V3 with negligible computational cost.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Bike Chen",
      "Antti Tikanmäki",
      "Juha Röning"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14953v1",
    "title": "Aligning Information Capacity Between Vision and Language via Dense-to-Sparse Feature Distillation for Image-Text Matching",
    "abstract": "Enabling Visual Semantic Models to effectively handle multi-view description\nmatching has been a longstanding challenge. Existing methods typically learn a\nset of embeddings to find the optimal match for each view's text and compute\nsimilarity. However, the visual and text embeddings learned through these\napproaches have limited information capacity and are prone to interference from\nlocally similar negative samples. To address this issue, we argue that the\ninformation capacity of embeddings is crucial and propose Dense-to-Sparse\nFeature Distilled Visual Semantic Embedding (D2S-VSE), which enhances the\ninformation capacity of sparse text by leveraging dense text distillation.\nSpecifically, D2S-VSE is a two-stage framework. In the pre-training stage, we\nalign images with dense text to enhance the information capacity of visual\nsemantic embeddings. In the fine-tuning stage, we optimize two tasks\nsimultaneously, distilling dense text embeddings to sparse text embeddings\nwhile aligning images and sparse texts, enhancing the information capacity of\nsparse text embeddings. Our proposed D2S-VSE model is extensively evaluated on\nthe large-scale MS-COCO and Flickr30K datasets, demonstrating its superiority\nover recent state-of-the-art methods.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yang Liu",
      "Wentao Feng",
      "Zhuoyao Liu",
      "Shudong Huang",
      "Jiancheng Lv"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14950v1",
    "title": "USAM-Net: A U-Net-based Network for Improved Stereo Correspondence and Scene Depth Estimation using Features from a Pre-trained Image Segmentation network",
    "abstract": "The increasing demand for high-accuracy depth estimation in autonomous\ndriving and augmented reality applications necessitates advanced neural\narchitectures capable of effectively leveraging multiple data modalities. In\nthis context, we introduce the Unified Segmentation Attention Mechanism Network\n(USAM-Net), a novel convolutional neural network that integrates stereo image\ninputs with semantic segmentation maps and attention to enhance depth\nestimation performance. USAM-Net employs a dual-pathway architecture, which\ncombines a pre-trained segmentation model (SAM) and a depth estimation model.\nThe segmentation pathway preprocesses the stereo images to generate semantic\nmasks, which are then concatenated with the stereo images as inputs to the\ndepth estimation pathway. This integration allows the model to focus on\nimportant features such as object boundaries and surface textures which are\ncrucial for accurate depth perception. Empirical evaluation on the\nDrivingStereo dataset demonstrates that USAM-Net achieves superior performance\nmetrics, including a Global Difference (GD) of 3.61\\% and an End-Point Error\n(EPE) of 0.88, outperforming traditional models such as CFNet, SegStereo, and\niResNet. These results underscore the effectiveness of integrating segmentation\ninformation into stereo depth estimation tasks, highlighting the potential of\nUSAM-Net in applications demanding high-precision depth data.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Joseph Emmanuel DL Dayo",
      "Prospero C. Naval Jr"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14948v1",
    "title": "ChatStitch: Visualizing Through Structures via Surround-View Unsupervised Deep Image Stitching with Collaborative LLM-Agents",
    "abstract": "Collaborative perception has garnered significant attention for its ability\nto enhance the perception capabilities of individual vehicles through the\nexchange of information with surrounding vehicle-agents. However, existing\ncollaborative perception systems are limited by inefficiencies in user\ninteraction and the challenge of multi-camera photorealistic visualization. To\naddress these challenges, this paper introduces ChatStitch, the first\ncollaborative perception system capable of unveiling obscured blind spot\ninformation through natural language commands integrated with external digital\nassets. To adeptly handle complex or abstract commands, ChatStitch employs a\nmulti-agent collaborative framework based on Large Language Models. For\nachieving the most intuitive perception for humans, ChatStitch proposes\nSV-UDIS, the first surround-view unsupervised deep image stitching method under\nthe non-global-overlapping condition. We conducted extensive experiments on the\nUDIS-D, MCOV-SLAM open datasets, and our real-world dataset. Specifically, our\nSV-UDIS method achieves state-of-the-art performance on the UDIS-D dataset for\n3, 4, and 5 image stitching tasks, with PSNR improvements of 9%, 17%, and 21%,\nand SSIM improvements of 8%, 18%, and 26%, respectively.",
    "categories": [
      "cs.CV",
      "cs.HC"
    ],
    "authors": [
      "Hao Liang",
      "Zhipeng Dong",
      "Yi Yang",
      "Mengyin Fu"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14945v1",
    "title": "Generating Multimodal Driving Scenes via Next-Scene Prediction",
    "abstract": "Generative models in Autonomous Driving (AD) enable diverse scene creation,\nyet existing methods fall short by only capturing a limited range of\nmodalities, restricting the capability of generating controllable scenes for\ncomprehensive evaluation of AD systems. In this paper, we introduce a\nmultimodal generation framework that incorporates four major data modalities,\nincluding a novel addition of map modality. With tokenized modalities, our\nscene sequence generation framework autoregressively predicts each scene while\nmanaging computational demands through a two-stage approach. The Temporal\nAutoRegressive (TAR) component captures inter-frame dynamics for each modality\nwhile the Ordered AutoRegressive (OAR) component aligns modalities within each\nscene by sequentially predicting tokens in a fixed order. To maintain coherence\nbetween map and ego-action modalities, we introduce the Action-aware Map\nAlignment (AMA) module, which applies a transformation based on the ego-action\nto maintain coherence between these modalities. Our framework effectively\ngenerates complex, realistic driving scenes over extended sequences, ensuring\nmultimodal consistency and offering fine-grained control over scene elements.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yanhao Wu",
      "Haoyang Zhang",
      "Tianwei Lin",
      "Lichao Huang",
      "Shujie Luo",
      "Rui Wu",
      "Congpei Qiu",
      "Wei Ke",
      "Tong Zhang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14944v1",
    "title": "MMAIF: Multi-task and Multi-degradation All-in-One for Image Fusion with Language Guidance",
    "abstract": "Image fusion, a fundamental low-level vision task, aims to integrate multiple\nimage sequences into a single output while preserving as much information as\npossible from the input. However, existing methods face several significant\nlimitations: 1) requiring task- or dataset-specific models; 2) neglecting\nreal-world image degradations (\\textit{e.g.}, noise), which causes failure when\nprocessing degraded inputs; 3) operating in pixel space, where attention\nmechanisms are computationally expensive; and 4) lacking user interaction\ncapabilities. To address these challenges, we propose a unified framework for\nmulti-task, multi-degradation, and language-guided image fusion. Our framework\nincludes two key components: 1) a practical degradation pipeline that simulates\nreal-world image degradations and generates interactive prompts to guide the\nmodel; 2) an all-in-one Diffusion Transformer (DiT) operating in latent space,\nwhich fuses a clean image conditioned on both the degraded inputs and the\ngenerated prompts. Furthermore, we introduce principled modifications to the\noriginal DiT architecture to better suit the fusion task. Based on this\nframework, we develop two versions of the model: Regression-based and Flow\nMatching-based variants. Extensive qualitative and quantitative experiments\ndemonstrate that our approach effectively addresses the aforementioned\nlimitations and outperforms previous restoration+fusion and all-in-one\npipelines. Codes are available at https://github.com/294coder/MMAIF.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Zihan Cao",
      "Yu Zhong",
      "Ziqi Wang",
      "Liang-Jian Deng"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14943v1",
    "title": "3D Engine-ready Photorealistic Avatars via Dynamic Textures",
    "abstract": "As the digital and physical worlds become more intertwined, there has been a\nlot of interest in digital avatars that closely resemble their real-world\ncounterparts. Current digitization methods used in 3D production pipelines\nrequire costly capture setups, making them impractical for mass usage among\ncommon consumers. Recent academic literature has found success in\nreconstructing humans from limited data using implicit representations (e.g.,\nvoxels used in NeRFs), which are able to produce impressive videos. However,\nthese methods are incompatible with traditional rendering pipelines, making it\ndifficult to use them in applications such as games. In this work, we propose\nan end-to-end pipeline that builds explicitly-represented photorealistic 3D\navatars using standard 3D assets. Our key idea is the use of\ndynamically-generated textures to enhance the realism and visually mask\ndeficiencies in the underlying mesh geometry. This allows for seamless\nintegration with current graphics pipelines while achieving comparable visual\nquality to state-of-the-art 3D avatar generation methods.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yifan Wang",
      "Ivan Molodetskikh",
      "Ondrej Texler",
      "Dimitar Dinev"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14941v1",
    "title": "UPME: An Unsupervised Peer Review Framework for Multimodal Large Language Model Evaluation",
    "abstract": "Multimodal Large Language Models (MLLMs) have emerged to tackle the\nchallenges of Visual Question Answering (VQA), sparking a new research focus on\nconducting objective evaluations of these models. Existing evaluation methods\nface limitations due to the significant human workload required to design Q&A\npairs for visual images, which inherently restricts the scale and scope of\nevaluations. Although automated MLLM-as-judge approaches attempt to reduce the\nhuman workload through automatic evaluations, they often introduce biases. To\naddress these problems, we propose an Unsupervised Peer review MLLM Evaluation\nframework. It utilizes only image data, allowing models to automatically\ngenerate questions and conduct peer review assessments of answers from other\nmodels, effectively alleviating the reliance on human workload. Additionally,\nwe introduce the vision-language scoring system to mitigate the bias issues,\nwhich focuses on three aspects: (i) response correctness; (ii) visual\nunderstanding and reasoning; and (iii) image-text correlation. Experimental\nresults demonstrate that UPME achieves a Pearson correlation of 0.944 with\nhuman evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset,\nindicating that our framework closely aligns with human-designed benchmarks and\ninherent human preferences.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Qihui Zhang",
      "Munan Ning",
      "Zheyuan Liu",
      "Yanbo Wang",
      "Jiayi Ye",
      "Yue Huang",
      "Shuo Yang",
      "Xiao Chen",
      "Yibing Song",
      "Li Yuan"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14939v1",
    "title": "VisNumBench: Evaluating Number Sense of Multimodal Large Language Models",
    "abstract": "Can Multimodal Large Language Models (MLLMs) develop an intuitive number\nsense similar to humans? Targeting this problem, we introduce Visual Number\nBenchmark (VisNumBench) to evaluate the number sense abilities of MLLMs across\na wide range of visual numerical tasks. VisNumBench consists of about 1,900\nmultiple-choice question-answer pairs derived from both synthetic and\nreal-world visual data, covering seven visual numerical attributes and four\ntypes of visual numerical estimation tasks. Our experiments on VisNumBench led\nto the following key findings: (i) The 17 MLLMs we tested, including\nopen-source models such as Qwen2.5-VL and InternVL2.5, as well as proprietary\nmodels like GPT-4o and Gemini 2.0 Flash, perform significantly below human\nlevels in number sense-related tasks. (ii) Multimodal mathematical models and\nmultimodal chain-of-thought (CoT) models did not exhibit significant\nimprovements in number sense abilities. (iii) Stronger MLLMs with larger\nparameter sizes and broader general abilities demonstrate modest gains in\nnumber sense abilities. We believe VisNumBench will serve as a valuable\nresource for the research community, encouraging further advancements in\nenhancing MLLMs' number sense abilities. All benchmark resources, including\ncode and datasets, will be publicly available at\nhttps://wwwtttjjj.github.io/VisNumBench/.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Tengjin Weng",
      "Jingyi Wang",
      "Wenhao Jiang",
      "Zhong Ming"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14938v1",
    "title": "Optimal Transport Adapter Tuning for Bridging Modality Gaps in Few-Shot Remote Sensing Scene Classification",
    "abstract": "Few-Shot Remote Sensing Scene Classification (FS-RSSC) presents the challenge\nof classifying remote sensing images with limited labeled samples. Existing\nmethods typically emphasize single-modal feature learning, neglecting the\npotential benefits of optimizing multi-modal representations. To address this\nlimitation, we propose a novel Optimal Transport Adapter Tuning (OTAT)\nframework aimed at constructing an ideal Platonic representational space\nthrough optimal transport (OT) theory. This framework seeks to harmonize rich\nvisual information with less dense textual cues, enabling effective cross-modal\ninformation transfer and complementarity. Central to this approach is the\nOptimal Transport Adapter (OTA), which employs a cross-modal attention\nmechanism to enrich textual representations and facilitate subsequent better\ninformation interaction. By transforming the network optimization into an OT\noptimization problem, OTA establishes efficient pathways for balanced\ninformation exchange between modalities. Moreover, we introduce a sample-level\nEntropy-Aware Weighted (EAW) loss, which combines difficulty-weighted\nsimilarity scores with entropy-based regularization. This loss function\nprovides finer control over the OT optimization process, enhancing its\nsolvability and stability. Our framework offers a scalable and efficient\nsolution for advancing multimodal learning in remote sensing applications.\nExtensive experiments on benchmark datasets demonstrate that OTAT achieves\nstate-of-the-art performance in FS-RSSC, significantly improving the model\nperformance and generalization.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Zhong Ji",
      "Ci Liu",
      "Jingren Liu",
      "Chen Tang",
      "Yanwei Pang",
      "Xuelong Li"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14937v1",
    "title": "Proceedings of the 3rd Italian Conference on Big Data and Data Science (ITADATA2024)",
    "abstract": "Proceedings of the 3rd Italian Conference on Big Data and Data Science\n(ITADATA2024), held in Pisa, Italy, September 17-19, 2024.\n  The Italian Conference on Big Data and Data Science (ITADATA2024) is the\nannual event supported by the CINI Big Data National Laboratory and ISTI CNR\nthat aims to put together Italian researchers and professionals from academia,\nindustry, government, and public administration working in the field of big\ndata and data science, as well as related fields (e.g., security and privacy,\nHPC, Cloud).\n  ITADATA2024 covered research on all theoretical and practical aspects of Big\nData and data science including data governance, data processing, data\nanalysis, data reporting, data protection, as well as experimental studies and\nlessons learned. In particular, ITADATA2024 focused on\n  - Data spaces\n  - Data processing life cycle\n  - Machine learning and Large Language Models\n  - Applications of big data and data science in healthcare, finance, industry\n5.0, and beyond\n  - Data science for social network analysis",
    "categories": [
      "cs.DB",
      "cs.LG"
    ],
    "authors": [
      "Nicola Bena",
      "Claudia Diamantini",
      "Michela Natilli",
      "Luigi Romano",
      "Giovanni Stilo",
      "Valentina Pansanella",
      "Claudio A. Ardagna",
      "Anna Monreale",
      "Roberto Trasarti"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14936v1",
    "title": "Enhancing Code LLM Training with Programmer Attention",
    "abstract": "Human attention provides valuable yet underexploited signals for code LLM\ntraining, offering a perspective beyond purely machine-driven attention.\nDespite the complexity and cost of collecting eye-tracking data, there has also\nbeen limited progress in systematically using these signals for code LLM\ntraining. To address both issues, we propose a cohesive pipeline spanning\naugmentation and reward-based fine-tuning. Specifically, we introduce (1) an\neye-tracking path augmentation method to expand programmer attention datasets,\n(2) a pattern abstraction step that refines raw fixations into learnable\nattention motifs, and (3) a reward-guided strategy for integrating these\ninsights directly into a CodeT5 supervised fine-tuning process. Our experiments\nyield +7.16 in CodeBLEU on the CodeXGlue benchmark for code summarization,\nunderscoring how uniting human and machine attention can boost code\nintelligence. We hope this work encourages broader exploration of human-centric\nmethods in next-generation AI4SE.",
    "categories": [
      "cs.SE",
      "cs.HC",
      "cs.LG"
    ],
    "authors": [
      "Yifan Zhang",
      "Chen Huang",
      "Zachary Karas",
      "Dung Thuy Nguyen",
      "Kevin Leach",
      "Yu Huang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14935v1",
    "title": "FAVOR-Bench: A Comprehensive Benchmark for Fine-Grained Video Motion Understanding",
    "abstract": "Multimodal Large Language Models (MLLMs) have shown remarkable capabilities\nin video content understanding but still struggle with fine-grained motion\ncomprehension. To comprehensively assess the motion understanding ability of\nexisting MLLMs, we introduce FAVOR-Bench, comprising 1,776 videos with\nstructured manual annotations of various motions. Our benchmark includes both\nclose-ended and open-ended tasks. For close-ended evaluation, we carefully\ndesign 8,184 multiple-choice question-answer pairs spanning six distinct\nsub-tasks. For open-ended evaluation, we develop both a novel cost-efficient\nLLM-free and a GPT-assisted caption assessment method, where the former can\nenhance benchmarking interpretability and reproducibility. Comprehensive\nexperiments with 21 state-of-the-art MLLMs reveal significant limitations in\ntheir ability to comprehend and describe detailed temporal dynamics in video\nmotions. To alleviate this limitation, we further build FAVOR-Train, a dataset\nconsisting of 17,152 videos with fine-grained motion annotations. The results\nof finetuning Qwen2.5-VL on FAVOR-Train yield consistent improvements on\nmotion-related tasks of TVBench, MotionBench and our FAVOR-Bench. Comprehensive\nassessment results demonstrate that the proposed FAVOR-Bench and FAVOR-Train\nprovide valuable tools to the community for developing more powerful video\nunderstanding models. Project page:\n\\href{https://favor-bench.github.io/}{https://favor-bench.github.io/}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Chongjun Tu",
      "Lin Zhang",
      "Pengtao Chen",
      "Peng Ye",
      "Xianfang Zeng",
      "Wei Cheng",
      "Gang Yu",
      "Tao Chen"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14933v1",
    "title": "A Language Vision Model Approach for Automated Tumor Contouring in Radiation Oncology",
    "abstract": "Background: Lung cancer ranks as the leading cause of cancer-related\nmortality worldwide. The complexity of tumor delineation, crucial for radiation\ntherapy, requires expertise often unavailable in resource-limited settings.\nArtificial Intelligence(AI), particularly with advancements in deep learning\n(DL) and natural language processing (NLP), offers potential solutions yet is\nchallenged by high false positive rates. Purpose: The Oncology Contouring\nCopilot (OCC) system is developed to leverage oncologist expertise for precise\ntumor contouring using textual descriptions, aiming to increase the efficiency\nof oncological workflows by combining the strengths of AI with human oversight.\nMethods: Our OCC system initially identifies nodule candidates from CT scans.\nEmploying Language Vision Models (LVMs) like GPT-4V, OCC then effectively\nreduces false positives with clinical descriptive texts, merging textual and\nvisual data to automate tumor delineation, designed to elevate the quality of\noncology care by incorporating knowledge from experienced domain experts.\nResults: Deployments of the OCC system resulted in a significant reduction in\nthe false discovery rate by 35.0%, a 72.4% decrease in false positives per\nscan, and an F1-score of 0.652 across our dataset for unbiased evaluation.\nConclusions: OCC represents a significant advance in oncology care,\nparticularly through the use of the latest LVMs to improve contouring results\nby (1) streamlining oncology treatment workflows by optimizing tumor\ndelineation, reducing manual processes; (2) offering a scalable and intuitive\nframework to reduce false positives in radiotherapy planning using LVMs; (3)\nintroducing novel medical language vision prompt techniques to minimize LVMs\nhallucinations with ablation study, and (4) conducting a comparative analysis\nof LVMs, highlighting their potential in addressing medical language vision\nchallenges.",
    "categories": [
      "eess.IV",
      "cs.CV",
      "physics.med-ph"
    ],
    "authors": [
      "Yi Luo",
      "Hamed Hooshangnejad",
      "Xue Feng",
      "Gaofeng Huang",
      "Xiaojian Chen",
      "Rui Zhang",
      "Quan Chen",
      "Wil Ngwa",
      "Kai Ding"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14932v1",
    "title": "Prada: Black-Box LLM Adaptation with Private Data on Resource-Constrained Devices",
    "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\nabilities in various natural language processing tasks. However, adapting these\nmodels to specialized domains using private datasets stored on\nresource-constrained edge devices, such as smartphones and personal computers,\nremains challenging due to significant privacy concerns and limited\ncomputational resources. Existing model adaptation methods either compromise\ndata privacy by requiring data transmission or jeopardize model privacy by\nexposing proprietary LLM parameters. To address these challenges, we propose\nPrada, a novel privacy-preserving and efficient black-box LLM adaptation system\nusing private on-device datasets. Prada employs a lightweight proxy model\nfine-tuned with Low-Rank Adaptation (LoRA) locally on user devices. During\ninference, Prada leverages the logits offset, i.e., difference in outputs\nbetween the base and adapted proxy models, to iteratively refine outputs from a\nremote black-box LLM. This offset-based adaptation approach preserves both data\nprivacy and model privacy, as there is no need to share sensitive data or\nproprietary model parameters. Furthermore, we incorporate speculative decoding\nto further speed up the inference process of Prada, making the system\npractically deployable on bandwidth-constrained edge devices, enabling a more\npractical deployment of Prada. Extensive experiments on various downstream\ntasks demonstrate that Prada achieves performance comparable to centralized\nfine-tuning methods while significantly reducing computational overhead by up\nto 60% and communication costs by up to 80%.",
    "categories": [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ],
    "authors": [
      "Ziyao Wang",
      "Yexiao He",
      "Zheyu Shen",
      "Yu Li",
      "Guoheng Sun",
      "Myungjin Lee",
      "Ang Li"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15566v1",
    "title": "Enforcing Consistency and Fairness in Multi-level Hierarchical Classification with a Mask-based Output Layer",
    "abstract": "Traditional Multi-level Hierarchical Classification (MLHC) classifiers often\nrely on backbone models with $n$ independent output layers. This structure\ntends to overlook the hierarchical relationships between classes, leading to\ninconsistent predictions that violate the underlying taxonomy. Additionally,\nonce a backbone architecture for an MLHC classifier is selected, adapting the\nmodel to accommodate new tasks can be challenging. For example, incorporating\nfairness to protect sensitive attributes within a hierarchical classifier\nnecessitates complex adjustments to maintain the class hierarchy while\nenforcing fairness constraints. In this paper, we extend this concept to\nhierarchical classification by introducing a fair, model-agnostic layer\ndesigned to enforce taxonomy and optimize specific objectives, including\nconsistency, fairness, and exact match. Our evaluations demonstrate that the\nproposed layer not only improves the fairness of predictions but also enforces\nthe taxonomy, resulting in consistent predictions and superior performance.\nCompared to Large Language Models (LLMs) employing in-processing de-biasing\ntechniques and models without any bias correction, our approach achieves better\noutcomes in both fairness and accuracy, making it particularly valuable in\nsectors like e-commerce, healthcare, and education, where predictive\nreliability is crucial.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Shijing Chen",
      "Shoaib Jameel",
      "Mohamed Reda Bouadjenek",
      "Feilong Tang",
      "Usman Naseem",
      "Basem Suleiman",
      "Hakim Hacid",
      "Flora D. Salim",
      "Imran Razzak"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14929v1",
    "title": "ACE: A Cardinality Estimator for Set-Valued Queries",
    "abstract": "Cardinality estimation is a fundamental functionality in database systems.\nMost existing cardinality estimators focus on handling predicates over numeric\nor categorical data. They have largely omitted an important data type,\nset-valued data, which frequently occur in contemporary applications such as\ninformation retrieval and recommender systems. The few existing estimators for\nsuch data either favor high-frequency elements or rely on a partial\nindependence assumption, which limits their practical applicability. We propose\nACE, an Attention-based Cardinality Estimator for estimating the cardinality of\nqueries over set-valued data. We first design a distillation-based data encoder\nto condense the dataset into a compact matrix. We then design an\nattention-based query analyzer to capture correlations among query elements. To\nhandle variable-sized queries, a pooling module is introduced, followed by a\nregression model (MLP) to generate final cardinality estimates. We evaluate ACE\non three datasets with varying query element distributions, demonstrating that\nACE outperforms the state-of-the-art competitors in terms of both accuracy and\nefficiency.",
    "categories": [
      "cs.DB",
      "cs.LG"
    ],
    "authors": [
      "Yufan Sheng",
      "Xin Cao",
      "Kaiqi Zhao",
      "Yixiang Fang",
      "Jianzhong Qi",
      "Wenjie Zhang",
      "Christian S. Jensen"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14928v1",
    "title": "Shushing! Let's Imagine an Authentic Speech from the Silent Video",
    "abstract": "Vision-guided speech generation aims to produce authentic speech from facial\nappearance or lip motions without relying on auditory signals, offering\nsignificant potential for applications such as dubbing in filmmaking and\nassisting individuals with aphonia. Despite recent progress, existing methods\nstruggle to achieve unified cross-modal alignment across semantics, timbre, and\nemotional prosody from visual cues, prompting us to propose Consistent\nVideo-to-Speech (CV2S) as an extended task to enhance cross-modal consistency.\nTo tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal\ndiffusion framework that generates faithful speech using only visual input,\noperating within a discrete space. Specifically, we propose a discrete lip\naligner that predicts discrete speech tokens from lip videos to capture\nsemantic information, while an error detector identifies misaligned tokens,\nwhich are subsequently refined through masked language modeling with BERT. To\nfurther enhance the expressiveness of the generated speech, we develop a style\ndiffusion transformer equipped with a face-style adapter that adaptively\ncustomizes identity and prosody dynamics across both the channel and temporal\ndimensions while ensuring synchronization with lip-aware semantic features.\nExtensive experiments demonstrate that ImaginTalk can generate high-fidelity\nspeech with more accurate semantic details and greater expressiveness in timbre\nand emotion compared to state-of-the-art baselines. Demos are shown at our\nproject page: https://imagintalk.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "authors": [
      "Jiaxin Ye",
      "Hongming Shan"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14927v1",
    "title": "Semi-Gradient SARSA Routing with Theoretical Guarantee on Traffic Stability and Weight Convergence",
    "abstract": "We consider the traffic control problem of dynamic routing over parallel\nservers, which arises in a variety of engineering systems such as\ntransportation and data transmission. We propose a semi-gradient, on-policy\nalgorithm that learns an approximate optimal routing policy. The algorithm uses\ngeneric basis functions with flexible weights to approximate the value function\nacross the unbounded state space. Consequently, the training process lacks\nLipschitz continuity of the gradient, boundedness of the temporal-difference\nerror, and a prior guarantee on ergodicity, which are the standard\nprerequisites in existing literature on reinforcement learning theory. To\naddress this, we combine a Lyapunov approach and an ordinary differential\nequation-based method to jointly characterize the behavior of traffic state and\napproximation weights. Our theoretical analysis proves that the training scheme\nguarantees traffic state stability and ensures almost surely convergence of the\nweights to the approximate optimum. We also demonstrate via simulations that\nour algorithm attains significantly faster convergence than neural\nnetwork-based methods with an insignificant approximation error.",
    "categories": [
      "cs.LG",
      "cs.SY",
      "eess.SY",
      "math.DS"
    ],
    "authors": [
      "Yidan Wu",
      "Yu Yu",
      "Jianan Zhang",
      "Li Jin"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14926v1",
    "title": "Covering Cracks in Content Moderation: Delexicalized Distant Supervision for Illicit Drug Jargon Detection",
    "abstract": "In light of rising drug-related concerns and the increasing role of social\nmedia, sales and discussions of illicit drugs have become commonplace online.\nSocial media platforms hosting user-generated content must therefore perform\ncontent moderation, which is a difficult task due to the vast amount of jargon\nused in drug discussions. Previous works on drug jargon detection were limited\nto extracting a list of terms, but these approaches have fundamental problems\nin practical application. First, they are trivially evaded using word\nsubstitutions. Second, they cannot distinguish whether euphemistic terms such\nas \"pot\" or \"crack\" are being used as drugs or in their benign meanings. We\nargue that drug content moderation should be done using contexts rather than\nrelying on a banlist. However, manually annotated datasets for training such a\ntask are not only expensive but also prone to becoming obsolete. We present\nJEDIS, a framework for detecting illicit drug jargon terms by analyzing their\ncontexts. JEDIS utilizes a novel approach that combines distant supervision and\ndelexicalization, which allows JEDIS to be trained without human-labeled data\nwhile being robust to new terms and euphemisms. Experiments on two manually\nannotated datasets show JEDIS significantly outperforms state-of-the-art\nword-based baselines in terms of F1-score and detection coverage in drug jargon\ndetection. We also conduct qualitative analysis that demonstrates JEDIS is\nrobust against pitfalls faced by existing approaches.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Minkyoo Song",
      "Eugene Jang",
      "Jaehan Kim",
      "Seungwon Shin"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14925v1",
    "title": "pFedFair: Towards Optimal Group Fairness-Accuracy Trade-off in Heterogeneous Federated Learning",
    "abstract": "Federated learning (FL) algorithms commonly aim to maximize clients' accuracy\nby training a model on their collective data. However, in several FL\napplications, the model's decisions should meet a group fairness constraint to\nbe independent of sensitive attributes such as gender or race. While such group\nfairness constraints can be incorporated into the objective function of the FL\noptimization problem, in this work, we show that such an approach would lead to\nsuboptimal classification accuracy in an FL setting with heterogeneous client\ndistributions. To achieve an optimal accuracy-group fairness trade-off, we\npropose the Personalized Federated Learning for Client-Level Group Fairness\n(pFedFair) framework, where clients locally impose their fairness constraints\nover the distributed training process. Leveraging the image embedding models,\nwe extend the application of pFedFair to computer vision settings, where we\nnumerically show that pFedFair achieves an optimal group fairness-accuracy\ntrade-off in heterogeneous FL settings. We present the results of several\nnumerical experiments on benchmark and synthetic datasets, which highlight the\nsuboptimality of non-personalized FL algorithms and the improvements made by\nthe pFedFair method.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Haoyu Lei",
      "Shizhan Gong",
      "Qi Dou",
      "Farzan Farnia"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.16544v1",
    "title": "Causal Discovery and Counterfactual Reasoning to Optimize Persuasive Dialogue Policies",
    "abstract": "Tailoring persuasive conversations to users leads to more effective\npersuasion. However, existing dialogue systems often struggle to adapt to\ndynamically evolving user states. This paper presents a novel method that\nleverages causal discovery and counterfactual reasoning for optimizing system\npersuasion capability and outcomes. We employ the Greedy Relaxation of the\nSparsest Permutation (GRaSP) algorithm to identify causal relationships between\nuser and system utterance strategies, treating user strategies as states and\nsystem strategies as actions. GRaSP identifies user strategies as causal\nfactors influencing system responses, which inform Bidirectional Conditional\nGenerative Adversarial Networks (BiCoGAN) in generating counterfactual\nutterances for the system. Subsequently, we use the Dueling Double Deep\nQ-Network (D3QN) model to utilize counterfactual data to determine the best\npolicy for selecting system utterances. Our experiments with the\nPersuasionForGood dataset show measurable improvements in persuasion outcomes\nusing our approach over baseline methods. The observed increase in cumulative\nrewards and Q-values highlights the effectiveness of causal discovery in\nenhancing counterfactual reasoning and optimizing reinforcement learning\npolicies for online dialogue systems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "authors": [
      "Donghuo Zeng",
      "Roberto Legaspi",
      "Yuewen Sun",
      "Xinshuai Dong",
      "Kazushi Ikeda",
      "Peter Spirtes",
      "Kun Zhang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14922v1",
    "title": "A Semantic and Clean-label Backdoor Attack against Graph Convolutional Networks",
    "abstract": "Graph Convolutional Networks (GCNs) have shown excellent performance in\ngraph-structured tasks such as node classification and graph classification.\nHowever, recent research has shown that GCNs are vulnerable to a new type of\nthreat called the backdoor attack, where the adversary can inject a hidden\nbackdoor into the GCNs so that the backdoored model performs well on benign\nsamples, whereas its prediction will be maliciously changed to the\nattacker-specified target label if the hidden backdoor is activated by the\nattacker-defined trigger. Clean-label backdoor attack and semantic backdoor\nattack are two new backdoor attacks to Deep Neural Networks (DNNs), they are\nmore imperceptible and have posed new and serious threats. The semantic and\nclean-label backdoor attack is not fully explored in GCNs. In this paper, we\npropose a semantic and clean-label backdoor attack against GCNs under the\ncontext of graph classification to reveal the existence of this security\nvulnerability in GCNs. Specifically, SCLBA conducts an importance analysis on\ngraph samples to select one type of node as semantic trigger, which is then\ninserted into the graph samples to create poisoning samples without changing\nthe labels of the poisoning samples to the attacker-specified target label. We\nevaluate SCLBA on multiple datasets and the results show that SCLBA can achieve\nattack success rates close to 99% with poisoning rates of less than 3%, and\nwith almost no impact on the performance of model on benign samples.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "authors": [
      "Jiazhu Dai",
      "Haoyu Sun"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14919v1",
    "title": "GenM$^3$: Generative Pretrained Multi-path Motion Model for Text Conditional Human Motion Generation",
    "abstract": "Scaling up motion datasets is crucial to enhance motion generation\ncapabilities. However, training on large-scale multi-source datasets introduces\ndata heterogeneity challenges due to variations in motion content. To address\nthis, we propose Generative Pretrained Multi-path Motion Model (GenM$^3$), a\ncomprehensive framework designed to learn unified motion representations.\nGenM$^3$ comprises two components: 1) a Multi-Expert VQ-VAE (MEVQ-VAE) that\nadapts to different dataset distributions to learn a unified discrete motion\nrepresentation, and 2) a Multi-path Motion Transformer (MMT) that improves\nintra-modal representations by using separate modality-specific pathways, each\nwith densely activated experts to accommodate variations within that modality,\nand improves inter-modal alignment by the text-motion shared pathway. To enable\nlarge-scale training, we integrate and unify 11 high-quality motion datasets\n(approximately 220 hours of motion data) and augment it with textual\nannotations (nearly 10,000 motion sequences labeled by a large language model\nand 300+ by human experts). After training on our integrated dataset, GenM$^3$\nachieves a state-of-the-art FID of 0.035 on the HumanML3D benchmark, surpassing\nstate-of-the-art methods by a large margin. It also demonstrates strong\nzero-shot generalization on IDEA400 dataset, highlighting its effectiveness and\nadaptability across diverse motion scenarios.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Junyu Shi",
      "Lijiang Liu",
      "Yong Sun",
      "Zhiyuan Zhang",
      "Jinni Zhou",
      "Qiang Nie"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14917v1",
    "title": "MASS: Mathematical Data Selection via Skill Graphs for Pretraining Large Language Models",
    "abstract": "High-quality data plays a critical role in the pretraining and fine-tuning of\nlarge language models (LLMs), even determining their performance ceiling to\nsome degree. Consequently, numerous data selection methods have been proposed\nto identify subsets of data that can effectively and efficiently enhance model\nperformance. However, most of these methods focus on general data selection and\ntend to overlook the specific nuances of domain-related data. In this paper, we\nintroduce MASS, a \\textbf{MA}thematical data \\textbf{S}election framework using\nthe \\textbf{S}kill graph for pretraining LLMs in the mathematical reasoning\ndomain. By taking into account the unique characteristics of mathematics and\nreasoning, we construct a skill graph that captures the mathematical skills and\ntheir interrelations from a reference dataset. This skill graph guides us in\nassigning quality scores to the target dataset, enabling us to select the\ntop-ranked subset which is further used to pretrain LLMs. Experimental results\ndemonstrate the efficiency and effectiveness of MASS across different model\nsizes (1B and 7B) and pretraining datasets (web data and synthetic data).\nSpecifically, in terms of efficiency, models trained on subsets selected by\nMASS can achieve similar performance to models trained on the original\ndatasets, with a significant reduction in the number of trained tokens -\nranging from 50\\% to 70\\% fewer tokens. In terms of effectiveness, when trained\non the same amount of tokens, models trained on the data selected by MASS\noutperform those trained on the original datasets by 3.3\\% to 5.9\\%. These\nresults underscore the potential of MASS to improve both the efficiency and\neffectiveness of pretraining LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Jiazheng Li",
      "Lu Yu",
      "Qing Cui",
      "Zhiqiang Zhang",
      "Jun Zhou",
      "Yanfang Ye",
      "Chuxu Zhang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14912v1",
    "title": "Deep Polycuboid Fitting for Compact 3D Representation of Indoor Scenes",
    "abstract": "This paper presents a novel framework for compactly representing a 3D indoor\nscene using a set of polycuboids through a deep learning-based fitting method.\nIndoor scenes mainly consist of man-made objects, such as furniture, which\noften exhibit rectilinear geometry. This property allows indoor scenes to be\nrepresented using combinations of polycuboids, providing a compact\nrepresentation that benefits downstream applications like furniture\nrearrangement. Our framework takes a noisy point cloud as input and first\ndetects six types of cuboid faces using a transformer network. Then, a graph\nneural network is used to validate the spatial relationships of the detected\nfaces to form potential polycuboids. Finally, each polycuboid instance is\nreconstructed by forming a set of boxes based on the aggregated face labels. To\ntrain our networks, we introduce a synthetic dataset encompassing a diverse\nrange of cuboid and polycuboid shapes that reflect the characteristics of\nindoor scenes. Our framework generalizes well to real-world indoor scene\ndatasets, including Replica, ScanNet, and scenes captured with an iPhone. The\nversatility of our method is demonstrated through practical applications, such\nas virtual room tours and scene editing.",
    "categories": [
      "cs.CV",
      "I.4.8; I.3.5"
    ],
    "authors": [
      "Gahye Lee",
      "Hyejeong Yoon",
      "Jungeon Kim",
      "Seungyong Lee"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14911v1",
    "title": "Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology",
    "abstract": "The emergence of vision-language models has transformed medical AI, enabling\nunprecedented advances in diagnostic capability and clinical applications.\nHowever, progress in dermatology has lagged behind other medical domains due to\nthe lack of standard image-text pairs. Existing dermatological datasets are\nlimited in both scale and depth, offering only single-label annotations across\na narrow range of diseases instead of rich textual descriptions, and lacking\nthe crucial clinical context needed for real-world applications. To address\nthese limitations, we present Derm1M, the first large-scale vision-language\ndataset for dermatology, comprising 1,029,761 image-text pairs. Built from\ndiverse educational resources and structured around a standard ontology\ncollaboratively developed by experts, Derm1M provides comprehensive coverage\nfor over 390 skin conditions across four hierarchical levels and 130 clinical\nconcepts with rich contextual information such as medical history, symptoms,\nand skin tone. To demonstrate Derm1M potential in advancing both AI research\nand clinical application, we pretrained a series of CLIP-like models,\ncollectively called DermLIP, on this dataset. The DermLIP family significantly\noutperforms state-of-the-art foundation models on eight diverse datasets across\nmultiple tasks, including zero-shot skin disease classification, clinical and\nartifacts concept identification, few-shot/full-shot learning, and cross-modal\nretrieval. Our dataset and code will be public.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Siyuan Yan",
      "Ming Hu",
      "Yiwen Jiang",
      "Xieji Li",
      "Hao Fei",
      "Philipp Tschandl",
      "Harald Kittler",
      "Zongyuan Ge"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14910v1",
    "title": "Robust Distribution Alignment for Industrial Anomaly Detection under Distribution Shift",
    "abstract": "Anomaly detection plays a crucial role in quality control for industrial\napplications. However, ensuring robustness under unseen domain shifts such as\nlighting variations or sensor drift remains a significant challenge. Existing\nmethods attempt to address domain shifts by training generalizable models but\noften rely on prior knowledge of target distributions and can hardly generalise\nto backbones designed for other data modalities. To overcome these limitations,\nwe build upon memory-bank-based anomaly detection methods, optimizing a robust\nSinkhorn distance on limited target training data to enhance generalization to\nunseen target domains. We evaluate the effectiveness on both 2D and 3D anomaly\ndetection benchmarks with simulated distribution shifts. Our proposed method\ndemonstrates superior results compared with state-of-the-art anomaly detection\nand domain adaptation methods.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jingyi Liao",
      "Xun Xu",
      "Yongyi Su",
      "Rong-Cheng Tu",
      "Yifan Liu",
      "Dacheng Tao",
      "Xulei Yang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14908v1",
    "title": "POSTA: A Go-to Framework for Customized Artistic Poster Generation",
    "abstract": "Poster design is a critical medium for visual communication. Prior work has\nexplored automatic poster design using deep learning techniques, but these\napproaches lack text accuracy, user customization, and aesthetic appeal,\nlimiting their applicability in artistic domains such as movies and\nexhibitions, where both clear content delivery and visual impact are essential.\nTo address these limitations, we present POSTA: a modular framework powered by\ndiffusion models and multimodal large language models (MLLMs) for customized\nartistic poster generation. The framework consists of three modules. Background\nDiffusion creates a themed background based on user input. Design MLLM then\ngenerates layout and typography elements that align with and complement the\nbackground style. Finally, to enhance the poster's aesthetic appeal, ArtText\nDiffusion applies additional stylization to key text elements. The final result\nis a visually cohesive and appealing poster, with a fully modular process that\nallows for complete customization. To train our models, we develop the\nPosterArt dataset, comprising high-quality artistic posters annotated with\nlayout, typography, and pixel-level stylized text segmentation. Our\ncomprehensive experimental analysis demonstrates POSTA's exceptional\ncontrollability and design diversity, outperforming existing models in both\ntext accuracy and aesthetic quality.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Haoyu Chen",
      "Xiaojie Xu",
      "Wenbo Li",
      "Jingjing Ren",
      "Tian Ye",
      "Songhua Liu",
      "Ying-Cong Chen",
      "Lei Zhu",
      "Xinchao Wang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14906v1",
    "title": "FetalFlex: Anatomy-Guided Diffusion Model for Flexible Control on Fetal Ultrasound Image Synthesis",
    "abstract": "Fetal ultrasound (US) examinations require the acquisition of multiple\nplanes, each providing unique diagnostic information to evaluate fetal\ndevelopment and screening for congenital anomalies. However, obtaining a\ncomprehensive, multi-plane annotated fetal US dataset remains challenging,\nparticularly for rare or complex anomalies owing to their low incidence and\nnumerous subtypes. This poses difficulties in training novice radiologists and\ndeveloping robust AI models, especially for detecting abnormal fetuses. In this\nstudy, we introduce a Flexible Fetal US image generation framework (FetalFlex)\nto address these challenges, which leverages anatomical structures and\nmultimodal information to enable controllable synthesis of fetal US images\nacross diverse planes. Specifically, FetalFlex incorporates a pre-alignment\nmodule to enhance controllability and introduces a repaint strategy to ensure\nconsistent texture and appearance. Moreover, a two-stage adaptive sampling\nstrategy is developed to progressively refine image quality from coarse to fine\nlevels. We believe that FetalFlex is the first method capable of generating\nboth in-distribution normal and out-of-distribution abnormal fetal US images,\nwithout requiring any abnormal data. Experiments on multi-center datasets\ndemonstrate that FetalFlex achieved state-of-the-art performance across\nmultiple image quality metrics. A reader study further confirms the close\nalignment of the generated results with expert visual assessments. Furthermore,\nsynthetic images by FetalFlex significantly improve the performance of six\ntypical deep models in downstream classification and anomaly detection tasks.\nLastly, FetalFlex's anatomy-level controllable generation offers a unique\nadvantage for anomaly simulation and creating paired or counterfactual data at\nthe pixel level. The demo is available at:\nhttps://dyf1023.github.io/FetalFlex/.",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Yaofei Duan",
      "Tao Tan",
      "Zhiyuan Zhu",
      "Yuhao Huang",
      "Yuanji Zhang",
      "Rui Gao",
      "Patrick Cheong-Iao Pang",
      "Xinru Gao",
      "Guowei Tao",
      "Xiang Cong",
      "Zhou Li",
      "Lianying Liang",
      "Guangzhi He",
      "Linliang Yin",
      "Xuedong Deng",
      "Xin Yang",
      "Dong Ni"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14905v1",
    "title": "Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection with Artifact Explanation",
    "abstract": "With the rapid advancement of Artificial Intelligence Generated Content\n(AIGC) technologies, synthetic images have become increasingly prevalent in\neveryday life, posing new challenges for authenticity assessment and detection.\nDespite the effectiveness of existing methods in evaluating image authenticity\nand locating forgeries, these approaches often lack human interpretability and\ndo not fully address the growing complexity of synthetic data. To tackle these\nchallenges, we introduce FakeVLM, a specialized large multimodal model designed\nfor both general synthetic image and DeepFake detection tasks. FakeVLM not only\nexcels in distinguishing real from fake images but also provides clear, natural\nlanguage explanations for image artifacts, enhancing interpretability.\nAdditionally, we present FakeClue, a comprehensive dataset containing over\n100,000 images across seven categories, annotated with fine-grained artifact\nclues in natural language. FakeVLM demonstrates performance comparable to\nexpert models while eliminating the need for additional classifiers, making it\na robust solution for synthetic data detection. Extensive evaluations across\nmultiple datasets confirm the superiority of FakeVLM in both authenticity\nclassification and artifact explanation tasks, setting a new benchmark for\nsynthetic image detection. The dataset and code will be released in:\nhttps://github.com/opendatalab/FakeVLM.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Siwei Wen",
      "Junyan Ye",
      "Peilin Feng",
      "Hengrui Kang",
      "Zichen Wen",
      "Yize Chen",
      "Jiang Wu",
      "Wenjun Wu",
      "Conghui He",
      "Weijia Li"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14900v1",
    "title": "Deep Contrastive Unlearning for Language Models",
    "abstract": "The past a few years have witnessed the great success of large language\nmodels, demonstrating powerful capabilities in comprehending textual data and\ngenerating human-like languages. Large language models achieve success by being\ntrained on vast amounts of textual data, including online sources with\ncopyrighted content and user-generated knowledge. However, this comes at a\ncost: the potential risk of exposing users' privacy and violating copyright\nprotections. Thus, to safeguard individuals' \"right to be forgotten\", there has\nbeen increasing interests in machine unlearning -- the process of removing\ninformation carried by particular training samples from a model while not\ndeteriorating its predictive quality. This is a challenging task due to the\nblack-box nature of language models. Most existing studies focus on mitigating\nthe impact of those forgot samples upon a model's outputs, and do not\nexplicitly consider the geometric distributions of samples in the latent space\nof a model. To address this issue, we propose a machine unlearning framework,\nnamed Deep Contrastive Unlearning for fine-Tuning (DeepCUT) language models.\nOur proposed model achieves machine unlearning by directly optimizing the\nlatent space of a model. Comprehensive experiments on real-world datasets\ndemonstrate the effectiveness and efficiency of DeepCUT with consistent and\nsignificant improvement over baseline methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Estrid He",
      "Tabinda Sarwar",
      "Ibrahim Khalil",
      "Xun Yi",
      "Ke Wang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14897v2",
    "title": "When Domain Generalization meets Generalized Category Discovery: An Adaptive Task-Arithmetic Driven Approach",
    "abstract": "Generalized Class Discovery (GCD) clusters base and novel classes in a target\ndomain using supervision from a source domain with only base classes. Current\nmethods often falter with distribution shifts and typically require access to\ntarget data during training, which can sometimes be impractical. To address\nthis issue, we introduce the novel paradigm of Domain Generalization in GCD\n(DG-GCD), where only source data is available for training, while the target\ndomain, with a distinct data distribution, remains unseen until inference. To\nthis end, our solution, DG2CD-Net, aims to construct a domain-independent,\ndiscriminative embedding space for GCD. The core innovation is an episodic\ntraining strategy that enhances cross-domain generalization by adapting a base\nmodel on tasks derived from source and synthetic domains generated by a\nfoundation model. Each episode focuses on a cross-domain GCD task, diversifying\ntask setups over episodes and combining open-set domain adaptation with a novel\nmargin loss and representation learning for optimizing the feature space\nprogressively. To capture the effects of fine-tuning on the base model, we\nextend task arithmetic by adaptively weighting the local task vectors\nconcerning the fine-tuned models based on their GCD performance on a validation\ndistribution. This episodic update mechanism boosts the adaptability of the\nbase model to unseen targets. Experiments across three datasets confirm that\nDG2CD-Net outperforms existing GCD methods customized for DG-GCD.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Vaibhav Rathore",
      "Shubhranil B",
      "Saikat Dutta",
      "Sarthak Mehrotra",
      "Zsolt Kira",
      "Biplab Banerjee"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.14895v1",
    "title": "Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations",
    "abstract": "Recently, multimodal large language models (MLLMs) have demonstrated\nremarkable performance in visual-language tasks. However, the authenticity of\nthe responses generated by MLLMs is often compromised by object hallucinations.\nWe identify that a key cause of these hallucinations is the model's\nover-susceptibility to specific image frequency features in detecting objects.\nIn this paper, we introduce Multi-Frequency Perturbations (MFP), a simple,\ncost-effective, and pluggable method that leverages both low-frequency and\nhigh-frequency features of images to perturb visual feature representations and\nexplicitly suppress redundant frequency-domain features during inference,\nthereby mitigating hallucinations. Experimental results demonstrate that our\nmethod significantly mitigates object hallucinations across various model\narchitectures. Furthermore, as a training-time method, MFP can be combined with\ninference-time methods to achieve state-of-the-art performance on the CHAIR\nbenchmark.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Shuo Li",
      "Jiajun Sun",
      "Guodong Zheng",
      "Xiaoran Fan",
      "Yujiong Shen",
      "Yi Lu",
      "Zhiheng Xi",
      "Yuming Yang",
      "Wenming Tan",
      "Tao Ji",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14892v1",
    "title": "Degradation Alchemy: Self-Supervised Unknown-to-Known Transformation for Blind Hyperspectral Image Fusion",
    "abstract": "Hyperspectral image (HSI) fusion is an efficient technique that combines\nlow-resolution HSI (LR-HSI) and high-resolution multispectral images (HR-MSI)\nto generate high-resolution HSI (HR-HSI). Existing supervised learning methods\n(SLMs) can yield promising results when test data degradation matches the\ntraining ones, but they face challenges in generalizing to unknown\ndegradations. To unleash the potential and generalization ability of SLMs, we\npropose a novel self-supervised unknown-to-known degradation transformation\nframework (U2K) for blind HSI fusion, which adaptively transforms unknown\ndegradation into the same type of degradation as those handled by pre-trained\nSLMs. Specifically, the proposed U2K framework consists of: (1) spatial and\nspectral Degradation Wrapping (DW) modules that map HR-HSI to unknown degraded\nHR-MSI and LR-HSI, and (2) Degradation Transformation (DT) modules that convert\nthese wrapped data into predefined degradation patterns. The transformed HR-MSI\nand LR-HSI pairs are then processed by a pre-trained network to reconstruct the\ntarget HR-HSI. We train the U2K framework in a self-supervised manner using\nconsistency loss and greedy alternating optimization, significantly improving\nthe flexibility of blind HSI fusion. Extensive experiments confirm the\neffectiveness of our proposed U2K framework in boosting the adaptability of\nfive existing SLMs under various degradation settings and surpassing\nstate-of-the-art blind methods.",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "He Huang",
      "Yong Chen",
      "Yujun Guo",
      "Wei He"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14891v1",
    "title": "MetaLadder: Ascending Mathematical Solution Quality via Analogical-Problem Reasoning Transfer",
    "abstract": "Large Language Models (LLMs) have demonstrated promising capabilities in\nsolving mathematical reasoning tasks, leveraging Chain-of-Thought (CoT) data as\na vital component in guiding answer generation. Current paradigms typically\ngenerate CoT and answers directly for a given problem, diverging from human\nproblem-solving strategies to some extent. Humans often solve problems by\nrecalling analogous cases and leveraging their solutions to reason about the\ncurrent task. Inspired by this cognitive process, we propose\n\\textbf{MetaLadder}, a novel framework that explicitly prompts LLMs to recall\nand reflect on meta-problems, those structurally or semantically analogous\nproblems, alongside their CoT solutions before addressing the target problem.\nAdditionally, we introduce a problem-restating mechanism to enhance the model's\ncomprehension of the target problem by regenerating the original question,\nwhich further improves reasoning accuracy. Therefore, the model can achieve\nreasoning transfer from analogical problems, mimicking human-like \"learning\nfrom examples\" and generalization abilities. Extensive experiments on\nmathematical benchmarks demonstrate that our MetaLadder significantly boosts\nLLMs' problem-solving accuracy, largely outperforming standard CoT-based\nmethods (\\textbf{10.3\\%} accuracy gain) and other methods. Our code and data\nhas been released at https://github.com/LHL3341/MetaLadder.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Honglin Lin",
      "Zhuoshi Pan",
      "Yu Li",
      "Qizhi Pei",
      "Xin Gao",
      "Mengzhang Cai",
      "Conghui He",
      "Lijun Wu"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14887v1",
    "title": "Pseudo-Relevance Feedback Can Improve Zero-Shot LLM-Based Dense Retrieval",
    "abstract": "Pseudo-relevance feedback (PRF) refines queries by leveraging initially\nretrieved documents to improve retrieval effectiveness. In this paper, we\ninvestigate how large language models (LLMs) can facilitate PRF for zero-shot\nLLM-based dense retrieval, extending the recently proposed PromptReps method.\nSpecifically, our approach uses LLMs to extract salient passage features-such\nas keywords and summaries-from top-ranked documents, which are then integrated\ninto PromptReps to produce enhanced query representations. Experiments on\npassage retrieval benchmarks demonstrate that incorporating PRF significantly\nboosts retrieval performance. Notably, smaller rankers with PRF can match the\neffectiveness of larger rankers without PRF, highlighting PRF's potential to\nimprove LLM-driven search while maintaining an efficient balance between\neffectiveness and resource usage.",
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "authors": [
      "Hang Li",
      "Xiao Wang",
      "Bevan Koopman",
      "Guido Zuccon"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14883v1",
    "title": "Envisioning an AI-Enhanced Mental Health Ecosystem",
    "abstract": "The rapid advancement of Large Language Models (LLMs), reasoning models, and\nagentic AI approaches coincides with a growing global mental health crisis,\nwhere increasing demand has not translated into adequate access to professional\nsupport, particularly for underserved populations. This presents a unique\nopportunity for AI to complement human-led interventions, offering scalable and\ncontext-aware support while preserving human connection in this sensitive\ndomain. We explore various AI applications in peer support, self-help\ninterventions, proactive monitoring, and data-driven insights, using a\nhuman-centred approach that ensures AI supports rather than replaces human\ninteraction. However, AI deployment in mental health fields presents challenges\nsuch as ethical concerns, transparency, privacy risks, and risks of\nover-reliance. We propose a hybrid ecosystem where where AI assists but does\nnot replace human providers, emphasising responsible deployment and evaluation.\nWe also present some of our early work and findings in several of these AI\napplications. Finally, we outline future research directions for refining\nAI-enhanced interventions while adhering to ethical and culturally sensitive\nguidelines.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.5.0"
    ],
    "authors": [
      "Kellie Yu Hui Sim",
      "Kenny Tsu Wei Choo"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14881v1",
    "title": "Exploring the Limits of KV Cache Compression in Visual Autoregressive Transformers",
    "abstract": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Bo Chen",
      "Xiaoyu Li",
      "Yekun Ke",
      "Yingyu Liang",
      "Zhenmei Shi",
      "Zhao Song"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14880v1",
    "title": "DPFlow: Adaptive Optical Flow Estimation with a Dual-Pyramid Framework",
    "abstract": "Optical flow estimation is essential for video processing tasks, such as\nrestoration and action recognition. The quality of videos is constantly\nincreasing, with current standards reaching 8K resolution. However, optical\nflow methods are usually designed for low resolution and do not generalize to\nlarge inputs due to their rigid architectures. They adopt downscaling or input\ntiling to reduce the input size, causing a loss of details and global\ninformation. There is also a lack of optical flow benchmarks to judge the\nactual performance of existing methods on high-resolution samples. Previous\nworks only conducted qualitative high-resolution evaluations on hand-picked\nsamples. This paper fills this gap in optical flow estimation in two ways. We\npropose DPFlow, an adaptive optical flow architecture capable of generalizing\nup to 8K resolution inputs while trained with only low-resolution samples. We\nalso introduce Kubric-NK, a new benchmark for evaluating optical flow methods\nwith input resolutions ranging from 1K to 8K. Our high-resolution evaluation\npushes the boundaries of existing methods and reveals new insights about their\ngeneralization capabilities. Extensive experimental results show that DPFlow\nachieves state-of-the-art results on the MPI-Sintel, KITTI 2015, Spring, and\nother high-resolution benchmarks.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Henrique Morimitsu",
      "Xiaobin Zhu",
      "Roberto M. Cesar Jr.",
      "Xiangyang Ji",
      "Xu-Cheng Yin"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15564v1",
    "title": "GReaTER: Generate Realistic Tabular data after data Enhancement and Reduction",
    "abstract": "Tabular data synthesis involves not only multi-table synthesis but also\ngenerating multi-modal data (e.g., strings and categories), which enables\ndiverse knowledge synthesis. However, separating numerical and categorical data\nhas limited the effectiveness of tabular data generation. The GReaT (Generate\nRealistic Tabular Data) framework uses Large Language Models (LLMs) to encode\nentire rows, eliminating the need to partition data types. Despite this, the\nframework's performance is constrained by two issues: (1) tabular data entries\nlack sufficient semantic meaning, limiting LLM's ability to leverage\npre-trained knowledge for in-context learning, and (2) complex multi-table\ndatasets struggle to establish effective relationships for collaboration. To\naddress these, we propose GReaTER (Generate Realistic Tabular Data after data\nEnhancement and Reduction), which includes: (1) a data semantic enhancement\nsystem that improves LLM's understanding of tabular data through mapping,\nenabling better in-context learning, and (2) a cross-table connecting method to\nestablish efficient relationships across complex tables. Experimental results\nshow that GReaTER outperforms the GReaT framework.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Tung Sum Thomas Kwok",
      "Chi-Hua Wang",
      "Guang Cheng"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14873v1",
    "title": "Robust Support Vector Machines for Imbalanced and Noisy Data via Benders Decomposition",
    "abstract": "This study introduces a novel formulation to enhance Support Vector Machines\n(SVMs) in handling class imbalance and noise. Unlike the conventional Soft\nMargin SVM, which penalizes the magnitude of constraint violations, the\nproposed model quantifies the number of violations and aims to minimize their\nfrequency. To achieve this, a binary variable is incorporated into the\nobjective function of the primal SVM formulation, replacing the traditional\nslack variable. Furthermore, each misclassified sample is assigned a priority\nand an associated constraint. The resulting formulation is a mixed-integer\nprogramming model, efficiently solved using Benders decomposition. The proposed\nmodel's performance was benchmarked against existing models, including Soft\nMargin SVM, weighted SVM, and NuSVC. Two primary hypotheses were examined: 1)\nThe proposed model improves the F1-score for the minority class in imbalanced\nclassification tasks. 2) The proposed model enhances classification accuracy in\nnoisy datasets. These hypotheses were evaluated using a Wilcoxon test across\nmultiple publicly available datasets from the OpenML repository. The results\nsupported both hypotheses (\\( p < 0.05 \\)). In addition, the proposed model\nexhibited several interesting properties, such as improved robustness to noise,\na decision boundary shift favoring the minority class, a reduced number of\nsupport vectors, and decreased prediction time. The open-source Python\nimplementation of the proposed SVM model is available.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Seyed Mojtaba Mohasel",
      "Hamidreza Koosha"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14869v1",
    "title": "Evaluating Time Series Models with Knowledge Discovery",
    "abstract": "Time series data is one of the most ubiquitous data modalities existing in a\ndiverse critical domains such as healthcare, seismology, manufacturing and\nenergy. Recent years, there are increasing interest of the data mining\ncommunity to develop time series deep learning models to pursue better\nperformance. The models performance often evaluate by certain evaluation\nmetrics such as RMSE, Accuracy, and F1-score. Yet time series data are often\nhard to interpret and are collected with unknown environmental factors, sensor\nconfiguration, latent physic mechanisms, and non-stationary evolving behavior.\nAs a result, a model that is better on standard metric-based evaluation may not\nalways perform better in real-world tasks. In this blue sky paper, we aim to\nexplore the challenge that exists in the metric-based evaluation framework for\ntime series data mining and propose a potential blue-sky idea -- developing a\nknowledge-discovery-based evaluation framework, which aims to effectively\nutilize domain-expertise knowledge to evaluate a model. We demonstrate that an\nevidence-seeking explanation can potentially have stronger persuasive power\nthan metric-based evaluation and obtain better generalization ability for time\nseries data mining tasks.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Li Zhang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14868v1",
    "title": "Efficient Personalization of Quantized Diffusion Model without Backpropagation",
    "abstract": "Diffusion models have shown remarkable performance in image synthesis, but\nthey demand extensive computational and memory resources for training,\nfine-tuning and inference. Although advanced quantization techniques have\nsuccessfully minimized memory usage for inference, training and fine-tuning\nthese quantized models still require large memory possibly due to\ndequantization for accurate computation of gradients and/or backpropagation for\ngradient-based algorithms. However, memory-efficient fine-tuning is\nparticularly desirable for applications such as personalization that often must\nbe run on edge devices like mobile phones with private data. In this work, we\naddress this challenge by quantizing a diffusion model with personalization via\nTextual Inversion and by leveraging a zeroth-order optimization on\npersonalization tokens without dequantization so that it does not require\ngradient and activation storage for backpropagation that consumes considerable\nmemory. Since a gradient estimation using zeroth-order optimization is quite\nnoisy for a single or a few images in personalization, we propose to denoise\nthe estimated gradient by projecting it onto a subspace that is constructed\nwith the past history of the tokens, dubbed Subspace Gradient. In addition, we\ninvestigated the influence of text embedding in image generation, leading to\nour proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for\nsampling with effective diffusion timesteps. Our method achieves comparable\nperformance to prior methods in image and text alignment scores for\npersonalizing Stable Diffusion with only forward passes while reducing training\nmemory demand up to $8.2\\times$.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Hoigi Seo",
      "Wongi Jeong",
      "Kyungryeol Lee",
      "Se Young Chun"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14867v1",
    "title": "DVHGNN: Multi-Scale Dilated Vision HGNN for Efficient Vision Recognition",
    "abstract": "Recently, Vision Graph Neural Network (ViG) has gained considerable attention\nin computer vision. Despite its groundbreaking innovation, Vision Graph Neural\nNetwork encounters key issues including the quadratic computational complexity\ncaused by its K-Nearest Neighbor (KNN) graph construction and the limitation of\npairwise relations of normal graphs. To address the aforementioned challenges,\nwe propose a novel vision architecture, termed Dilated Vision HyperGraph Neural\nNetwork (DVHGNN), which is designed to leverage multi-scale hypergraph to\nefficiently capture high-order correlations among objects. Specifically, the\nproposed method tailors Clustering and Dilated HyperGraph Construction (DHGC)\nto adaptively capture multi-scale dependencies among the data samples.\nFurthermore, a dynamic hypergraph convolution mechanism is proposed to\nfacilitate adaptive feature exchange and fusion at the hypergraph level.\nExtensive qualitative and quantitative evaluations of the benchmark image\ndatasets demonstrate that the proposed DVHGNN significantly outperforms the\nstate-of-the-art vision backbones. For instance, our DVHGNN-S achieves an\nimpressive top-1 accuracy of 83.1% on ImageNet-1K, surpassing ViG-S by +1.0%\nand ViHGNN-S by +0.6%.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Caoshuo Li",
      "Tanzhe Li",
      "Xiaobin Hu",
      "Donghao Luo",
      "Taisong Jin"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14863v1",
    "title": "Temporal-Consistent Video Restoration with Pre-trained Diffusion Models",
    "abstract": "Video restoration (VR) aims to recover high-quality videos from degraded\nones. Although recent zero-shot VR methods using pre-trained diffusion models\n(DMs) show good promise, they suffer from approximation errors during reverse\ndiffusion and insufficient temporal consistency. Moreover, dealing with 3D\nvideo data, VR is inherently computationally intensive. In this paper, we\nadvocate viewing the reverse process in DMs as a function and present a novel\nMaximum a Posterior (MAP) framework that directly parameterizes video frames in\nthe seed space of DMs, eliminating approximation errors. We also introduce\nstrategies to promote bilevel temporal consistency: semantic consistency by\nleveraging clustering structures in the seed space, and pixel-level consistency\nby progressive warping with optical flow refinements. Extensive experiments on\nmultiple virtual reality tasks demonstrate superior visual quality and temporal\nconsistency achieved by our method compared to the state-of-the-art.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Hengkang Wang",
      "Yang Liu",
      "Huidong Liu",
      "Chien-Chih Wang",
      "Yanhui Guo",
      "Hongdong Li",
      "Bryan Wang",
      "Ju Sun"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14862v2",
    "title": "Fine-Grained Open-Vocabulary Object Detection with Fined-Grained Prompts: Task, Dataset and Benchmark",
    "abstract": "Open-vocabulary detectors are proposed to locate and recognize objects in\nnovel classes. However, variations in vision-aware language vocabulary data\nused for open-vocabulary learning can lead to unfair and unreliable\nevaluations. Recent evaluation methods have attempted to address this issue by\nincorporating object properties or adding locations and characteristics to the\ncaptions. Nevertheless, since these properties and locations depend on the\nspecific details of the images instead of classes, detectors can not make\naccurate predictions without precise descriptions provided through human\nannotation. This paper introduces 3F-OVD, a novel task that extends supervised\nfine-grained object detection to the open-vocabulary setting. Our task is\nintuitive and challenging, requiring a deep understanding of Fine-grained\ncaptions and careful attention to Fine-grained details in images in order to\naccurately detect Fine-grained objects. Additionally, due to the scarcity of\nqualified fine-grained object detection datasets, we have created a new\ndataset, NEU-171K, tailored for both supervised and open-vocabulary settings.\nWe benchmark state-of-the-art object detectors on our dataset for both\nsettings. Furthermore, we propose a simple yet effective post-processing\ntechnique.",
    "categories": [
      "cs.CV",
      "I.2.0"
    ],
    "authors": [
      "Ying Liu",
      "Yijing Hua",
      "Haojiang Chai",
      "Yanbo Wang",
      "TengQi Ye"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.14860v1",
    "title": "Global Renewables Watch: A Temporal Dataset of Solar and Wind Energy Derived from Satellite Imagery",
    "abstract": "We present a comprehensive global temporal dataset of commercial solar\nphotovoltaic (PV) farms and onshore wind turbines, derived from high-resolution\nsatellite imagery analyzed quarterly from the fourth quarter of 2017 to the\nsecond quarter of 2024. We create this dataset by training deep learning-based\nsegmentation models to identify these renewable energy installations from\nsatellite imagery, then deploy them on over 13 trillion pixels covering the\nworld. For each detected feature, we estimate the construction date and the\npreceding land use type. This dataset offers crucial insights into progress\ntoward sustainable development goals and serves as a valuable resource for\npolicymakers, researchers, and stakeholders aiming to assess and promote\neffective strategies for renewable energy deployment. Our final spatial dataset\nincludes 375,197 individual wind turbines and 86,410 solar PV installations. We\naggregate our predictions to the country level -- estimating total power\ncapacity based on construction date, solar PV area, and number of windmills --\nand find an $r^2$ value of $0.96$ and $0.93$ for solar PV and onshore wind\nrespectively compared to IRENA's most recent 2023 country-level capacity\nestimates.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Caleb Robinson",
      "Anthony Ortiz",
      "Allen Kim",
      "Rahul Dodhia",
      "Andrew Zolli",
      "Shivaprakash K Nagaraju",
      "James Oakleaf",
      "Joe Kiesecker",
      "Juan M. Lavista Ferres"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14858v1",
    "title": "1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities",
    "abstract": "Scaling up self-supervised learning has driven breakthroughs in language and\nvision, yet comparable progress has remained elusive in reinforcement learning\n(RL). In this paper, we study building blocks for self-supervised RL that\nunlock substantial improvements in scalability, with network depth serving as a\ncritical factor. Whereas most RL papers in recent years have relied on shallow\narchitectures (around 2 - 5 layers), we demonstrate that increasing the depth\nup to 1024 layers can significantly boost performance. Our experiments are\nconducted in an unsupervised goal-conditioned setting, where no demonstrations\nor rewards are provided, so an agent must explore (from scratch) and learn how\nto maximize the likelihood of reaching commanded goals. Evaluated on simulated\nlocomotion and manipulation tasks, our approach increases performance by\n$2\\times$ - $50\\times$. Increasing the model depth not only increases success\nrates but also qualitatively changes the behaviors learned.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Kevin Wang",
      "Ishaan Javali",
      "Michał Bortkiewicz",
      "Tomasz Trzciński",
      "Benjamin Eysenbach"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.16543v1",
    "title": "Comprehensive Review of Reinforcement Learning for Medical Ultrasound Imaging",
    "abstract": "Medical Ultrasound (US) imaging has seen increasing demands over the past\nyears, becoming one of the most preferred imaging modalities in clinical\npractice due to its affordability, portability, and real-time capabilities.\nHowever, it faces several challenges that limit its applicability, such as\noperator dependency, variability in interpretation, and limited resolution,\nwhich are amplified by the low availability of trained experts. This calls for\nthe need of autonomous systems that are capable of reducing the dependency on\nhumans for increased efficiency and throughput. Reinforcement Learning (RL)\ncomes as a rapidly advancing field under Artificial Intelligence (AI) that\nallows the development of autonomous and intelligent agents that are capable of\nexecuting complex tasks through rewarded interactions with their environments.\nExisting surveys on advancements in the US scanning domain predominantly focus\non partially autonomous solutions leveraging AI for scanning guidance, organ\nidentification, plane recognition, and diagnosis. However, none of these\nsurveys explore the intersection between the stages of the US process and the\nrecent advancements in RL solutions. To bridge this gap, this review proposes a\ncomprehensive taxonomy that integrates the stages of the US process with the RL\ndevelopment pipeline. This taxonomy not only highlights recent RL advancements\nin the US domain but also identifies unresolved challenges crucial for\nachieving fully autonomous US systems. This work aims to offer a thorough\nreview of current research efforts, highlighting the potential of RL in\nbuilding autonomous US solutions while identifying limitations and\nopportunities for further advancements in this field.",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Hanae Elmekki",
      "Saidul Islam",
      "Ahmed Alagha",
      "Hani Sami",
      "Amanda Spilkin",
      "Ehsan Zakeri",
      "Antonela Mariel Zanuttini",
      "Jamal Bentahar",
      "Lyes Kadem",
      "Wen-Fang Xie",
      "Philippe Pibarot",
      "Rabeb Mizouni",
      "Hadi Otrok",
      "Shakti Singh",
      "Azzam Mourad"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14853v1",
    "title": "Unlocking the Capabilities of Vision-Language Models for Generalizable and Explainable Deepfake Detection",
    "abstract": "Current vision-language models (VLMs) have demonstrated remarkable\ncapabilities in understanding multimodal data, but their potential remains\nunderexplored for deepfake detection due to the misaligned of their knowledge\nand forensics patterns. To this end, we present a novel paradigm that unlocks\nVLMs' potential capabilities through three components: (1) A knowledge-guided\nforgery adaptation module that aligns VLM's semantic space with forensic\nfeatures through contrastive learning with external manipulation knowledge; (2)\nA multi-modal prompt tuning framework that jointly optimizes visual-textual\nembeddings for both localization and explainability; (3) An iterative\nrefinement strategy enabling multi-turn dialog for evidence-based reasoning.\nOur framework includes a VLM-based Knowledge-guided Forgery Detector (KFD), a\nVLM image encoder, and a Large Language Model (LLM). The VLM image encoder\nextracts visual prompt embeddings from images, while the LLM receives visual\nand question prompt embeddings for inference. The KFD is used to calculate\ncorrelations between image features and pristine/deepfake class embeddings,\nenabling forgery classification and localization. The outputs from these\ncomponents are used to construct forgery prompt embeddings. Finally, we feed\nthese prompt embeddings into the LLM to generate textual detection responses to\nassist judgment. Extensive experiments on multiple benchmarks, including FF++,\nCDF2, DFD, DFDCP, and DFDC, demonstrate that our scheme surpasses\nstate-of-the-art methods in generalization performance, while also supporting\nmulti-turn dialogue capabilities.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Peipeng Yu",
      "Jianwei Fei",
      "Hui Gao",
      "Xuan Feng",
      "Zhihua Xia",
      "Chip Hong Chang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15563v1",
    "title": "Dynamic Power Flow Analysis and Fault Characteristics: A Graph Attention Neural Network",
    "abstract": "We propose the joint graph attention neural network (GAT), clustering with\nadaptive neighbors (CAN) and probabilistic graphical model for dynamic power\nflow analysis and fault characteristics. In fact, computational efficiency is\nthe main focus to enhance, whilst we ensure the performance accuracy at the\naccepted level. Note that Machine Learning (ML) based schemes have a\nrequirement of sufficient labeled data during training, which is not easily\nsatisfied in practical applications. Also, there are unknown data due to new\narrived measurements or incompatible smart devices in complex smart grid\nsystems. These problems would be resolved by our proposed GAT based framework,\nwhich models the label dependency between the network data and learns object\nrepresentations such that it could achieve the semi-supervised fault diagnosis.\nTo create the joint label dependency, we develop the graph construction from\nthe raw acquired signals by using CAN. Next, we develop the probabilistic\ngraphical model of Markov random field for graph representation, which supports\nfor the GAT based framework. We then evaluate the proposed framework in the\nuse-case application in smart grid and make a fair comparison to the existing\nmethods.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Tan Le",
      "Van Le"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14849v1",
    "title": "LogLLaMA: Transformer-based log anomaly detection with LLaMA",
    "abstract": "Log anomaly detection refers to the task that distinguishes the anomalous log\nmessages from normal log messages. Transformer-based large language models\n(LLMs) are becoming popular for log anomaly detection because of their superb\nability to understand complex and long language patterns. In this paper, we\npropose LogLLaMA, a novel framework that leverages LLaMA2. LogLLaMA is first\nfinetuned on normal log messages from three large-scale datasets to learn their\npatterns. After finetuning, the model is capable of generating successive log\nmessages given previous log messages. Our generative model is further trained\nto identify anomalous log messages using reinforcement learning (RL). The\nexperimental results show that LogLLaMA outperforms the state-of-the-art\napproaches for anomaly detection on BGL, Thunderbird, and HDFS datasets.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Zhuoyi Yang",
      "Ian G. Harris"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14847v1",
    "title": "Project Jenkins: Turning Monkey Neural Data into Robotic Arm Movement, and Back",
    "abstract": "Project Jenkins explores how neural activity in the brain can be decoded into\nrobotic movement and, conversely, how movement patterns can be used to generate\nsynthetic neural data. Using real neural data recorded from motor and premotor\ncortex areas of a macaque monkey named Jenkins, we develop models for decoding\n(converting brain signals into robotic arm movements) and encoding (simulating\nbrain activity corresponding to a given movement). For the interface between\nthe brain simulation and the physical world, we utilized Koch v1.1 leader and\nfollower robotic arms. We developed an interactive web console that allows\nusers to generate synthetic brain data from joystick movements in real time.\nOur results are a step towards brain-controlled robotics, prosthetics, and\nenhancing normal motor function. By accurately modeling brain activity, we take\na step toward flexible brain-computer interfaces that generalize beyond\npredefined movements. To support the research community, we provide open source\ntools for both synthetic data generation and neural decoding, fostering\nreproducibility and accelerating progress. The project is available at\nhttps://www.808robots.com/projects/jenkins",
    "categories": [
      "cs.RO",
      "cs.AI",
      "eess.SP",
      "q-bio.NC"
    ],
    "authors": [
      "Andrii Zahorodnii",
      "Dima Yanovsky"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14845v1",
    "title": "ClimateGS: Real-Time Climate Simulation with 3D Gaussian Style Transfer",
    "abstract": "Adverse climate conditions pose significant challenges for autonomous\nsystems, demanding reliable perception and decision-making across diverse\nenvironments. To better simulate these conditions, physically-based NeRF\nrendering methods have been explored for their ability to generate realistic\nscene representations. However, these methods suffer from slow rendering speeds\nand long preprocessing times, making them impractical for real-time testing and\nuser interaction. This paper presents ClimateGS, a novel framework integrating\n3D Gaussian representations with physical simulation to enable real-time\nclimate effects rendering. The novelty of this work is threefold: 1) developing\na linear transformation for 3D Gaussian photorealistic style transfer, enabling\ndirect modification of spherical harmonics across bands for efficient and\nconsistent style adaptation; 2) developing a joint training strategy for 3D\nstyle transfer, combining supervised and self-supervised learning to accelerate\nconvergence while preserving original scene details; 3) developing a real-time\nrendering method for climate simulation, integrating physics-based effects with\n3D Gaussian to achieve efficient and realistic rendering. We evaluate ClimateGS\non MipNeRF360 and Tanks and Temples, demonstrating real-time rendering with\ncomparable or superior visual quality to SOTA 2D/3D methods, making it suitable\nfor interactive applications.",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Yuezhen Xie",
      "Meiying Zhang",
      "Qi Hao"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14837v1",
    "title": "SemanticFlow: A Self-Supervised Framework for Joint Scene Flow Prediction and Instance Segmentation in Dynamic Environments",
    "abstract": "Accurate perception of dynamic traffic scenes is crucial for high-level\nautonomous driving systems, requiring robust object motion estimation and\ninstance segmentation. However, traditional methods often treat them as\nseparate tasks, leading to suboptimal performance, spatio-temporal\ninconsistencies, and inefficiency in complex scenarios due to the absence of\ninformation sharing. This paper proposes a multi-task SemanticFlow framework to\nsimultaneously predict scene flow and instance segmentation of full-resolution\npoint clouds. The novelty of this work is threefold: 1) developing a\ncoarse-to-fine prediction based multi-task scheme, where an initial coarse\nsegmentation of static backgrounds and dynamic objects is used to provide\ncontextual information for refining motion and semantic information through a\nshared feature processing module; 2) developing a set of loss functions to\nenhance the performance of scene flow estimation and instance segmentation,\nwhile can help ensure spatial and temporal consistency of both static and\ndynamic objects within traffic scenes; 3) developing a self-supervised learning\nscheme, which utilizes coarse segmentation to detect rigid objects and compute\ntheir transformation matrices between sequential frames, enabling the\ngeneration of self-supervised labels. The proposed framework is validated on\nthe Argoverse and Waymo datasets, demonstrating superior performance in\ninstance segmentation accuracy, scene flow estimation, and computational\nefficiency, establishing a new benchmark for self-supervised methods in dynamic\nscene understanding.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Yinqi Chen",
      "Meiying Zhang",
      "Qi Hao",
      "Guang Zhou"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14836v1",
    "title": "On the Robustness Tradeoff in Fine-Tuning",
    "abstract": "Fine-tuning has become the standard practice for adapting pre-trained\n(upstream) models to downstream tasks. However, the impact on model robustness\nis not well understood. In this work, we characterize the robustness-accuracy\ntrade-off in fine-tuning. We evaluate the robustness and accuracy of fine-tuned\nmodels over 6 benchmark datasets and 7 different fine-tuning strategies. We\nobserve a consistent trade-off between adversarial robustness and accuracy.\nPeripheral updates such as BitFit are more effective for simple tasks--over 75%\nabove the average measured with area under the Pareto frontiers on CIFAR-10 and\nCIFAR-100. In contrast, fine-tuning information-heavy layers, such as attention\nlayers via Compacter, achieves a better Pareto frontier on more complex\ntasks--57.5% and 34.6% above the average on Caltech-256 and CUB-200,\nrespectively. Lastly, we observe that robustness of fine-tuning against\nout-of-distribution data closely tracks accuracy. These insights emphasize the\nneed for robustness-aware fine-tuning to ensure reliable real-world\ndeployments.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Kunyang Li",
      "Jean-Charles Noirot Ferrand",
      "Ryan Sheatsley",
      "Blaine Hoak",
      "Yohan Beugin",
      "Eric Pauley",
      "Patrick McDaniel"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14833v1",
    "title": "Curiosity-Diffuser: Curiosity Guide Diffusion Models for Reliability",
    "abstract": "One of the bottlenecks in robotic intelligence is the instability of neural\nnetwork models, which, unlike control models, lack a well-defined convergence\ndomain and stability. This leads to risks when applying intelligence in the\nphysical world. Specifically, imitation policy based on neural network may\ngenerate hallucinations, leading to inaccurate behaviors that impact the safety\nof real-world applications. To address this issue, this paper proposes the\nCuriosity-Diffuser, aimed at guiding the conditional diffusion model to\ngenerate trajectories with lower curiosity, thereby improving the reliability\nof policy. The core idea is to use a Random Network Distillation (RND)\ncuriosity module to assess whether the model's behavior aligns with the\ntraining data, and then minimize curiosity by classifier guidance diffusion to\nreduce overgeneralization during inference. Additionally, we propose a\ncomputationally efficient metric for evaluating the reliability of the policy,\nmeasuring the similarity between the generated behaviors and the training\ndataset, to facilitate research about reliability learning. Finally, simulation\nverify the effectiveness and applicability of the proposed method to a variety\nof scenarios, showing that Curiosity-Diffuser significantly improves task\nperformance and produces behaviors that are more similar to the training data.\nThe code for this work is available at: github.com/CarlDegio/Curiosity-Diffuser",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Zihao Liu",
      "Xing Liu",
      "Yizhai Zhang",
      "Zhengxiong Liu",
      "Panfeng Huang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14832v1",
    "title": "H2ST: Hierarchical Two-Sample Tests for Continual Out-of-Distribution Detection",
    "abstract": "Task Incremental Learning (TIL) is a specialized form of Continual Learning\n(CL) in which a model incrementally learns from non-stationary data streams.\nExisting TIL methodologies operate under the closed-world assumption, presuming\nthat incoming data remains in-distribution (ID). However, in an open-world\nsetting, incoming samples may originate from out-of-distribution (OOD) sources,\nwith their task identities inherently unknown. Continually detecting OOD\nsamples presents several challenges for current OOD detection methods: reliance\non model outputs leads to excessive dependence on model performance, selecting\nsuitable thresholds is difficult, hindering real-world deployment, and binary\nID/OOD classification fails to provide task-level identification. To address\nthese issues, we propose a novel continual OOD detection method called the\nHierarchical Two-sample Tests (H2ST). H2ST eliminates the need for threshold\nselection through hypothesis testing and utilizes feature maps to better\nexploit model capabilities without excessive dependence on model performance.\nThe proposed hierarchical architecture enables task-level detection with\nsuperior performance and lower overhead compared to non-hierarchical classifier\ntwo-sample tests. Extensive experiments and analysis validate the effectiveness\nof H2ST in open-world TIL scenarios and its superiority to the existing\nmethods. Code is available at\n\\href{https://github.com/YuhangLiuu/H2ST}{https://github.com/YuhangLiuu/H2ST}.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Yuhang Liu",
      "Wenjie Zhao",
      "Yunhui Guo"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14831v1",
    "title": "Robust Transmission of Punctured Text with Large Language Model-based Recovery",
    "abstract": "With the recent advancements in deep learning, semantic communication which\ntransmits only task-oriented features, has rapidly emerged. However, since\nfeature extraction relies on learning-based models, its performance\nfundamentally depends on the training dataset or tasks. For practical\nscenarios, it is essential to design a model that demonstrates robust\nperformance regardless of dataset or tasks. In this correspondence, we propose\na novel text transmission model that selects and transmits only a few\ncharacters and recovers the missing characters at the receiver using a large\nlanguage model (LLM). Additionally, we propose a novel importance character\nextractor (ICE), which selects transmitted characters to enhance LLM recovery\nperformance. Simulations demonstrate that the proposed filter selection by ICE\noutperforms random filter selection, which selects transmitted characters\nrandomly. Moreover, the proposed model exhibits robust performance across\ndifferent datasets and tasks and outperforms traditional bit-based\ncommunication in low signal-to-noise ratio conditions.",
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "authors": [
      "Sojeong Park",
      "Hyeonho Noh",
      "Hyun Jong Yang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14830v1",
    "title": "Decompositional Neural Scene Reconstruction with Generative Diffusion Prior",
    "abstract": "Decompositional reconstruction of 3D scenes, with complete shapes and\ndetailed texture of all objects within, is intriguing for downstream\napplications but remains challenging, particularly with sparse views as input.\nRecent approaches incorporate semantic or geometric regularization to address\nthis issue, but they suffer significant degradation in underconstrained areas\nand fail to recover occluded regions. We argue that the key to solving this\nproblem lies in supplementing missing information for these areas. To this end,\nwe propose DP-Recon, which employs diffusion priors in the form of Score\nDistillation Sampling (SDS) to optimize the neural representation of each\nindividual object under novel views. This provides additional information for\nthe underconstrained areas, but directly incorporating diffusion prior raises\npotential conflicts between the reconstruction and generative guidance.\nTherefore, we further introduce a visibility-guided approach to dynamically\nadjust the per-pixel SDS loss weights. Together these components enhance both\ngeometry and appearance recovery while remaining faithful to input images.\nExtensive experiments across Replica and ScanNet++ demonstrate that our method\nsignificantly outperforms SOTA methods. Notably, it achieves better object\nreconstruction under 10 views than the baselines under 100 views. Our method\nenables seamless text-based editing for geometry and appearance through SDS\noptimization and produces decomposed object meshes with detailed UV maps that\nsupport photorealistic Visual effects (VFX) editing. The project page is\navailable at https://dp-recon.github.io/.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Junfeng Ni",
      "Yu Liu",
      "Ruijie Lu",
      "Zirui Zhou",
      "Song-Chun Zhu",
      "Yixin Chen",
      "Siyuan Huang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14828v1",
    "title": "The CLEF-2025 CheckThat! Lab: Subjectivity, Fact-Checking, Claim Normalization, and Retrieval",
    "abstract": "The CheckThat! lab aims to advance the development of innovative technologies\ndesigned to identify and counteract online disinformation and manipulation\nefforts across various languages and platforms. The first five editions focused\non key tasks in the information verification pipeline, including\ncheck-worthiness, evidence retrieval and pairing, and verification. Since the\n2023 edition, the lab has expanded its scope to address auxiliary tasks that\nsupport research and decision-making in verification. In the 2025 edition, the\nlab revisits core verification tasks while also considering auxiliary\nchallenges. Task 1 focuses on the identification of subjectivity (a follow-up\nfrom CheckThat! 2024), Task 2 addresses claim normalization, Task 3 targets\nfact-checking numerical claims, and Task 4 explores scientific web discourse\nprocessing. These tasks present challenging classification and retrieval\nproblems at both the document and span levels, including multilingual settings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "I.2; I.2.7"
    ],
    "authors": [
      "Firoj Alam",
      "Julia Maria Struß",
      "Tanmoy Chakraborty",
      "Stefan Dietze",
      "Salim Hafid",
      "Katerina Korre",
      "Arianna Muti",
      "Preslav Nakov",
      "Federico Ruggeri",
      "Sebastian Schellhammer",
      "Vinay Setty",
      "Megha Sundriyal",
      "Konstantin Todorov",
      "Venktesh V"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14827v1",
    "title": "MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models",
    "abstract": "Multimodal foundation models (MMFMs) play a crucial role in various\napplications, including autonomous driving, healthcare, and virtual assistants.\nHowever, several studies have revealed vulnerabilities in these models, such as\ngenerating unsafe content by text-to-image models. Existing benchmarks on\nmultimodal models either predominantly assess the helpfulness of these models,\nor only focus on limited perspectives such as fairness and privacy. In this\npaper, we present the first unified platform, MMDT (Multimodal DecodingTrust),\ndesigned to provide a comprehensive safety and trustworthiness evaluation for\nMMFMs. Our platform assesses models from multiple perspectives, including\nsafety, hallucination, fairness/bias, privacy, adversarial robustness, and\nout-of-distribution (OOD) generalization. We have designed various evaluation\nscenarios and red teaming algorithms under different tasks for each perspective\nto generate challenging data, forming a high-quality benchmark. We evaluate a\nrange of multimodal models using MMDT, and our findings reveal a series of\nvulnerabilities and areas for improvement across these perspectives. This work\nintroduces the first comprehensive and unique safety and trustworthiness\nevaluation platform for MMFMs, paving the way for developing safer and more\nreliable MMFMs and systems. Our platform and benchmark are available at\nhttps://mmdecodingtrust.github.io/.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "authors": [
      "Chejian Xu",
      "Jiawei Zhang",
      "Zhaorun Chen",
      "Chulin Xie",
      "Mintong Kang",
      "Yujin Potter",
      "Zhun Wang",
      "Zhuowen Yuan",
      "Alexander Xiong",
      "Zidi Xiong",
      "Chenhui Zhang",
      "Lingzhi Yuan",
      "Yi Zeng",
      "Peiyang Xu",
      "Chengquan Guo",
      "Andy Zhou",
      "Jeffrey Ziwei Tan",
      "Xuandong Zhao",
      "Francesco Pinto",
      "Zhen Xiang",
      "Yu Gai",
      "Zinan Lin",
      "Dan Hendrycks",
      "Bo Li",
      "Dawn Song"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.16542v1",
    "title": "Defending Against Gradient Inversion Attacks for Biomedical Images via Learnable Data Perturbation",
    "abstract": "The increasing need for sharing healthcare data and collaborating on clinical\nresearch has raised privacy concerns. Health information leakage due to\nmalicious attacks can lead to serious problems such as misdiagnoses and patient\nidentification issues. Privacy-preserving machine learning (PPML) and\nprivacy-enhancing technologies, particularly federated learning (FL), have\nemerged in recent years as innovative solutions to balance privacy protection\nwith data utility; however, they also suffer from inherent privacy\nvulnerabilities. Gradient inversion attacks constitute major threats to data\nsharing in federated learning. Researchers have proposed many defenses against\ngradient inversion attacks. However, current defense methods for healthcare\ndata lack generalizability, i.e., existing solutions may not be applicable to\ndata from a broader range of populations. In addition, most existing defense\nmethods are tested using non-healthcare data, which raises concerns about their\napplicability to real-world healthcare systems. In this study, we present a\ndefense against gradient inversion attacks in federated learning. We achieve\nthis using latent data perturbation and minimax optimization, utilizing both\ngeneral and medical image datasets. Our method is compared to two baselines,\nand the results show that our approach can outperform the baselines with a\nreduction of 12.5% in the attacker's accuracy in classifying reconstructed\nimages. The proposed method also yields an increase of over 12.4% in Mean\nSquared Error (MSE) between the original and reconstructed images at the same\nlevel of model utility of around 90% client classification accuracy. The\nresults suggest the potential of a generalizable defense for healthcare data.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Shiyi Jiang",
      "Farshad Firouzi",
      "Krishnendu Chakrabarty"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.16541v1",
    "title": "Poly-FEVER: A Multilingual Fact Verification Benchmark for Hallucination Detection in Large Language Models",
    "abstract": "Hallucinations in generative AI, particularly in Large Language Models\n(LLMs), pose a significant challenge to the reliability of multilingual\napplications. Existing benchmarks for hallucination detection focus primarily\non English and a few widely spoken languages, lacking the breadth to assess\ninconsistencies in model performance across diverse linguistic contexts. To\naddress this gap, we introduce Poly-FEVER, a large-scale multilingual fact\nverification benchmark specifically designed for evaluating hallucination\ndetection in LLMs. Poly-FEVER comprises 77,973 labeled factual claims spanning\n11 languages, sourced from FEVER, Climate-FEVER, and SciFact. It provides the\nfirst large-scale dataset tailored for analyzing hallucination patterns across\nlanguages, enabling systematic evaluation of LLMs such as ChatGPT and the LLaMA\nseries. Our analysis reveals how topic distribution and web resource\navailability influence hallucination frequency, uncovering language-specific\nbiases that impact model accuracy. By offering a multilingual benchmark for\nfact verification, Poly-FEVER facilitates cross-linguistic comparisons of\nhallucination detection and contributes to the development of more reliable,\nlanguage-inclusive AI systems. The dataset is publicly available to advance\nresearch in responsible AI, fact-checking methodologies, and multilingual NLP,\npromoting greater transparency and robustness in LLM performance. The proposed\nPoly-FEVER is available at:\nhttps://huggingface.co/datasets/HanzhiZhang/Poly-FEVER.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Hanzhi Zhang",
      "Sumera Anjum",
      "Heng Fan",
      "Weijian Zheng",
      "Yan Huang",
      "Yunhe Feng"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14824v1",
    "title": "Prototype Perturbation for Relaxing Alignment Constraints in Backward-Compatible Learning",
    "abstract": "The traditional paradigm to update retrieval models requires re-computing the\nembeddings of the gallery data, a time-consuming and computationally intensive\nprocess known as backfilling. To circumvent backfilling, Backward-Compatible\nLearning (BCL) has been widely explored, which aims to train a new model\ncompatible with the old one. Many previous works focus on effectively aligning\nthe embeddings of the new model with those of the old one to enhance the\nbackward-compatibility. Nevertheless, such strong alignment constraints would\ncompromise the discriminative ability of the new model, particularly when\ndifferent classes are closely clustered and hard to distinguish in the old\nfeature space. To address this issue, we propose to relax the constraints by\nintroducing perturbations to the old feature prototypes. This allows us to\nalign the new feature space with a pseudo-old feature space defined by these\nperturbed prototypes, thereby preserving the discriminative ability of the new\nmodel in backward-compatible learning. We have developed two approaches for\ncalculating the perturbations: Neighbor-Driven Prototype Perturbation (NDPP)\nand Optimization-Driven Prototype Perturbation (ODPP). Particularly, they take\ninto account the feature distributions of not only the old but also the new\nmodels to obtain proper perturbations along with new model updating. Extensive\nexperiments on the landmark and commodity datasets demonstrate that our\napproaches perform favorably against state-of-the-art BCL algorithms.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Zikun Zhou",
      "Yushuai Sun",
      "Wenjie Pei",
      "Xin Li",
      "Yaowei Wang"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14813v1",
    "title": "Scaled Supervision is an Implicit Lipschitz Regularizer",
    "abstract": "In modern social media, recommender systems (RecSys) rely on the\nclick-through rate (CTR) as the standard metric to evaluate user engagement.\nCTR prediction is traditionally framed as a binary classification task to\npredict whether a user will interact with a given item. However, this approach\noverlooks the complexity of real-world social modeling, where the user, item,\nand their interactive features change dynamically in fast-paced online\nenvironments. This dynamic nature often leads to model instability, reflected\nin overfitting short-term fluctuations rather than higher-level interactive\npatterns. While overfitting calls for more scaled and refined supervisions,\ncurrent solutions often rely on binary labels that overly simplify fine-grained\nuser preferences through the thresholding process, which significantly reduces\nthe richness of the supervision. Therefore, we aim to alleviate the overfitting\nproblem by increasing the supervision bandwidth in CTR training. Specifically,\n(i) theoretically, we formulate the impact of fine-grained preferences on model\nstability as a Lipschitz constrain; (ii) empirically, we discover that scaling\nthe supervision bandwidth can act as an implicit Lipschitz regularizer, stably\noptimizing existing CTR models to achieve better generalizability. Extensive\nexperiments show that this scaled supervision significantly and consistently\nimproves the optimization process and the performance of existing CTR models,\neven without the need for additional hyperparameter tuning.",
    "categories": [
      "cs.LG",
      "cs.IR"
    ],
    "authors": [
      "Zhongyu Ouyang",
      "Chunhui Zhang",
      "Yaning Jia",
      "Soroush Vosoughi"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14809v1",
    "title": "Learning with Expert Abstractions for Efficient Multi-Task Continuous Control",
    "abstract": "Decision-making in complex, continuous multi-task environments is often\nhindered by the difficulty of obtaining accurate models for planning and the\ninefficiency of learning purely from trial and error. While precise environment\ndynamics may be hard to specify, human experts can often provide high-fidelity\nabstractions that capture the essential high-level structure of a task and user\npreferences in the target environment. Existing hierarchical approaches often\ntarget discrete settings and do not generalize across tasks. We propose a\nhierarchical reinforcement learning approach that addresses these limitations\nby dynamically planning over the expert-specified abstraction to generate\nsubgoals to learn a goal-conditioned policy. To overcome the challenges of\nlearning under sparse rewards, we shape the reward based on the optimal state\nvalue in the abstract model. This structured decision-making process enhances\nsample efficiency and facilitates zero-shot generalization. Our empirical\nevaluation on a suite of procedurally generated continuous control environments\ndemonstrates that our approach outperforms existing hierarchical reinforcement\nlearning methods in terms of sample efficiency, task completion rate,\nscalability to complex tasks, and generalization to novel scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jeff Jewett",
      "Sandhya Saisubramanian"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.15562v1",
    "title": "Shap-MeD",
    "abstract": "We present Shap-MeD, a text-to-3D object generative model specialized in the\nbiomedical domain. The objective of this study is to develop an assistant that\nfacilitates the 3D modeling of medical objects, thereby reducing development\ntime. 3D modeling in medicine has various applications, including surgical\nprocedure simulation and planning, the design of personalized prosthetic\nimplants, medical education, the creation of anatomical models, and the\ndevelopment of research prototypes. To achieve this, we leverage Shap-e, an\nopen-source text-to-3D generative model developed by OpenAI, and fine-tune it\nusing a dataset of biomedical objects. Our model achieved a mean squared error\n(MSE) of 0.089 in latent generation on the evaluation set, compared to Shap-e's\nMSE of 0.147. Additionally, we conducted a qualitative evaluation, comparing\nour model with others in the generation of biomedical objects. Our results\nindicate that Shap-MeD demonstrates higher structural accuracy in biomedical\nobject generation.",
    "categories": [
      "cs.GR",
      "cs.CE",
      "cs.CV"
    ],
    "authors": [
      "Nicolás Laverde",
      "Melissa Robles",
      "Johan Rodríguez"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14800v1",
    "title": "Long Context Modeling with Ranked Memory-Augmented Retrieval",
    "abstract": "Effective long-term memory management is crucial for language models handling\nextended contexts. We introduce a novel framework that dynamically ranks memory\nentries based on relevance. Unlike previous works, our model introduces a novel\nrelevance scoring and a pointwise re-ranking model for key-value embeddings,\ninspired by learning-to-rank techniques in information retrieval. Enhanced\nRanked Memory Augmented Retrieval ERMAR achieves state-of-the-art results on\nstandard benchmarks.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Ghadir Alselwi",
      "Hao Xue",
      "Shoaib Jameel",
      "Basem Suleiman",
      "Flora D. Salim",
      "Imran Razzak"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14799v1",
    "title": "Pruning-Based TinyML Optimization of Machine Learning Models for Anomaly Detection in Electric Vehicle Charging Infrastructure",
    "abstract": "With the growing need for real-time processing on IoT devices, optimizing\nmachine learning (ML) models' size, latency, and computational efficiency is\nessential. This paper investigates a pruning method for anomaly detection in\nresource-constrained environments, specifically targeting Electric Vehicle\nCharging Infrastructure (EVCI). Using the CICEVSE2024 dataset, we trained and\noptimized three models-Multi-Layer Perceptron (MLP), Long Short-Term Memory\n(LSTM), and XGBoost-through hyperparameter tuning with Optuna, further refining\nthem using SHapley Additive exPlanations (SHAP)-based feature selection (FS)\nand unstructured pruning techniques. The optimized models achieved significant\nreductions in model size and inference times, with only a marginal impact on\ntheir performance. Notably, our findings indicate that, in the context of EVCI,\npruning and FS can enhance computational efficiency while retaining critical\nanomaly detection capabilities.",
    "categories": [
      "cs.LG",
      "eess.SP"
    ],
    "authors": [
      "Fatemeh Dehrouyeh",
      "Ibrahim Shaer",
      "Soodeh Nikan",
      "Firouz Badrkhani Ajaei",
      "Abdallah Shami"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14797v1",
    "title": "FACTS&EVIDENCE: An Interactive Tool for Transparent Fine-Grained Factual Verification of Machine-Generated Text",
    "abstract": "With the widespread consumption of AI-generated content, there has been an\nincreased focus on developing automated tools to verify the factual accuracy of\nsuch content. However, prior research and tools developed for fact verification\ntreat it as a binary classification or a linear regression problem. Although\nthis is a useful mechanism as part of automatic guardrails in systems, we argue\nthat such tools lack transparency in the prediction reasoning and diversity in\nsource evidence to provide a trustworthy user experience. We develop\nFacts&Evidence - an interactive and transparent tool for user-driven\nverification of complex text. The tool facilitates the intricate\ndecision-making involved in fact-verification, presenting its users a breakdown\nof complex input texts to visualize the credibility of individual claims along\nwith an explanation of model decisions and attribution to multiple, diverse\nevidence sources. Facts&Evidence aims to empower consumers of machine-generated\ntext and give them agency to understand, verify, selectively trust and use such\ntext.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Varich Boonsanong",
      "Vidhisha Balachandran",
      "Xiaochuang Han",
      "Shangbin Feng",
      "Lucy Lu Wang",
      "Yulia Tsvetkov"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14796v1",
    "title": "A New Benchmark for Online Learning with Budget-Balancing Constraints",
    "abstract": "The adversarial Bandit with Knapsack problem is a multi-armed bandits problem\nwith budget constraints and adversarial rewards and costs. In each round, a\nlearner selects an action to take and observes the reward and cost of the\nselected action. The goal is to maximize the sum of rewards while satisfying\nthe budget constraint. The classical benchmark to compare against is the best\nfixed distribution over actions that satisfies the budget constraint in\nexpectation. Unlike its stochastic counterpart, where rewards and costs are\ndrawn from some fixed distribution (Badanidiyuru et al., 2018), the adversarial\nBwK problem does not admit a no-regret algorithm for every problem instance due\nto the \"spend-or-save\" dilemma (Immorlica et al., 2022).\n  A key problem left open by existing works is whether there exists a weaker\nbut still meaningful benchmark to compare against such that no-regret learning\nis still possible. In this work, we present a new benchmark to compare against,\nmotivated both by real-world applications such as autobidding and by its\nunderlying mathematical structure. The benchmark is based on the Earth Mover's\nDistance (EMD), and we show that sublinear regret is attainable against any\nstrategy whose spending pattern is within EMD $o(T^2)$ of any sub-pacing\nspending pattern.\n  As a special case, we obtain results against the \"pacing over windows\"\nbenchmark, where we partition time into disjoint windows of size $w$ and allow\nthe benchmark strategies to choose a different distribution over actions for\neach window while satisfying a pacing budget constraint. Against this\nbenchmark, our algorithm obtains a regret bound of\n$\\tilde{O}(T/\\sqrt{w}+\\sqrt{wT})$. We also show a matching lower bound, proving\nthe optimality of our algorithm in this important special case. In addition, we\nprovide further evidence of the necessity of the EMD condition for obtaining a\nsublinear regret.",
    "categories": [
      "cs.LG",
      "cs.GT"
    ],
    "authors": [
      "Mark Braverman",
      "Jingyi Liu",
      "Jieming Mao",
      "Jon Schneider",
      "Eric Xue"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14795v1",
    "title": "The Hardness of Validating Observational Studies with Experimental Data",
    "abstract": "Observational data is often readily available in large quantities, but can\nlead to biased causal effect estimates due to the presence of unobserved\nconfounding. Recent works attempt to remove this bias by supplementing\nobservational data with experimental data, which, when available, is typically\non a smaller scale due to the time and cost involved in running a randomised\ncontrolled trial. In this work, we prove a theorem that places fundamental\nlimits on this ``best of both worlds'' approach. Using the framework of\nimpossible inference, we show that although it is possible to use experimental\ndata to \\emph{falsify} causal effect estimates from observational data, in\ngeneral it is not possible to \\emph{validate} such estimates. Our theorem\nproves that while experimental data can be used to detect bias in observational\nstudies, without additional assumptions on the smoothness of the correction\nfunction, it can not be used to remove it. We provide a practical example of\nsuch an assumption, developing a novel Gaussian Process based approach to\nconstruct intervals which contain the true treatment effect with high\nprobability, both inside and outside of the support of the experimental data.\nWe demonstrate our methodology on both simulated and semi-synthetic datasets\nand make the \\href{https://github.com/Jakefawkes/Obs_and_exp_data}{code\navailable}.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "authors": [
      "Jake Fawkes",
      "Michael O'Riordan",
      "Athanasios Vlontzos",
      "Oriol Corcoll",
      "Ciarán Mark Gilligan-Lee"
    ],
    "published_date": "2025-03-19",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14786v1",
    "title": "SketchSplat: 3D Edge Reconstruction via Differentiable Multi-view Sketch Splatting",
    "abstract": "Edges are one of the most basic parametric primitives to describe structural\ninformation in 3D. In this paper, we study parametric 3D edge reconstruction\nfrom calibrated multi-view images. Previous methods usually reconstruct a 3D\nedge point set from multi-view 2D edge images, and then fit 3D edges to the\npoint set. However, noise in the point set may cause gaps among fitted edges,\nand the recovered edges may not align with input multi-view images since the\nedge fitting depends only on the reconstructed 3D point set. To mitigate these\nproblems, we propose SketchSplat, a method to reconstruct accurate, complete,\nand compact 3D edges via differentiable multi-view sketch splatting. We\nrepresent 3D edges as sketches, which are parametric lines and curves defined\nby attributes including control points, scales, and opacity. During edge\nreconstruction, we iteratively sample Gaussian points from a set of sketches\nand rasterize the Gaussians onto 2D edge images. Then the gradient of the image\nerror with respect to the input 2D edge images can be back-propagated to\noptimize the sketch attributes. Our method bridges 2D edge images and 3D edges\nin a differentiable manner, which ensures that 3D edges align well with 2D\nimages and leads to accurate and complete results. We also propose a series of\nadaptive topological operations and apply them along with the sketch\noptimization. The topological operations help reduce the number of sketches\nrequired while ensuring high accuracy, yielding a more compact reconstruction.\nFinally, we contribute an accurate 2D edge detector that improves the\nperformance of both ours and existing methods. Experiments show that our method\nachieves state-of-the-art accuracy, completeness, and compactness on a\nbenchmark CAD dataset.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Haiyang Ying",
      "Matthias Zwicker"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14785v1",
    "title": "SEEK: Self-adaptive Explainable Kernel For Nonstationary Gaussian Processes",
    "abstract": "Gaussian processes (GPs) are powerful probabilistic models that define\nflexible priors over functions, offering strong interpretability and\nuncertainty quantification. However, GP models often rely on simple, stationary\nkernels which can lead to suboptimal predictions and miscalibrated uncertainty\nestimates, especially in nonstationary real-world applications. In this paper,\nwe introduce SEEK, a novel class of learnable kernels to model complex,\nnonstationary functions via GPs. Inspired by artificial neurons, SEEK is\nderived from first principles to ensure symmetry and positive\nsemi-definiteness, key properties of valid kernels. The proposed method\nachieves flexible and adaptive nonstationarity by learning a mapping from a set\nof base kernels. Compared to existing techniques, our approach is more\ninterpretable and much less prone to overfitting. We conduct comprehensive\nsensitivity analyses and comparative studies to demonstrate that our approach\nis not robust to only many of its design choices, but also outperforms existing\nstationary/nonstationary kernels in both mean prediction accuracy and\nuncertainty quantification.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Nima Negarandeh",
      "Carlos Mora",
      "Ramin Bostanabad"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14783v1",
    "title": "RAT: Boosting Misclassification Detection Ability without Extra Data",
    "abstract": "As deep neural networks(DNN) become increasingly prevalent, particularly in\nhigh-stakes areas such as autonomous driving and healthcare, the ability to\ndetect incorrect predictions of models and intervene accordingly becomes\ncrucial for safety. In this work, we investigate the detection of misclassified\ninputs for image classification models from the lens of adversarial\nperturbation: we propose to use robust radius (a.k.a. input-space margin) as a\nconfidence metric and design two efficient estimation algorithms, RR-BS and\nRR-Fast, for misclassification detection. Furthermore, we design a training\nmethod called Radius Aware Training (RAT) to boost models' ability to identify\nmistakes. Extensive experiments show our method could achieve up to 29.3%\nreduction on AURC and 21.62% reduction in FPR@95TPR, compared with previous\nmethods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Ge Yan",
      "Tsui-Wei Weng"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14781v1",
    "title": "Fake Runs, Real Fixes -- Analyzing xPU Performance Through Simulation",
    "abstract": "As models become larger, ML accelerators are a scarce resource whose\nperformance must be continually optimized to improve efficiency. Existing\nperformance analysis tools are coarse grained, and fail to capture model\nperformance at the machine-code level. In addition, these tools often do not\nprovide specific recommendations for optimizations. We present xPU-Shark, a\nfine-grained methodology for analyzing ML models at the machine-code level that\nprovides actionable optimization suggestions. Our core insight is to use a\nhardware-level simulator, an artifact of the hardware design process that we\ncan re-purpose for performance analysis. xPU-Shark captures traces from\nproduction deployments running on accelerators and replays them in a modified\nmicroarchitecture simulator to gain low-level insights into the model's\nperformance. We implement xPU-Shark for our in-house accelerator and used it to\nanalyze the performance of several of our production LLMs, revealing several\npreviously-unknown microarchitecture inefficiencies. Leveraging these insights,\nwe optimize a common communication collective by up to 15% and reduce token\ngeneration latency by up to 4.1%.",
    "categories": [
      "cs.PF",
      "cs.DC",
      "cs.LG"
    ],
    "authors": [
      "Ioannis Zarkadas",
      "Amanda Tomlinson",
      "Asaf Cidon",
      "Baris Kasikci",
      "Ofir Weisse"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14779v1",
    "title": "Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution",
    "abstract": "Single Image Super-Resolution (SISR) aims to reconstruct high-resolution (HR)\nimages from low-resolution (LR) inputs. Deep learning, especially Convolutional\nNeural Networks (CNNs), has advanced SISR. However, increasing network depth\nincreases parameters, and memory usage, and slows training, which is\nproblematic for resource-limited devices. To address this, lightweight models\nare developed to balance accuracy and efficiency. We propose the Involution &\nBSConv Multi-Depth Distillation Network (IBMDN), combining Involution & BSConv\nMulti-Depth Distillation Block (IBMDB) and the Contrast and High-Frequency\nAttention Block (CHFAB). IBMDB integrates Involution and BSConv to balance\ncomputational efficiency and feature extraction. CHFAB enhances high-frequency\ndetails for better visual quality. IBMDB is compatible with other SISR\narchitectures and reduces complexity, improving evaluation metrics like PSNR\nand SSIM. In transformer-based models, IBMDB reduces memory usage while\nimproving feature extraction. In GANs, it enhances perceptual quality,\nbalancing pixel-level accuracy with perceptual details. Our experiments show\nthat the method achieves high accuracy with minimal computational cost. The\ncode is available at GitHub.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Akram Khatami-Rizi",
      "Ahmad Mahmoudi-Aznaveh"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14774v1",
    "title": "Revisiting Image Fusion for Multi-Illuminant White-Balance Correction",
    "abstract": "White balance (WB) correction in scenes with multiple illuminants remains a\npersistent challenge in computer vision. Recent methods explored fusion-based\napproaches, where a neural network linearly blends multiple sRGB versions of an\ninput image, each processed with predefined WB presets. However, we demonstrate\nthat these methods are suboptimal for common multi-illuminant scenarios.\nAdditionally, existing fusion-based methods rely on sRGB WB datasets lacking\ndedicated multi-illuminant images, limiting both training and evaluation. To\naddress these challenges, we introduce two key contributions. First, we propose\nan efficient transformer-based model that effectively captures spatial\ndependencies across sRGB WB presets, substantially improving upon linear fusion\ntechniques. Second, we introduce a large-scale multi-illuminant dataset\ncomprising over 16,000 sRGB images rendered with five different WB settings,\nalong with WB-corrected images. Our method achieves up to 100\\% improvement\nover existing techniques on our new multi-illuminant image fusion dataset.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "David Serrano-Lozano",
      "Aditya Arora",
      "Luis Herranz",
      "Konstantinos G. Derpanis",
      "Michael S. Brown",
      "Javier Vazquez-Corral"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.15561v1",
    "title": "Localized Physics-informed Gaussian Processes with Curriculum Training for Topology Optimization",
    "abstract": "We introduce a simultaneous and meshfree topology optimization (TO) framework\nbased on physics-informed Gaussian processes (GPs). Our framework endows all\ndesign and state variables via GP priors which have a shared, multi-output mean\nfunction that is parametrized via a customized deep neural network (DNN). The\nparameters of this mean function are estimated by minimizing a multi-component\nloss function that depends on the performance metric, design constraints, and\nthe residuals on the state equations. Our TO approach yields well-defined\nmaterial interfaces and has a built-in continuation nature that promotes global\noptimality. Other unique features of our approach include (1) its customized\nDNN which, unlike fully connected feed-forward DNNs, has a localized learning\ncapacity that enables capturing intricate topologies and reducing residuals in\nhigh gradient fields, (2) its loss function that leverages localized weights to\npromote solution accuracy around interfaces, and (3) its use of curriculum\ntraining to avoid local optimality.To demonstrate the power of our framework,\nwe validate it against commercial TO package COMSOL on three problems involving\ndissipated power minimization in Stokes flow.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Amin Yousefpour",
      "Shirin Hosseinmardi",
      "Xiangyu Sun",
      "Ramin Bostanabad"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.15560v1",
    "title": "Temporal Context Awareness: A Defense Framework Against Multi-turn Manipulation Attacks on Large Language Models",
    "abstract": "Large Language Models (LLMs) are increasingly vulnerable to sophisticated\nmulti-turn manipulation attacks, where adversaries strategically build context\nthrough seemingly benign conversational turns to circumvent safety measures and\nelicit harmful or unauthorized responses. These attacks exploit the temporal\nnature of dialogue to evade single-turn detection methods, representing a\ncritical security vulnerability with significant implications for real-world\ndeployments.\n  This paper introduces the Temporal Context Awareness (TCA) framework, a novel\ndefense mechanism designed to address this challenge by continuously analyzing\nsemantic drift, cross-turn intention consistency and evolving conversational\npatterns. The TCA framework integrates dynamic context embedding analysis,\ncross-turn consistency verification, and progressive risk scoring to detect and\nmitigate manipulation attempts effectively. Preliminary evaluations on\nsimulated adversarial scenarios demonstrate the framework's potential to\nidentify subtle manipulation patterns often missed by traditional detection\ntechniques, offering a much-needed layer of security for conversational AI\nsystems. In addition to outlining the design of TCA , we analyze diverse attack\nvectors and their progression across multi-turn conversation, providing\nvaluable insights into adversarial tactics and their impact on LLM\nvulnerabilities. Our findings underscore the pressing need for robust,\ncontext-aware defenses in conversational AI systems and highlight TCA framework\nas a promising direction for securing LLMs while preserving their utility in\nlegitimate applications. We make our implementation available to support\nfurther research in this emerging area of AI security.",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "Prashant Kulkarni",
      "Assaf Namer"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14760v1",
    "title": "Validation of Human Pose Estimation and Human Mesh Recovery for Extracting Clinically Relevant Motion Data from Videos",
    "abstract": "This work aims to discuss the current landscape of kinematic analysis tools,\nranging from the state-of-the-art in sports biomechanics such as inertial\nmeasurement units (IMUs) and retroreflective marker-based optical motion\ncapture (MoCap) to more novel approaches from the field of computing such as\nhuman pose estimation and human mesh recovery. Primarily, this comparative\nanalysis aims to validate the use of marker-less MoCap techniques in a clinical\nsetting by showing that these marker-less techniques are within a reasonable\nrange for kinematics analysis compared to the more cumbersome and less portable\nstate-of-the-art tools. Not only does marker-less motion capture using human\npose estimation produce results in-line with the results of both the IMU and\nMoCap kinematics but also benefits from a reduced set-up time and reduced\npractical knowledge and expertise to set up. Overall, while there is still room\nfor improvement when it comes to the quality of the data produced, we believe\nthat this compromise is within the room of error that these low-speed actions\nthat are used in small clinical tests.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Kai Armstrong",
      "Alexander Rodrigues",
      "Alexander P. Willmott",
      "Lei Zhang",
      "Xujiong Ye"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.15559v1",
    "title": "Advanced Relay-Based Collaborative Framework for Optimizing Synchronization in Split Federated Learning over Wireless Networks",
    "abstract": "Split Federated Learning (SFL) offers a promising approach for distributed\nmodel training in edge computing, combining the strengths of split learning in\nreducing computational demands on edge devices and enhancing data privacy, with\nthe role of federated aggregation to ensure model convergence and\nsynchronization across users. However, synchronization issues caused by user\nheterogeneity have hindered the development of the framework. To optimize\nsynchronization efficiency among users and improve overall system performance,\nwe propose a collaborative SFL framework (CSFL). Based on the model's\npartitioning capabilities, we design a mechanism called the collaborative relay\noptimization mechanism (CROM), where the assistance provided by high-efficiency\nusers is seen as a relay process, with the portion of the model they compute\nacting as the relay point. Wireless communication between users facilitates\nreal-time collaboration, allowing high-efficiency users to assist bottleneck\nusers in handling part of the model's computation, thereby alleviating the\ncomputational load on bottleneck users. Simulation results show that our\nproposed CSFL framework reduces synchronization delays and improves overall\nsystem throughput while maintaining similar performance and convergence rate to\nthe SFL framework. This demonstrates that the collaboration not only reduces\nsynchronization waiting time but also accelerates model convergence.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Haoran Gao",
      "Samuel D. Okegbile",
      "Jun Cai"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.15558v1",
    "title": "Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning",
    "abstract": "Physical AI systems need to perceive, understand, and perform complex actions\nin the physical world. In this paper, we present the Cosmos-Reason1 models that\ncan understand the physical world and generate appropriate embodied decisions\n(e.g., next step action) in natural language through long chain-of-thought\nreasoning processes. We begin by defining key capabilities for Physical AI\nreasoning, with a focus on physical common sense and embodied reasoning. To\nrepresent physical common sense, we use a hierarchical ontology that captures\nfundamental knowledge about space, time, and physics. For embodied reasoning,\nwe rely on a two-dimensional ontology that generalizes across different\nphysical embodiments. Building on these capabilities, we develop two multimodal\nlarge language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data\nand train our models in four stages: vision pre-training, general supervised\nfine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL)\nas the post-training. To evaluate our models, we build comprehensive benchmarks\nfor physical common sense and embodied reasoning according to our ontologies.\nEvaluation results show that Physical AI SFT and reinforcement learning bring\nsignificant improvements. To facilitate the development of Physical AI, we will\nmake our code and pre-trained models available under the NVIDIA Open Model\nLicense at https://github.com/nvidia-cosmos/cosmos-reason1.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "authors": [
      "NVIDIA",
      ":",
      "Alisson Azzolini",
      "Hannah Brandon",
      "Prithvijit Chattopadhyay",
      "Huayu Chen",
      "Jinju Chu",
      "Yin Cui",
      "Jenna Diamond",
      "Yifan Ding",
      "Francesco Ferroni",
      "Rama Govindaraju",
      "Jinwei Gu",
      "Siddharth Gururani",
      "Imad El Hanafi",
      "Zekun Hao",
      "Jacob Huffman",
      "Jingyi Jin",
      "Brendan Johnson",
      "Rizwan Khan",
      "George Kurian",
      "Elena Lantz",
      "Nayeon Lee",
      "Zhaoshuo Li",
      "Xuan Li",
      "Tsung-Yi Lin",
      "Yen-Chen Lin",
      "Ming-Yu Liu",
      "Andrew Mathau",
      "Yun Ni",
      "Lindsey Pavao",
      "Wei Ping",
      "David W. Romero",
      "Misha Smelyanskiy",
      "Shuran Song",
      "Lyne Tchapmi",
      "Andrew Z. Wang",
      "Boxin Wang",
      "Haoxiang Wang",
      "Fangyin Wei",
      "Jiashu Xu",
      "Yao Xu",
      "Xiaodong Yang",
      "Zhuolin Yang",
      "Xiaohui Zeng",
      "Zhe Zhang"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14757v1",
    "title": "RETHINED: A New Benchmark and Baseline for Real-Time High-Resolution Image Inpainting On Edge Devices",
    "abstract": "Existing image inpainting methods have shown impressive completion results\nfor low-resolution images. However, most of these algorithms fail at high\nresolutions and require powerful hardware, limiting their deployment on edge\ndevices. Motivated by this, we propose the first baseline for REal-Time\nHigh-resolution image INpainting on Edge Devices (RETHINED) that is able to\ninpaint at ultra-high-resolution and can run in real-time ($\\leq$ 30ms) in a\nwide variety of mobile devices. A simple, yet effective novel method formed by\na lightweight Convolutional Neural Network (CNN) to recover structure, followed\nby a resolution-agnostic patch replacement mechanism to provide detailed\ntexture. Specially our pipeline leverages the structural capacity of CNN and\nthe high-level detail of patch-based methods, which is a key component for\nhigh-resolution image inpainting. To demonstrate the real application of our\nmethod, we conduct an extensive analysis on various mobile-friendly devices and\ndemonstrate similar inpainting performance while being $\\mathrm{100 \\times\nfaster}$ than existing state-of-the-art methods. Furthemore, we realease\nDF8K-Inpainting, the first free-form mask UHD inpainting dataset.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Marcelo Sanchez",
      "Gil Triginer",
      "Ignacio Sarasua",
      "Lara Raad",
      "Coloma Ballester"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14756v1",
    "title": "SceneEval: Evaluating Semantic Coherence in Text-Conditioned 3D Indoor Scene Synthesis",
    "abstract": "Despite recent advances in text-conditioned 3D indoor scene generation, there\nremain gaps in the evaluation of these methods. Existing metrics primarily\nassess the realism of generated scenes by comparing them to a set of\nground-truth scenes, often overlooking alignment with the input text - a\ncritical factor in determining how effectively a method meets user\nrequirements. We present SceneEval, an evaluation framework designed to address\nthis limitation. SceneEval includes metrics for both explicit user\nrequirements, such as the presence of specific objects and their attributes\ndescribed in the input text, and implicit expectations, like the absence of\nobject collisions, providing a comprehensive assessment of scene quality. To\nfacilitate evaluation, we introduce SceneEval-100, a dataset of scene\ndescriptions with annotated ground-truth scene properties. We evaluate recent\nscene generation methods using SceneEval and demonstrate its ability to provide\ndetailed assessments of the generated scenes, highlighting strengths and areas\nfor improvement across multiple dimensions. Our results show that current\nmethods struggle at generating scenes that meet user requirements, underscoring\nthe need for further research in this direction.",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Hou In Ivan Tam",
      "Hou In Derek Pun",
      "Austin T. Wang",
      "Angel X. Chang",
      "Manolis Savva"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14755v1",
    "title": "Language Independent Named Entity Recognition via Orthogonal Transformation of Word Vectors",
    "abstract": "Word embeddings have been a key building block for NLP in which models relied\nheavily on word embeddings in many different tasks. In this paper, a model is\nproposed based on using Bidirectional LSTM/CRF with word embeddings to perform\nnamed entity recognition for any language. This is done by training a model on\na source language (English) and transforming word embeddings from the target\nlanguage into word embeddings of the source language by using an orthogonal\nlinear transformation matrix. Evaluation of the model shows that by training a\nmodel on an English dataset the model was capable of detecting named entities\nin an Arabic dataset without neither training or fine tuning the model on an\nArabic language dataset.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Omar E. Rakha",
      "Hazem M. Abbas"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14754v1",
    "title": "Bayesian Modeling of Zero-Shot Classifications for Urban Flood Detection",
    "abstract": "Street scene datasets, collected from Street View or dashboard cameras, offer\na promising means of detecting urban objects and incidents like street\nflooding. However, a major challenge in using these datasets is their lack of\nreliable labels: there are myriad types of incidents, many types occur rarely,\nand ground-truth measures of where incidents occur are lacking. Here, we\npropose BayFlood, a two-stage approach which circumvents this difficulty.\nFirst, we perform zero-shot classification of where incidents occur using a\npretrained vision-language model (VLM). Second, we fit a spatial Bayesian model\non the VLM classifications. The zero-shot approach avoids the need to annotate\nlarge training sets, and the Bayesian model provides frequent desiderata in\nurban settings - principled measures of uncertainty, smoothing across\nlocations, and incorporation of external data like stormwater accumulation\nzones. We comprehensively validate this two-stage approach, showing that VLMs\nprovide strong zero-shot signal for floods across multiple cities and time\nperiods, the Bayesian model improves out-of-sample prediction relative to\nbaseline methods, and our inferred flood risk correlates with known external\npredictors of risk. Having validated our approach, we show it can be used to\nimprove urban flood detection: our analysis reveals 113,738 people who are at\nhigh risk of flooding overlooked by current methods, identifies demographic\nbiases in existing methods, and suggests locations for new flood sensors. More\nbroadly, our results showcase how Bayesian modeling of zero-shot LM annotations\nrepresents a promising paradigm because it avoids the need to collect large\nlabeled datasets and leverages the power of foundation models while providing\nthe expressiveness and uncertainty quantification of Bayesian models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Matt Franchi",
      "Nikhil Garg",
      "Wendy Ju",
      "Emma Pierson"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14751v1",
    "title": "LipShiFT: A Certifiably Robust Shift-based Vision Transformer",
    "abstract": "Deriving tight Lipschitz bounds for transformer-based architectures presents\na significant challenge. The large input sizes and high-dimensional attention\nmodules typically prove to be crucial bottlenecks during the training process\nand leads to sub-optimal results. Our research highlights practical constraints\nof these methods in vision tasks. We find that Lipschitz-based margin training\nacts as a strong regularizer while restricting weights in successive layers of\nthe model. Focusing on a Lipschitz continuous variant of the ShiftViT model, we\naddress significant training challenges for transformer-based architectures\nunder norm-constrained input setting. We provide an upper bound estimate for\nthe Lipschitz constants of this model using the $l_2$ norm on common image\nclassification datasets. Ultimately, we demonstrate that our method scales to\nlarger models and advances the state-of-the-art in certified robustness for\ntransformer-based architectures.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Rohan Menon",
      "Nicola Franco",
      "Stephan Günnemann"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14749v1",
    "title": "Uncertainty Distillation: Teaching Language Models to Express Semantic Confidence",
    "abstract": "As large language models (LLMs) are increasingly used for factual\nquestion-answering, it becomes more important for LLMs to have the capability\nto communicate the likelihood that their answer is correct. For these\nverbalized expressions of uncertainty to be meaningful, they should reflect the\nerror rates at the expressed level of confidence. However, when prompted to\nexpress confidence, the error rates of current LLMs are inconsistent with their\ncommunicated confidences, highlighting the need for uncertainty quantification\nmethods. Many prior methods calculate lexical uncertainty, estimating a model's\nconfidence in the specific string it generated. In some cases, however, it may\nbe more useful to estimate semantic uncertainty, or the model's confidence in\nthe answer regardless of how it is verbalized. We propose a simple procedure,\nuncertainty distillation, to teach an LLM to verbalize calibrated semantic\nconfidences. Using held-out data to map initial uncertainty estimates to\nmeaningful probabilities, we create examples annotated with verbalized\nprobabilities for supervised fine-tuning. We demonstrate our method yields\nverbalized confidences that correlate with observed error rates with a small\nfine-tuned language model as well as with larger instruction-tuned models, and\nfind that our semantic uncertainty correlates well with lexical uncertainty on\nshort answers.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Sophia Hager",
      "David Mueller",
      "Kevin Duh",
      "Nicholas Andrews"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.15557v1",
    "title": "Motion Synthesis with Sparse and Flexible Keyjoint Control",
    "abstract": "Creating expressive character animations is labor-intensive, requiring\nintricate manual adjustment of animators across space and time. Previous works\non controllable motion generation often rely on a predefined set of dense\nspatio-temporal specifications (e.g., dense pelvis trajectories with exact\nper-frame timing), limiting practicality for animators. To process high-level\nintent and intuitive control in diverse scenarios, we propose a practical\ncontrollable motions synthesis framework that respects sparse and flexible\nkeyjoint signals. Our approach employs a decomposed diffusion-based motion\nsynthesis framework that first synthesizes keyjoint movements from sparse input\ncontrol signals and then synthesizes full-body motion based on the completed\nkeyjoint trajectories. The low-dimensional keyjoint movements can easily adapt\nto various control signal types, such as end-effector position for diverse\ngoal-driven motion synthesis, or incorporate functional constraints on a subset\nof keyjoints. Additionally, we introduce a time-agnostic control formulation,\neliminating the need for frame-specific timing annotations and enhancing\ncontrol flexibility. Then, the shared second stage can synthesize a natural\nwhole-body motion that precisely satisfies the task requirement from dense\nkeyjoint movements. We demonstrate the effectiveness of sparse and flexible\nkeyjoint control through comprehensive experiments on diverse datasets and\nscenarios.",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Inwoo Hwang",
      "Jinseok Bae",
      "Donggeun Lim",
      "Young Min Kim"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14736v1",
    "title": "HandSplat: Embedding-Driven Gaussian Splatting for High-Fidelity Hand Rendering",
    "abstract": "Existing 3D Gaussian Splatting (3DGS) methods for hand rendering rely on\nrigid skeletal motion with an oversimplified non-rigid motion model, which\nfails to capture fine geometric and appearance details. Additionally, they\nperform densification based solely on per-point gradients and process poses\nindependently, ignoring spatial and temporal correlations. These limitations\nlead to geometric detail loss, temporal instability, and inefficient point\ndistribution. To address these issues, we propose HandSplat, a novel Gaussian\nSplatting-based framework that enhances both fidelity and stability for hand\nrendering. To improve fidelity, we extend standard 3DGS attributes with\nimplicit geometry and appearance embeddings for finer non-rigid motion modeling\nwhile preserving the static hand characteristic modeled by original 3DGS\nattributes. Additionally, we introduce a local gradient-aware densification\nstrategy that dynamically refines Gaussian density in high-variation regions.\nTo improve stability, we incorporate pose-conditioned attribute regularization\nto encourage attribute consistency across similar poses, mitigating temporal\nartifacts. Extensive experiments on InterHand2.6M demonstrate that HandSplat\nsurpasses existing methods in fidelity and stability while achieving real-time\nperformance. We will release the code and pre-trained models upon acceptance.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yilan Dong",
      "Haohe Liu",
      "Qing Wang",
      "Jiahao Yang",
      "Wenqing Wang",
      "Gregory Slabaugh",
      "Shanxin Yuan"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14734v1",
    "title": "GR00T N1: An Open Foundation Model for Generalist Humanoid Robots",
    "abstract": "General-purpose robots need a versatile body and an intelligent mind. Recent\nadvancements in humanoid robots have shown great promise as a hardware platform\nfor building generalist autonomy in the human world. A robot foundation model,\ntrained on massive and diverse data sources, is essential for enabling the\nrobots to reason about novel situations, robustly handle real-world\nvariability, and rapidly learn new tasks. To this end, we introduce GR00T N1,\nan open foundation model for humanoid robots. GR00T N1 is a\nVision-Language-Action (VLA) model with a dual-system architecture. The\nvision-language module (System 2) interprets the environment through vision and\nlanguage instructions. The subsequent diffusion transformer module (System 1)\ngenerates fluid motor actions in real time. Both modules are tightly coupled\nand jointly trained end-to-end. We train GR00T N1 with a heterogeneous mixture\nof real-robot trajectories, human videos, and synthetically generated datasets.\nWe show that our generalist robot model GR00T N1 outperforms the\nstate-of-the-art imitation learning baselines on standard simulation benchmarks\nacross multiple robot embodiments. Furthermore, we deploy our model on the\nFourier GR-1 humanoid robot for language-conditioned bimanual manipulation\ntasks, achieving strong performance with high data efficiency.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "NVIDIA",
      "Johan Bjorck",
      "Fernando Castañeda",
      "Nikita Cherniadev",
      "Xingye Da",
      "Runyu Ding",
      "Linxi \"Jim\" Fan",
      "Yu Fang",
      "Dieter Fox",
      "Fengyuan Hu",
      "Spencer Huang",
      "Joel Jang",
      "Zhenyu Jiang",
      "Jan Kautz",
      "Kaushil Kundalia",
      "Lawrence Lao",
      "Zhiqi Li",
      "Zongyu Lin",
      "Kevin Lin",
      "Guilin Liu",
      "Edith Llontop",
      "Loic Magne",
      "Ajay Mandlekar",
      "Avnish Narayan",
      "Soroush Nasiriany",
      "Scott Reed",
      "You Liang Tan",
      "Guanzhi Wang",
      "Zu Wang",
      "Jing Wang",
      "Qi Wang",
      "Jiannan Xiang",
      "Yuqi Xie",
      "Yinzhen Xu",
      "Zhenjia Xu",
      "Seonghyeon Ye",
      "Zhiding Yu",
      "Ao Zhang",
      "Hao Zhang",
      "Yizhou Zhao",
      "Ruijie Zheng",
      "Yuke Zhu"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14728v1",
    "title": "Strategic resource allocation in memory encoding: An efficiency principle shaping language processing",
    "abstract": "How is the limited capacity of working memory efficiently used to support\nhuman linguistic behaviors? In this paper, we investigate strategic resource\nallocation as an efficiency principle for memory encoding in sentence\nprocessing. The idea is that working memory resources are dynamically and\nstrategically allocated to prioritize novel and unexpected information,\nenhancing their representations to make them less susceptible to memory decay\nand interference. Theoretically, from a resource-rational perspective, we argue\nthat this efficiency principle naturally arises from two functional assumptions\nabout working memory, namely, its limited capacity and its noisy\nrepresentation. Empirically, through naturalistic corpus data, we find\nconverging evidence for strategic resource allocation in the context of\ndependency locality from both the production and the comprehension side, where\nnon-local dependencies with less predictable antecedents are associated with\nreduced locality effect. However, our results also reveal considerable\ncross-linguistic variability, highlighting the need for a closer examination of\nhow strategic resource allocation, as a universal efficiency principle,\ninteracts with language-specific phrase structures.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Weijie Xu",
      "Richard Futrell"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14720v1",
    "title": "ShapeShift: Towards Text-to-Shape Arrangement Synthesis with Content-Aware Geometric Constraints",
    "abstract": "While diffusion-based models excel at generating photorealistic images from\ntext, a more nuanced challenge emerges when constrained to using only a fixed\nset of rigid shapes, akin to solving tangram puzzles or arranging real-world\nobjects to match semantic descriptions. We formalize this problem as\nshape-based image generation, a new text-guided image-to-image translation task\nthat requires rearranging the input set of rigid shapes into non-overlapping\nconfigurations and visually communicating the target concept. Unlike\npixel-manipulation approaches, our method, ShapeShift, explicitly parameterizes\neach shape within a differentiable vector graphics pipeline, iteratively\noptimizing placement and orientation through score distillation sampling from\npretrained diffusion models. To preserve arrangement clarity, we introduce a\ncontent-aware collision resolution mechanism that applies minimal semantically\ncoherent adjustments when overlaps occur, ensuring smooth convergence toward\nphysically valid configurations. By bridging diffusion-based semantic guidance\nwith explicit geometric constraints, our approach yields interpretable\ncompositions where spatial relationships clearly embody the textual prompt.\nExtensive experiments demonstrate compelling results across diverse scenarios,\nwith quantitative and qualitative advantages over alternative techniques.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Vihaan Misra",
      "Peter Schaldenbrand",
      "Jean Oh"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14719v1",
    "title": "ViVa-SAFELAND: a New Freeware for Safe Validation of Vision-based Navigation in Aerial Vehicles",
    "abstract": "ViVa-SAFELAND is an open source software library, aimed to test and evaluate\nvision-based navigation strategies for aerial vehicles, with special interest\nin autonomous landing, while complying with legal regulations and people's\nsafety. It consists of a collection of high definition aerial videos, focusing\non real unstructured urban scenarios, recording moving obstacles of interest,\nsuch as cars and people. Then, an Emulated Aerial Vehicle (EAV) with a virtual\nmoving camera is implemented in order to ``navigate\" inside the video,\naccording to high-order commands. ViVa-SAFELAND provides a new, safe, simple\nand fair comparison baseline to evaluate and compare different visual\nnavigation solutions under the same conditions, and to randomize variables\nalong several trials. It also facilitates the development of autonomous landing\nand navigation strategies, as well as the generation of image datasets for\ndifferent training tasks. Moreover, it is useful for training either human of\nautonomous pilots using deep learning. The effectiveness of the framework for\nvalidating vision algorithms is demonstrated through two case studies,\ndetection of moving objects and risk assessment segmentation. To our knowledge,\nthis is the first safe validation framework of its kind, to test and compare\nvisual navigation solution for aerial vehicles, which is a crucial aspect for\nurban deployment in complex real scenarios.",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Miguel S. Soriano-García",
      "Diego A. Mercado-Ravell"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14718v1",
    "title": "Second language Korean Universal Dependency treebank v1.2: Focus on data augmentation and annotation scheme refinement",
    "abstract": "We expand the second language (L2) Korean Universal Dependencies (UD)\ntreebank with 5,454 manually annotated sentences. The annotation guidelines are\nalso revised to better align with the UD framework. Using this enhanced\ntreebank, we fine-tune three Korean language models and evaluate their\nperformance on in-domain and out-of-domain L2-Korean datasets. The results show\nthat fine-tuning significantly improves their performance across various\nmetrics, thus highlighting the importance of using well-tailored L2 datasets\nfor fine-tuning first-language-based, general-purpose language models for the\nmorphosyntactic analysis of L2 data.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Hakyung Sung",
      "Gyu-Ho Shin"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14716v1",
    "title": "Construction Site Scaffolding Completeness Detection Based on Mask R-CNN and Hough Transform",
    "abstract": "Construction site scaffolding is essential for many building projects, and\nensuring its safety is crucial to prevent accidents. The safety inspector must\ncheck the scaffolding's completeness and integrity, where most violations\noccur. The inspection process includes ensuring all the components are in the\nright place since workers often compromise safety for convenience and\ndisassemble parts such as cross braces. This paper proposes a deep\nlearning-based approach to detect the scaffolding and its cross braces using\ncomputer vision. A scaffold image dataset with annotated labels is used to\ntrain a convolutional neural network (CNN) model. With the proposed approach,\nwe can automatically detect the completeness of cross braces from images taken\nat construction sites, without the need for manual inspection, saving a\nsignificant amount of time and labor costs. This non-invasive and efficient\nsolution for detecting scaffolding completeness can help improve safety in\nconstruction sites.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Pei-Hsin Lin",
      "Jacob J. Lin",
      "Shang-Hsien Hsieh"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.15555v1",
    "title": "Whole-Body Image-to-Image Translation for a Virtual Scanner in a Healthcare Digital Twin",
    "abstract": "Generating positron emission tomography (PET) images from computed tomography\n(CT) scans via deep learning offers a promising pathway to reduce radiation\nexposure and costs associated with PET imaging, improving patient care and\naccessibility to functional imaging. Whole-body image translation presents\nchallenges due to anatomical heterogeneity, often limiting generalized models.\nWe propose a framework that segments whole-body CT images into four\nregions-head, trunk, arms, and legs-and uses district-specific Generative\nAdversarial Networks (GANs) for tailored CT-to-PET translation. Synthetic PET\nimages from each region are stitched together to reconstruct the whole-body\nscan. Comparisons with a baseline non-segmented GAN and experiments with\nPix2Pix and CycleGAN architectures tested paired and unpaired scenarios.\nQuantitative evaluations at district, whole-body, and lesion levels\ndemonstrated significant improvements with our district-specific GANs. Pix2Pix\nyielded superior metrics, ensuring precise, high-quality image synthesis. By\naddressing anatomical heterogeneity, this approach achieves state-of-the-art\nresults in whole-body CT-to-PET translation. This methodology supports\nhealthcare Digital Twins by enabling accurate virtual PET scans from CT data,\ncreating virtual imaging representations to monitor, predict, and optimize\nhealth outcomes.",
    "categories": [
      "eess.IV",
      "cs.AI"
    ],
    "authors": [
      "Valerio Guarrasi",
      "Francesco Di Feola",
      "Rebecca Restivo",
      "Lorenzo Tronchin",
      "Paolo Soda"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14710v1",
    "title": "Variational Autoencoded Multivariate Spatial Fay-Herriot Models",
    "abstract": "Small area estimation models are essential for estimating population\ncharacteristics in regions with limited sample sizes, thereby supporting policy\ndecisions, demographic studies, and resource allocation, among other use cases.\nThe spatial Fay-Herriot model is one such approach that incorporates spatial\ndependence to improve estimation by borrowing strength from neighboring\nregions. However, this approach often requires substantial computational\nresources, limiting its scalability for high-dimensional datasets, especially\nwhen considering multiple (multivariate) responses. This paper proposes two\nmethods that integrate the multivariate spatial Fay-Herriot model with spatial\nrandom effects, learned through variational autoencoders, to efficiently\nleverage spatial structure. Importantly, after training the variational\nautoencoder to represent spatial dependence for a given set of geographies, it\nmay be used again in future modeling efforts, without the need for retraining.\nAdditionally, the use of the variational autoencoder to represent spatial\ndependence results in extreme improvements in computational efficiency, even\nfor massive datasets. We demonstrate the effectiveness of our approach using\n5-year period estimates from the American Community Survey over all census\ntracts in California.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Zhenhua Wang",
      "Paul A. Parker",
      "Scott H. Holan"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.16538v1",
    "title": "Leveraging Vision-Language Models for Open-Vocabulary Instance Segmentation and Tracking",
    "abstract": "This paper introduces a novel approach that leverages the capabilities of\nvision-language models (VLMs) by integrating them with established approaches\nfor open-vocabulary detection (OVD), instance segmentation, and tracking. We\nutilize VLM-generated structured descriptions to identify visible object\ninstances, collect application-relevant attributes, and inform an\nopen-vocabulary detector to extract corresponding bounding boxes that are\npassed to a video segmentation model providing precise segmentation masks and\ntracking capabilities. Once initialized, this model can then directly extract\nsegmentation masks, allowing processing of image streams in real time with\nminimal computational overhead. Tracks can be updated online as needed by\ngenerating new structured descriptions and corresponding open-vocabulary\ndetections. This combines the descriptive power of VLMs with the grounding\ncapability of OVD and the pixel-level understanding and speed of video\nsegmentation. Our evaluation across datasets and robotics platforms\ndemonstrates the broad applicability of this approach, showcasing its ability\nto extract task-specific attributes from non-standard objects in dynamic\nenvironments.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Bastian Pätzold",
      "Jan Nogga",
      "Sven Behnke"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14709v1",
    "title": "Better Private Distribution Testing by Leveraging Unverified Auxiliary Data",
    "abstract": "We extend the framework of augmented distribution testing (Aliakbarpour,\nIndyk, Rubinfeld, and Silwal, NeurIPS 2024) to the differentially private\nsetting. This captures scenarios where a data analyst must perform hypothesis\ntesting tasks on sensitive data, but is able to leverage prior knowledge\n(public, but possibly erroneous or untrusted) about the data distribution.\n  We design private algorithms in this augmented setting for three flagship\ndistribution testing tasks, uniformity, identity, and closeness testing, whose\nsample complexity smoothly scales with the claimed quality of the auxiliary\ninformation. We complement our algorithms with information-theoretic lower\nbounds, showing that their sample complexity is optimal (up to logarithmic\nfactors).",
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.DS"
    ],
    "authors": [
      "Maryam Aliakbarpour",
      "Arnav Burudgunte",
      "Clément Cannone",
      "Ronitt Rubinfeld"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.15554v1",
    "title": "A Comprehensive Study of LLM Secure Code Generation",
    "abstract": "LLMs are widely used in software development. However, the code generated by\nLLMs often contains vulnerabilities. Several secure code generation methods\nhave been proposed to address this issue, but their current evaluation schemes\nleave several concerns unaddressed. Specifically, most existing studies\nevaluate security and functional correctness separately, using different\ndatasets. That is, they assess vulnerabilities using security-related code\ndatasets while validating functionality with general code datasets. In\naddition, prior research primarily relies on a single static analyzer, CodeQL,\nto detect vulnerabilities in generated code, which limits the scope of security\nevaluation.\n  In this work, we conduct a comprehensive study to systematically assess the\nimprovements introduced by four state-of-the-art secure code generation\ntechniques. Specifically, we apply both security inspection and functionality\nvalidation to the same generated code and evaluate these two aspects together.\nWe also employ three popular static analyzers and two LLMs to identify\npotential vulnerabilities in the generated code. Our study reveals that\nexisting techniques often compromise the functionality of generated code to\nenhance security. Their overall performance remains limited when evaluating\nsecurity and functionality together. In fact, many techniques even degrade the\nperformance of the base LLM. Our further inspection reveals that these\ntechniques often either remove vulnerable lines of code entirely or generate\n``garbage code'' that is unrelated to the intended task. Moreover, the commonly\nused static analyzer CodeQL fails to detect several vulnerabilities, further\nobscuring the actual security improvements achieved by existing techniques. Our\nstudy serves as a guideline for a more rigorous and comprehensive evaluation of\nsecure code generation performance in future work.",
    "categories": [
      "cs.CR",
      "cs.LG",
      "cs.SE"
    ],
    "authors": [
      "Shih-Chieh Dai",
      "Jun Xu",
      "Guanhong Tao"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14701v1",
    "title": "ARC-Calib: Autonomous Markerless Camera-to-Robot Calibration via Exploratory Robot Motions",
    "abstract": "Camera-to-robot (also known as eye-to-hand) calibration is a critical\ncomponent of vision-based robot manipulation. Traditional marker-based methods\noften require human intervention for system setup. Furthermore, existing\nautonomous markerless calibration methods typically rely on pre-trained robot\ntracking models that impede their application on edge devices and require\nfine-tuning for novel robot embodiments. To address these limitations, this\npaper proposes a model-based markerless camera-to-robot calibration framework,\nARC-Calib, that is fully autonomous and generalizable across diverse robots and\nscenarios without requiring extensive data collection or learning. First,\nexploratory robot motions are introduced to generate easily trackable\ntrajectory-based visual patterns in the camera's image frames. Then, a\ngeometric optimization framework is proposed to exploit the coplanarity and\ncollinearity constraints from the observed motions to iteratively refine the\nestimated calibration result. Our approach eliminates the need for extra effort\nin either environmental marker setup or data collection and model training,\nrendering it highly adaptable across a wide range of real-world autonomous\nsystems. Extensive experiments are conducted in both simulation and the real\nworld to validate its robustness and generalizability.",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Podshara Chanrungmaneekul",
      "Yiting Chen",
      "Joshua T. Grace",
      "Aaron M. Dollar",
      "Kaiyu Hang"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14698v1",
    "title": "SplatVoxel: History-Aware Novel View Streaming without Temporal Training",
    "abstract": "We study the problem of novel view streaming from sparse-view videos, which\naims to generate a continuous sequence of high-quality, temporally consistent\nnovel views as new input frames arrive. However, existing novel view synthesis\nmethods struggle with temporal coherence and visual fidelity, leading to\nflickering and inconsistency. To address these challenges, we introduce\nhistory-awareness, leveraging previous frames to reconstruct the scene and\nimprove quality and stability. We propose a hybrid splat-voxel feed-forward\nscene reconstruction approach that combines Gaussian Splatting to propagate\ninformation over time, with a hierarchical voxel grid for temporal fusion.\nGaussian primitives are efficiently warped over time using a motion graph that\nextends 2D tracking models to 3D motion, while a sparse voxel transformer\nintegrates new temporal observations in an error-aware manner. Crucially, our\nmethod does not require training on multi-view video datasets, which are\ncurrently limited in size and diversity, and can be directly applied to\nsparse-view video streams in a history-aware manner at inference time. Our\napproach achieves state-of-the-art performance in both static and streaming\nscene reconstruction, effectively reducing temporal artifacts and visual\nartifacts while running at interactive rates (15 fps with 350ms delay) on a\nsingle H100 GPU. Project Page: https://19reborn.github.io/SplatVoxel/",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yiming Wang",
      "Lucy Chai",
      "Xuan Luo",
      "Michael Niemeyer",
      "Manuel Lagunas",
      "Stephen Lombardi",
      "Siyu Tang",
      "Tiancheng Sun"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14681v1",
    "title": "DPImageBench: A Unified Benchmark for Differentially Private Image Synthesis",
    "abstract": "Differentially private (DP) image synthesis aims to generate artificial\nimages that retain the properties of sensitive images while protecting the\nprivacy of individual images within the dataset. Despite recent advancements,\nwe find that inconsistent--and sometimes flawed--evaluation protocols have been\napplied across studies. This not only impedes the understanding of current\nmethods but also hinders future advancements.\n  To address the issue, this paper introduces DPImageBench for DP image\nsynthesis, with thoughtful design across several dimensions: (1) Methods. We\nstudy eleven prominent methods and systematically characterize each based on\nmodel architecture, pretraining strategy, and privacy mechanism. (2)\nEvaluation. We include nine datasets and seven fidelity and utility metrics to\nthoroughly assess them. Notably, we find that a common practice of selecting\ndownstream classifiers based on the highest accuracy on the sensitive test set\nnot only violates DP but also overestimates the utility scores. DPImageBench\ncorrects for these mistakes. (3) Platform. Despite the methods and evaluation\nprotocols, DPImageBench provides a standardized interface that accommodates\ncurrent and future implementations within a unified framework. With\nDPImageBench, we have several noteworthy findings. For example, contrary to the\ncommon wisdom that pretraining on public image datasets is usually beneficial,\nwe find that the distributional similarity between pretraining and sensitive\nimages significantly impacts the performance of the synthetic images and does\nnot always yield improvements. In addition, adding noise to low-dimensional\nfeatures, such as the high-level characteristics of sensitive images, is less\naffected by the privacy budget compared to adding noise to high-dimensional\nfeatures, like weight gradients. The former methods perform better than the\nlatter under a low privacy budget.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Chen Gong",
      "Kecen Li",
      "Zinan Lin",
      "Tianhao Wang"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14674v1",
    "title": "Elevating Visual Question Answering through Implicitly Learned Reasoning Pathways in LVLMs",
    "abstract": "Large Vision-Language Models (LVLMs) have shown remarkable progress in\nvarious multimodal tasks, yet they often struggle with complex visual reasoning\nthat requires multi-step inference. To address this limitation, we propose\nMF-SQ-LLaVA, a novel approach that enhances LVLMs by enabling implicit\nself-questioning through end-to-end training. Our method involves augmenting\nvisual question answering datasets with reasoning chains consisting of\nsub-question and answer pairs, and training the LVLM with a multi-task loss\nthat encourages the generation and answering of these intermediate steps, as\nwell as the prediction of the final answer. We conduct extensive experiments on\nthe ScienceQA and VQAv2 datasets, demonstrating that MF-SQ-LLaVA significantly\noutperforms existing state-of-the-art models, including the base LLaVA and the\noriginal SQ-LLaVA. Ablation studies further validate the contribution of each\ncomponent of our approach, and human evaluation confirms the improved accuracy\nand coherence of the reasoning process enabled by our method.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Liu Jing",
      "Amirul Rahman"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14671v1",
    "title": "Generating Medically-Informed Explanations for Depression Detection using LLMs",
    "abstract": "Early detection of depression from social media data offers a valuable\nopportunity for timely intervention. However, this task poses significant\nchallenges, requiring both professional medical knowledge and the development\nof accurate and explainable models. In this paper, we propose LLM-MTD (Large\nLanguage Model for Multi-Task Depression Detection), a novel approach that\nleverages a pre-trained large language model to simultaneously classify social\nmedia posts for depression and generate textual explanations grounded in\nmedical diagnostic criteria. We train our model using a multi-task learning\nframework with a combined loss function that optimizes both classification\naccuracy and explanation quality. We evaluate LLM-MTD on the benchmark Reddit\nSelf-Reported Depression Dataset (RSDD) and compare its performance against\nseveral competitive baseline methods, including traditional machine learning\nand fine-tuned BERT. Our experimental results demonstrate that LLM-MTD achieves\nstate-of-the-art performance in depression detection, showing significant\nimprovements in AUPRC and other key metrics. Furthermore, human evaluation of\nthe generated explanations reveals their relevance, completeness, and medical\naccuracy, highlighting the enhanced interpretability of our approach. This work\ncontributes a novel methodology for depression detection that combines the\npower of large language models with the crucial aspect of explainability.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Xiangyong Chen",
      "Xiaochuan Lin"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.15552v1",
    "title": "Personalized Attacks of Social Engineering in Multi-turn Conversations -- LLM Agents for Simulation and Detection",
    "abstract": "The rapid advancement of conversational agents, particularly chatbots powered\nby Large Language Models (LLMs), poses a significant risk of social engineering\n(SE) attacks on social media platforms. SE detection in multi-turn, chat-based\ninteractions is considerably more complex than single-instance detection due to\nthe dynamic nature of these conversations. A critical factor in mitigating this\nthreat is understanding the mechanisms through which SE attacks operate,\nspecifically how attackers exploit vulnerabilities and how victims' personality\ntraits contribute to their susceptibility. In this work, we propose an\nLLM-agentic framework, SE-VSim, to simulate SE attack mechanisms by generating\nmulti-turn conversations. We model victim agents with varying personality\ntraits to assess how psychological profiles influence susceptibility to\nmanipulation. Using a dataset of over 1000 simulated conversations, we examine\nattack scenarios in which adversaries, posing as recruiters, funding agencies,\nand journalists, attempt to extract sensitive information. Based on this\nanalysis, we present a proof of concept, SE-OmniGuard, to offer personalized\nprotection to users by leveraging prior knowledge of the victims personality,\nevaluating attack strategies, and monitoring information exchanges in\nconversations to identify potential SE attempts.",
    "categories": [
      "cs.CR",
      "cs.CL"
    ],
    "authors": [
      "Tharindu Kumarage",
      "Cameron Johnson",
      "Jadie Adams",
      "Lin Ai",
      "Matthias Kirchner",
      "Anthony Hoogs",
      "Joshua Garland",
      "Julia Hirschberg",
      "Arslan Basharat",
      "Huan Liu"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14665v2",
    "title": "These Magic Moments: Differentiable Uncertainty Quantification of Radiance Field Models",
    "abstract": "This paper introduces a novel approach to uncertainty quantification for\nradiance fields by leveraging higher-order moments of the rendering equation.\nUncertainty quantification is crucial for downstream tasks including view\nplanning and scene understanding, where safety and robustness are paramount.\nHowever, the high dimensionality and complexity of radiance fields pose\nsignificant challenges for uncertainty quantification, limiting the use of\nthese uncertainty quantification methods in high-speed decision-making. We\ndemonstrate that the probabilistic nature of the rendering process enables\nefficient and differentiable computation of higher-order moments for radiance\nfield outputs, including color, depth, and semantic predictions. Our method\noutperforms existing radiance field uncertainty estimation techniques while\noffering a more direct, computationally efficient, and differentiable\nformulation without the need for post-processing. Beyond uncertainty\nquantification, we also illustrate the utility of our approach in downstream\napplications such as next-best-view (NBV) selection and active ray sampling for\nneural radiance field training. Extensive experiments on synthetic and\nreal-world scenes confirm the efficacy of our approach, which achieves\nstate-of-the-art performance while maintaining simplicity.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Parker Ewen",
      "Hao Chen",
      "Seth Isaacson",
      "Joey Wilson",
      "Katherine A. Skinner",
      "Ram Vasudevan"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-20"
  },
  {
    "id": "2503.14663v1",
    "title": "Sepsyn-OLCP: An Online Learning-based Framework for Early Sepsis Prediction with Uncertainty Quantification using Conformal Prediction",
    "abstract": "Sepsis is a life-threatening syndrome with high morbidity and mortality in\nhospitals. Early prediction of sepsis plays a crucial role in facilitating\nearly interventions for septic patients. However, early sepsis prediction\nsystems with uncertainty quantification and adaptive learning are scarce. This\npaper proposes Sepsyn-OLCP, a novel online learning algorithm for early sepsis\nprediction by integrating conformal prediction for uncertainty quantification\nand Bayesian bandits for adaptive decision-making. By combining the robustness\nof Bayesian models with the statistical uncertainty guarantees of conformal\nprediction methodologies, this algorithm delivers accurate and trustworthy\npredictions, addressing the critical need for reliable and adaptive systems in\nhigh-stakes healthcare applications such as early sepsis prediction. We\nevaluate the performance of Sepsyn-OLCP in terms of regret in stochastic bandit\nsetting, the area under the receiver operating characteristic curve (AUROC),\nand F-measure. Our results show that Sepsyn-OLCP outperforms existing\nindividual models, increasing AUROC of a neural network from 0.64 to 0.73\nwithout retraining and high computational costs. And the model selection policy\nconverges to the optimal strategy in the long run. We propose a novel\nreinforcement learning-based framework integrated with conformal prediction\ntechniques to provide uncertainty quantification for early sepsis prediction.\nThe proposed methodology delivers accurate and trustworthy predictions,\naddressing a critical need in high-stakes healthcare applications like early\nsepsis prediction.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Anni Zhou",
      "Beyah Raheem",
      "Rishikesan Kamaleswaran",
      "Yao Xie"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14662v1",
    "title": "ConQuer: A Framework for Concept-Based Quiz Generation",
    "abstract": "Quizzes play a crucial role in education by reinforcing students'\nunderstanding of key concepts and encouraging self-directed exploration.\nHowever, compiling high-quality quizzes can be challenging and require deep\nexpertise and insight into specific subject matter. Although LLMs have greatly\nenhanced the efficiency of quiz generation, concerns remain regarding the\nquality of these AI-generated quizzes and their educational impact on students.\nTo address these issues, we introduce ConQuer, a concept-based quiz generation\nframework that leverages external knowledge sources. We employ comprehensive\nevaluation dimensions to assess the quality of the generated quizzes, using\nLLMs as judges. Our experiment results demonstrate a 4.8% improvement in\nevaluation scores and a 77.52% win rate in pairwise comparisons against\nbaseline quiz sets. Ablation studies further underscore the effectiveness of\neach component in our framework. Code available at\nhttps://github.com/sofyc/ConQuer.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yicheng Fu",
      "Zikui Wang",
      "Liuxin Yang",
      "Meiqing Huo",
      "Zhongdongming Dai"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.16537v1",
    "title": "Do Multimodal Large Language Models Understand Welding?",
    "abstract": "This paper examines the performance of Multimodal LLMs (MLLMs) in skilled\nproduction work, with a focus on welding. Using a novel data set of real-world\nand online weld images, annotated by a domain expert, we evaluate the\nperformance of two state-of-the-art MLLMs in assessing weld acceptability\nacross three contexts: RV \\& Marine, Aeronautical, and Farming. While both\nmodels perform better on online images, likely due to prior exposure or\nmemorization, they also perform relatively well on unseen, real-world weld\nimages. Additionally, we introduce WeldPrompt, a prompting strategy that\ncombines Chain-of-Thought generation with in-context learning to mitigate\nhallucinations and improve reasoning. WeldPrompt improves model recall in\ncertain contexts but exhibits inconsistent performance across others. These\nresults underscore the limitations and potentials of MLLMs in high-stakes\ntechnical domains and highlight the importance of fine-tuning, domain-specific\ndata, and more sophisticated prompting strategies to improve model reliability.\nThe study opens avenues for further research into multimodal learning in\nindustry applications.",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "authors": [
      "Grigorii Khvatskii",
      "Yong Suk Lee",
      "Corey Angst",
      "Maria Gibbs",
      "Robert Landers",
      "Nitesh V. Chawla"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14655v1",
    "title": "Core-Periphery Principle Guided State Space Model for Functional Connectome Classification",
    "abstract": "Understanding the organization of human brain networks has become a central\nfocus in neuroscience, particularly in the study of functional connectivity,\nwhich plays a crucial role in diagnosing neurological disorders. Advances in\nfunctional magnetic resonance imaging and machine learning techniques have\nsignificantly improved brain network analysis. However, traditional machine\nlearning approaches struggle to capture the complex relationships between brain\nregions, while deep learning methods, particularly Transformer-based models,\nface computational challenges due to their quadratic complexity in\nlong-sequence modeling. To address these limitations, we propose a\nCore-Periphery State-Space Model (CP-SSM), an innovative framework for\nfunctional connectome classification. Specifically, we introduce Mamba, a\nselective state-space model with linear complexity, to effectively capture\nlong-range dependencies in functional brain networks. Furthermore, inspired by\nthe core-periphery (CP) organization, a fundamental characteristic of brain\nnetworks that enhances efficient information transmission, we design CP-MoE, a\nCP-guided Mixture-of-Experts that improves the representation learning of brain\nconnectivity patterns. We evaluate CP-SSM on two benchmark fMRI datasets: ABIDE\nand ADNI. Experimental results demonstrate that CP-SSM surpasses\nTransformer-based models in classification performance while significantly\nreducing computational complexity. These findings highlight the effectiveness\nand efficiency of CP-SSM in modeling brain functional connectivity, offering a\npromising direction for neuroimaging-based neurological disease diagnosis.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "authors": [
      "Minheng Chen",
      "Xiaowei Yu",
      "Jing Zhang",
      "Tong Chen",
      "Chao Cao",
      "Yan Zhuang",
      "Yanjun Lyu",
      "Lu Zhang",
      "Tianming Liu",
      "Dajiang Zhu"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14654v1",
    "title": "A Simple Combination of Diffusion Models for Better Quality Trade-Offs in Image Denoising",
    "abstract": "Diffusion models have garnered considerable interest in computer vision,\nowing both to their capacity to synthesize photorealistic images and to their\nproven effectiveness in image reconstruction tasks. However, existing\napproaches fail to efficiently balance the high visual quality of diffusion\nmodels with the low distortion achieved by previous image reconstruction\nmethods. Specifically, for the fundamental task of additive Gaussian noise\nremoval, we first illustrate an intuitive method for leveraging pretrained\ndiffusion models. Further, we introduce our proposed Linear Combination\nDiffusion Denoiser (LCDD), which unifies two complementary inference procedures\n- one that leverages the model's generative potential and another that ensures\nfaithful signal recovery. By exploiting the inherent structure of the denoising\nsamples, LCDD achieves state-of-the-art performance and offers controlled,\nwell-behaved trade-offs through a simple scalar hyperparameter adjustment.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jonas Dornbusch",
      "Emanuel Pfarr",
      "Florin-Alexandru Vasluianu",
      "Frank Werner",
      "Radu Timofte"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14649v2",
    "title": "RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving",
    "abstract": "Retrieval-augmented generation (RAG), which combines large language models\n(LLMs) with retrievals from external knowledge databases, is emerging as a\npopular approach for reliable LLM serving. However, efficient RAG serving\nremains an open challenge due to the rapid emergence of many RAG variants and\nthe substantial differences in workload characteristics across them. In this\npaper, we make three fundamental contributions to advancing RAG serving. First,\nwe introduce RAGSchema, a structured abstraction that captures the wide range\nof RAG algorithms, serving as a foundation for performance optimization.\nSecond, we analyze several representative RAG workloads with distinct\nRAGSchema, revealing significant performance variability across these\nworkloads. Third, to address this variability and meet diverse performance\nrequirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a\nsystem optimization framework for efficient RAG serving. Our evaluation shows\nthat RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in\ntime-to-first-token latency compared to RAG systems built on LLM-system\nextensions.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.DC",
      "C.1; C.4; H.3"
    ],
    "authors": [
      "Wenqi Jiang",
      "Suvinay Subramanian",
      "Cat Graves",
      "Gustavo Alonso",
      "Amir Yazdanbakhsh",
      "Vidushi Dadu"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-21"
  },
  {
    "id": "2503.14640v1",
    "title": "Dynamic Accumulated Attention Map for Interpreting Evolution of Decision-Making in Vision Transformer",
    "abstract": "Various Vision Transformer (ViT) models have been widely used for image\nrecognition tasks. However, existing visual explanation methods can not display\nthe attention flow hidden inside the inner structure of ViT models, which\nexplains how the final attention regions are formed inside a ViT for its\ndecision-making. In this paper, a novel visual explanation approach, Dynamic\nAccumulated Attention Map (DAAM), is proposed to provide a tool that can\nvisualize, for the first time, the attention flow from the top to the bottom\nthrough ViT networks. To this end, a novel decomposition module is proposed to\nconstruct and store the spatial feature information by unlocking the [class]\ntoken generated by the self-attention module of each ViT block. The module can\nalso obtain the channel importance coefficients by decomposing the\nclassification score for supervised ViT models. Because of the lack of\nclassification score in self-supervised ViT models, we propose dimension-wise\nimportance weights to compute the channel importance coefficients. Such spatial\nfeatures are linearly combined with the corresponding channel importance\ncoefficients, forming the attention map for each block. The dynamic attention\nflow is revealed by block-wisely accumulating each attention map. The\ncontribution of this work focuses on visualizing the evolution dynamic of the\ndecision-making attention for any intermediate block inside a ViT model by\nproposing a novel decomposition module and dimension-wise importance weights.\nThe quantitative and qualitative analysis consistently validate the\neffectiveness and superior capacity of the proposed DAAM for not only\ninterpreting ViT models with the fully-connected layers as the classifier but\nalso self-supervised ViT models. The code is available at\nhttps://github.com/ly9802/DynamicAccumulatedAttentionMap.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Yi Liao",
      "Yongsheng Gao",
      "Weichuan Zhang"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.16536v1",
    "title": "Word2Minecraft: Generating 3D Game Levels through Large Language Models",
    "abstract": "We present Word2Minecraft, a system that leverages large language models to\ngenerate playable game levels in Minecraft based on structured stories. The\nsystem transforms narrative elements-such as protagonist goals, antagonist\nchallenges, and environmental settings-into game levels with both spatial and\ngameplay constraints. We introduce a flexible framework that allows for the\ncustomization of story complexity, enabling dynamic level generation. The\nsystem employs a scaling algorithm to maintain spatial consistency while\nadapting key game elements. We evaluate Word2Minecraft using both metric-based\nand human-based methods. Our results show that GPT-4-Turbo outperforms\nGPT-4o-Mini in most areas, including story coherence and objective enjoyment,\nwhile the latter excels in aesthetic appeal. We also demonstrate the system' s\nability to generate levels with high map enjoyment, offering a promising step\nforward in the intersection of story generation and game design. We open-source\nthe code at https://github.com/JMZ-kk/Word2Minecraft/tree/word2mc_v0",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Shuo Huang",
      "Muhammad Umair Nasir",
      "Steven James",
      "Julian Togelius"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14637v1",
    "title": "Reinforcement learning-based motion imitation for physiologically plausible musculoskeletal motor control",
    "abstract": "How do humans move? The quest to understand human motion has broad\napplications in numerous fields, ranging from computer animation and motion\nsynthesis to neuroscience, human prosthetics and rehabilitation. Although\nadvances in reinforcement learning (RL) have produced impressive results in\ncapturing human motion using simplified humanoids, controlling physiologically\naccurate models of the body remains an open challenge. In this work, we present\na model-free motion imitation framework (KINESIS) to advance the understanding\nof muscle-based motor control. Using a musculoskeletal model of the lower body\nwith 80 muscle actuators and 20 DoF, we demonstrate that KINESIS achieves\nstrong imitation performance on 1.9 hours of motion capture data, is\ncontrollable by natural language through pre-trained text-to-motion generative\nmodels, and can be fine-tuned to carry out high-level tasks such as target goal\nreaching. Importantly, KINESIS generates muscle activity patterns that\ncorrelate well with human EMG activity. The physiological plausibility makes\nKINESIS a promising model for tackling challenging problems in human motor\ncontrol theory, which we highlight by investigating Bernstein's redundancy\nproblem in the context of locomotion. Code, videos and benchmarks will be\navailable at https://github.com/amathislab/Kinesis.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "q-bio.NC"
    ],
    "authors": [
      "Merkourios Simos",
      "Alberto Silvio Chiappa",
      "Alexander Mathis"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14630v1",
    "title": "Assessing Large Language Models for Automated Feedback Generation in Learning Programming Problem Solving",
    "abstract": "Providing effective feedback is important for student learning in programming\nproblem-solving. In this sense, Large Language Models (LLMs) have emerged as\npotential tools to automate feedback generation. However, their reliability and\nability to identify reasoning errors in student code remain not well\nunderstood. This study evaluates the performance of four LLMs (GPT-4o, GPT-4o\nmini, GPT-4-Turbo, and Gemini-1.5-pro) on a benchmark dataset of 45 student\nsolutions. We assessed the models' capacity to provide accurate and insightful\nfeedback, particularly in identifying reasoning mistakes. Our analysis reveals\nthat 63\\% of feedback hints were accurate and complete, while 37\\% contained\nmistakes, including incorrect line identification, flawed explanations, or\nhallucinated issues. These findings highlight the potential and limitations of\nLLMs in programming education and underscore the need for improvements to\nenhance reliability and minimize risks in educational applications.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Priscylla Silva",
      "Evandro Costa"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14626v1",
    "title": "An Explainable Framework for Misinformation Identification via Critical Question Answering",
    "abstract": "Natural language misinformation detection approaches have been, to date,\nlargely dependent on sequence classification methods, producing opaque systems\nin which the reasons behind classification as misinformation are unclear. While\nan effort has been made in the area of automated fact-checking to propose\nexplainable approaches to the problem, this is not the case for automated\nreason-checking systems. In this paper, we propose a new explainable framework\nfor both factual and rational misinformation detection based on the theory of\nArgumentation Schemes and Critical Questions. For that purpose, we create and\nrelease NLAS-CQ, the first corpus combining 3,566 textbook-like natural\nlanguage argumentation scheme instances and 4,687 corresponding answers to\ncritical questions related to these arguments. On the basis of this corpus, we\nimplement and validate our new framework which combines classification with\nquestion answering to analyse arguments in search of misinformation, and\nprovides the explanations in form of critical questions to the human user.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Ramon Ruiz-Dolz",
      "John Lawrence"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14621v1",
    "title": "Reducing False Ventricular Tachycardia Alarms in ICU Settings: A Machine Learning Approach",
    "abstract": "False arrhythmia alarms in intensive care units (ICUs) are a significant\nchallenge, contributing to alarm fatigue and potentially compromising patient\nsafety. Ventricular tachycardia (VT) alarms are particularly difficult to\ndetect accurately due to their complex nature. This paper presents a machine\nlearning approach to reduce false VT alarms using the VTaC dataset, a benchmark\ndataset of annotated VT alarms from ICU monitors. We extract time-domain and\nfrequency-domain features from waveform data, preprocess the data, and train\ndeep learning models to classify true and false VT alarms. Our results\ndemonstrate high performance, with ROC-AUC scores exceeding 0.96 across various\ntraining configurations. This work highlights the potential of machine learning\nto improve the accuracy of VT alarm detection in clinical settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Grace Funmilayo Farayola",
      "Akinyemi Sadeeq Akintola",
      "Oluwole Fagbohun",
      "Chukwuka Michael Oforgu",
      "Bisola Faith Kayode",
      "Christian Chimezie",
      "Temitope Kadri",
      "Abiola Oludotun",
      "Nelson Ogbeide",
      "Mgbame Michael",
      "Adeseye Ifaturoti",
      "Toyese Oloyede"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14620v1",
    "title": "Retrieval-Augmented Simulacra: Generative Agents for Up-to-date and Knowledge-Adaptive Simulations",
    "abstract": "In the 2023 edition of the White Paper on Information and Communications, it\nis estimated that the population of social networking services in Japan will\nexceed 100 million by 2022, and the influence of social networking services in\nJapan is growing significantly. In addition, marketing using SNS and research\non the propagation of emotions and information on SNS are being actively\nconducted, creating the need for a system for predicting trends in SNS\ninteractions. We have already created a system that simulates the behavior of\nvarious communities on SNS by building a virtual SNS environment in which\nagents post and reply to each other in a chat community created by agents using\na LLMs. In this paper, we evaluate the impact of the search extension\ngeneration mechanism used to create posts and replies in a virtual SNS\nenvironment using a simulation system on the ability to generate posts and\nreplies. As a result of the evaluation, we confirmed that the proposed search\nextension generation mechanism, which mimics human search behavior, generates\nthe most natural exchange.",
    "categories": [
      "cs.CL",
      "cs.SI"
    ],
    "authors": [
      "Hikaru Shimadzu",
      "Takehito Utsuro",
      "Daisuke Kitayama"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14618v1",
    "title": "Anomaly-Flow: A Multi-domain Federated Generative Adversarial Network for Distributed Denial-of-Service Detection",
    "abstract": "Distributed denial-of-service (DDoS) attacks remain a critical threat to\nInternet services, causing costly disruptions. While machine learning (ML) has\nshown promise in DDoS detection, current solutions struggle with multi-domain\nenvironments where attacks must be detected across heterogeneous networks and\norganizational boundaries. This limitation severely impacts the practical\ndeployment of ML-based defenses in real-world settings.\n  This paper introduces Anomaly-Flow, a novel framework that addresses this\ncritical gap by combining Federated Learning (FL) with Generative Adversarial\nNetworks (GANs) for privacy-preserving, multi-domain DDoS detection. Our\nproposal enables collaborative learning across diverse network domains while\npreserving data privacy through synthetic flow generation. Through extensive\nevaluation across three distinct network datasets, Anomaly-Flow achieves an\naverage F1-score of $0.747$, outperforming baseline models. Importantly, our\nframework enables organizations to share attack detection capabilities without\nexposing sensitive network data, making it particularly valuable for critical\ninfrastructure and privacy-sensitive sectors.\n  Beyond immediate technical contributions, this work provides insights into\nthe challenges and opportunities in multi-domain DDoS detection, establishing a\nfoundation for future research in collaborative network defense systems. Our\nfindings have important implications for academic research and industry\npractitioners working to deploy practical ML-based security solutions.",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "Leonardo Henrique de Melo",
      "Gustavo de Carvalho Bertoli",
      "Michele Nogueira",
      "Aldri Luiz dos Santos",
      "Lourenço Alves Pereira Junior"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14615v1",
    "title": "Unique Hard Attention: A Tale of Two Sides",
    "abstract": "Understanding the expressive power of transformers has recently attracted\nattention, as it offers insights into their abilities and limitations. Many\nstudies analyze unique hard attention transformers, where attention selects a\nsingle position that maximizes the attention scores. When multiple positions\nachieve the maximum score, either the rightmost or the leftmost of those is\nchosen. In this paper, we highlight the importance of this seeming triviality.\nRecently, finite-precision transformers with both leftmost- and rightmost-hard\nattention were shown to be equivalent to Linear Temporal Logic (LTL). We show\nthat this no longer holds with only leftmost-hard attention -- in that case,\nthey correspond to a \\emph{strictly weaker} fragment of LTL. Furthermore, we\nshow that models with leftmost-hard attention are equivalent to \\emph{soft}\nattention, suggesting they may better approximate real-world transformers than\nright-attention models. These findings refine the landscape of transformer\nexpressivity and underscore the role of attention directionality.",
    "categories": [
      "cs.LG",
      "cs.CC",
      "cs.CL",
      "cs.FL"
    ],
    "authors": [
      "Selim Jerad",
      "Anej Svete",
      "Jiaoda Li",
      "Ryan Cotterell"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14607v1",
    "title": "Can Large Vision Language Models Read Maps Like a Human?",
    "abstract": "In this paper, we introduce MapBench-the first dataset specifically designed\nfor human-readable, pixel-based map-based outdoor navigation, curated from\ncomplex path finding scenarios. MapBench comprises over 1600 pixel space map\npath finding problems from 100 diverse maps. In MapBench, LVLMs generate\nlanguage-based navigation instructions given a map image and a query with\nbeginning and end landmarks. For each map, MapBench provides Map Space Scene\nGraph (MSSG) as an indexing data structure to convert between natural language\nand evaluate LVLM-generated results. We demonstrate that MapBench significantly\nchallenges state-of-the-art LVLMs both zero-shot prompting and a\nChain-of-Thought (CoT) augmented reasoning framework that decomposes map\nnavigation into sequential cognitive processes. Our evaluation of both\nopen-source and closed-source LVLMs underscores the substantial difficulty\nposed by MapBench, revealing critical limitations in their spatial reasoning\nand structured decision-making capabilities. We release all the code and\ndataset in https://github.com/taco-group/MapBench.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Shuo Xing",
      "Zezhou Sun",
      "Shuangyu Xie",
      "Kaiyuan Chen",
      "Yanjia Huang",
      "Yuping Wang",
      "Jiachen Li",
      "Dezhen Song",
      "Zhengzhong Tu"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.16535v1",
    "title": "Vision-Language Embodiment for Monocular Depth Estimation",
    "abstract": "Depth estimation is a core problem in robotic perception and vision tasks,\nbut 3D reconstruction from a single image presents inherent uncertainties.\nCurrent depth estimation models primarily rely on inter-image relationships for\nsupervised training, often overlooking the intrinsic information provided by\nthe camera itself. We propose a method that embodies the camera model and its\nphysical characteristics into a deep learning model, computing embodied scene\ndepth through real-time interactions with road environments. The model can\ncalculate embodied scene depth in real-time based on immediate environmental\nchanges using only the intrinsic properties of the camera, without any\nadditional equipment. By combining embodied scene depth with RGB image\nfeatures, the model gains a comprehensive perspective on both geometric and\nvisual details. Additionally, we incorporate text descriptions containing\nenvironmental content and depth information as priors for scene understanding,\nenriching the model's perception of objects. This integration of image and\nlanguage - two inherently ambiguous modalities - leverages their complementary\nstrengths for monocular depth estimation. The real-time nature of the embodied\nlanguage and depth prior model ensures that the model can continuously adjust\nits perception and behavior in dynamic environments. Experimental results show\nthat the embodied depth estimation method enhances model performance across\ndifferent scenes.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jinchang Zhang",
      "Guoyu Lu"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14604v1",
    "title": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives",
    "abstract": "The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Sara Sarto",
      "Marcella Cornia",
      "Rita Cucchiara"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14603v1",
    "title": "Command R7B Arabic: A Small, Enterprise Focused, Multilingual, and Culturally Aware Arabic LLM",
    "abstract": "Building high-quality large language models (LLMs) for enterprise Arabic\napplications remains challenging due to the limited availability of digitized\nArabic data. In this work, we present a data synthesis and refinement strategy\nto help address this problem, namely, by leveraging synthetic data generation\nand human-in-the-loop annotation to expand our Arabic training corpus. We\nfurther present our iterative post training recipe that is essential to\nachieving state-of-the-art performance in aligning the model with human\npreferences, a critical aspect to enterprise use cases. The culmination of this\neffort is the release of a small, 7B, open-weight model that outperforms\nsimilarly sized peers in head-to-head comparisons and on Arabic-focused\nbenchmarks covering cultural knowledge, instruction following, RAG, and\ncontextual faithfulness.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Yazeed Alnumay",
      "Alexandre Barbet",
      "Anna Bialas",
      "William Darling",
      "Shaan Desai",
      "Joan Devassy",
      "Kyle Duffy",
      "Stephanie Howe",
      "Olivia Lasche",
      "Justin Lee",
      "Anirudh Shrinivason",
      "Jennifer Tracey"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14505v1",
    "title": "MusicInfuser: Making Video Diffusion Listen and Dance",
    "abstract": "We introduce MusicInfuser, an approach for generating high-quality dance\nvideos that are synchronized to a specified music track. Rather than attempting\nto design and train a new multimodal audio-video model, we show how existing\nvideo diffusion models can be adapted to align with musical inputs by\nintroducing lightweight music-video cross-attention and a low-rank adapter.\nUnlike prior work requiring motion capture data, our approach fine-tunes only\non dance videos. MusicInfuser achieves high-quality music-driven video\ngeneration while preserving the flexibility and generative capabilities of the\nunderlying models. We introduce an evaluation framework using Video-LLMs to\nassess multiple dimensions of dance generation quality. The project page and\ncode are available at https://susunghong.github.io/MusicInfuser.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Susung Hong",
      "Ira Kemelmacher-Shlizerman",
      "Brian Curless",
      "Steven M. Seitz"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14504v1",
    "title": "Aligning Multimodal LLM with Human Preference: A Survey",
    "abstract": "Large language models (LLMs) can handle a wide variety of general tasks with\nsimple prompts, without the need for task-specific training. Multimodal Large\nLanguage Models (MLLMs), built upon LLMs, have demonstrated impressive\npotential in tackling complex tasks involving visual, auditory, and textual\ndata. However, critical issues related to truthfulness, safety, o1-like\nreasoning, and alignment with human preference remain insufficiently addressed.\nThis gap has spurred the emergence of various alignment algorithms, each\ntargeting different application scenarios and optimization goals. Recent\nstudies have shown that alignment algorithms are a powerful approach to\nresolving the aforementioned challenges. In this paper, we aim to provide a\ncomprehensive and systematic review of alignment algorithms for MLLMs.\nSpecifically, we explore four key aspects: (1) the application scenarios\ncovered by alignment algorithms, including general image understanding,\nmulti-image, video, and audio, and extended multimodal applications; (2) the\ncore factors in constructing alignment datasets, including data sources, model\nresponses, and preference annotations; (3) the benchmarks used to evaluate\nalignment algorithms; and (4) a discussion of potential future directions for\nthe development of alignment algorithms. This work seeks to help researchers\norganize current advancements in the field and inspire better alignment\nmethods. The project page of this paper is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Tao Yu",
      "Yi-Fan Zhang",
      "Chaoyou Fu",
      "Junkang Wu",
      "Jinda Lu",
      "Kun Wang",
      "Xingyu Lu",
      "Yunhang Shen",
      "Guibin Zhang",
      "Dingjie Song",
      "Yibo Yan",
      "Tianlong Xu",
      "Qingsong Wen",
      "Zhang Zhang",
      "Yan Huang",
      "Liang Wang",
      "Tieniu Tan"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14503v1",
    "title": "The Power of Context: How Multimodality Improves Image Super-Resolution",
    "abstract": "Single-image super-resolution (SISR) remains challenging due to the inherent\ndifficulty of recovering fine-grained details and preserving perceptual quality\nfrom low-resolution inputs. Existing methods often rely on limited image\npriors, leading to suboptimal results. We propose a novel approach that\nleverages the rich contextual information available in multiple modalities --\nincluding depth, segmentation, edges, and text prompts -- to learn a powerful\ngenerative prior for SISR within a diffusion model framework. We introduce a\nflexible network architecture that effectively fuses multimodal information,\naccommodating an arbitrary number of input modalities without requiring\nsignificant modifications to the diffusion process. Crucially, we mitigate\nhallucinations, often introduced by text prompts, by using spatial information\nfrom other modalities to guide regional text-based conditioning. Each\nmodality's guidance strength can also be controlled independently, allowing\nsteering outputs toward different directions, such as increasing bokeh through\ndepth or adjusting object prominence via segmentation. Extensive experiments\ndemonstrate that our model surpasses state-of-the-art generative SISR methods,\nachieving superior visual quality and fidelity. See project page at\nhttps://mmsr.kfmei.com/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Kangfu Mei",
      "Hossein Talebi",
      "Mojtaba Ardakani",
      "Vishal M. Patel",
      "Peyman Milanfar",
      "Mauricio Delbracio"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14501v2",
    "title": "Advances in 4D Generation: A Survey",
    "abstract": "Generative artificial intelligence (AI) has made significant progress across\nvarious domains in recent years. Building on the rapid advancements in 2D,\nvideo, and 3D content generation fields, 4D generation has emerged as a novel\nand rapidly evolving research area, attracting growing attention. 4D generation\nfocuses on creating dynamic 3D assets with spatiotemporal consistency based on\nuser input, offering greater creative freedom and richer immersive experiences.\nThis paper presents a comprehensive survey of the 4D generation field,\nsystematically summarizing its core technologies, developmental trajectory, key\nchallenges, and practical applications, while also exploring potential future\nresearch directions. The survey begins by introducing various fundamental 4D\nrepresentation models, followed by a review of 4D generation frameworks built\nupon these representations and the key technologies that incorporate motion and\ngeometry priors into 4D assets. We summarize five major challenges of 4D\ngeneration: consistency, controllability, diversity, efficiency, and fidelity,\naccompanied by an outline of existing solutions to address these issues. We\nsystematically analyze applications of 4D generation, spanning dynamic object\ngeneration, scene generation, digital human synthesis, 4D editing, and\nautonomous driving. Finally, we provide an in-depth discussion of the obstacles\ncurrently hindering the development of the 4D generation. This survey offers a\nclear and comprehensive overview of 4D generation, aiming to stimulate further\nexploration and innovation in this rapidly evolving field. Our code is publicly\navailable at: https://github.com/MiaoQiaowei/Awesome-4D.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Qiaowei Miao",
      "Kehan Li",
      "Jinsheng Quan",
      "Zhiyuan Min",
      "Shaojie Ma",
      "Yichao Xu",
      "Yi Yang",
      "Yawei Luo"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14500v1",
    "title": "Utilization of Neighbor Information for Image Classification with Different Levels of Supervision",
    "abstract": "We propose to bridge the gap between semi-supervised and unsupervised image\nrecognition with a flexible method that performs well for both generalized\ncategory discovery (GCD) and image clustering. Despite the overlap in\nmotivation between these tasks, the methods themselves are restricted to a\nsingle task -- GCD methods are reliant on the labeled portion of the data, and\ndeep image clustering methods have no built-in way to leverage the labels\nefficiently. We connect the two regimes with an innovative approach that\nUtilizes Neighbor Information for Classification (UNIC) both in the\nunsupervised (clustering) and semisupervised (GCD) setting. State-of-the-art\nclustering methods already rely heavily on nearest neighbors. We improve on\ntheir results substantially in two parts, first with a sampling and cleaning\nstrategy where we identify accurate positive and negative neighbors, and\nsecondly by finetuning the backbone with clustering losses computed by sampling\nboth types of neighbors. We then adapt this pipeline to GCD by utilizing the\nlabelled images as ground truth neighbors. Our method yields state-of-the-art\nresults for both clustering (+3% ImageNet-100, Imagenet200) and GCD (+0.8%\nImageNet-100, +5% CUB, +2% SCars, +4% Aircraft).",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Gihan Jayatilaka",
      "Abhinav Shrivastava",
      "Matthew Gwilliam"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14499v1",
    "title": "Measuring AI Ability to Complete Long Tasks",
    "abstract": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark\nperformance remains unclear. To quantify the capabilities of AI systems in\nterms of human capabilities, we propose a new metric: 50%-task-completion time\nhorizon. This is the time humans typically take to complete tasks that AI\nmodels can complete with 50% success rate. We first timed humans with relevant\ndomain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter\ntasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet\nhave a 50% time horizon of around 50 minutes. Furthermore, frontier AI time\nhorizon has been doubling approximately every seven months since 2019, though\nthe trend may have accelerated in 2024. The increase in AI models' time\nhorizons seems to be primarily driven by greater reliability and ability to\nadapt to mistakes, combined with better logical reasoning and tool use\ncapabilities. We discuss the limitations of our results -- including their\ndegree of external validity -- and the implications of increased autonomy for\ndangerous capabilities. If these results generalize to real-world software\ntasks, extrapolation of this trend predicts that within 5 years, AI systems\nwill be capable of automating many software tasks that currently take humans a\nmonth.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Thomas Kwa",
      "Ben West",
      "Joel Becker",
      "Amy Deng",
      "Katharyn Garcia",
      "Max Hasin",
      "Sami Jawhar",
      "Megan Kinniment",
      "Nate Rush",
      "Sydney Von Arx",
      "Ryan Bloom",
      "Thomas Broadley",
      "Haoxing Du",
      "Brian Goodrich",
      "Nikola Jurkovic",
      "Luke Harold Miles",
      "Seraphina Nix",
      "Tao Lin",
      "Neev Parikh",
      "David Rein",
      "Lucas Jun Koba Sato",
      "Hjalmar Wijk",
      "Daniel M. Ziegler",
      "Elizabeth Barnes",
      "Lawrence Chan"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14498v1",
    "title": "Tracking Meets Large Multimodal Models for Driving Scenario Understanding",
    "abstract": "Large Multimodal Models (LMMs) have recently gained prominence in autonomous\ndriving research, showcasing promising capabilities across various emerging\nbenchmarks. LMMs specifically designed for this domain have demonstrated\neffective perception, planning, and prediction skills. However, many of these\nmethods underutilize 3D spatial and temporal elements, relying mainly on image\ndata. As a result, their effectiveness in dynamic driving environments is\nlimited. We propose to integrate tracking information as an additional input to\nrecover 3D spatial and temporal details that are not effectively captured in\nthe images. We introduce a novel approach for embedding this tracking\ninformation into LMMs to enhance their spatiotemporal understanding of driving\nscenarios. By incorporating 3D tracking data through a track encoder, we enrich\nvisual queries with crucial spatial and temporal cues while avoiding the\ncomputational overhead associated with processing lengthy video sequences or\nextensive 3D inputs. Moreover, we employ a self-supervised approach to pretrain\nthe tracking encoder to provide LMMs with additional contextual information,\nsignificantly improving their performance in perception, planning, and\nprediction tasks for autonomous driving. Experimental results demonstrate the\neffectiveness of our approach, with a gain of 9.5% in accuracy, an increase of\n7.04 points in the ChatGPT score, and 9.4% increase in the overall score over\nbaseline models on DriveLM-nuScenes benchmark, along with a 3.7% final score\nimprovement on DriveLM-CARLA. Our code is available at\nhttps://github.com/mbzuai-oryx/TrackingMeetsLMM",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Ayesha Ishaq",
      "Jean Lahoud",
      "Fahad Shahbaz Khan",
      "Salman Khan",
      "Hisham Cholakkal",
      "Rao Muhammad Anwer"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14495v1",
    "title": "Temporal Consistency for LLM Reasoning Process Error Identification",
    "abstract": "Verification is crucial for effective mathematical reasoning. We present a\nnew temporal consistency method where verifiers iteratively refine their\njudgments based on the previous assessment. Unlike one-round verification or\nmulti-model debate approaches, our method leverages consistency in a sequence\nof self-reflection actions to improve verification accuracy. Empirical\nevaluations across diverse mathematical process error identification benchmarks\n(Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements\nover baseline methods. When applied to the recent DeepSeek R1 distilled models,\nour method demonstrates strong performance, enabling 7B/8B distilled models to\noutperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the\ndistilled 14B model with our method achieves performance comparable to\nDeepseek-R1. Our codes are available at\nhttps://github.com/jcguo123/Temporal-Consistency",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Jiacheng Guo",
      "Yue Wu",
      "Jiahao Qiu",
      "Kaixuan Huang",
      "Xinzhe Juan",
      "Ling Yang",
      "Mengdi Wang"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14494v1",
    "title": "Deeply Supervised Flow-Based Generative Models",
    "abstract": "Flow based generative models have charted an impressive path across multiple\nvisual generation tasks by adhering to a simple principle: learning velocity\nrepresentations of a linear interpolant. However, we observe that training\nvelocity solely from the final layer output underutilizes the rich inter layer\nrepresentations, potentially impeding model convergence. To address this\nlimitation, we introduce DeepFlow, a novel framework that enhances velocity\nrepresentation through inter layer communication. DeepFlow partitions\ntransformer layers into balanced branches with deep supervision and inserts a\nlightweight Velocity Refiner with Acceleration (VeRA) block between adjacent\nbranches, which aligns the intermediate velocity features within transformer\nblocks. Powered by the improved deep supervision via the internal velocity\nalignment, DeepFlow converges 8 times faster on ImageNet with equivalent\nperformance and further reduces FID by 2.6 while halving training time compared\nto previous flow based models without a classifier free guidance. DeepFlow also\noutperforms baselines in text to image generation tasks, as evidenced by\nevaluations on MSCOCO and zero shot GenEval.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Inkyu Shin",
      "Chenglin Yang",
      "Liang-Chieh Chen"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14493v2",
    "title": "State Space Model Meets Transformer: A New Paradigm for 3D Object Detection",
    "abstract": "DETR-based methods, which use multi-layer transformer decoders to refine\nobject queries iteratively, have shown promising performance in 3D indoor\nobject detection. However, the scene point features in the transformer decoder\nremain fixed, leading to minimal contributions from later decoder layers,\nthereby limiting performance improvement. Recently, State Space Models (SSM)\nhave shown efficient context modeling ability with linear complexity through\niterative interactions between system states and inputs. Inspired by SSMs, we\npropose a new 3D object DEtection paradigm with an interactive STate space\nmodel (DEST). In the interactive SSM, we design a novel state-dependent SSM\nparameterization method that enables system states to effectively serve as\nqueries in 3D indoor detection tasks. In addition, we introduce four key\ndesigns tailored to the characteristics of point cloud and SSM: The\nserialization and bidirectional scanning strategies enable bidirectional\nfeature interaction among scene points within the SSM. The inter-state\nattention mechanism models the relationships between state points, while the\ngated feed-forward network enhances inter-channel correlations. To the best of\nour knowledge, this is the first method to model queries as system states and\nscene points as system inputs, which can simultaneously update scene point\nfeatures and query features with linear complexity. Extensive experiments on\ntwo challenging datasets demonstrate the effectiveness of our DEST-based\nmethod. Our method improves the GroupFree baseline in terms of AP50 on ScanNet\nV2 (+5.3) and SUN RGB-D (+3.2) datasets. Based on the VDETR baseline, Our\nmethod sets a new SOTA on the ScanNetV2 and SUN RGB-D datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Chuxin Wang",
      "Wenfei Yang",
      "Xiang Liu",
      "Tianzhu Zhang"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14492v1",
    "title": "Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control",
    "abstract": "We introduce Cosmos-Transfer, a conditional world generation model that can\ngenerate world simulations based on multiple spatial control inputs of various\nmodalities such as segmentation, depth, and edge. In the design, the spatial\nconditional scheme is adaptive and customizable. It allows weighting different\nconditional inputs differently at different spatial locations. This enables\nhighly controllable world generation and finds use in various world-to-world\ntransfer use cases, including Sim2Real. We conduct extensive evaluations to\nanalyze the proposed model and demonstrate its applications for Physical AI,\nincluding robotics Sim2Real and autonomous vehicle data enrichment. We further\ndemonstrate an inference scaling strategy to achieve real-time world generation\nwith an NVIDIA GB200 NVL72 rack. To help accelerate research development in the\nfield, we open-source our models and code at\nhttps://github.com/nvidia-cosmos/cosmos-transfer1.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "authors": [
      "NVIDIA",
      ":",
      "Hassan Abu Alhaija",
      "Jose Alvarez",
      "Maciej Bala",
      "Tiffany Cai",
      "Tianshi Cao",
      "Liz Cha",
      "Joshua Chen",
      "Mike Chen",
      "Francesco Ferroni",
      "Sanja Fidler",
      "Dieter Fox",
      "Yunhao Ge",
      "Jinwei Gu",
      "Ali Hassani",
      "Michael Isaev",
      "Pooya Jannaty",
      "Shiyi Lan",
      "Tobias Lasser",
      "Huan Ling",
      "Ming-Yu Liu",
      "Xian Liu",
      "Yifan Lu",
      "Alice Luo",
      "Qianli Ma",
      "Hanzi Mao",
      "Fabio Ramos",
      "Xuanchi Ren",
      "Tianchang Shen",
      "Shitao Tang",
      "Ting-Chun Wang",
      "Jay Wu",
      "Jiashu Xu",
      "Stella Xu",
      "Kevin Xie",
      "Yuchong Ye",
      "Xiaodong Yang",
      "Xiaohui Zeng",
      "Yu Zeng"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14489v1",
    "title": "Stable Virtual Camera: Generative View Synthesis with Diffusion Models",
    "abstract": "We present Stable Virtual Camera (Seva), a generalist diffusion model that\ncreates novel views of a scene, given any number of input views and target\ncameras. Existing works struggle to generate either large viewpoint changes or\ntemporally smooth samples, while relying on specific task configurations. Our\napproach overcomes these limitations through simple model design, optimized\ntraining recipe, and flexible sampling strategy that generalize across view\nsynthesis tasks at test time. As a result, our samples maintain high\nconsistency without requiring additional 3D representation-based distillation,\nthus streamlining view synthesis in the wild. Furthermore, we show that our\nmethod can generate high-quality videos lasting up to half a minute with\nseamless loop closure. Extensive benchmarking demonstrates that Seva\noutperforms existing methods across different datasets and settings.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Jensen",
      "Zhou",
      "Hang Gao",
      "Vikram Voleti",
      "Aaryaman Vasishta",
      "Chun-Han Yao",
      "Mark Boss",
      "Philip Torr",
      "Christian Rupprecht",
      "Varun Jampani"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14488v1",
    "title": "Engineering Scientific Assistants using Interactive Structured Induction of Programs",
    "abstract": "We are interested in the construction of software that can act as scientific\nassistants to domain specialists. It is expected that such assistants will be\nneeded to accelerate the identification of ways to address complex problems\nrequiring urgent solutions. In this paper, our focus is not on a specific\nscientific problem, but on the software-engineering of such 'science\naccelerators'. Recent developments in 'No Code' techniques would seem to\nsuggest that scientist can simply hypothesise solutions simply by conversing\nwith a large language model (LLM). However, for complex scientific problems,\nthis seems unlikely given the current state of LLM technology. What does appear\nfeasible is that a software engineer can use LLMs to rapidly construct programs\nfor use by a domain-specialist, including the specialist's requirements\nexpressed in natural language. We propose the design of an interactive form of\n'structured' inductive programming in which a software-engineer and an LLM\ncollaboratively construct an 'assistant' for a scientific data analysis. The\npaper describes a simple implementation called iStrucInd that adapts a '2-way\nIntelligibility' protocol to implement the interaction between the software\nengineer and the LLM. We test the tool on two different non-trivial scientific\ndata analysis tasks. Specifically, we compare the system constructed by\niStrucInd against systems constructed manually and by Low Code/No Code methods\nalong dimensions of: (a) program performance; (b) program quality; and (c)\nprogramming effort. The results show iStrucInd allows a software engineer to\ndevelop better programs faster suggesting interactive structured induction can\nplay a useful role in the rapid construction of scientific assistants.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "authors": [
      "Shraddha Surana",
      "Ashwin Srinivasan"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14487v1",
    "title": "DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers",
    "abstract": "Diffusion models have demonstrated remarkable success in various image\ngeneration tasks, but their performance is often limited by the uniform\nprocessing of inputs across varying conditions and noise levels. To address\nthis limitation, we propose a novel approach that leverages the inherent\nheterogeneity of the diffusion process. Our method, DiffMoE, introduces a\nbatch-level global token pool that enables experts to access global token\ndistributions during training, promoting specialized expert behavior. To\nunleash the full potential of the diffusion process, DiffMoE incorporates a\ncapacity predictor that dynamically allocates computational resources based on\nnoise levels and sample complexity. Through comprehensive evaluation, DiffMoE\nachieves state-of-the-art performance among diffusion models on ImageNet\nbenchmark, substantially outperforming both dense architectures with 3x\nactivated parameters and existing MoE approaches while maintaining 1x activated\nparameters. The effectiveness of our approach extends beyond class-conditional\ngeneration to more challenging tasks such as text-to-image generation,\ndemonstrating its broad applicability across different diffusion model\napplications. Project Page: https://shiml20.github.io/DiffMoE/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Minglei Shi",
      "Ziyang Yuan",
      "Haotian Yang",
      "Xintao Wang",
      "Mingwu Zheng",
      "Xin Tao",
      "Wenliang Zhao",
      "Wenzhao Zheng",
      "Jie Zhou",
      "Jiwen Lu",
      "Pengfei Wan",
      "Di Zhang",
      "Kun Gai"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14485v1",
    "title": "Lux Post Facto: Learning Portrait Performance Relighting with Conditional Video Diffusion and a Hybrid Dataset",
    "abstract": "Video portrait relighting remains challenging because the results need to be\nboth photorealistic and temporally stable. This typically requires a strong\nmodel design that can capture complex facial reflections as well as intensive\ntraining on a high-quality paired video dataset, such as dynamic\none-light-at-a-time (OLAT). In this work, we introduce Lux Post Facto, a novel\nportrait video relighting method that produces both photorealistic and\ntemporally consistent lighting effects. From the model side, we design a new\nconditional video diffusion model built upon state-of-the-art pre-trained video\ndiffusion model, alongside a new lighting injection mechanism to enable precise\ncontrol. This way we leverage strong spatial and temporal generative capability\nto generate plausible solutions to the ill-posed relighting problem. Our\ntechnique uses a hybrid dataset consisting of static expression OLAT data and\nin-the-wild portrait performance videos to jointly learn relighting and\ntemporal modeling. This avoids the need to acquire paired video data in\ndifferent lighting conditions. Our extensive experiments show that our model\nproduces state-of-the-art results both in terms of photorealism and temporal\nconsistency.",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Yiqun Mei",
      "Mingming He",
      "Li Ma",
      "Julien Philip",
      "Wenqi Xian",
      "David M George",
      "Xueming Yu",
      "Gabriel Dedic",
      "Ahmet Levent Taşel",
      "Ning Yu",
      "Vishal M. Patel",
      "Paul Debevec"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14484v1",
    "title": "Gricean Norms as a Basis for Effective Collaboration",
    "abstract": "Effective human-AI collaboration hinges not only on the AI agent's ability to\nfollow explicit instructions but also on its capacity to navigate ambiguity,\nincompleteness, invalidity, and irrelevance in communication. Gricean\nconversational and inference norms facilitate collaboration by aligning unclear\ninstructions with cooperative principles. We propose a normative framework that\nintegrates Gricean norms and cognitive frameworks -- common ground, relevance\ntheory, and theory of mind -- into large language model (LLM) based agents. The\nnormative framework adopts the Gricean maxims of quantity, quality, relation,\nand manner, along with inference, as Gricean norms to interpret unclear\ninstructions, which are: ambiguous, incomplete, invalid, or irrelevant. Within\nthis framework, we introduce Lamoids, GPT-4 powered agents designed to\ncollaborate with humans. To assess the influence of Gricean norms in human-AI\ncollaboration, we evaluate two versions of a Lamoid: one with norms and one\nwithout. In our experiments, a Lamoid collaborates with a human to achieve\nshared goals in a grid world (Doors, Keys, and Gems) by interpreting both clear\nand unclear natural language instructions. Our results reveal that the Lamoid\nwith Gricean norms achieves higher task accuracy and generates clearer, more\naccurate, and contextually relevant responses than the Lamoid without norms.\nThis improvement stems from the normative framework, which enhances the agent's\npragmatic reasoning, fostering effective human-AI collaboration and enabling\ncontext-aware communication in LLM-based agents.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Fardin Saad",
      "Pradeep K. Murukannaiah",
      "Munindar P. Singh"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14483v1",
    "title": "Multi-view Reconstruction via SfM-guided Monocular Depth Estimation",
    "abstract": "In this paper, we present a new method for multi-view geometric\nreconstruction. In recent years, large vision models have rapidly developed,\nperforming excellently across various tasks and demonstrating remarkable\ngeneralization capabilities. Some works use large vision models for monocular\ndepth estimation, which have been applied to facilitate multi-view\nreconstruction tasks in an indirect manner. Due to the ambiguity of the\nmonocular depth estimation task, the estimated depth values are usually not\naccurate enough, limiting their utility in aiding multi-view reconstruction. We\npropose to incorporate SfM information, a strong multi-view prior, into the\ndepth estimation process, thus enhancing the quality of depth prediction and\nenabling their direct application in multi-view geometric reconstruction.\nExperimental results on public real-world datasets show that our method\nsignificantly improves the quality of depth estimation compared to previous\nmonocular depth estimation works. Additionally, we evaluate the reconstruction\nquality of our approach in various types of scenes including indoor,\nstreetscape, and aerial views, surpassing state-of-the-art MVS methods. The\ncode and supplementary materials are available at\nhttps://zju3dv.github.io/murre/ .",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Haoyu Guo",
      "He Zhu",
      "Sida Peng",
      "Haotong Lin",
      "Yunzhi Yan",
      "Tao Xie",
      "Wenguan Wang",
      "Xiaowei Zhou",
      "Hujun Bao"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14482v1",
    "title": "ICE-Bench: A Unified and Comprehensive Benchmark for Image Creating and Editing",
    "abstract": "Image generation has witnessed significant advancements in the past few\nyears. However, evaluating the performance of image generation models remains a\nformidable challenge. In this paper, we propose ICE-Bench, a unified and\ncomprehensive benchmark designed to rigorously assess image generation models.\nIts comprehensiveness could be summarized in the following key features: (1)\nCoarse-to-Fine Tasks: We systematically deconstruct image generation into four\ntask categories: No-ref/Ref Image Creating/Editing, based on the presence or\nabsence of source images and reference images. And further decompose them into\n31 fine-grained tasks covering a broad spectrum of image generation\nrequirements, culminating in a comprehensive benchmark. (2) Multi-dimensional\nMetrics: The evaluation framework assesses image generation capabilities across\n6 dimensions: aesthetic quality, imaging quality, prompt following, source\nconsistency, reference consistency, and controllability. 11 metrics are\nintroduced to support the multi-dimensional evaluation. Notably, we introduce\nVLLM-QA, an innovative metric designed to assess the success of image editing\nby leveraging large models. (3) Hybrid Data: The data comes from real scenes\nand virtual generation, which effectively improves data diversity and\nalleviates the bias problem in model evaluation. Through ICE-Bench, we conduct\na thorough analysis of existing generation models, revealing both the\nchallenging nature of our benchmark and the gap between current model\ncapabilities and real-world generation requirements. To foster further\nadvancements in the field, we will open-source ICE-Bench, including its\ndataset, evaluation code, and models, thereby providing a valuable resource for\nthe research community.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yulin Pan",
      "Xiangteng He",
      "Chaojie Mao",
      "Zhen Han",
      "Zeyinzi Jiang",
      "Jingfeng Zhang",
      "Yu Liu"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14481v1",
    "title": "Don't lie to your friends: Learning what you know from collaborative self-play",
    "abstract": "To be helpful assistants, AI agents must be aware of their own capabilities\nand limitations. This includes knowing when to answer from parametric knowledge\nversus using tools, when to trust tool outputs, and when to abstain or hedge.\nSuch capabilities are hard to teach through supervised fine-tuning because they\nrequire constructing examples that reflect the agent's specific capabilities.\nWe therefore propose a radically new approach to teaching agents what they\nknow: \\emph{collaborative self-play}. We construct multi-agent collaborations\nin which the group is rewarded for collectively arriving at correct answers.\nThe desired meta-knowledge emerges from the incentives built into the structure\nof the interaction. We focus on small societies of agents that have access to\nheterogeneous tools (corpus-specific retrieval), and therefore must collaborate\nto maximize their success while minimizing their effort. Experiments show that\ngroup-level rewards for multi-agent communities can induce policies that\n\\emph{transfer} to improve tool use and selective prediction in settings where\nindividual agents are deployed in isolation.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Jacob Eisenstein",
      "Reza Aghajani",
      "Adam Fisch",
      "Dheeru Dua",
      "Fantine Huot",
      "Mirella Lapata",
      "Vicky Zayats",
      "Jonathan Berant"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14478v2",
    "title": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM",
    "abstract": "Creativity is a fundamental aspect of intelligence, involving the ability to\ngenerate novel and appropriate solutions across diverse contexts. While Large\nLanguage Models (LLMs) have been extensively evaluated for their creative\ncapabilities, the assessment of Multimodal Large Language Models (MLLMs) in\nthis domain remains largely unexplored. To address this gap, we introduce\nCreation-MMBench, a multimodal benchmark specifically designed to evaluate the\ncreative capabilities of MLLMs in real-world, image-based tasks. The benchmark\ncomprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous\nevaluation, we define instance-specific evaluation criteria for each test case,\nguiding the assessment of both general response quality and factual consistency\nwith visual inputs. Experimental results reveal that current open-source MLLMs\nsignificantly underperform compared to proprietary models in creative tasks.\nFurthermore, our analysis demonstrates that visual fine-tuning can negatively\nimpact the base LLM's creative abilities. Creation-MMBench provides valuable\ninsights for advancing MLLM creativity and establishes a foundation for future\nimprovements in multimodal generative intelligence. Full data and evaluation\ncode is released on https://github.com/open-compass/Creation-MMBench.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Xinyu Fang",
      "Zhijian Chen",
      "Kai Lan",
      "Lixin Ma",
      "Shengyuan Ding",
      "Yingji Liang",
      "Xiangyu Zhao",
      "Farong Wen",
      "Zicheng Zhang",
      "Guofeng Zhang",
      "Haodong Duan",
      "Kai Chen",
      "Dahua Lin"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14477v1",
    "title": "Calibrating Verbal Uncertainty as a Linear Feature to Reduce Hallucinations",
    "abstract": "LLMs often adopt an assertive language style also when making false claims.\nSuch ``overconfident hallucinations'' mislead users and erode trust. Achieving\nthe ability to express in language the actual degree of uncertainty around a\nclaim is therefore of great importance. We find that ``verbal uncertainty'' is\ngoverned by a single linear feature in the representation space of LLMs, and\nshow that this has only moderate correlation with the actual ``semantic\nuncertainty'' of the model. We apply this insight and show that (1) the\nmismatch between semantic and verbal uncertainty is a better predictor of\nhallucinations than semantic uncertainty alone and (2) we can intervene on\nverbal uncertainty at inference time and reduce hallucinations on short-form\nanswers, achieving an average relative reduction of 32%.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Ziwei Ji",
      "Lei Yu",
      "Yeskendir Koishekenov",
      "Yejin Bang",
      "Anthony Hartshorn",
      "Alan Schelten",
      "Cheng Zhang",
      "Pascale Fung",
      "Nicola Cancedda"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14476v1",
    "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
    "abstract": "Inference scaling empowers LLMs with unprecedented reasoning ability, with\nreinforcement learning as the core technique to elicit complex reasoning.\nHowever, key technical details of state-of-the-art reasoning LLMs are concealed\n(such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the\ncommunity still struggles to reproduce their RL training results. We propose\nthe $\\textbf{D}$ecoupled Clip and $\\textbf{D}$ynamic s$\\textbf{A}$mpling\n$\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{DAPO}$) algorithm, and\nfully open-source a state-of-the-art large-scale RL system that achieves 50\npoints on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that\nwithhold training details, we introduce four key techniques of our algorithm\nthat make large-scale LLM RL a success. In addition, we open-source our\ntraining code, which is built on the verl framework, along with a carefully\ncurated and processed dataset. These components of our open-source system\nenhance reproducibility and support future research in large-scale LLM RL.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Qiying Yu",
      "Zheng Zhang",
      "Ruofei Zhu",
      "Yufeng Yuan",
      "Xiaochen Zuo",
      "Yu Yue",
      "Tiantian Fan",
      "Gaohong Liu",
      "Lingjun Liu",
      "Xin Liu",
      "Haibin Lin",
      "Zhiqi Lin",
      "Bole Ma",
      "Guangming Sheng",
      "Yuxuan Tong",
      "Chi Zhang",
      "Mofan Zhang",
      "Wang Zhang",
      "Hang Zhu",
      "Jinhua Zhu",
      "Jiaze Chen",
      "Jiangjie Chen",
      "Chengyi Wang",
      "Hongli Yu",
      "Weinan Dai",
      "Yuxuan Song",
      "Xiangpeng Wei",
      "Hao Zhou",
      "Jingjing Liu",
      "Wei-Ying Ma",
      "Ya-Qin Zhang",
      "Lin Yan",
      "Mu Qiao",
      "Yonghui Wu",
      "Mingxuan Wang"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14475v1",
    "title": "Optimized 3D Gaussian Splatting using Coarse-to-Fine Image Frequency Modulation",
    "abstract": "The field of Novel View Synthesis has been revolutionized by 3D Gaussian\nSplatting (3DGS), which enables high-quality scene reconstruction that can be\nrendered in real-time. 3DGS-based techniques typically suffer from high GPU\nmemory and disk storage requirements which limits their practical application\non consumer-grade devices. We propose Opti3DGS, a novel frequency-modulated\ncoarse-to-fine optimization framework that aims to minimize the number of\nGaussian primitives used to represent a scene, thus reducing memory and storage\ndemands. Opti3DGS leverages image frequency modulation, initially enforcing a\ncoarse scene representation and progressively refining it by modulating\nfrequency details in the training images. On the baseline 3DGS, we demonstrate\nan average reduction of 62% in Gaussians, a 40% reduction in the training GPU\nmemory requirements and a 20% reduction in optimization time without\nsacrificing the visual quality. Furthermore, we show that our method integrates\nseamlessly with many 3DGS-based techniques, consistently reducing the number of\nGaussian primitives while maintaining, and often improving, visual quality.\nAdditionally, Opti3DGS inherently produces a level-of-detail scene\nrepresentation at no extra cost, a natural byproduct of the optimization\npipeline. Results and code will be made publicly available.",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Umar Farooq",
      "Jean-Yves Guillemaut",
      "Adrian Hilton",
      "Marco Volino"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14473v1",
    "title": "EnQode: Fast Amplitude Embedding for Quantum Machine Learning Using Classical Data",
    "abstract": "Amplitude embedding (AE) is essential in quantum machine learning (QML) for\nencoding classical data onto quantum circuits. However, conventional AE methods\nsuffer from deep, variable-length circuits that introduce high output error due\nto extensive gate usage and variable error rates across samples, resulting in\nnoise-driven inconsistencies that degrade model accuracy. We introduce EnQode,\na fast AE technique based on symbolic representation that addresses these\nlimitations by clustering dataset samples and solving for cluster mean states\nthrough a low-depth, machine-specific ansatz. Optimized to reduce physical\ngates and SWAP operations, EnQode ensures all samples face consistent, low\nnoise levels by standardizing circuit depth and composition. With over 90%\nfidelity in data mapping, EnQode enables robust, high-performance QML on noisy\nintermediate-scale quantum (NISQ) devices. Our open-source solution provides a\nscalable and efficient alternative for integrating classical data with quantum\nmodels.",
    "categories": [
      "quant-ph",
      "cs.ET",
      "cs.LG"
    ],
    "authors": [
      "Jason Han",
      "Nicholas S. DiBrita",
      "Younghyun Cho",
      "Hengrui Luo",
      "Tirthak Patel"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14469v1",
    "title": "Attribution Score Alignment in Explainable Data Management",
    "abstract": "Different attribution-scores have been proposed to quantify the relevance of\ndatabase tuples for a query answer from a database. Among them, we find Causal\nResponsibility, the Shapley Value, the Banzhaf Power-Index, and the Causal\nEffect. They have been analyzed in isolation, mainly in terms of computational\nproperties. In this work, we start an investigation into the alignment of these\nscores on the basis of the queries at hand; that is, on whether they induce\ncompatible rankings of tuples. We are able to identify vast classes of queries\nfor which some pairs of scores are always aligned, and others for which they\nare not. It turns out that the presence of exogenous tuples makes a crucial\ndifference in this regard.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "authors": [
      "Felipe Azua",
      "Leopoldo Bertossi"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14463v1",
    "title": "SIR-DIFF: Sparse Image Sets Restoration with Multi-View Diffusion Model",
    "abstract": "The computer vision community has developed numerous techniques for digitally\nrestoring true scene information from single-view degraded photographs, an\nimportant yet extremely ill-posed task. In this work, we tackle image\nrestoration from a different perspective by jointly denoising multiple\nphotographs of the same scene. Our core hypothesis is that degraded images\ncapturing a shared scene contain complementary information that, when combined,\nbetter constrains the restoration problem. To this end, we implement a powerful\nmulti-view diffusion model that jointly generates uncorrupted views by\nextracting rich information from multi-view relationships. Our experiments show\nthat our multi-view approach outperforms existing single-view image and even\nvideo-based methods on image deblurring and super-resolution tasks. Critically,\nour model is trained to output 3D consistent images, making it a promising tool\nfor applications requiring robust multi-view integration, such as 3D\nreconstruction or pose estimation.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Yucheng Mao",
      "Boyang Wang",
      "Nilesh Kulkarni",
      "Jeong Joon Park"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14459v1",
    "title": "Doubly robust identification of treatment effects from multiple environments",
    "abstract": "Practical and ethical constraints often require the use of observational data\nfor causal inference, particularly in medicine and social sciences. Yet,\nobservational datasets are prone to confounding, potentially compromising the\nvalidity of causal conclusions. While it is possible to correct for biases if\nthe underlying causal graph is known, this is rarely a feasible ask in\npractical scenarios. A common strategy is to adjust for all available\ncovariates, yet this approach can yield biased treatment effect estimates,\nespecially when post-treatment or unobserved variables are present. We propose\nRAMEN, an algorithm that produces unbiased treatment effect estimates by\nleveraging the heterogeneity of multiple data sources without the need to know\nor learn the underlying causal graph. Notably, RAMEN achieves doubly robust\nidentification: it can identify the treatment effect whenever the causal\nparents of the treatment or those of the outcome are observed, and the node\nwhose parents are observed satisfies an invariance assumption. Empirical\nevaluations on synthetic and real-world datasets show that our approach\noutperforms existing methods.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "authors": [
      "Piersilvio De Bartolomeis",
      "Julia Kostin",
      "Javier Abad",
      "Yixin Wang",
      "Fanny Yang"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14456v1",
    "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
    "abstract": "We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with\npre-trained language models that establish a new state-of-the-art in downstream\nperformance at the 3 billion parameter scale on multilingual tasks, and match\ncurrent SoTA English language performance despite being trained on dramatically\nfewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only\nconstant memory usage and constant inference time per token. RWKV-7 introduces\na newly generalized formulation of the delta rule with vector-valued gating and\nin-context learning rates, as well as a relaxed value replacement rule. We show\nthat RWKV-7 can perform state tracking and recognize all regular languages,\nwhile retaining parallelizability of training. This exceeds the capabilities of\nTransformers under standard complexity conjectures, which are limited to\n$\\mathsf{TC}^0$. To demonstrate RWKV-7's language modeling capability, we also\npresent an extended open source 3.1 trillion token multilingual corpus, and\ntrain four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on\nthis dataset.\n  To foster openness, reproduction, and adoption, we release our models and\ndataset component listing at https://huggingface.co/RWKV, and our training and\ninference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0\nLicense.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.0; I.2.7"
    ],
    "authors": [
      "Bo Peng",
      "Ruichong Zhang",
      "Daniel Goldstein",
      "Eric Alcaide",
      "Haowen Hou",
      "Janna Lu",
      "William Merrill",
      "Guangyu Song",
      "Kaifeng Tan",
      "Saiteja Utpala",
      "Nathan Wilce",
      "Johan S. Wind",
      "Tianyi Wu",
      "Daniel Wuttke",
      "Christian Zhou-Zheng"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14453v1",
    "title": "Online Conformal Probabilistic Numerics via Adaptive Edge-Cloud Offloading",
    "abstract": "Consider an edge computing setting in which a user submits queries for the\nsolution of a linear system to an edge processor, which is subject to\ntime-varying computing availability. The edge processor applies a probabilistic\nlinear solver (PLS) so as to be able to respond to the user's query within the\nallotted time and computing budget. Feedback to the user is in the form of an\nuncertainty set. Due to model misspecification, the uncertainty set obtained\nvia a direct application of PLS does not come with coverage guarantees with\nrespect to the true solution of the linear system. This work introduces a new\nmethod to calibrate the uncertainty sets produced by PLS with the aim of\nguaranteeing long-term coverage requirements. The proposed method, referred to\nas online conformal prediction-PLS (OCP-PLS), assumes sporadic feedback from\ncloud to edge. This enables the online calibration of uncertainty thresholds\nvia online conformal prediction (OCP), an online optimization method previously\nstudied in the context of prediction models. The validity of OCP-PLS is\nverified via experiments that bring insights into trade-offs between coverage,\nprediction set size, and cloud usage.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Qiushuo Hou",
      "Sangwoo Park",
      "Matteo Zecchin",
      "Yunlong Cai",
      "Guanding Yu",
      "Osvaldo Simeone"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14448v1",
    "title": "Pauli Network Circuit Synthesis with Reinforcement Learning",
    "abstract": "We introduce a Reinforcement Learning (RL)-based method for re-synthesis of\nquantum circuits containing arbitrary Pauli rotations alongside Clifford\noperations. By collapsing each sub-block to a compact representation and then\nsynthesizing it step-by-step through a learned heuristic, we obtain circuits\nthat are both shorter and compliant with hardware connectivity constraints. We\nfind that the method is fast enough and good enough to work as an optimization\nprocedure: in direct comparisons on 6-qubit random Pauli Networks against\nstate-of-the-art heuristic methods, our RL approach yields over 2x reduction in\ntwo-qubit gate count, while executing in under 10 milliseconds per circuit. We\nfurther integrate the method into a collect-and-re-synthesize pipeline, applied\nas a Qiskit transpiler pass, where we observe average improvements of 20% in\ntwo-qubit gate count and depth, reaching up to 60% for many instances, across\nthe Benchpress benchmark. These results highlight the potential of RL-driven\nsynthesis to significantly improve circuit quality in realistic, large-scale\nquantum transpilation workloads.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "authors": [
      "Ayushi Dubal",
      "David Kremer",
      "Simon Martiel",
      "Victor Villar",
      "Derek Wang",
      "Juan Cruz-Benito"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14445v1",
    "title": "Bolt3D: Generating 3D Scenes in Seconds",
    "abstract": "We present a latent diffusion model for fast feed-forward 3D scene\ngeneration. Given one or more images, our model Bolt3D directly samples a 3D\nscene representation in less than seven seconds on a single GPU. We achieve\nthis by leveraging powerful and scalable existing 2D diffusion network\narchitectures to produce consistent high-fidelity 3D scene representations. To\ntrain this model, we create a large-scale multiview-consistent dataset of 3D\ngeometry and appearance by applying state-of-the-art dense 3D reconstruction\ntechniques to existing multiview image datasets. Compared to prior multiview\ngenerative models that require per-scene optimization for 3D reconstruction,\nBolt3D reduces the inference cost by a factor of up to 300 times.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Stanislaw Szymanowicz",
      "Jason Y. Zhang",
      "Pratul Srinivasan",
      "Ruiqi Gao",
      "Arthur Brussee",
      "Aleksander Holynski",
      "Ricardo Martin-Brualla",
      "Jonathan T. Barron",
      "Philipp Henzler"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14443v1",
    "title": "EnvBench: A Benchmark for Automated Environment Setup",
    "abstract": "Recent advances in Large Language Models (LLMs) have enabled researchers to\nfocus on practical repository-level tasks in software engineering domain. In\nthis work, we consider a cornerstone task for automating work with software\nrepositories-environment setup, i.e., a task of configuring a\nrepository-specific development environment on a system. Existing studies on\nenvironment setup introduce innovative agentic strategies, but their evaluation\nis often based on small datasets that may not capture the full range of\nconfiguration challenges encountered in practice. To address this gap, we\nintroduce a comprehensive environment setup benchmark EnvBench. It encompasses\n329 Python and 665 JVM-based (Java, Kotlin) repositories, with a focus on\nrepositories that present genuine configuration challenges, excluding projects\nthat can be fully configured by simple deterministic scripts. To enable further\nbenchmark extension and usage for model tuning, we implement two automatic\nmetrics: a static analysis check for missing imports in Python and a\ncompilation check for JVM languages. We demonstrate the applicability of our\nbenchmark by evaluating three environment setup approaches, including a simple\nzero-shot baseline and two agentic workflows, that we test with two powerful\nLLM backbones, GPT-4o and GPT-4o-mini. The best approach manages to\nsuccessfully configure 6.69% repositories for Python and 29.47% repositories\nfor JVM, suggesting that EnvBench remains challenging for current approaches.\nOur benchmark suite is publicly available at\nhttps://github.com/JetBrains-Research/EnvBench. The dataset and experiment\ntrajectories are available at https://jb.gg/envbench.",
    "categories": [
      "cs.LG",
      "cs.SE"
    ],
    "authors": [
      "Aleksandra Eliseeva",
      "Alexander Kovrigin",
      "Ilia Kholkin",
      "Egor Bogomolov",
      "Yaroslav Zharov"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14442v1",
    "title": "Inducing Causal Structure for Interpretable Neural Networks Applied to Glucose Prediction for T1DM Patients",
    "abstract": "Causal abstraction techniques such as Interchange Intervention Training (IIT)\nhave been proposed to infuse neural network with expert knowledge encoded in\ncausal models, but their application to real-world problems remains limited.\nThis article explores the application of IIT in predicting blood glucose levels\nin Type 1 Diabetes Mellitus (T1DM) patients. The study utilizes an acyclic\nversion of the simglucose simulator approved by the FDA to train a Multi-Layer\nPerceptron (MLP) model, employing IIT to impose causal relationships. Results\nshow that the model trained with IIT effectively abstracted the causal\nstructure and outperformed the standard one in terms of predictive performance\nacross different prediction horizons (PHs) post-meal. Furthermore, the\nbreakdown of the counterfactual loss can be leveraged to explain which part of\nthe causal mechanism are more or less effectively captured by the model. These\npreliminary results suggest the potential of IIT in enhancing predictive models\nin healthcare by effectively complying with expert knowledge.",
    "categories": [
      "cs.LG",
      "q-bio.BM"
    ],
    "authors": [
      "Ana Esponera",
      "Giovanni Cinà"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14439v1",
    "title": "Graph-CNNs for RF Imaging: Learning the Electric Field Integral Equations",
    "abstract": "Radio-Frequency (RF) imaging concerns the digital recreation of the surfaces\nof scene objects based on the scattered field at distributed receivers. To\nsolve this difficult inverse scattering problems, data-driven methods are often\nemployed that extract patterns from similar training examples, while offering\nminimal latency. In this paper, we first provide an approximate yet fast\nelectromagnetic model, which is based on the electric field integral equations,\nfor data generation, and subsequently propose a Deep Neural Network (DNN)\narchitecture to learn the corresponding inverse model. A graph-attention\nbackbone allows for the system geometry to be passed to the DNN, where residual\nconvolutional layers extract features about the objects, while a UNet head\nperforms the final image reconstruction. Our quantitative and qualitative\nevaluations on two synthetic data sets of different characteristics showcase\nthe performance gains of thee proposed advanced architecture and its relative\nresilience to signal noise levels and various reception configurations.",
    "categories": [
      "cs.LG",
      "eess.SP"
    ],
    "authors": [
      "Kyriakos Stylianopoulos",
      "Panagiotis Gavriilidis",
      "Gabriele Gradoni",
      "George C. Alexandropoulos"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14434v1",
    "title": "LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as Evolutionary Optimizers",
    "abstract": "Automated feature engineering plays a critical role in improving predictive\nmodel performance for tabular learning tasks. Traditional automated feature\nengineering methods are limited by their reliance on pre-defined\ntransformations within fixed, manually designed search spaces, often neglecting\ndomain knowledge. Recent advances using Large Language Models (LLMs) have\nenabled the integration of domain knowledge into the feature engineering\nprocess. However, existing LLM-based approaches use direct prompting or rely\nsolely on validation scores for feature selection, failing to leverage insights\nfrom prior feature discovery experiments or establish meaningful reasoning\nbetween feature generation and data-driven performance. To address these\nchallenges, we propose LLM-FE, a novel framework that combines evolutionary\nsearch with the domain knowledge and reasoning capabilities of LLMs to\nautomatically discover effective features for tabular learning tasks. LLM-FE\nformulates feature engineering as a program search problem, where LLMs propose\nnew feature transformation programs iteratively, and data-driven feedback\nguides the search process. Our results demonstrate that LLM-FE consistently\noutperforms state-of-the-art baselines, significantly enhancing the performance\nof tabular prediction models across diverse classification and regression\nbenchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.NE"
    ],
    "authors": [
      "Nikhil Abhyankar",
      "Parshin Shojaee",
      "Chandan K. Reddy"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14433v1",
    "title": "Splintering Nonconcatenative Languages for Better Tokenization",
    "abstract": "Common subword tokenization algorithms like BPE and UnigramLM assume that\ntext can be split into meaningful units by concatenative measures alone. This\nis not true for languages such as Hebrew and Arabic, where morphology is\nencoded in root-template patterns, or Malay and Georgian, where split affixes\nare common. We present SPLINTER, a pre-processing step which rearranges text\ninto a linear form that better represents such nonconcatenative morphologies,\nenabling meaningful contiguous segments to be found by the tokenizer. We\ndemonstrate SPLINTER's merit using both intrinsic measures evaluating token\nvocabularies in Hebrew, Arabic, and Malay; as well as on downstream tasks using\nBERT-architecture models trained for Hebrew.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Bar Gazit",
      "Shaltiel Shmidman",
      "Avi Shmidman",
      "Yuval Pinter"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14432v1",
    "title": "PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play",
    "abstract": "Large language models (LLMs) are increasingly integrated with specialized\nexternal tools, yet many tasks demand zero-shot tool usage with minimal or\nnoisy documentation. Existing solutions rely on manual rewriting or labeled\ndata for validation, making them inapplicable in true zero-shot settings. To\naddress these challenges, we propose PLAY2PROMPT, an automated framework that\nsystematically \"plays\" with each tool to explore its input-output behaviors.\nThrough this iterative trial-and-error process, PLAY2PROMPT refines tool\ndocumentation and generates usage examples without any labeled data. These\nexamples not only guide LLM inference but also serve as validation to further\nenhance tool utilization. Extensive experiments on real-world tasks demonstrate\nthat PLAY2PROMPT significantly improves zero-shot tool performance across both\nopen and closed models, offering a scalable and effective solution for\ndomain-specific tool integration.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Wei Fang",
      "Yang Zhang",
      "Kaizhi Qian",
      "James Glass",
      "Yada Zhu"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14428v1",
    "title": "MagicComp: Training-free Dual-Phase Refinement for Compositional Video Generation",
    "abstract": "Text-to-video (T2V) generation has made significant strides with diffusion\nmodels. However, existing methods still struggle with accurately binding\nattributes, determining spatial relationships, and capturing complex action\ninteractions between multiple subjects. To address these limitations, we\npropose MagicComp, a training-free method that enhances compositional T2V\ngeneration through dual-phase refinement. Specifically, (1) During the\nConditioning Stage: We introduce the Semantic Anchor Disambiguation to\nreinforces subject-specific semantics and resolve inter-subject ambiguity by\nprogressively injecting the directional vectors of semantic anchors into\noriginal text embedding; (2) During the Denoising Stage: We propose Dynamic\nLayout Fusion Attention, which integrates grounding priors and model-adaptive\nspatial perception to flexibly bind subjects to their spatiotemporal regions\nthrough masked attention modulation. Furthermore, MagicComp is a model-agnostic\nand versatile approach, which can be seamlessly integrated into existing T2V\narchitectures. Extensive experiments on T2V-CompBench and VBench demonstrate\nthat MagicComp outperforms state-of-the-art methods, highlighting its potential\nfor applications such as complex prompt-based and trajectory-controllable video\ngeneration. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Hongyu Zhang",
      "Yufan Deng",
      "Shenghai Yuan",
      "Peng Jin",
      "Zesen Cheng",
      "Yian Zhao",
      "Chang Liu",
      "Jie Chen"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14427v1",
    "title": "VisEscape: A Benchmark for Evaluating Exploration-driven Decision-making in Virtual Escape Rooms",
    "abstract": "Escape rooms present a unique cognitive challenge that demands\nexploration-driven planning: players should actively search their environment,\ncontinuously update their knowledge based on new discoveries, and connect\ndisparate clues to determine which elements are relevant to their objectives.\nMotivated by this, we introduce VisEscape, a benchmark of 20 virtual escape\nrooms specifically designed to evaluate AI models under these challenging\nconditions, where success depends not only on solving isolated puzzles but also\non iteratively constructing and refining spatial-temporal knowledge of a\ndynamically changing environment. On VisEscape, we observed that even\nstate-of-the-art multimodal models generally fail to escape the rooms, showing\nconsiderable variation in their levels of progress and trajectories. To address\nthis issue, we propose VisEscaper, which effectively integrates Memory,\nFeedback, and ReAct modules, demonstrating significant improvements by\nperforming 3.7 times more effectively and 5.0 times more efficiently on\naverage.",
    "categories": [
      "cs.AI"
    ],
    "authors": [
      "Seungwon Lim",
      "Sungwoong Kim",
      "Jihwan Yu",
      "Sungjae Lee",
      "Jiwan Chung",
      "Youngjae Yu"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14421v1",
    "title": "ExDDV: A New Dataset for Explainable Deepfake Detection in Video",
    "abstract": "The ever growing realism and quality of generated videos makes it\nincreasingly harder for humans to spot deepfake content, who need to rely more\nand more on automatic deepfake detectors. However, deepfake detectors are also\nprone to errors, and their decisions are not explainable, leaving humans\nvulnerable to deepfake-based fraud and misinformation. To this end, we\nintroduce ExDDV, the first dataset and benchmark for Explainable Deepfake\nDetection in Video. ExDDV comprises around 5.4K real and deepfake videos that\nare manually annotated with text descriptions (to explain the artifacts) and\nclicks (to point out the artifacts). We evaluate a number of vision-language\nmodels on ExDDV, performing experiments with various fine-tuning and in-context\nlearning strategies. Our results show that text and click supervision are both\nrequired to develop robust explainable models for deepfake videos, which are\nable to localize and describe the observed artifacts. Our novel dataset and\ncode to reproduce the results are available at\nhttps://github.com/vladhondru25/ExDDV.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "authors": [
      "Vlad Hondru",
      "Eduard Hogea",
      "Darian Onchis",
      "Radu Tudor Ionescu"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14412v1",
    "title": "Iffy-Or-Not: Extending the Web to Support the Critical Evaluation of Fallacious Texts",
    "abstract": "Social platforms have expanded opportunities for deliberation with the\ncomments being used to inform one's opinion. However, using such information to\nform opinions is challenged by unsubstantiated or false content. To enhance the\nquality of opinion formation and potentially confer resistance to\nmisinformation, we developed Iffy-Or-Not (ION), a browser extension that seeks\nto invoke critical thinking when reading texts. With three features guided by\nargumentation theory, ION highlights fallacious content, suggests diverse\nqueries to probe them with, and offers deeper questions to consider and chat\nwith others about. From a user study (N=18), we found that ION encourages users\nto be more attentive to the content, suggests queries that align with or are\npreferable to their own, and poses thought-provoking questions that expands\ntheir perspectives. However, some participants expressed aversion to ION due to\nmisalignments with their information goals and thinking predispositions.\nPotential backfiring effects with ION are discussed.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "authors": [
      "Gionnieve Lim",
      "Juho Kim",
      "Simon T. Perrault"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14411v1",
    "title": "Unifying Text Semantics and Graph Structures for Temporal Text-attributed Graphs with Large Language Models",
    "abstract": "Temporal graph neural networks (TGNNs) have shown remarkable performance in\ntemporal graph modeling. However, real-world temporal graphs often possess rich\ntextual information, giving rise to temporal text-attributed graphs (TTAGs).\nSuch combination of dynamic text semantics and evolving graph structures\nintroduces heightened complexity. Existing TGNNs embed texts statically and\nrely heavily on encoding mechanisms that biasedly prioritize structural\ninformation, overlooking the temporal evolution of text semantics and the\nessential interplay between semantics and structures for synergistic\nreinforcement. To tackle these issues, we present \\textbf{{Cross}}, a novel\nframework that seamlessly extends existing TGNNs for TTAG modeling. The key\nidea is to employ the advanced large language models (LLMs) to extract the\ndynamic semantics in text space and then generate expressive representations\nunifying both semantics and structures. Specifically, we propose a Temporal\nSemantics Extractor in the {Cross} framework, which empowers the LLM to offer\nthe temporal semantic understanding of node's evolving contexts of textual\nneighborhoods, facilitating semantic dynamics. Subsequently, we introduce the\nSemantic-structural Co-encoder, which collaborates with the above Extractor for\nsynthesizing illuminating representations by jointly considering both semantic\nand structural information while encouraging their mutual reinforcement.\nExtensive experimental results on four public datasets and one practical\nindustrial dataset demonstrate {Cross}'s significant effectiveness and\nrobustness.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Siwei Zhang",
      "Yun Xiong",
      "Yateng Tang",
      "Xi Chen",
      "Zian Jia",
      "Zehao Gu",
      "Jiarong Xu",
      "Jiawei Zhang"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14408v1",
    "title": "Large Language Models for Virtual Human Gesture Selection",
    "abstract": "Co-speech gestures convey a wide variety of meanings and play an important\nrole in face-to-face human interactions. These gestures significantly influence\nthe addressee's engagement, recall, comprehension, and attitudes toward the\nspeaker. Similarly, they impact interactions between humans and embodied\nvirtual agents. The process of selecting and animating meaningful gestures has\nthus become a key focus in the design of these agents. However, automating this\ngesture selection process poses a significant challenge. Prior gesture\ngeneration techniques have varied from fully automated, data-driven methods,\nwhich often struggle to produce contextually meaningful gestures, to more\nmanual approaches that require crafting specific gesture expertise and are\ntime-consuming and lack generalizability. In this paper, we leverage the\nsemantic capabilities of Large Language Models to develop a gesture selection\napproach that suggests meaningful, appropriate co-speech gestures. We first\ndescribe how information on gestures is encoded into GPT-4. Then, we conduct a\nstudy to evaluate alternative prompting approaches for their ability to select\nmeaningful, contextually relevant gestures and to align them appropriately with\nthe co-speech utterance. Finally, we detail and demonstrate how this approach\nhas been implemented within a virtual agent system, automating the selection\nand subsequent animation of the selected gestures for enhanced human-agent\ninteractions.",
    "categories": [
      "cs.HC",
      "cs.CL"
    ],
    "authors": [
      "Parisa Ghanad Torshizi",
      "Laura B. Hensel",
      "Ari Shapiro",
      "Stacy C. Marsella"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14405v1",
    "title": "DUNE: Distilling a Universal Encoder from Heterogeneous 2D and 3D Teachers",
    "abstract": "Recent multi-teacher distillation methods have unified the encoders of\nmultiple foundation models into a single encoder, achieving competitive\nperformance on core vision tasks like classification, segmentation, and depth\nestimation. This led us to ask: Could similar success be achieved when the pool\nof teachers also includes vision models specialized in diverse tasks across\nboth 2D and 3D perception? In this paper, we define and investigate the problem\nof heterogeneous teacher distillation, or co-distillation, a challenging\nmulti-teacher distillation scenario where teacher models vary significantly in\nboth (a) their design objectives and (b) the data they were trained on. We\nexplore data-sharing strategies and teacher-specific encoding, and introduce\nDUNE, a single encoder excelling in 2D vision, 3D understanding, and 3D human\nperception. Our model achieves performance comparable to that of its larger\nteachers, sometimes even outperforming them, on their respective tasks.\nNotably, DUNE surpasses MASt3R in Map-free Visual Relocalization with a much\nsmaller encoder.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Mert Bulent Sariyildiz",
      "Philippe Weinzaepfel",
      "Thomas Lucas",
      "Pau de Jorge",
      "Diane Larlus",
      "Yannis Kalantidis"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14403v1",
    "title": "Landscape Complexity for the Empirical Risk of Generalized Linear Models: Discrimination between Structured Data",
    "abstract": "We use the Kac-Rice formula and results from random matrix theory to obtain\nthe average number of critical points of a family of high-dimensional empirical\nloss functions, where the data are correlated $d$-dimensional Gaussian vectors,\nwhose number has a fixed ratio with their dimension. The correlations are\nintroduced to model the existence of structure in the data, as is common in\ncurrent Machine-Learning systems. Under a technical hypothesis, our results are\nexact in the large-$d$ limit, and characterize the annealed landscape\ncomplexity, namely the logarithm of the expected number of critical points at a\ngiven value of the loss.\n  We first address in detail the landscape of the loss function of a single\nperceptron and then generalize it to the case where two competing data sets\nwith different covariance matrices are present, with the perceptron seeking to\ndiscriminate between them. The latter model can be applied to understand the\ninterplay between adversity and non-trivial data structure. For completeness,\nwe also treat the case of a loss function used in training Generalized Linear\nModels in the presence of correlated input data.",
    "categories": [
      "cs.LG",
      "cond-mat.stat-mech",
      "stat.ML"
    ],
    "authors": [
      "Theodoros G. Tsironis",
      "Aris L. Moustakas"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14402v1",
    "title": "Diffusion-based Facial Aesthetics Enhancement with 3D Structure Guidance",
    "abstract": "Facial Aesthetics Enhancement (FAE) aims to improve facial attractiveness by\nadjusting the structure and appearance of a facial image while preserving its\nidentity as much as possible. Most existing methods adopted deep feature-based\nor score-based guidance for generation models to conduct FAE. Although these\nmethods achieved promising results, they potentially produced excessively\nbeautified results with lower identity consistency or insufficiently improved\nfacial attractiveness. To enhance facial aesthetics with less loss of identity,\nwe propose the Nearest Neighbor Structure Guidance based on Diffusion\n(NNSG-Diffusion), a diffusion-based FAE method that beautifies a 2D facial\nimage with 3D structure guidance. Specifically, we propose to extract FAE\nguidance from a nearest neighbor reference face. To allow for less change of\nfacial structures in the FAE process, a 3D face model is recovered by referring\nto both the matched 2D reference face and the 2D input face, so that the depth\nand contour guidance can be extracted from the 3D face model. Then the depth\nand contour clues can provide effective guidance to Stable Diffusion with\nControlNet for FAE. Extensive experiments demonstrate that our method is\nsuperior to previous relevant methods in enhancing facial aesthetics while\npreserving facial identity.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Lisha Li",
      "Jingwen Hou",
      "Weide Liu",
      "Yuming Fang",
      "Jiebin Yan"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14396v2",
    "title": "Technical Report: Aggregation on Learnable Manifolds for Asynchronous Federated Optimization",
    "abstract": "In Federated Learning (FL), a primary challenge to the server-side\naggregation of client models is device heterogeneity in both loss landscape\ngeometry and computational capacity. This issue can be particularly pronounced\nin clinical contexts where variations in data distribution (aggravated by class\nimbalance), infrastructure requirements, and sample sizes are common. We\npropose AsyncManifold, a novel asynchronous FL framework to address these\nissues by taking advantage of underlying solution space geometry at each of the\nlocal training, delay-correction, and aggregation stages. Our proposal is\naccompanied by a convergence proof in a general form and, motivated through\nexploratory studies of local behaviour, a proof-of-concept algorithm which\nperforms aggregation along non-linear mode connections and hence avoids\nbarriers to convergence that techniques based on linear interpolation will\nencounter.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Archie Licudi"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14395v1",
    "title": "Weakly Supervised Spatial Implicit Neural Representation Learning for 3D MRI-Ultrasound Deformable Image Registration in HDR Prostate Brachytherapy",
    "abstract": "Purpose: Accurate 3D MRI-ultrasound (US) deformable registration is critical\nfor real-time guidance in high-dose-rate (HDR) prostate brachytherapy. We\npresent a weakly supervised spatial implicit neural representation (SINR)\nmethod to address modality differences and pelvic anatomy challenges.\n  Methods: The framework uses sparse surface supervision from MRI/US\nsegmentations instead of dense intensity matching. SINR models deformations as\ncontinuous spatial functions, with patient-specific surface priors guiding a\nstationary velocity field for biologically plausible deformations. Validation\nincluded 20 public Prostate-MRI-US-Biopsy cases and 10 institutional HDR cases,\nevaluated via Dice similarity coefficient (DSC), mean surface distance (MSD),\nand 95% Hausdorff distance (HD95).\n  Results: The proposed method achieved robust registration. For the public\ndataset, prostate DSC was $0.93 \\pm 0.05$, MSD $0.87 \\pm 0.10$ mm, and HD95\n$1.58 \\pm 0.37$ mm. For the institutional dataset, prostate CTV achieved DSC\n$0.88 \\pm 0.09$, MSD $1.21 \\pm 0.38$ mm, and HD95 $2.09 \\pm 1.48$ mm. Bladder\nand rectum performance was lower due to ultrasound's limited field of view.\nVisual assessments confirmed accurate alignment with minimal discrepancies.\n  Conclusion: This study introduces a novel weakly supervised SINR-based\napproach for 3D MRI-US deformable registration. By leveraging sparse surface\nsupervision and spatial priors, it achieves accurate, robust, and\ncomputationally efficient registration, enhancing real-time image guidance in\nHDR prostate brachytherapy and improving treatment precision.",
    "categories": [
      "physics.med-ph",
      "cs.CV"
    ],
    "authors": [
      "Jing Wang",
      "Ruirui Liu",
      "Yu Lei",
      "Michael J. Baine",
      "Tian Liu",
      "Yang Lei"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14393v1",
    "title": "On the clustering behavior of sliding windows",
    "abstract": "Things can go spectacularly wrong when clustering timeseries data that has\nbeen preprocessed with a sliding window. We highlight three surprising failures\nthat emerge depending on how the window size compares with the timeseries\nlength. In addition to computational examples, we present theoretical\nexplanations for each of these failure modes.",
    "categories": [
      "cs.LG"
    ],
    "authors": [
      "Boris Alexeev",
      "Wenyan Luo",
      "Dustin G. Mixon",
      "Yan X Zhang"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14392v1",
    "title": "From \"Hallucination\" to \"Suture\": Insights from Language Philosophy to Enhance Large Language Models",
    "abstract": "This paper explores hallucination phenomena in large language models (LLMs)\nthrough the lens of language philosophy and psychoanalysis. By incorporating\nLacan's concepts of the \"chain of signifiers\" and \"suture points,\" we propose\nthe Anchor-RAG framework as a novel approach to mitigate hallucinations. In\ncontrast to the predominant reliance on trial-and-error experiments, constant\nadjustments of mathematical formulas, or resource-intensive methods that\nemphasize quantity over quality, our approach returns to the fundamental\nprinciples of linguistics to analyze the root causes of hallucinations in LLMs.\nDrawing from robust theoretical foundations, we derive algorithms and models\nthat are not only effective in reducing hallucinations but also enhance LLM\nperformance and improve output quality. This paper seeks to establish a\ncomprehensive theoretical framework for understanding hallucinations in LLMs\nand aims to challenge the prevalent \"guess-and-test\" approach and rat race\nmentality in the field. We aspire to pave the way for a new era of\ninterpretable LLMs, offering deeper insights into the inner workings of\nlanguage-based AI systems.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Qiantong Wang"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14391v1",
    "title": "How much do LLMs learn from negative examples?",
    "abstract": "Large language models (LLMs) undergo a three-phase training process:\nunsupervised pre-training, supervised fine-tuning (SFT), and learning from\nhuman feedback (RLHF/DPO). Notably, it is during the final phase that these\nmodels are exposed to negative examples -- incorrect, rejected, or suboptimal\nresponses to queries. This paper delves into the role of negative examples in\nthe training of LLMs, using a likelihood-ratio (Likra) model on multiple-choice\nquestion answering benchmarks to precisely manage the influence and the volume\nof negative examples. Our findings reveal three key insights: (1) During a\ncritical phase in training, Likra with negative examples demonstrates a\nsignificantly larger improvement per training example compared to SFT using\nonly positive examples. This leads to a sharp jump in the learning curve for\nLikra unlike the smooth and gradual improvement of SFT; (2) negative examples\nthat are plausible but incorrect (near-misses) exert a greater influence; and\n(3) while training with positive examples fails to significantly decrease the\nlikelihood of plausible but incorrect answers, training with negative examples\nmore accurately identifies them. These results indicate a potentially\nsignificant role for negative examples in improving accuracy and reducing\nhallucinations for LLMs.",
    "categories": [
      "cs.CL",
      "68T50, 68T05",
      "I.2.6; I.2.7"
    ],
    "authors": [
      "Shadi Hamdan",
      "Deniz Yuret"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14382v1",
    "title": "Good/Evil Reputation Judgment of Celebrities by LLMs via Retrieval Augmented Generation",
    "abstract": "The purpose of this paper is to examine whether large language models (LLMs)\ncan understand what is good and evil with respect to judging good/evil\nreputation of celebrities. Specifically, we first apply a large language model\n(namely, ChatGPT) to the task of collecting sentences that mention the target\ncelebrity from articles about celebrities on Web pages. Next, the collected\nsentences are categorized based on their contents by ChatGPT, where ChatGPT\nassigns a category name to each of those categories. Those assigned category\nnames are referred to as \"aspects\" of each celebrity. Then, by applying the\nframework of retrieval augmented generation (RAG), we show that the large\nlanguage model is quite effective in the task of judging good/evil reputation\nof aspects and descriptions of each celebrity. Finally, also in terms of\nproving the advantages of the proposed method over existing services\nincorporating RAG functions, we show that the proposed method of judging\ngood/evil of aspects/descriptions of each celebrity significantly outperform an\nexisting service incorporating RAG functions.",
    "categories": [
      "cs.CL"
    ],
    "authors": [
      "Rikuto Tsuchida",
      "Hibiki Yokoyama",
      "Takehito Utsuro"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14381v1",
    "title": "Optimizing High-Dimensional Oblique Splits",
    "abstract": "Orthogonal-split trees perform well, but evidence suggests oblique splits can\nenhance their performance. This paper explores optimizing high-dimensional\n$s$-sparse oblique splits from $\\{(\\vec{w}, \\vec{w}^{\\top}\\boldsymbol{X}_{i}) :\ni\\in \\{1,\\dots, n\\}, \\vec{w} \\in \\mathbb{R}^p, \\| \\vec{w} \\|_{2} = 1, \\|\n\\vec{w} \\|_{0} \\leq s \\}$ for growing oblique trees, where $ s $ is a\nuser-defined sparsity parameter. We establish a connection between SID\nconvergence and $s_0$-sparse oblique splits with $s_0\\ge 1$, showing that the\nSID function class expands as $s_0$ increases, enabling the capture of more\ncomplex data-generating functions such as the $s_0$-dimensional XOR function.\nThus, $s_0$ represents the unknown potential complexity of the underlying\ndata-generating function. Learning these complex functions requires an\n$s$-sparse oblique tree with $s \\geq s_0$ and greater computational resources.\nThis highlights a trade-off between statistical accuracy, governed by the SID\nfunction class size depending on $s_0$, and computational cost. In contrast,\nprevious studies have explored the problem of SID convergence using orthogonal\nsplits with $ s_0 = s = 1 $, where runtime was less critical. Additionally, we\nintroduce a practical framework for oblique trees that integrates optimized\noblique splits alongside orthogonal splits into random forests. The proposed\napproach is assessed through simulations and real-data experiments, comparing\nits performance against various oblique tree models.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.ME",
      "stat.TH"
    ],
    "authors": [
      "Chien-Ming Chi"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14577v1",
    "title": "PHGNN: A Novel Prompted Hypergraph Neural Network to Diagnose Alzheimer's Disease",
    "abstract": "The accurate diagnosis of Alzheimer's disease (AD) and prognosis of mild\ncognitive impairment (MCI) conversion are crucial for early intervention.\nHowever, existing multimodal methods face several challenges, from the\nheterogeneity of input data, to underexplored modality interactions, missing\ndata due to patient dropouts, and limited data caused by the time-consuming and\ncostly data collection process. In this paper, we propose a novel Prompted\nHypergraph Neural Network (PHGNN) framework that addresses these limitations by\nintegrating hypergraph based learning with prompt learning. Hypergraphs capture\nhigher-order relationships between different modalities, while our prompt\nlearning approach for hypergraphs, adapted from NLP, enables efficient training\nwith limited data. Our model is validated through extensive experiments on the\nADNI dataset, outperforming SOTA methods in both AD diagnosis and the\nprediction of MCI conversion.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Chenyu Liu",
      "Luca Rossi"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14378v1",
    "title": "Impossible Videos",
    "abstract": "Synthetic videos nowadays is widely used to complement data scarcity and\ndiversity of real-world videos. Current synthetic datasets primarily replicate\nreal-world scenarios, leaving impossible, counterfactual and anti-reality video\nconcepts underexplored. This work aims to answer two questions: 1) Can today's\nvideo generation models effectively follow prompts to create impossible video\ncontent? 2) Are today's video understanding models good enough for\nunderstanding impossible videos? To this end, we introduce IPV-Bench, a novel\nbenchmark designed to evaluate and foster progress in video understanding and\ngeneration. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing\n4 domains, 14 categories. It features diverse scenes that defy physical,\nbiological, geographical, or social laws. Based on the taxonomy, a prompt suite\nis constructed to evaluate video generation models, challenging their prompt\nfollowing and creativity capabilities. In addition, a video benchmark is\ncurated to assess Video-LLMs on their ability of understanding impossible\nvideos, which particularly requires reasoning on temporal dynamics and world\nknowledge. Comprehensive evaluations reveal limitations and insights for future\ndirections of video models, paving the way for next-generation video models.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Zechen Bai",
      "Hai Ci",
      "Mike Zheng Shou"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14377v1",
    "title": "Advancing Medical Representation Learning Through High-Quality Data",
    "abstract": "Despite the growing scale of medical Vision-Language datasets, the impact of\ndataset quality on model performance remains under-explored. We introduce\nOpen-PMC, a high-quality medical dataset from PubMed Central, containing 2.2\nmillion image-text pairs, enriched with image modality annotations, subfigures,\nand summarized in-text references. Notably, the in-text references provide\nricher medical context, extending beyond the abstract information typically\nfound in captions. Through extensive experiments, we benchmark Open-PMC against\nlarger datasets across retrieval and zero-shot classification tasks. Our\nresults show that dataset quality-not just size-drives significant performance\ngains. We complement our benchmark with an in-depth analysis of feature\nrepresentation. Our findings highlight the crucial role of data curation\nquality in advancing multimodal medical AI. We release Open-PMC, along with the\ntrained models and our codebase.",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Negin Baghbanzadeh",
      "Adibvafa Fallahpour",
      "Yasaman Parhizkar",
      "Franklin Ogidi",
      "Shuvendu Roy",
      "Sajad Ashkezari",
      "Vahid Reza Khazaie",
      "Michael Colacci",
      "Ali Etemad",
      "Arash Afkanpour",
      "Elham Dolatabadi"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14376v1",
    "title": "Tiled Flash Linear Attention: More Efficient Linear RNN and xLSTM Kernels",
    "abstract": "Linear RNNs with gating recently demonstrated competitive performance\ncompared to Transformers in language modeling. Although their linear compute\nscaling in sequence length offers theoretical runtime advantages over\nTransformers, realizing these benefits in practice requires optimized custom\nkernels, as Transformers rely on the highly efficient Flash Attention kernels.\nLeveraging the chunkwise-parallel formulation of linear RNNs, Flash Linear\nAttention (FLA) shows that linear RNN kernels are faster than Flash Attention,\nby parallelizing over chunks of the input sequence. However, since the chunk\nsize of FLA is limited, many intermediate states must be materialized in GPU\nmemory. This leads to low arithmetic intensity and causes high memory\nconsumption and IO cost, especially for long-context pre-training. In this\nwork, we present Tiled Flash Linear Attention (TFLA), a novel kernel algorithm\nfor linear RNNs, that enables arbitrary large chunk sizes by introducing an\nadditional level of sequence parallelization within each chunk. First, we apply\nTFLA to the xLSTM with matrix memory, the mLSTM. Second, we propose an mLSTM\nvariant with sigmoid input gate and reduced computation for even faster kernel\nruntimes at equal language modeling performance. In our speed benchmarks, we\nshow that our new mLSTM kernels based on TFLA outperform highly optimized Flash\nAttention, Linear Attention and Mamba kernels, setting a new state of the art\nfor efficient long-context sequence modeling primitives.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Maximilian Beck",
      "Korbinian Pöppel",
      "Phillip Lippe",
      "Sepp Hochreiter"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14375v1",
    "title": "Evaluating Machine Learning Approaches for ASCII Art Generation",
    "abstract": "Generating structured ASCII art using computational techniques demands a\ncareful interplay between aesthetic representation and computational precision,\nrequiring models that can effectively translate visual information into\nsymbolic text characters. Although Convolutional Neural Networks (CNNs) have\nshown promise in this domain, the comparative performance of deep learning\narchitectures and classical machine learning methods remains unexplored. This\npaper explores the application of contemporary ML and DL methods to generate\nstructured ASCII art, focusing on three key criteria: fidelity, character\nclassification accuracy, and output quality. We investigate deep learning\narchitectures, including Multilayer Perceptrons (MLPs), ResNet, and\nMobileNetV2, alongside classical approaches such as Random Forests, Support\nVector Machines (SVMs) and k-Nearest Neighbors (k-NN), trained on an augmented\nsynthetic dataset of ASCII characters. Our results show that complex neural\nnetwork architectures often fall short in producing high-quality ASCII art,\nwhereas classical machine learning classifiers, despite their simplicity,\nachieve performance similar to CNNs. Our findings highlight the strength of\nclassical methods in bridging model simplicity with output quality, offering\nnew insights into ASCII art synthesis and machine learning on image data with\nlow dimensionality.",
    "categories": [
      "cs.GR",
      "cs.LG"
    ],
    "authors": [
      "Sai Coumar",
      "Zachary Kingston"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14576v1",
    "title": "SocialJax: An Evaluation Suite for Multi-agent Reinforcement Learning in Sequential Social Dilemmas",
    "abstract": "Social dilemmas pose a significant challenge in the field of multi-agent\nreinforcement learning (MARL). Melting Pot is an extensive framework designed\nto evaluate social dilemma environments, providing an evaluation protocol that\nmeasures generalization to new social partners across various test scenarios.\nHowever, running reinforcement learning algorithms in the official Melting Pot\nenvironments demands substantial computational resources. In this paper, we\nintroduce SocialJax, a suite of sequential social dilemma environments\nimplemented in JAX. JAX is a high-performance numerical computing library for\nPython that enables significant improvements in the operational efficiency of\nSocialJax on GPUs and TPUs. Our experiments demonstrate that the training\npipeline of SocialJax achieves a 50\\texttimes{} speedup in real-time\nperformance compared to Melting Pot's RLlib baselines. Additionally, we\nvalidate the effectiveness of baseline algorithms within the SocialJax\nenvironments. Finally, we use Schelling diagrams to verify the social dilemma\nproperties of these environments, ensuring they accurately capture the dynamics\nof social dilemmas.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zihao Guo",
      "Richard Willis",
      "Shuqing Shi",
      "Tristan Tomilin",
      "Joel Z. Leibo",
      "Yali Du"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14369v1",
    "title": "C(NN)FD -- Deep Learning Modelling of Multi-Stage Axial Compressors Aerodynamics",
    "abstract": "The field of scientific machine learning and its applications to numerical\nanalyses such as CFD has recently experienced a surge in interest. While its\nviability has been demonstrated in different domains, it has not yet reached a\nlevel of robustness and scalability to make it practical for industrial\napplications in the turbomachinery field. The highly complex, turbulent, and\nthree-dimensional flows of multi-stage axial compressors for gas turbine\napplications represent a remarkably challenging case. This is due to the\nhigh-dimensionality of the regression of the flow-field from geometrical and\noperational variables, and the high computational cost associated with the\nlarge scale of the CFD domains. This paper demonstrates the development and\napplication of a generalized deep learning framework for predictions of the\nflow field and aerodynamic performance of multi-stage axial compressors, also\npotentially applicable to any type of turbomachinery. A physics-based\ndimensionality reduction unlocks the potential for flow-field predictions for\nlarge-scale domains, re-formulating the regression problem from an unstructured\nto a structured one. The relevant physical equations are used to define a\nmulti-dimensional physical loss function. Compared to \"black-box\" approaches,\nthe proposed framework has the advantage of physically explainable predictions\nof overall performance, as the corresponding aerodynamic drivers can be\nidentified on a 0D/1D/2D/3D level. An iterative architecture is employed,\nimproving the accuracy of the predictions, as well as estimating the associated\nuncertainty. The model is trained on a series of dataset including\nmanufacturing and build variations, different geometries, compressor designs\nand operating conditions. This demonstrates the capability to predict the\nflow-field and the overall performance in a generalizable manner, with accuracy\ncomparable to the benchmark.",
    "categories": [
      "physics.flu-dyn",
      "cs.LG"
    ],
    "authors": [
      "Giuseppe Bruni",
      "Sepehr Maleki",
      "Senthil K Krishnababu"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14575v1",
    "title": "The Exoplanet Citizen Science Pipeline: Human Factors and Machine Learning",
    "abstract": "We present the progress of work to streamline and simplify the process of\nexoplanet observation by citizen scientists. International collaborations such\nas ExoClock and Exoplanet Watch enable citizen scientists to use small\ntelescopes to carry out transit observations. These studies provide essential\nsupports for space missions such as JWST and ARIEL. Contributions include\nmaintenance or recovery of ephemerides, follow up confirmation and transit time\nvariations. Ongoing observation programs benefit from a large pool of\nobservers, with a wide variety of experience levels. Our projects work closely\nwith these communities to streamline their observation pipelines and enable\nwider participation. Two complementary approaches are taken: Star Guide applies\nhuman-centric design and community consultation to identify points of friction\nwithin existing systems and provide complementary online tools and resources to\nreduce barriers to entry to the observing community. Machine Learning is used\nto accelerate data processing and automate steps which are currently manual,\nproviding a streamlined tool for citizen science and a scalable solution for\nlarge-scale archival research.",
    "categories": [
      "astro-ph.IM",
      "astro-ph.EP",
      "cs.LG"
    ],
    "authors": [
      "Oisín Creaner",
      "Anna Preis",
      "Cormac Ryan",
      "Nika Gorchakova"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14359v1",
    "title": "ImViD: Immersive Volumetric Videos for Enhanced VR Engagement",
    "abstract": "User engagement is greatly enhanced by fully immersive multi-modal\nexperiences that combine visual and auditory stimuli. Consequently, the next\nfrontier in VR/AR technologies lies in immersive volumetric videos with\ncomplete scene capture, large 6-DoF interaction space, multi-modal feedback,\nand high resolution & frame-rate contents. To stimulate the reconstruction of\nimmersive volumetric videos, we introduce ImViD, a multi-view, multi-modal\ndataset featuring complete space-oriented data capture and various\nindoor/outdoor scenarios. Our capture rig supports multi-view video-audio\ncapture while on the move, a capability absent in existing datasets,\nsignificantly enhancing the completeness, flexibility, and efficiency of data\ncapture.\n  The captured multi-view videos (with synchronized audios) are in 5K\nresolution at 60FPS, lasting from 1-5 minutes, and include rich\nforeground-background elements, and complex dynamics. We benchmark existing\nmethods using our dataset and establish a base pipeline for constructing\nimmersive volumetric videos from multi-view audiovisual inputs for 6-DoF\nmulti-modal immersive VR experiences. The benchmark and the reconstruction and\ninteraction results demonstrate the effectiveness of our dataset and baseline\nmethod, which we believe will stimulate future research on immersive volumetric\nvideo production.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Zhengxian Yang",
      "Shi Pan",
      "Shengqi Wang",
      "Haoxiang Wang",
      "Li Lin",
      "Guanjun Li",
      "Zhengqi Wen",
      "Borong Lin",
      "Jianhua Tao",
      "Tao Yu"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14358v1",
    "title": "RFMI: Estimating Mutual Information on Rectified Flow for Text-to-Image Alignment",
    "abstract": "Rectified Flow (RF) models trained with a Flow matching framework have\nachieved state-of-the-art performance on Text-to-Image (T2I) conditional\ngeneration. Yet, multiple benchmarks show that synthetic images can still\nsuffer from poor alignment with the prompt, i.e., images show wrong attribute\nbinding, subject positioning, numeracy, etc. While the literature offers many\nmethods to improve T2I alignment, they all consider only Diffusion Models, and\nrequire auxiliary datasets, scoring models, and linguistic analysis of the\nprompt. In this paper we aim to address these gaps. First, we introduce RFMI, a\nnovel Mutual Information (MI) estimator for RF models that uses the pre-trained\nmodel itself for the MI estimation. Then, we investigate a self-supervised\nfine-tuning approach for T2I alignment based on RFMI that does not require\nauxiliary information other than the pre-trained model itself. Specifically, a\nfine-tuning set is constructed by selecting synthetic images generated from the\npre-trained RF model and having high point-wise MI between images and prompts.\nOur experiments on MI estimation benchmarks demonstrate the validity of RFMI,\nand empirical fine-tuning on SD3.5-Medium confirms the effectiveness of RFMI\nfor improving T2I alignment while maintaining image quality.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Chao Wang",
      "Giulio Franzese",
      "Alessandro Finamore",
      "Pietro Michiardi"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14357v1",
    "title": "Wasserstein-based Kernels for Clustering: Application to Power Distribution Graphs",
    "abstract": "Many data clustering applications must handle objects that cannot be\nrepresented as vector data. In this context, the bag-of-vectors representation\ncan be leveraged to describe complex objects through discrete distributions,\nand the Wasserstein distance can effectively measure the dissimilarity between\nthem. Additionally, kernel methods can be used to embed data into feature\nspaces that are easier to analyze. Despite significant progress in data\nclustering, a method that simultaneously accounts for distributional and\nvectorial dissimilarity measures is still lacking. To tackle this gap, this\nwork explores kernel methods and Wasserstein distance metrics to develop a\ncomputationally tractable clustering framework. The compositional properties of\nkernels allow the simultaneous handling of different metrics, enabling the\nintegration of both vectors and discrete distributions for object\nrepresentation. This approach is flexible enough to be applied in various\ndomains, such as graph analysis and image processing. The framework consists of\nthree main components. First, we efficiently approximate pairwise Wasserstein\ndistances using multiple reference distributions. Second, we employ kernel\nfunctions based on Wasserstein distances and present ways of composing kernels\nto express different types of information. Finally, we use the kernels to\ncluster data and evaluate the quality of the results using scalable and\ndistance-agnostic validity indices. A case study involving two datasets of 879\nand 34,920 power distribution graphs demonstrates the framework's effectiveness\nand efficiency.",
    "categories": [
      "cs.LG",
      "stat.AP"
    ],
    "authors": [
      "Alfredo Oneto",
      "Blazhe Gjorgiev",
      "Giovanni Sansavini"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14574v1",
    "title": "Sequence Analysis Using the Bezier Curve",
    "abstract": "The analysis of sequences (e.g., protein, DNA, and SMILES string) is\nessential for disease diagnosis, biomaterial engineering, genetic engineering,\nand drug discovery domains. Conventional analytical methods focus on\ntransforming sequences into numerical representations for applying machine\nlearning/deep learning-based sequence characterization. However, their efficacy\nis constrained by the intrinsic nature of deep learning (DL) models, which tend\nto exhibit suboptimal performance when applied to tabular data. An alternative\ngroup of methodologies endeavors to convert biological sequences into image\nforms by applying the concept of Chaos Game Representation (CGR). However, a\nnoteworthy drawback of these methods lies in their tendency to map individual\nelements of the sequence onto a relatively small subset of designated pixels\nwithin the generated image. The resulting sparse image representation may not\nadequately encapsulate the comprehensive sequence information, potentially\nresulting in suboptimal predictions. In this study, we introduce a novel\napproach to transform sequences into images using the B\\'ezier curve concept\nfor element mapping. Mapping the elements onto a curve enhances the sequence\ninformation representation in the respective images, hence yielding better\nDL-based classification performance. We employed different sequence datasets to\nvalidate our system by using different classification tasks, and the results\nillustrate that our B\\'ezier curve method is able to achieve good performance\nfor all the tasks.",
    "categories": [
      "q-bio.QM",
      "cs.LG"
    ],
    "authors": [
      "Taslim Murad",
      "Sarwan Ali",
      "Murray Patterson"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14356v1",
    "title": "Benchmarking community drug response prediction models: datasets, models, tools, and metrics for cross-dataset generalization analysis",
    "abstract": "Deep learning (DL) and machine learning (ML) models have shown promise in\ndrug response prediction (DRP), yet their ability to generalize across datasets\nremains an open question, raising concerns about their real-world\napplicability. Due to the lack of standardized benchmarking approaches, model\nevaluations and comparisons often rely on inconsistent datasets and evaluation\ncriteria, making it difficult to assess true predictive capabilities. In this\nwork, we introduce a benchmarking framework for evaluating cross-dataset\nprediction generalization in DRP models. Our framework incorporates five\npublicly available drug screening datasets, six standardized DRP models, and a\nscalable workflow for systematic evaluation. To assess model generalization, we\nintroduce a set of evaluation metrics that quantify both absolute performance\n(e.g., predictive accuracy across datasets) and relative performance (e.g.,\nperformance drop compared to within-dataset results), enabling a more\ncomprehensive assessment of model transferability. Our results reveal\nsubstantial performance drops when models are tested on unseen datasets,\nunderscoring the importance of rigorous generalization assessments. While\nseveral models demonstrate relatively strong cross-dataset generalization, no\nsingle model consistently outperforms across all datasets. Furthermore, we\nidentify CTRPv2 as the most effective source dataset for training, yielding\nhigher generalization scores across target datasets. By sharing this\nstandardized evaluation framework with the community, our study aims to\nestablish a rigorous foundation for model comparison, and accelerate the\ndevelopment of robust DRP models for real-world applications.",
    "categories": [
      "cs.LG",
      "q-bio.QM"
    ],
    "authors": [
      "Alexander Partin",
      "Priyanka Vasanthakumari",
      "Oleksandr Narykov",
      "Andreas Wilke",
      "Natasha Koussa",
      "Sara E. Jones",
      "Yitan Zhu",
      "Jamie C. Overbeek",
      "Rajeev Jain",
      "Gayara Demini Fernando",
      "Cesar Sanchez-Villalobos",
      "Cristina Garcia-Cardona",
      "Jamaludin Mohd-Yusof",
      "Nicholas Chia",
      "Justin M. Wozniak",
      "Souparno Ghosh",
      "Ranadip Pal",
      "Thomas S. Brettin",
      "M. Ryan Weil",
      "Rick L. Stevens"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14355v1",
    "title": "MAST-Pro: Dynamic Mixture-of-Experts for Adaptive Segmentation of Pan-Tumors with Knowledge-Driven Prompts",
    "abstract": "Accurate tumor segmentation is crucial for cancer diagnosis and treatment.\nWhile foundation models have advanced general-purpose segmentation, existing\nmethods still struggle with: (1) limited incorporation of medical priors, (2)\nimbalance between generic and tumor-specific features, and (3) high\ncomputational costs for clinical adaptation. To address these challenges, we\npropose MAST-Pro (Mixture-of-experts for Adaptive Segmentation of pan-Tumors\nwith knowledge-driven Prompts), a novel framework that integrates dynamic\nMixture-of-Experts (D-MoE) and knowledge-driven prompts for pan-tumor\nsegmentation. Specifically, text and anatomical prompts provide domain-specific\npriors, guiding tumor representation learning, while D-MoE dynamically selects\nexperts to balance generic and tumor-specific feature learning, improving\nsegmentation accuracy across diverse tumor types. To enhance efficiency, we\nemploy Parameter-Efficient Fine-Tuning (PEFT), optimizing MAST-Pro with\nsignificantly reduced computational overhead. Experiments on multi-anatomical\ntumor datasets demonstrate that MAST-Pro outperforms state-of-the-art\napproaches, achieving up to a 5.20% improvement in average DSC while reducing\ntrainable parameters by 91.04%, without compromising accuracy.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "Runqi Meng",
      "Sifan Song",
      "Pengfei Jin",
      "Yujin Oh",
      "Lin Teng",
      "Yulin Wang",
      "Yiqun Sun",
      "Ling Chen",
      "Xiang Li",
      "Quanzheng Li",
      "Ning Guo",
      "Dinggang Shen"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14354v1",
    "title": "Retrospective: A CORDIC Based Configurable Activation Function for NN Applications",
    "abstract": "A CORDIC-based configuration for the design of Activation Functions (AF) was\npreviously suggested to accelerate ASIC hardware design for\nresource-constrained systems by providing functional reconfigurability. Since\nits introduction, this new approach for neural network acceleration has gained\nwidespread popularity, influencing numerous designs for activation functions in\nboth academic and commercial AI processors. In this retrospective analysis, we\nexplore the foundational aspects of this initiative, summarize key developments\nover recent years, and introduce the DA-VINCI AF tailored for the evolving\nneeds of AI applications. This new generation of dynamically configurable and\nprecision-adjustable activation function cores promise greater adaptability for\na range of activation functions in AI workloads, including Swish, SoftMax,\nSeLU, and GeLU, utilizing the Shift-and-Add CORDIC technique. The previously\npresented design has been optimized for MAC, Sigmoid, and Tanh functionalities\nand incorporated into ReLU AFs, culminating in an accumulative NEURIC compute\nunit. These enhancements position NEURIC as a fundamental component in the\nresource-efficient vector engine for the realization of AI accelerators that\nfocus on DNNs, RNNs/LSTMs, and Transformers, achieving a quality of results\n(QoR) of 98.5%.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.CV",
      "cs.ET",
      "eess.IV"
    ],
    "authors": [
      "Omkar Kokane",
      "Gopal Raut",
      "Salim Ullah",
      "Mukul Lokhande",
      "Adam Teman",
      "Akash Kumar",
      "Santosh Kumar Vishvakarma"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14353v1",
    "title": "Unified Analysis of Decentralized Gradient Descent: a Contraction Mapping Framework",
    "abstract": "The decentralized gradient descent (DGD) algorithm, and its sibling,\ndiffusion, are workhorses in decentralized machine learning, distributed\ninference and estimation, and multi-agent coordination. We propose a novel,\nprincipled framework for the analysis of DGD and diffusion for strongly convex,\nsmooth objectives, and arbitrary undirected topologies, using contraction\nmappings coupled with a result called the mean Hessian theorem (MHT). The use\nof these tools yields tight convergence bounds, both in the noise-free and\nnoisy regimes. While these bounds are qualitatively similar to results found in\nthe literature, our approach using contractions together with the MHT decouples\nthe algorithm dynamics (how quickly the algorithm converges to its fixed point)\nfrom its asymptotic convergence properties (how far the fixed point is from the\nglobal optimum). This yields a simple, intuitive analysis that is accessible to\na broader audience. Extensions are provided to multiple local gradient updates,\ntime-varying step sizes, noisy gradients (stochastic DGD and diffusion),\ncommunication noise, and random topologies.",
    "categories": [
      "eess.SP",
      "cs.DC",
      "cs.LG"
    ],
    "authors": [
      "Erik G. Larsson",
      "Nicolo Michelusi"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14350v2",
    "title": "VEGGIE: Instructional Editing and Reasoning of Video Concepts with Grounded Generation",
    "abstract": "Recent video diffusion models have enhanced video editing, but it remains\nchallenging to handle instructional editing and diverse tasks (e.g., adding,\nremoving, changing) within a unified framework. In this paper, we introduce\nVEGGIE, a Video Editor with Grounded Generation from Instructions, a simple\nend-to-end framework that unifies video concept editing, grounding, and\nreasoning based on diverse user instructions. Specifically, given a video and\ntext query, VEGGIE first utilizes an MLLM to interpret user intentions in\ninstructions and ground them to the video contexts, generating frame-specific\ngrounded task queries for pixel-space responses. A diffusion model then renders\nthese plans and generates edited videos that align with user intent. To support\ndiverse tasks and complex instructions, we employ a curriculum learning\nstrategy: first aligning the MLLM and video diffusion model with large-scale\ninstructional image editing data, followed by end-to-end fine-tuning on\nhigh-quality multitask video data. Additionally, we introduce a novel data\nsynthesis pipeline to generate paired instructional video editing data for\nmodel training. It transforms static image data into diverse, high-quality\nvideo editing samples by leveraging Image-to-Video models to inject dynamics.\nVEGGIE shows strong performance in instructional video editing with different\nediting skills, outperforming the best instructional baseline as a versatile\nmodel, while other models struggle with multi-tasking. VEGGIE also excels in\nvideo object grounding and reasoning segmentation, where other baselines fail.\nWe further reveal how the multiple tasks help each other and highlight\npromising applications like zero-shot multimodal instructional and in-context\nvideo editing.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Shoubin Yu",
      "Difan Liu",
      "Ziqiao Ma",
      "Yicong Hong",
      "Yang Zhou",
      "Hao Tan",
      "Joyce Chai",
      "Mohit Bansal"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.16534v1",
    "title": "Gender and content bias in Large Language Models: a case study on Google Gemini 2.0 Flash Experimental",
    "abstract": "This study evaluates the biases in Gemini 2.0 Flash Experimental, a\nstate-of-the-art large language model (LLM) developed by Google, focusing on\ncontent moderation and gender disparities. By comparing its performance to\nChatGPT-4o, examined in a previous work of the author, the analysis highlights\nsome differences in ethical moderation practices. Gemini 2.0 demonstrates\nreduced gender bias, notably with female-specific prompts achieving a\nsubstantial rise in acceptance rates compared to results obtained by\nChatGPT-4o. It adopts a more permissive stance toward sexual content and\nmaintains relatively high acceptance rates for violent prompts, including\ngender-specific cases. Despite these changes, whether they constitute an\nimprovement is debatable. While gender bias has been reduced, this reduction\ncomes at the cost of permitting more violent content toward both males and\nfemales, potentially normalizing violence rather than mitigating harm.\nMale-specific prompts still generally receive higher acceptance rates than\nfemale-specific ones. These findings underscore the complexities of aligning AI\nsystems with ethical standards, highlighting progress in reducing certain\nbiases while raising concerns about the broader implications of the model's\npermissiveness. Ongoing refinements are essential to achieve moderation\npractices that ensure transparency, fairness, and inclusivity without\namplifying harmful content.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "authors": [
      "Roberto Balestri"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14346v1",
    "title": "3D Densification for Multi-Map Monocular VSLAM in Endoscopy",
    "abstract": "Multi-map Sparse Monocular visual Simultaneous Localization and Mapping\napplied to monocular endoscopic sequences has proven efficient to robustly\nrecover tracking after the frequent losses in endoscopy due to motion blur,\ntemporal occlusion, tools interaction or water jets. The sparse multi-maps are\nadequate for robust camera localization, however they are very poor for\nenvironment representation, they are noisy, with a high percentage of\ninaccurately reconstructed 3D points, including significant outliers, and more\nimportantly with an unacceptable low density for clinical applications.\n  We propose a method to remove outliers and densify the maps of the state of\nthe art for sparse endoscopy multi-map CudaSIFT-SLAM. The NN LightDepth for\nup-to-scale depth dense predictions are aligned with the sparse CudaSIFT\nsubmaps by means of the robust to spurious LMedS. Our system mitigates the\ninherent scale ambiguity in monocular depth estimation while filtering\noutliers, leading to reliable densified 3D maps.\n  We provide experimental evidence of accurate densified maps 4.15 mm RMS\naccuracy at affordable computing time in the C3VD phantom colon dataset. We\nreport qualitative results on the real colonoscopy from the Endomapper dataset.",
    "categories": [
      "cs.CV"
    ],
    "authors": [
      "X. Anadón",
      "Javier Rodríguez-Puigvert",
      "J. M. M. Montiel"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  },
  {
    "id": "2503.14345v2",
    "title": "MoonCast: High-Quality Zero-Shot Podcast Generation",
    "abstract": "Recent advances in text-to-speech synthesis have achieved notable success in\ngenerating high-quality short utterances for individual speakers. However,\nthese systems still face challenges when extending their capabilities to long,\nmulti-speaker, and spontaneous dialogues, typical of real-world scenarios such\nas podcasts. These limitations arise from two primary challenges: 1) long\nspeech: podcasts typically span several minutes, exceeding the upper limit of\nmost existing work; 2) spontaneity: podcasts are marked by their spontaneous,\noral nature, which sharply contrasts with formal, written contexts; existing\nworks often fall short in capturing this spontaneity. In this paper, we propose\nMoonCast, a solution for high-quality zero-shot podcast generation, aiming to\nsynthesize natural podcast-style speech from text-only sources (e.g., stories,\ntechnical reports, news in TXT, PDF, or Web URL formats) using the voices of\nunseen speakers. To generate long audio, we adopt a long-context language\nmodel-based audio modeling approach utilizing large-scale long-context speech\ndata. To enhance spontaneity, we utilize a podcast generation module to\ngenerate scripts with spontaneous details, which have been empirically shown to\nbe as crucial as the text-to-speech modeling itself. Experiments demonstrate\nthat MoonCast outperforms baselines, with particularly notable improvements in\nspontaneity and coherence.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "authors": [
      "Zeqian Ju",
      "Dongchao Yang",
      "Jianwei Yu",
      "Kai Shen",
      "Yichong Leng",
      "Zhengtao Wang",
      "Xu Tan",
      "Xinyu Zhou",
      "Tao Qin",
      "Xiangyang Li"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-19"
  },
  {
    "id": "2503.14343v1",
    "title": "Multi-Prototype Embedding Refinement for Semi-Supervised Medical Image Segmentation",
    "abstract": "Medical image segmentation aims to identify anatomical structures at the\nvoxel-level. Segmentation accuracy relies on distinguishing voxel differences.\nCompared to advancements achieved in studies of the inter-class variance, the\nintra-class variance receives less attention. Moreover, traditional linear\nclassifiers, limited by a single learnable weight per class, struggle to\ncapture this finer distinction. To address the above challenges, we propose a\nMulti-Prototype-based Embedding Refinement method for semi-supervised medical\nimage segmentation. Specifically, we design a multi-prototype-based\nclassification strategy, rethinking the segmentation from the perspective of\nstructural relationships between voxel embeddings. The intra-class variations\nare explored by clustering voxels along the distribution of multiple prototypes\nin each class. Next, we introduce a consistency constraint to alleviate the\nlimitation of linear classifiers. This constraint integrates different\nclassification granularities from a linear classifier and the proposed\nprototype-based classifier. In the thorough evaluation on two popular\nbenchmarks, our method achieves superior performance compared with\nstate-of-the-art methods. Code is available at\nhttps://github.com/Briley-byl123/MPER.",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Yali Bi",
      "Enyu Che",
      "Yinan Chen",
      "Yuanpeng He",
      "Jingwei Qu"
    ],
    "published_date": "2025-03-18",
    "updated_date": "2025-03-18"
  }
]